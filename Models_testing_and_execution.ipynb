{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from helpers import*\n",
    "from build_polynomial import*\n",
    "from utils_predictions_manipulation import*\n",
    "from utils_nans_manipulation import*\n",
    "from cross_validation import*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model testing on training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = \"i8,S5,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,i8,f8,f8,f8,f8,f8,f8,f8\"\n",
    "with open('Data/train.csv', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    headerstrain = next(reader)\n",
    "datatrain = np.genfromtxt('Data/train.csv', delimiter=\",\",names=True, dtype=dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format training data, add fake feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building y\n",
    "Y_total = np.array((datatrain['Prediction']==b's').astype(int))\n",
    "\n",
    "# Filtering out useless data from x\n",
    "names = list(datatrain.dtype.names)[2:]\n",
    "datatrain_filtered = datatrain[names]\n",
    "\n",
    "# Building x\n",
    "x = np.empty((len(datatrain_filtered),len(datatrain_filtered[0].item())))\n",
    "for i in range(len(datatrain_filtered)):\n",
    "    x[i] = np.array(datatrain_filtered[i].item()).reshape(1,-1)\n",
    "    \n",
    "# Adding 1s at the beginning\n",
    "X_total = np.c_[np.ones(len(x)),x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating data points with and without NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing undefined data with NaNs\n",
    "\n",
    "X_nans = replace_bad_data_with_nans(X_total, -999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset based on presence of NaNs\n",
    "\n",
    "X_no_nans, Y_no_nans = delete_nan_points(X_nans, Y_total)\n",
    "X_with_nans, Y_with_nans = store_nan_points(X_nans, Y_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing NaNs with median\n",
    "\n",
    "X_cleaned = replace_nans_with_median(X_with_nans, threshold=0.5)\n",
    "Y_cleaned = Y_with_nans  # As we do not delete any data point, Y is the same for the cleaned data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaned Data names in the cell below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68114, 31) (68114,)\n",
      "(181886, 21) (181886,)\n"
     ]
    }
   ],
   "source": [
    "print(X_no_nans.shape, Y_no_nans.shape)\n",
    "print(X_cleaned.shape, Y_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LPM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999): loss=0.2344471850285804\n",
      "Gradient Descent(1/999): loss=0.12572120799910913\n",
      "Gradient Descent(2/999): loss=0.1246789471861695\n",
      "Gradient Descent(3/999): loss=0.12434172030491897\n",
      "Gradient Descent(4/999): loss=0.1240301876036757\n",
      "Gradient Descent(5/999): loss=0.12373514673728662\n",
      "Gradient Descent(6/999): loss=0.12345326030185856\n",
      "Gradient Descent(7/999): loss=0.1231823034183267\n",
      "Gradient Descent(8/999): loss=0.12292075770498662\n",
      "Gradient Descent(9/999): loss=0.12266756497252462\n",
      "Gradient Descent(10/999): loss=0.12242196769899616\n",
      "Gradient Descent(11/999): loss=0.12218340561150044\n",
      "Gradient Descent(12/999): loss=0.12195144861125863\n",
      "Gradient Descent(13/999): loss=0.12172575324182368\n",
      "Gradient Descent(14/999): loss=0.12150603440949644\n",
      "Gradient Descent(15/999): loss=0.12129204698575002\n",
      "Gradient Descent(16/999): loss=0.1210835738132422\n",
      "Gradient Descent(17/999): loss=0.12088041786231343\n",
      "Gradient Descent(18/999): loss=0.12068239707851944\n",
      "Gradient Descent(19/999): loss=0.12048934097580141\n",
      "Gradient Descent(20/999): loss=0.12030108836285847\n",
      "Gradient Descent(21/999): loss=0.12011748580595032\n",
      "Gradient Descent(22/999): loss=0.1199383865710484\n",
      "Gradient Descent(23/999): loss=0.11976364987873535\n",
      "Gradient Descent(24/999): loss=0.11959314036386157\n",
      "Gradient Descent(25/999): loss=0.11942672766993115\n",
      "Gradient Descent(26/999): loss=0.11926428613278253\n",
      "Gradient Descent(27/999): loss=0.11910569452406114\n",
      "Gradient Descent(28/999): loss=0.11895083583530225\n",
      "Gradient Descent(29/999): loss=0.11879959709013133\n",
      "Gradient Descent(30/999): loss=0.11865186917642292\n",
      "Gradient Descent(31/999): loss=0.11850754669306972\n",
      "Gradient Descent(32/999): loss=0.11836652780783509\n",
      "Gradient Descent(33/999): loss=0.11822871412394596\n",
      "Gradient Descent(34/999): loss=0.11809401055385017\n",
      "Gradient Descent(35/999): loss=0.11796232519906295\n",
      "Gradient Descent(36/999): loss=0.1178335692353524\n",
      "Gradient Descent(37/999): loss=0.11770765680272688\n",
      "Gradient Descent(38/999): loss=0.11758450489982715\n",
      "Gradient Descent(39/999): loss=0.11746403328241822\n",
      "Gradient Descent(40/999): loss=0.1173461643657381\n",
      "Gradient Descent(41/999): loss=0.11723082313050086\n",
      "Gradient Descent(42/999): loss=0.11711793703238199\n",
      "Gradient Descent(43/999): loss=0.11700743591483204\n",
      "Gradient Descent(44/999): loss=0.11689925192508051\n",
      "Gradient Descent(45/999): loss=0.11679331943320237\n",
      "Gradient Descent(46/999): loss=0.1166895749541275\n",
      "Gradient Descent(47/999): loss=0.11658795707248083\n",
      "Gradient Descent(48/999): loss=0.11648840637014687\n",
      "Gradient Descent(49/999): loss=0.11639086535645596\n",
      "Gradient Descent(50/999): loss=0.11629527840089549\n",
      "Gradient Descent(51/999): loss=0.11620159166825288\n",
      "Gradient Descent(52/999): loss=0.11610975305609994\n",
      "Gradient Descent(53/999): loss=0.11601971213453312\n",
      "Gradient Descent(54/999): loss=0.11593142008808649\n",
      "Gradient Descent(55/999): loss=0.1158448296597379\n",
      "Gradient Descent(56/999): loss=0.11575989509693134\n",
      "Gradient Descent(57/999): loss=0.11567657209954169\n",
      "Gradient Descent(58/999): loss=0.11559481776971114\n",
      "Gradient Descent(59/999): loss=0.11551459056348788\n",
      "Gradient Descent(60/999): loss=0.11543585024420217\n",
      "Gradient Descent(61/999): loss=0.1153585578375152\n",
      "Gradient Descent(62/999): loss=0.11528267558808072\n",
      "Gradient Descent(63/999): loss=0.11520816691775929\n",
      "Gradient Descent(64/999): loss=0.11513499638532927\n",
      "Gradient Descent(65/999): loss=0.1150631296476397\n",
      "Gradient Descent(66/999): loss=0.11499253342215214\n",
      "Gradient Descent(67/999): loss=0.11492317545082104\n",
      "Gradient Descent(68/999): loss=0.11485502446526369\n",
      "Gradient Descent(69/999): loss=0.11478805015317288\n",
      "Gradient Descent(70/999): loss=0.11472222312592659\n",
      "Gradient Descent(71/999): loss=0.11465751488735147\n",
      "Gradient Descent(72/999): loss=0.11459389780359795\n",
      "Gradient Descent(73/999): loss=0.11453134507408627\n",
      "Gradient Descent(74/999): loss=0.11446983070348474\n",
      "Gradient Descent(75/999): loss=0.11440932947468263\n",
      "Gradient Descent(76/999): loss=0.11434981692272109\n",
      "Gradient Descent(77/999): loss=0.11429126930964814\n",
      "Gradient Descent(78/999): loss=0.11423366360026294\n",
      "Gradient Descent(79/999): loss=0.11417697743871834\n",
      "Gradient Descent(80/999): loss=0.11412118912594928\n",
      "Gradient Descent(81/999): loss=0.11406627759789814\n",
      "Gradient Descent(82/999): loss=0.11401222240450745\n",
      "Gradient Descent(83/999): loss=0.11395900368945219\n",
      "Gradient Descent(84/999): loss=0.11390660217058524\n",
      "Gradient Descent(85/999): loss=0.11385499912106993\n",
      "Gradient Descent(86/999): loss=0.11380417635117454\n",
      "Gradient Descent(87/999): loss=0.11375411619070519\n",
      "Gradient Descent(88/999): loss=0.11370480147205392\n",
      "Gradient Descent(89/999): loss=0.1136562155138396\n",
      "Gradient Descent(90/999): loss=0.11360834210512025\n",
      "Gradient Descent(91/999): loss=0.11356116549015634\n",
      "Gradient Descent(92/999): loss=0.11351467035370495\n",
      "Gradient Descent(93/999): loss=0.11346884180682575\n",
      "Gradient Descent(94/999): loss=0.11342366537318015\n",
      "Gradient Descent(95/999): loss=0.11337912697580643\n",
      "Gradient Descent(96/999): loss=0.1133352129243528\n",
      "Gradient Descent(97/999): loss=0.11329190990275285\n",
      "Gradient Descent(98/999): loss=0.11324920495732667\n",
      "Gradient Descent(99/999): loss=0.11320708548529289\n",
      "Gradient Descent(100/999): loss=0.11316553922367673\n",
      "Gradient Descent(101/999): loss=0.11312455423859966\n",
      "Gradient Descent(102/999): loss=0.11308411891493732\n",
      "Gradient Descent(103/999): loss=0.11304422194633204\n",
      "Gradient Descent(104/999): loss=0.11300485232554783\n",
      "Gradient Descent(105/999): loss=0.11296599933515505\n",
      "Gradient Descent(106/999): loss=0.11292765253853333\n",
      "Gradient Descent(107/999): loss=0.11288980177118113\n",
      "Gradient Descent(108/999): loss=0.11285243713232117\n",
      "Gradient Descent(109/999): loss=0.11281554897679107\n",
      "Gradient Descent(110/999): loss=0.11277912790720904\n",
      "Gradient Descent(111/999): loss=0.11274316476640482\n",
      "Gradient Descent(112/999): loss=0.11270765063010649\n",
      "Gradient Descent(113/999): loss=0.11267257679987401\n",
      "Gradient Descent(114/999): loss=0.11263793479627064\n",
      "Gradient Descent(115/999): loss=0.11260371635226392\n",
      "Gradient Descent(116/999): loss=0.11256991340684802\n",
      "Gradient Descent(117/999): loss=0.11253651809887963\n",
      "Gradient Descent(118/999): loss=0.11250352276111981\n",
      "Gradient Descent(119/999): loss=0.11247091991447439\n",
      "Gradient Descent(120/999): loss=0.11243870226242625\n",
      "Gradient Descent(121/999): loss=0.1124068626856524\n",
      "Gradient Descent(122/999): loss=0.11237539423681936\n",
      "Gradient Descent(123/999): loss=0.11234429013555065\n",
      "Gradient Descent(124/999): loss=0.11231354376356048\n",
      "Gradient Descent(125/999): loss=0.11228314865994739\n",
      "Gradient Descent(126/999): loss=0.1122530985166427\n",
      "Gradient Descent(127/999): loss=0.1122233871740079\n",
      "Gradient Descent(128/999): loss=0.11219400861657614\n",
      "Gradient Descent(129/999): loss=0.11216495696893254\n",
      "Gradient Descent(130/999): loss=0.11213622649172848\n",
      "Gradient Descent(131/999): loss=0.11210781157782537\n",
      "Gradient Descent(132/999): loss=0.11207970674856317\n",
      "Gradient Descent(133/999): loss=0.11205190665014947\n",
      "Gradient Descent(134/999): loss=0.11202440605016487\n",
      "Gradient Descent(135/999): loss=0.11199719983418055\n",
      "Gradient Descent(136/999): loss=0.11197028300248453\n",
      "Gradient Descent(137/999): loss=0.11194365066691192\n",
      "Gradient Descent(138/999): loss=0.11191729804777695\n",
      "Gradient Descent(139/999): loss=0.11189122047090187\n",
      "Gradient Descent(140/999): loss=0.11186541336474051\n",
      "Gradient Descent(141/999): loss=0.11183987225759237\n",
      "Gradient Descent(142/999): loss=0.1118145927749046\n",
      "Gradient Descent(143/999): loss=0.11178957063665898\n",
      "Gradient Descent(144/999): loss=0.11176480165484035\n",
      "Gradient Descent(145/999): loss=0.11174028173098459\n",
      "Gradient Descent(146/999): loss=0.11171600685380247\n",
      "Gradient Descent(147/999): loss=0.11169197309687758\n",
      "Gradient Descent(148/999): loss=0.11166817661643552\n",
      "Gradient Descent(149/999): loss=0.11164461364918159\n",
      "Gradient Descent(150/999): loss=0.11162128051020537\n",
      "Gradient Descent(151/999): loss=0.11159817359094933\n",
      "Gradient Descent(152/999): loss=0.11157528935723929\n",
      "Gradient Descent(153/999): loss=0.11155262434737544\n",
      "Gradient Descent(154/999): loss=0.11153017517028053\n",
      "Gradient Descent(155/999): loss=0.1115079385037048\n",
      "Gradient Descent(156/999): loss=0.11148591109248449\n",
      "Gradient Descent(157/999): loss=0.1114640897468528\n",
      "Gradient Descent(158/999): loss=0.11144247134080162\n",
      "Gradient Descent(159/999): loss=0.11142105281049175\n",
      "Gradient Descent(160/999): loss=0.1113998311527108\n",
      "Gradient Descent(161/999): loss=0.11137880342337605\n",
      "Gradient Descent(162/999): loss=0.11135796673608254\n",
      "Gradient Descent(163/999): loss=0.1113373182606927\n",
      "Gradient Descent(164/999): loss=0.11131685522196805\n",
      "Gradient Descent(165/999): loss=0.11129657489824052\n",
      "Gradient Descent(166/999): loss=0.11127647462012231\n",
      "Gradient Descent(167/999): loss=0.11125655176925334\n",
      "Gradient Descent(168/999): loss=0.11123680377708459\n",
      "Gradient Descent(169/999): loss=0.11121722812369643\n",
      "Gradient Descent(170/999): loss=0.11119782233665085\n",
      "Gradient Descent(171/999): loss=0.11117858398987628\n",
      "Gradient Descent(172/999): loss=0.1111595107025841\n",
      "Gradient Descent(173/999): loss=0.1111406001382158\n",
      "Gradient Descent(174/999): loss=0.11112185000341963\n",
      "Gradient Descent(175/999): loss=0.11110325804705612\n",
      "Gradient Descent(176/999): loss=0.11108482205923124\n",
      "Gradient Descent(177/999): loss=0.11106653987035645\n",
      "Gradient Descent(178/999): loss=0.11104840935023474\n",
      "Gradient Descent(179/999): loss=0.11103042840717198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(180/999): loss=0.11101259498711236\n",
      "Gradient Descent(181/999): loss=0.11099490707279779\n",
      "Gradient Descent(182/999): loss=0.11097736268295005\n",
      "Gradient Descent(183/999): loss=0.11095995987147483\n",
      "Gradient Descent(184/999): loss=0.11094269672668763\n",
      "Gradient Descent(185/999): loss=0.11092557137056024\n",
      "Gradient Descent(186/999): loss=0.11090858195798721\n",
      "Gradient Descent(187/999): loss=0.11089172667607208\n",
      "Gradient Descent(188/999): loss=0.11087500374343248\n",
      "Gradient Descent(189/999): loss=0.11085841140952336\n",
      "Gradient Descent(190/999): loss=0.11084194795397799\n",
      "Gradient Descent(191/999): loss=0.11082561168596654\n",
      "Gradient Descent(192/999): loss=0.11080940094357089\n",
      "Gradient Descent(193/999): loss=0.11079331409317568\n",
      "Gradient Descent(194/999): loss=0.11077734952887539\n",
      "Gradient Descent(195/999): loss=0.11076150567189623\n",
      "Gradient Descent(196/999): loss=0.11074578097003285\n",
      "Gradient Descent(197/999): loss=0.1107301738970996\n",
      "Gradient Descent(198/999): loss=0.11071468295239535\n",
      "Gradient Descent(199/999): loss=0.11069930666018203\n",
      "Gradient Descent(200/999): loss=0.11068404356917586\n",
      "Gradient Descent(201/999): loss=0.11066889225205175\n",
      "Gradient Descent(202/999): loss=0.11065385130495956\n",
      "Gradient Descent(203/999): loss=0.1106389193470525\n",
      "Gradient Descent(204/999): loss=0.11062409502002714\n",
      "Gradient Descent(205/999): loss=0.11060937698767447\n",
      "Gradient Descent(206/999): loss=0.11059476393544208\n",
      "Gradient Descent(207/999): loss=0.11058025457000686\n",
      "Gradient Descent(208/999): loss=0.11056584761885789\n",
      "Gradient Descent(209/999): loss=0.11055154182988956\n",
      "Gradient Descent(210/999): loss=0.11053733597100417\n",
      "Gradient Descent(211/999): loss=0.11052322882972394\n",
      "Gradient Descent(212/999): loss=0.11050921921281266\n",
      "Gradient Descent(213/999): loss=0.11049530594590541\n",
      "Gradient Descent(214/999): loss=0.1104814878731477\n",
      "Gradient Descent(215/999): loss=0.11046776385684264\n",
      "Gradient Descent(216/999): loss=0.11045413277710625\n",
      "Gradient Descent(217/999): loss=0.11044059353153102\n",
      "Gradient Descent(218/999): loss=0.11042714503485673\n",
      "Gradient Descent(219/999): loss=0.1104137862186493\n",
      "Gradient Descent(220/999): loss=0.11040051603098651\n",
      "Gradient Descent(221/999): loss=0.11038733343615108\n",
      "Gradient Descent(222/999): loss=0.11037423741433047\n",
      "Gradient Descent(223/999): loss=0.11036122696132367\n",
      "Gradient Descent(224/999): loss=0.11034830108825425\n",
      "Gradient Descent(225/999): loss=0.11033545882128981\n",
      "Gradient Descent(226/999): loss=0.11032269920136797\n",
      "Gradient Descent(227/999): loss=0.1103100212839278\n",
      "Gradient Descent(228/999): loss=0.11029742413864764\n",
      "Gradient Descent(229/999): loss=0.11028490684918833\n",
      "Gradient Descent(230/999): loss=0.11027246851294216\n",
      "Gradient Descent(231/999): loss=0.11026010824078705\n",
      "Gradient Descent(232/999): loss=0.11024782515684628\n",
      "Gradient Descent(233/999): loss=0.1102356183982531\n",
      "Gradient Descent(234/999): loss=0.11022348711492035\n",
      "Gradient Descent(235/999): loss=0.11021143046931534\n",
      "Gradient Descent(236/999): loss=0.11019944763623878\n",
      "Gradient Descent(237/999): loss=0.11018753780260908\n",
      "Gradient Descent(238/999): loss=0.11017570016725066\n",
      "Gradient Descent(239/999): loss=0.11016393394068685\n",
      "Gradient Descent(240/999): loss=0.11015223834493705\n",
      "Gradient Descent(241/999): loss=0.11014061261331802\n",
      "Gradient Descent(242/999): loss=0.11012905599024936\n",
      "Gradient Descent(243/999): loss=0.11011756773106285\n",
      "Gradient Descent(244/999): loss=0.11010614710181564\n",
      "Gradient Descent(245/999): loss=0.1100947933791074\n",
      "Gradient Descent(246/999): loss=0.11008350584990101\n",
      "Gradient Descent(247/999): loss=0.1100722838113468\n",
      "Gradient Descent(248/999): loss=0.11006112657061048\n",
      "Gradient Descent(249/999): loss=0.11005003344470428\n",
      "Gradient Descent(250/999): loss=0.11003900376032173\n",
      "Gradient Descent(251/999): loss=0.11002803685367527\n",
      "Gradient Descent(252/999): loss=0.11001713207033754\n",
      "Gradient Descent(253/999): loss=0.1100062887650854\n",
      "Gradient Descent(254/999): loss=0.10999550630174709\n",
      "Gradient Descent(255/999): loss=0.10998478405305259\n",
      "Gradient Descent(256/999): loss=0.1099741214004864\n",
      "Gradient Descent(257/999): loss=0.10996351773414367\n",
      "Gradient Descent(258/999): loss=0.10995297245258874\n",
      "Gradient Descent(259/999): loss=0.10994248496271647\n",
      "Gradient Descent(260/999): loss=0.10993205467961627\n",
      "Gradient Descent(261/999): loss=0.10992168102643869\n",
      "Gradient Descent(262/999): loss=0.10991136343426451\n",
      "Gradient Descent(263/999): loss=0.10990110134197616\n",
      "Gradient Descent(264/999): loss=0.1098908941961319\n",
      "Gradient Descent(265/999): loss=0.10988074145084206\n",
      "Gradient Descent(266/999): loss=0.10987064256764749\n",
      "Gradient Descent(267/999): loss=0.10986059701540066\n",
      "Gradient Descent(268/999): loss=0.10985060427014864\n",
      "Gradient Descent(269/999): loss=0.10984066381501824\n",
      "Gradient Descent(270/999): loss=0.10983077514010346\n",
      "Gradient Descent(271/999): loss=0.10982093774235491\n",
      "Gradient Descent(272/999): loss=0.10981115112547099\n",
      "Gradient Descent(273/999): loss=0.10980141479979158\n",
      "Gradient Descent(274/999): loss=0.10979172828219301\n",
      "Gradient Descent(275/999): loss=0.10978209109598557\n",
      "Gradient Descent(276/999): loss=0.10977250277081235\n",
      "Gradient Descent(277/999): loss=0.10976296284255016\n",
      "Gradient Descent(278/999): loss=0.10975347085321219\n",
      "Gradient Descent(279/999): loss=0.10974402635085222\n",
      "Gradient Descent(280/999): loss=0.1097346288894709\n",
      "Gradient Descent(281/999): loss=0.10972527802892329\n",
      "Gradient Descent(282/999): loss=0.10971597333482828\n",
      "Gradient Descent(283/999): loss=0.10970671437847952\n",
      "Gradient Descent(284/999): loss=0.10969750073675799\n",
      "Gradient Descent(285/999): loss=0.1096883319920458\n",
      "Gradient Descent(286/999): loss=0.10967920773214207\n",
      "Gradient Descent(287/999): loss=0.10967012755017963\n",
      "Gradient Descent(288/999): loss=0.10966109104454361\n",
      "Gradient Descent(289/999): loss=0.1096520978187913\n",
      "Gradient Descent(290/999): loss=0.10964314748157326\n",
      "Gradient Descent(291/999): loss=0.10963423964655607\n",
      "Gradient Descent(292/999): loss=0.10962537393234603\n",
      "Gradient Descent(293/999): loss=0.10961654996241466\n",
      "Gradient Descent(294/999): loss=0.10960776736502487\n",
      "Gradient Descent(295/999): loss=0.10959902577315891\n",
      "Gradient Descent(296/999): loss=0.10959032482444735\n",
      "Gradient Descent(297/999): loss=0.10958166416109905\n",
      "Gradient Descent(298/999): loss=0.10957304342983269\n",
      "Gradient Descent(299/999): loss=0.10956446228180927\n",
      "Gradient Descent(300/999): loss=0.1095559203725655\n",
      "Gradient Descent(301/999): loss=0.10954741736194891\n",
      "Gradient Descent(302/999): loss=0.10953895291405344\n",
      "Gradient Descent(303/999): loss=0.10953052669715643\n",
      "Gradient Descent(304/999): loss=0.10952213838365646\n",
      "Gradient Descent(305/999): loss=0.10951378765001259\n",
      "Gradient Descent(306/999): loss=0.1095054741766841\n",
      "Gradient Descent(307/999): loss=0.10949719764807157\n",
      "Gradient Descent(308/999): loss=0.10948895775245893\n",
      "Gradient Descent(309/999): loss=0.10948075418195623\n",
      "Gradient Descent(310/999): loss=0.10947258663244355\n",
      "Gradient Descent(311/999): loss=0.10946445480351581\n",
      "Gradient Descent(312/999): loss=0.10945635839842839\n",
      "Gradient Descent(313/999): loss=0.10944829712404378\n",
      "Gradient Descent(314/999): loss=0.10944027069077879\n",
      "Gradient Descent(315/999): loss=0.10943227881255319\n",
      "Gradient Descent(316/999): loss=0.10942432120673844\n",
      "Gradient Descent(317/999): loss=0.10941639759410794\n",
      "Gradient Descent(318/999): loss=0.10940850769878763\n",
      "Gradient Descent(319/999): loss=0.10940065124820757\n",
      "Gradient Descent(320/999): loss=0.10939282797305429\n",
      "Gradient Descent(321/999): loss=0.1093850376072239\n",
      "Gradient Descent(322/999): loss=0.10937727988777579\n",
      "Gradient Descent(323/999): loss=0.10936955455488746\n",
      "Gradient Descent(324/999): loss=0.10936186135180953\n",
      "Gradient Descent(325/999): loss=0.1093542000248221\n",
      "Gradient Descent(326/999): loss=0.10934657032319106\n",
      "Gradient Descent(327/999): loss=0.10933897199912596\n",
      "Gradient Descent(328/999): loss=0.10933140480773768\n",
      "Gradient Descent(329/999): loss=0.10932386850699741\n",
      "Gradient Descent(330/999): loss=0.10931636285769601\n",
      "Gradient Descent(331/999): loss=0.10930888762340395\n",
      "Gradient Descent(332/999): loss=0.10930144257043212\n",
      "Gradient Descent(333/999): loss=0.10929402746779299\n",
      "Gradient Descent(334/999): loss=0.1092866420871626\n",
      "Gradient Descent(335/999): loss=0.10927928620284295\n",
      "Gradient Descent(336/999): loss=0.10927195959172514\n",
      "Gradient Descent(337/999): loss=0.10926466203325308\n",
      "Gradient Descent(338/999): loss=0.10925739330938755\n",
      "Gradient Descent(339/999): loss=0.1092501532045711\n",
      "Gradient Descent(340/999): loss=0.10924294150569337\n",
      "Gradient Descent(341/999): loss=0.10923575800205693\n",
      "Gradient Descent(342/999): loss=0.1092286024853436\n",
      "Gradient Descent(343/999): loss=0.10922147474958148\n",
      "Gradient Descent(344/999): loss=0.10921437459111234\n",
      "Gradient Descent(345/999): loss=0.1092073018085595\n",
      "Gradient Descent(346/999): loss=0.10920025620279625\n",
      "Gradient Descent(347/999): loss=0.10919323757691486\n",
      "Gradient Descent(348/999): loss=0.10918624573619586\n",
      "Gradient Descent(349/999): loss=0.10917928048807791\n",
      "Gradient Descent(350/999): loss=0.10917234164212813\n",
      "Gradient Descent(351/999): loss=0.1091654290100129\n",
      "Gradient Descent(352/999): loss=0.109158542405469\n",
      "Gradient Descent(353/999): loss=0.10915168164427544\n",
      "Gradient Descent(354/999): loss=0.10914484654422528\n",
      "Gradient Descent(355/999): loss=0.10913803692509834\n",
      "Gradient Descent(356/999): loss=0.10913125260863409\n",
      "Gradient Descent(357/999): loss=0.10912449341850503\n",
      "Gradient Descent(358/999): loss=0.10911775918029028\n",
      "Gradient Descent(359/999): loss=0.1091110497214499\n",
      "Gradient Descent(360/999): loss=0.10910436487129943\n",
      "Gradient Descent(361/999): loss=0.10909770446098459\n",
      "Gradient Descent(362/999): loss=0.10909106832345682\n",
      "Gradient Descent(363/999): loss=0.10908445629344884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(364/999): loss=0.10907786820745072\n",
      "Gradient Descent(365/999): loss=0.10907130390368627\n",
      "Gradient Descent(366/999): loss=0.10906476322208969\n",
      "Gradient Descent(367/999): loss=0.1090582460042829\n",
      "Gradient Descent(368/999): loss=0.10905175209355279\n",
      "Gradient Descent(369/999): loss=0.109045281334829\n",
      "Gradient Descent(370/999): loss=0.10903883357466218\n",
      "Gradient Descent(371/999): loss=0.10903240866120231\n",
      "Gradient Descent(372/999): loss=0.1090260064441774\n",
      "Gradient Descent(373/999): loss=0.10901962677487283\n",
      "Gradient Descent(374/999): loss=0.10901326950611039\n",
      "Gradient Descent(375/999): loss=0.1090069344922283\n",
      "Gradient Descent(376/999): loss=0.1090006215890609\n",
      "Gradient Descent(377/999): loss=0.10899433065391922\n",
      "Gradient Descent(378/999): loss=0.10898806154557139\n",
      "Gradient Descent(379/999): loss=0.10898181412422354\n",
      "Gradient Descent(380/999): loss=0.10897558825150098\n",
      "Gradient Descent(381/999): loss=0.10896938379042968\n",
      "Gradient Descent(382/999): loss=0.10896320060541785\n",
      "Gradient Descent(383/999): loss=0.10895703856223797\n",
      "Gradient Descent(384/999): loss=0.10895089752800913\n",
      "Gradient Descent(385/999): loss=0.10894477737117937\n",
      "Gradient Descent(386/999): loss=0.10893867796150852\n",
      "Gradient Descent(387/999): loss=0.1089325991700512\n",
      "Gradient Descent(388/999): loss=0.10892654086914012\n",
      "Gradient Descent(389/999): loss=0.10892050293236948\n",
      "Gradient Descent(390/999): loss=0.10891448523457879\n",
      "Gradient Descent(391/999): loss=0.10890848765183687\n",
      "Gradient Descent(392/999): loss=0.10890251006142598\n",
      "Gradient Descent(393/999): loss=0.10889655234182637\n",
      "Gradient Descent(394/999): loss=0.10889061437270083\n",
      "Gradient Descent(395/999): loss=0.10888469603487973\n",
      "Gradient Descent(396/999): loss=0.10887879721034595\n",
      "Gradient Descent(397/999): loss=0.10887291778222043\n",
      "Gradient Descent(398/999): loss=0.10886705763474755\n",
      "Gradient Descent(399/999): loss=0.10886121665328084\n",
      "Gradient Descent(400/999): loss=0.10885539472426914\n",
      "Gradient Descent(401/999): loss=0.10884959173524254\n",
      "Gradient Descent(402/999): loss=0.10884380757479896\n",
      "Gradient Descent(403/999): loss=0.10883804213259042\n",
      "Gradient Descent(404/999): loss=0.10883229529931009\n",
      "Gradient Descent(405/999): loss=0.108826566966679\n",
      "Gradient Descent(406/999): loss=0.10882085702743333\n",
      "Gradient Descent(407/999): loss=0.10881516537531159\n",
      "Gradient Descent(408/999): loss=0.10880949190504222\n",
      "Gradient Descent(409/999): loss=0.10880383651233115\n",
      "Gradient Descent(410/999): loss=0.10879819909384975\n",
      "Gradient Descent(411/999): loss=0.10879257954722278\n",
      "Gradient Descent(412/999): loss=0.10878697777101669\n",
      "Gradient Descent(413/999): loss=0.10878139366472778\n",
      "Gradient Descent(414/999): loss=0.10877582712877101\n",
      "Gradient Descent(415/999): loss=0.10877027806446841\n",
      "Gradient Descent(416/999): loss=0.10876474637403812\n",
      "Gradient Descent(417/999): loss=0.10875923196058328\n",
      "Gradient Descent(418/999): loss=0.10875373472808128\n",
      "Gradient Descent(419/999): loss=0.10874825458137297\n",
      "Gradient Descent(420/999): loss=0.10874279142615234\n",
      "Gradient Descent(421/999): loss=0.1087373451689559\n",
      "Gradient Descent(422/999): loss=0.10873191571715254\n",
      "Gradient Descent(423/999): loss=0.1087265029789335\n",
      "Gradient Descent(424/999): loss=0.10872110686330234\n",
      "Gradient Descent(425/999): loss=0.10871572728006508\n",
      "Gradient Descent(426/999): loss=0.10871036413982073\n",
      "Gradient Descent(427/999): loss=0.10870501735395152\n",
      "Gradient Descent(428/999): loss=0.10869968683461362\n",
      "Gradient Descent(429/999): loss=0.10869437249472792\n",
      "Gradient Descent(430/999): loss=0.1086890742479708\n",
      "Gradient Descent(431/999): loss=0.10868379200876503\n",
      "Gradient Descent(432/999): loss=0.1086785256922711\n",
      "Gradient Descent(433/999): loss=0.10867327521437833\n",
      "Gradient Descent(434/999): loss=0.10866804049169627\n",
      "Gradient Descent(435/999): loss=0.10866282144154607\n",
      "Gradient Descent(436/999): loss=0.10865761798195224\n",
      "Gradient Descent(437/999): loss=0.10865243003163426\n",
      "Gradient Descent(438/999): loss=0.10864725750999839\n",
      "Gradient Descent(439/999): loss=0.10864210033712972\n",
      "Gradient Descent(440/999): loss=0.10863695843378413\n",
      "Gradient Descent(441/999): loss=0.10863183172138052\n",
      "Gradient Descent(442/999): loss=0.10862672012199294\n",
      "Gradient Descent(443/999): loss=0.10862162355834318\n",
      "Gradient Descent(444/999): loss=0.10861654195379312\n",
      "Gradient Descent(445/999): loss=0.10861147523233727\n",
      "Gradient Descent(446/999): loss=0.10860642331859563\n",
      "Gradient Descent(447/999): loss=0.10860138613780629\n",
      "Gradient Descent(448/999): loss=0.10859636361581844\n",
      "Gradient Descent(449/999): loss=0.10859135567908532\n",
      "Gradient Descent(450/999): loss=0.10858636225465722\n",
      "Gradient Descent(451/999): loss=0.10858138327017489\n",
      "Gradient Descent(452/999): loss=0.10857641865386253\n",
      "Gradient Descent(453/999): loss=0.10857146833452122\n",
      "Gradient Descent(454/999): loss=0.10856653224152259\n",
      "Gradient Descent(455/999): loss=0.10856161030480202\n",
      "Gradient Descent(456/999): loss=0.10855670245485254\n",
      "Gradient Descent(457/999): loss=0.10855180862271838\n",
      "Gradient Descent(458/999): loss=0.10854692873998882\n",
      "Gradient Descent(459/999): loss=0.1085420627387921\n",
      "Gradient Descent(460/999): loss=0.1085372105517894\n",
      "Gradient Descent(461/999): loss=0.10853237211216873\n",
      "Gradient Descent(462/999): loss=0.10852754735363929\n",
      "Gradient Descent(463/999): loss=0.1085227362104255\n",
      "Gradient Descent(464/999): loss=0.10851793861726136\n",
      "Gradient Descent(465/999): loss=0.10851315450938476\n",
      "Gradient Descent(466/999): loss=0.10850838382253193\n",
      "Gradient Descent(467/999): loss=0.10850362649293206\n",
      "Gradient Descent(468/999): loss=0.1084988824573017\n",
      "Gradient Descent(469/999): loss=0.10849415165283954\n",
      "Gradient Descent(470/999): loss=0.10848943401722104\n",
      "Gradient Descent(471/999): loss=0.10848472948859338\n",
      "Gradient Descent(472/999): loss=0.1084800380055702\n",
      "Gradient Descent(473/999): loss=0.10847535950722655\n",
      "Gradient Descent(474/999): loss=0.10847069393309387\n",
      "Gradient Descent(475/999): loss=0.1084660412231552\n",
      "Gradient Descent(476/999): loss=0.10846140131784011\n",
      "Gradient Descent(477/999): loss=0.10845677415802003\n",
      "Gradient Descent(478/999): loss=0.1084521596850035\n",
      "Gradient Descent(479/999): loss=0.10844755784053148\n",
      "Gradient Descent(480/999): loss=0.10844296856677264\n",
      "Gradient Descent(481/999): loss=0.108438391806319\n",
      "Gradient Descent(482/999): loss=0.10843382750218122\n",
      "Gradient Descent(483/999): loss=0.10842927559778438\n",
      "Gradient Descent(484/999): loss=0.1084247360369634\n",
      "Gradient Descent(485/999): loss=0.10842020876395883\n",
      "Gradient Descent(486/999): loss=0.10841569372341256\n",
      "Gradient Descent(487/999): loss=0.10841119086036372\n",
      "Gradient Descent(488/999): loss=0.10840670012024425\n",
      "Gradient Descent(489/999): loss=0.10840222144887508\n",
      "Gradient Descent(490/999): loss=0.10839775479246197\n",
      "Gradient Descent(491/999): loss=0.10839330009759147\n",
      "Gradient Descent(492/999): loss=0.10838885731122708\n",
      "Gradient Descent(493/999): loss=0.10838442638070525\n",
      "Gradient Descent(494/999): loss=0.10838000725373166\n",
      "Gradient Descent(495/999): loss=0.1083755998783773\n",
      "Gradient Descent(496/999): loss=0.10837120420307483\n",
      "Gradient Descent(497/999): loss=0.10836682017661484\n",
      "Gradient Descent(498/999): loss=0.10836244774814212\n",
      "Gradient Descent(499/999): loss=0.10835808686715226\n",
      "Gradient Descent(500/999): loss=0.10835373748348792\n",
      "Gradient Descent(501/999): loss=0.10834939954733543\n",
      "Gradient Descent(502/999): loss=0.10834507300922112\n",
      "Gradient Descent(503/999): loss=0.10834075782000824\n",
      "Gradient Descent(504/999): loss=0.10833645393089322\n",
      "Gradient Descent(505/999): loss=0.10833216129340269\n",
      "Gradient Descent(506/999): loss=0.10832787985938984\n",
      "Gradient Descent(507/999): loss=0.1083236095810314\n",
      "Gradient Descent(508/999): loss=0.1083193504108244\n",
      "Gradient Descent(509/999): loss=0.10831510230158291\n",
      "Gradient Descent(510/999): loss=0.10831086520643499\n",
      "Gradient Descent(511/999): loss=0.10830663907881952\n",
      "Gradient Descent(512/999): loss=0.10830242387248332\n",
      "Gradient Descent(513/999): loss=0.10829821954147788\n",
      "Gradient Descent(514/999): loss=0.10829402604015666\n",
      "Gradient Descent(515/999): loss=0.10828984332317189\n",
      "Gradient Descent(516/999): loss=0.10828567134547185\n",
      "Gradient Descent(517/999): loss=0.10828151006229796\n",
      "Gradient Descent(518/999): loss=0.10827735942918194\n",
      "Gradient Descent(519/999): loss=0.10827321940194298\n",
      "Gradient Descent(520/999): loss=0.10826908993668502\n",
      "Gradient Descent(521/999): loss=0.108264970989794\n",
      "Gradient Descent(522/999): loss=0.10826086251793525\n",
      "Gradient Descent(523/999): loss=0.1082567644780507\n",
      "Gradient Descent(524/999): loss=0.10825267682735636\n",
      "Gradient Descent(525/999): loss=0.1082485995233396\n",
      "Gradient Descent(526/999): loss=0.10824453252375675\n",
      "Gradient Descent(527/999): loss=0.10824047578663047\n",
      "Gradient Descent(528/999): loss=0.10823642927024728\n",
      "Gradient Descent(529/999): loss=0.10823239293315501\n",
      "Gradient Descent(530/999): loss=0.1082283667341605\n",
      "Gradient Descent(531/999): loss=0.10822435063232709\n",
      "Gradient Descent(532/999): loss=0.10822034458697224\n",
      "Gradient Descent(533/999): loss=0.10821634855766528\n",
      "Gradient Descent(534/999): loss=0.1082123625042249\n",
      "Gradient Descent(535/999): loss=0.10820838638671709\n",
      "Gradient Descent(536/999): loss=0.10820442016545262\n",
      "Gradient Descent(537/999): loss=0.10820046380098501\n",
      "Gradient Descent(538/999): loss=0.1081965172541082\n",
      "Gradient Descent(539/999): loss=0.10819258048585442\n",
      "Gradient Descent(540/999): loss=0.10818865345749196\n",
      "Gradient Descent(541/999): loss=0.10818473613052308\n",
      "Gradient Descent(542/999): loss=0.10818082846668196\n",
      "Gradient Descent(543/999): loss=0.10817693042793247\n",
      "Gradient Descent(544/999): loss=0.10817304197646624\n",
      "Gradient Descent(545/999): loss=0.10816916307470058\n",
      "Gradient Descent(546/999): loss=0.10816529368527641\n",
      "Gradient Descent(547/999): loss=0.10816143377105639\n",
      "Gradient Descent(548/999): loss=0.10815758329512284\n",
      "Gradient Descent(549/999): loss=0.10815374222077594\n",
      "Gradient Descent(550/999): loss=0.10814991051153157\n",
      "Gradient Descent(551/999): loss=0.1081460881311197\n",
      "Gradient Descent(552/999): loss=0.1081422750434823\n",
      "Gradient Descent(553/999): loss=0.10813847121277162\n",
      "Gradient Descent(554/999): loss=0.10813467660334829\n",
      "Gradient Descent(555/999): loss=0.1081308911797795\n",
      "Gradient Descent(556/999): loss=0.10812711490683728\n",
      "Gradient Descent(557/999): loss=0.10812334774949665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(558/999): loss=0.108119589672934\n",
      "Gradient Descent(559/999): loss=0.10811584064252516\n",
      "Gradient Descent(560/999): loss=0.10811210062384388\n",
      "Gradient Descent(561/999): loss=0.10810836958266003\n",
      "Gradient Descent(562/999): loss=0.10810464748493806\n",
      "Gradient Descent(563/999): loss=0.10810093429683519\n",
      "Gradient Descent(564/999): loss=0.10809722998469984\n",
      "Gradient Descent(565/999): loss=0.1080935345150701\n",
      "Gradient Descent(566/999): loss=0.10808984785467202\n",
      "Gradient Descent(567/999): loss=0.10808616997041808\n",
      "Gradient Descent(568/999): loss=0.10808250082940567\n",
      "Gradient Descent(569/999): loss=0.1080788403989155\n",
      "Gradient Descent(570/999): loss=0.10807518864641007\n",
      "Gradient Descent(571/999): loss=0.1080715455395322\n",
      "Gradient Descent(572/999): loss=0.10806791104610358\n",
      "Gradient Descent(573/999): loss=0.10806428513412314\n",
      "Gradient Descent(574/999): loss=0.10806066777176579\n",
      "Gradient Descent(575/999): loss=0.10805705892738088\n",
      "Gradient Descent(576/999): loss=0.10805345856949075\n",
      "Gradient Descent(577/999): loss=0.10804986666678938\n",
      "Gradient Descent(578/999): loss=0.10804628318814097\n",
      "Gradient Descent(579/999): loss=0.10804270810257859\n",
      "Gradient Descent(580/999): loss=0.10803914137930282\n",
      "Gradient Descent(581/999): loss=0.1080355829876803\n",
      "Gradient Descent(582/999): loss=0.1080320328972426\n",
      "Gradient Descent(583/999): loss=0.1080284910776847\n",
      "Gradient Descent(584/999): loss=0.10802495749886377\n",
      "Gradient Descent(585/999): loss=0.10802143213079794\n",
      "Gradient Descent(586/999): loss=0.10801791494366496\n",
      "Gradient Descent(587/999): loss=0.1080144059078009\n",
      "Gradient Descent(588/999): loss=0.10801090499369902\n",
      "Gradient Descent(589/999): loss=0.10800741217200842\n",
      "Gradient Descent(590/999): loss=0.10800392741353289\n",
      "Gradient Descent(591/999): loss=0.10800045068922969\n",
      "Gradient Descent(592/999): loss=0.1079969819702083\n",
      "Gradient Descent(593/999): loss=0.10799352122772941\n",
      "Gradient Descent(594/999): loss=0.10799006843320348\n",
      "Gradient Descent(595/999): loss=0.10798662355818985\n",
      "Gradient Descent(596/999): loss=0.10798318657439539\n",
      "Gradient Descent(597/999): loss=0.10797975745367352\n",
      "Gradient Descent(598/999): loss=0.10797633616802303\n",
      "Gradient Descent(599/999): loss=0.10797292268958696\n",
      "Gradient Descent(600/999): loss=0.10796951699065148\n",
      "Gradient Descent(601/999): loss=0.10796611904364488\n",
      "Gradient Descent(602/999): loss=0.10796272882113643\n",
      "Gradient Descent(603/999): loss=0.1079593462958354\n",
      "Gradient Descent(604/999): loss=0.1079559714405899\n",
      "Gradient Descent(605/999): loss=0.10795260422838593\n",
      "Gradient Descent(606/999): loss=0.10794924463234631\n",
      "Gradient Descent(607/999): loss=0.10794589262572964\n",
      "Gradient Descent(608/999): loss=0.10794254818192933\n",
      "Gradient Descent(609/999): loss=0.10793921127447267\n",
      "Gradient Descent(610/999): loss=0.10793588187701965\n",
      "Gradient Descent(611/999): loss=0.10793255996336214\n",
      "Gradient Descent(612/999): loss=0.10792924550742293\n",
      "Gradient Descent(613/999): loss=0.10792593848325471\n",
      "Gradient Descent(614/999): loss=0.10792263886503908\n",
      "Gradient Descent(615/999): loss=0.10791934662708573\n",
      "Gradient Descent(616/999): loss=0.10791606174383143\n",
      "Gradient Descent(617/999): loss=0.10791278418983918\n",
      "Gradient Descent(618/999): loss=0.10790951393979714\n",
      "Gradient Descent(619/999): loss=0.10790625096851797\n",
      "Gradient Descent(620/999): loss=0.10790299525093774\n",
      "Gradient Descent(621/999): loss=0.1078997467621152\n",
      "Gradient Descent(622/999): loss=0.1078965054772307\n",
      "Gradient Descent(623/999): loss=0.10789327137158561\n",
      "Gradient Descent(624/999): loss=0.10789004442060114\n",
      "Gradient Descent(625/999): loss=0.10788682459981778\n",
      "Gradient Descent(626/999): loss=0.10788361188489434\n",
      "Gradient Descent(627/999): loss=0.10788040625160711\n",
      "Gradient Descent(628/999): loss=0.10787720767584892\n",
      "Gradient Descent(629/999): loss=0.10787401613362865\n",
      "Gradient Descent(630/999): loss=0.10787083160107014\n",
      "Gradient Descent(631/999): loss=0.10786765405441145\n",
      "Gradient Descent(632/999): loss=0.10786448347000413\n",
      "Gradient Descent(633/999): loss=0.10786131982431235\n",
      "Gradient Descent(634/999): loss=0.10785816309391233\n",
      "Gradient Descent(635/999): loss=0.10785501325549124\n",
      "Gradient Descent(636/999): loss=0.10785187028584671\n",
      "Gradient Descent(637/999): loss=0.10784873416188603\n",
      "Gradient Descent(638/999): loss=0.10784560486062522\n",
      "Gradient Descent(639/999): loss=0.10784248235918861\n",
      "Gradient Descent(640/999): loss=0.10783936663480784\n",
      "Gradient Descent(641/999): loss=0.10783625766482116\n",
      "Gradient Descent(642/999): loss=0.1078331554266729\n",
      "Gradient Descent(643/999): loss=0.10783005989791257\n",
      "Gradient Descent(644/999): loss=0.10782697105619422\n",
      "Gradient Descent(645/999): loss=0.1078238888792758\n",
      "Gradient Descent(646/999): loss=0.10782081334501836\n",
      "Gradient Descent(647/999): loss=0.10781774443138545\n",
      "Gradient Descent(648/999): loss=0.10781468211644236\n",
      "Gradient Descent(649/999): loss=0.10781162637835558\n",
      "Gradient Descent(650/999): loss=0.10780857719539202\n",
      "Gradient Descent(651/999): loss=0.10780553454591837\n",
      "Gradient Descent(652/999): loss=0.10780249840840045\n",
      "Gradient Descent(653/999): loss=0.1077994687614026\n",
      "Gradient Descent(654/999): loss=0.10779644558358704\n",
      "Gradient Descent(655/999): loss=0.10779342885371314\n",
      "Gradient Descent(656/999): loss=0.10779041855063694\n",
      "Gradient Descent(657/999): loss=0.10778741465331036\n",
      "Gradient Descent(658/999): loss=0.10778441714078074\n",
      "Gradient Descent(659/999): loss=0.1077814259921901\n",
      "Gradient Descent(660/999): loss=0.10777844118677463\n",
      "Gradient Descent(661/999): loss=0.10777546270386403\n",
      "Gradient Descent(662/999): loss=0.10777249052288092\n",
      "Gradient Descent(663/999): loss=0.10776952462334026\n",
      "Gradient Descent(664/999): loss=0.1077665649848488\n",
      "Gradient Descent(665/999): loss=0.10776361158710443\n",
      "Gradient Descent(666/999): loss=0.10776066440989562\n",
      "Gradient Descent(667/999): loss=0.10775772343310099\n",
      "Gradient Descent(668/999): loss=0.10775478863668844\n",
      "Gradient Descent(669/999): loss=0.10775186000071492\n",
      "Gradient Descent(670/999): loss=0.10774893750532569\n",
      "Gradient Descent(671/999): loss=0.10774602113075382\n",
      "Gradient Descent(672/999): loss=0.10774311085731958\n",
      "Gradient Descent(673/999): loss=0.10774020666543009\n",
      "Gradient Descent(674/999): loss=0.10773730853557847\n",
      "Gradient Descent(675/999): loss=0.1077344164483437\n",
      "Gradient Descent(676/999): loss=0.10773153038438968\n",
      "Gradient Descent(677/999): loss=0.10772865032446508\n",
      "Gradient Descent(678/999): loss=0.10772577624940255\n",
      "Gradient Descent(679/999): loss=0.10772290814011834\n",
      "Gradient Descent(680/999): loss=0.10772004597761183\n",
      "Gradient Descent(681/999): loss=0.1077171897429649\n",
      "Gradient Descent(682/999): loss=0.10771433941734149\n",
      "Gradient Descent(683/999): loss=0.10771149498198715\n",
      "Gradient Descent(684/999): loss=0.10770865641822848\n",
      "Gradient Descent(685/999): loss=0.10770582370747263\n",
      "Gradient Descent(686/999): loss=0.10770299683120699\n",
      "Gradient Descent(687/999): loss=0.10770017577099843\n",
      "Gradient Descent(688/999): loss=0.10769736050849309\n",
      "Gradient Descent(689/999): loss=0.10769455102541571\n",
      "Gradient Descent(690/999): loss=0.10769174730356933\n",
      "Gradient Descent(691/999): loss=0.10768894932483468\n",
      "Gradient Descent(692/999): loss=0.10768615707116981\n",
      "Gradient Descent(693/999): loss=0.10768337052460958\n",
      "Gradient Descent(694/999): loss=0.10768058966726532\n",
      "Gradient Descent(695/999): loss=0.10767781448132428\n",
      "Gradient Descent(696/999): loss=0.10767504494904916\n",
      "Gradient Descent(697/999): loss=0.10767228105277778\n",
      "Gradient Descent(698/999): loss=0.10766952277492249\n",
      "Gradient Descent(699/999): loss=0.10766677009796997\n",
      "Gradient Descent(700/999): loss=0.10766402300448057\n",
      "Gradient Descent(701/999): loss=0.10766128147708798\n",
      "Gradient Descent(702/999): loss=0.10765854549849883\n",
      "Gradient Descent(703/999): loss=0.10765581505149224\n",
      "Gradient Descent(704/999): loss=0.10765309011891942\n",
      "Gradient Descent(705/999): loss=0.1076503706837032\n",
      "Gradient Descent(706/999): loss=0.10764765672883773\n",
      "Gradient Descent(707/999): loss=0.10764494823738796\n",
      "Gradient Descent(708/999): loss=0.10764224519248934\n",
      "Gradient Descent(709/999): loss=0.1076395475773473\n",
      "Gradient Descent(710/999): loss=0.10763685537523703\n",
      "Gradient Descent(711/999): loss=0.10763416856950288\n",
      "Gradient Descent(712/999): loss=0.10763148714355819\n",
      "Gradient Descent(713/999): loss=0.10762881108088458\n",
      "Gradient Descent(714/999): loss=0.10762614036503204\n",
      "Gradient Descent(715/999): loss=0.1076234749796181\n",
      "Gradient Descent(716/999): loss=0.10762081490832766\n",
      "Gradient Descent(717/999): loss=0.10761816013491264\n",
      "Gradient Descent(718/999): loss=0.10761551064319151\n",
      "Gradient Descent(719/999): loss=0.10761286641704902\n",
      "Gradient Descent(720/999): loss=0.10761022744043573\n",
      "Gradient Descent(721/999): loss=0.10760759369736775\n",
      "Gradient Descent(722/999): loss=0.10760496517192626\n",
      "Gradient Descent(723/999): loss=0.10760234184825733\n",
      "Gradient Descent(724/999): loss=0.10759972371057135\n",
      "Gradient Descent(725/999): loss=0.10759711074314285\n",
      "Gradient Descent(726/999): loss=0.10759450293031003\n",
      "Gradient Descent(727/999): loss=0.10759190025647458\n",
      "Gradient Descent(728/999): loss=0.10758930270610105\n",
      "Gradient Descent(729/999): loss=0.1075867102637169\n",
      "Gradient Descent(730/999): loss=0.10758412291391171\n",
      "Gradient Descent(731/999): loss=0.10758154064133726\n",
      "Gradient Descent(732/999): loss=0.10757896343070697\n",
      "Gradient Descent(733/999): loss=0.10757639126679557\n",
      "Gradient Descent(734/999): loss=0.10757382413443882\n",
      "Gradient Descent(735/999): loss=0.10757126201853322\n",
      "Gradient Descent(736/999): loss=0.1075687049040356\n",
      "Gradient Descent(737/999): loss=0.10756615277596289\n",
      "Gradient Descent(738/999): loss=0.1075636056193917\n",
      "Gradient Descent(739/999): loss=0.10756106341945808\n",
      "Gradient Descent(740/999): loss=0.10755852616135715\n",
      "Gradient Descent(741/999): loss=0.10755599383034294\n",
      "Gradient Descent(742/999): loss=0.10755346641172778\n",
      "Gradient Descent(743/999): loss=0.1075509438908823\n",
      "Gradient Descent(744/999): loss=0.10754842625323494\n",
      "Gradient Descent(745/999): loss=0.10754591348427171\n",
      "Gradient Descent(746/999): loss=0.1075434055695359\n",
      "Gradient Descent(747/999): loss=0.10754090249462776\n",
      "Gradient Descent(748/999): loss=0.1075384042452042\n",
      "Gradient Descent(749/999): loss=0.10753591080697845\n",
      "Gradient Descent(750/999): loss=0.10753342216571991\n",
      "Gradient Descent(751/999): loss=0.1075309383072537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(752/999): loss=0.1075284592174605\n",
      "Gradient Descent(753/999): loss=0.10752598488227612\n",
      "Gradient Descent(754/999): loss=0.10752351528769138\n",
      "Gradient Descent(755/999): loss=0.10752105041975173\n",
      "Gradient Descent(756/999): loss=0.10751859026455689\n",
      "Gradient Descent(757/999): loss=0.10751613480826082\n",
      "Gradient Descent(758/999): loss=0.10751368403707122\n",
      "Gradient Descent(759/999): loss=0.1075112379372493\n",
      "Gradient Descent(760/999): loss=0.10750879649510957\n",
      "Gradient Descent(761/999): loss=0.10750635969701954\n",
      "Gradient Descent(762/999): loss=0.10750392752939941\n",
      "Gradient Descent(763/999): loss=0.10750149997872191\n",
      "Gradient Descent(764/999): loss=0.10749907703151189\n",
      "Gradient Descent(765/999): loss=0.10749665867434614\n",
      "Gradient Descent(766/999): loss=0.10749424489385316\n",
      "Gradient Descent(767/999): loss=0.10749183567671285\n",
      "Gradient Descent(768/999): loss=0.10748943100965622\n",
      "Gradient Descent(769/999): loss=0.1074870308794652\n",
      "Gradient Descent(770/999): loss=0.10748463527297243\n",
      "Gradient Descent(771/999): loss=0.10748224417706086\n",
      "Gradient Descent(772/999): loss=0.10747985757866356\n",
      "Gradient Descent(773/999): loss=0.10747747546476355\n",
      "Gradient Descent(774/999): loss=0.10747509782239346\n",
      "Gradient Descent(775/999): loss=0.10747272463863532\n",
      "Gradient Descent(776/999): loss=0.10747035590062033\n",
      "Gradient Descent(777/999): loss=0.10746799159552861\n",
      "Gradient Descent(778/999): loss=0.10746563171058891\n",
      "Gradient Descent(779/999): loss=0.10746327623307839\n",
      "Gradient Descent(780/999): loss=0.10746092515032249\n",
      "Gradient Descent(781/999): loss=0.10745857844969453\n",
      "Gradient Descent(782/999): loss=0.10745623611861559\n",
      "Gradient Descent(783/999): loss=0.10745389814455417\n",
      "Gradient Descent(784/999): loss=0.10745156451502612\n",
      "Gradient Descent(785/999): loss=0.1074492352175943\n",
      "Gradient Descent(786/999): loss=0.1074469102398683\n",
      "Gradient Descent(787/999): loss=0.10744458956950437\n",
      "Gradient Descent(788/999): loss=0.107442273194205\n",
      "Gradient Descent(789/999): loss=0.10743996110171893\n",
      "Gradient Descent(790/999): loss=0.10743765327984069\n",
      "Gradient Descent(791/999): loss=0.10743534971641056\n",
      "Gradient Descent(792/999): loss=0.10743305039931421\n",
      "Gradient Descent(793/999): loss=0.10743075531648262\n",
      "Gradient Descent(794/999): loss=0.10742846445589181\n",
      "Gradient Descent(795/999): loss=0.10742617780556246\n",
      "Gradient Descent(796/999): loss=0.10742389535356002\n",
      "Gradient Descent(797/999): loss=0.10742161708799423\n",
      "Gradient Descent(798/999): loss=0.10741934299701902\n",
      "Gradient Descent(799/999): loss=0.10741707306883228\n",
      "Gradient Descent(800/999): loss=0.10741480729167564\n",
      "Gradient Descent(801/999): loss=0.10741254565383426\n",
      "Gradient Descent(802/999): loss=0.10741028814363673\n",
      "Gradient Descent(803/999): loss=0.10740803474945465\n",
      "Gradient Descent(804/999): loss=0.10740578545970263\n",
      "Gradient Descent(805/999): loss=0.10740354026283798\n",
      "Gradient Descent(806/999): loss=0.10740129914736049\n",
      "Gradient Descent(807/999): loss=0.10739906210181241\n",
      "Gradient Descent(808/999): loss=0.10739682911477796\n",
      "Gradient Descent(809/999): loss=0.1073946001748834\n",
      "Gradient Descent(810/999): loss=0.1073923752707967\n",
      "Gradient Descent(811/999): loss=0.10739015439122734\n",
      "Gradient Descent(812/999): loss=0.10738793752492617\n",
      "Gradient Descent(813/999): loss=0.1073857246606852\n",
      "Gradient Descent(814/999): loss=0.10738351578733742\n",
      "Gradient Descent(815/999): loss=0.10738131089375652\n",
      "Gradient Descent(816/999): loss=0.10737910996885687\n",
      "Gradient Descent(817/999): loss=0.10737691300159315\n",
      "Gradient Descent(818/999): loss=0.10737471998096035\n",
      "Gradient Descent(819/999): loss=0.1073725308959934\n",
      "Gradient Descent(820/999): loss=0.10737034573576708\n",
      "Gradient Descent(821/999): loss=0.10736816448939589\n",
      "Gradient Descent(822/999): loss=0.10736598714603371\n",
      "Gradient Descent(823/999): loss=0.10736381369487381\n",
      "Gradient Descent(824/999): loss=0.10736164412514851\n",
      "Gradient Descent(825/999): loss=0.10735947842612911\n",
      "Gradient Descent(826/999): loss=0.10735731658712562\n",
      "Gradient Descent(827/999): loss=0.10735515859748668\n",
      "Gradient Descent(828/999): loss=0.10735300444659929\n",
      "Gradient Descent(829/999): loss=0.10735085412388874\n",
      "Gradient Descent(830/999): loss=0.1073487076188183\n",
      "Gradient Descent(831/999): loss=0.10734656492088922\n",
      "Gradient Descent(832/999): loss=0.1073444260196404\n",
      "Gradient Descent(833/999): loss=0.10734229090464824\n",
      "Gradient Descent(834/999): loss=0.10734015956552664\n",
      "Gradient Descent(835/999): loss=0.10733803199192661\n",
      "Gradient Descent(836/999): loss=0.10733590817353625\n",
      "Gradient Descent(837/999): loss=0.10733378810008044\n",
      "Gradient Descent(838/999): loss=0.10733167176132093\n",
      "Gradient Descent(839/999): loss=0.10732955914705584\n",
      "Gradient Descent(840/999): loss=0.10732745024711977\n",
      "Gradient Descent(841/999): loss=0.10732534505138351\n",
      "Gradient Descent(842/999): loss=0.10732324354975391\n",
      "Gradient Descent(843/999): loss=0.10732114573217366\n",
      "Gradient Descent(844/999): loss=0.10731905158862129\n",
      "Gradient Descent(845/999): loss=0.10731696110911078\n",
      "Gradient Descent(846/999): loss=0.10731487428369164\n",
      "Gradient Descent(847/999): loss=0.10731279110244855\n",
      "Gradient Descent(848/999): loss=0.1073107115555014\n",
      "Gradient Descent(849/999): loss=0.10730863563300483\n",
      "Gradient Descent(850/999): loss=0.10730656332514853\n",
      "Gradient Descent(851/999): loss=0.10730449462215663\n",
      "Gradient Descent(852/999): loss=0.10730242951428784\n",
      "Gradient Descent(853/999): loss=0.10730036799183516\n",
      "Gradient Descent(854/999): loss=0.10729831004512586\n",
      "Gradient Descent(855/999): loss=0.1072962556645211\n",
      "Gradient Descent(856/999): loss=0.10729420484041605\n",
      "Gradient Descent(857/999): loss=0.10729215756323958\n",
      "Gradient Descent(858/999): loss=0.10729011382345412\n",
      "Gradient Descent(859/999): loss=0.10728807361155557\n",
      "Gradient Descent(860/999): loss=0.1072860369180731\n",
      "Gradient Descent(861/999): loss=0.10728400373356911\n",
      "Gradient Descent(862/999): loss=0.10728197404863886\n",
      "Gradient Descent(863/999): loss=0.10727994785391065\n",
      "Gradient Descent(864/999): loss=0.10727792514004536\n",
      "Gradient Descent(865/999): loss=0.10727590589773653\n",
      "Gradient Descent(866/999): loss=0.10727389011771007\n",
      "Gradient Descent(867/999): loss=0.10727187779072427\n",
      "Gradient Descent(868/999): loss=0.1072698689075695\n",
      "Gradient Descent(869/999): loss=0.1072678634590682\n",
      "Gradient Descent(870/999): loss=0.10726586143607465\n",
      "Gradient Descent(871/999): loss=0.10726386282947493\n",
      "Gradient Descent(872/999): loss=0.10726186763018666\n",
      "Gradient Descent(873/999): loss=0.10725987582915901\n",
      "Gradient Descent(874/999): loss=0.1072578874173724\n",
      "Gradient Descent(875/999): loss=0.1072559023858385\n",
      "Gradient Descent(876/999): loss=0.10725392072560005\n",
      "Gradient Descent(877/999): loss=0.10725194242773073\n",
      "Gradient Descent(878/999): loss=0.10724996748333498\n",
      "Gradient Descent(879/999): loss=0.10724799588354798\n",
      "Gradient Descent(880/999): loss=0.10724602761953539\n",
      "Gradient Descent(881/999): loss=0.1072440626824933\n",
      "Gradient Descent(882/999): loss=0.1072421010636481\n",
      "Gradient Descent(883/999): loss=0.1072401427542563\n",
      "Gradient Descent(884/999): loss=0.1072381877456045\n",
      "Gradient Descent(885/999): loss=0.1072362360290091\n",
      "Gradient Descent(886/999): loss=0.10723428759581637\n",
      "Gradient Descent(887/999): loss=0.10723234243740214\n",
      "Gradient Descent(888/999): loss=0.10723040054517184\n",
      "Gradient Descent(889/999): loss=0.10722846191056025\n",
      "Gradient Descent(890/999): loss=0.10722652652503137\n",
      "Gradient Descent(891/999): loss=0.1072245943800785\n",
      "Gradient Descent(892/999): loss=0.10722266546722381\n",
      "Gradient Descent(893/999): loss=0.1072207397780185\n",
      "Gradient Descent(894/999): loss=0.10721881730404242\n",
      "Gradient Descent(895/999): loss=0.1072168980369042\n",
      "Gradient Descent(896/999): loss=0.10721498196824097\n",
      "Gradient Descent(897/999): loss=0.10721306908971824\n",
      "Gradient Descent(898/999): loss=0.1072111593930299\n",
      "Gradient Descent(899/999): loss=0.10720925286989799\n",
      "Gradient Descent(900/999): loss=0.10720734951207257\n",
      "Gradient Descent(901/999): loss=0.10720544931133179\n",
      "Gradient Descent(902/999): loss=0.10720355225948142\n",
      "Gradient Descent(903/999): loss=0.10720165834835517\n",
      "Gradient Descent(904/999): loss=0.10719976756981424\n",
      "Gradient Descent(905/999): loss=0.10719787991574735\n",
      "Gradient Descent(906/999): loss=0.10719599537807054\n",
      "Gradient Descent(907/999): loss=0.10719411394872719\n",
      "Gradient Descent(908/999): loss=0.1071922356196878\n",
      "Gradient Descent(909/999): loss=0.10719036038294988\n",
      "Gradient Descent(910/999): loss=0.10718848823053792\n",
      "Gradient Descent(911/999): loss=0.10718661915450314\n",
      "Gradient Descent(912/999): loss=0.10718475314692362\n",
      "Gradient Descent(913/999): loss=0.10718289019990383\n",
      "Gradient Descent(914/999): loss=0.10718103030557494\n",
      "Gradient Descent(915/999): loss=0.10717917345609429\n",
      "Gradient Descent(916/999): loss=0.10717731964364563\n",
      "Gradient Descent(917/999): loss=0.10717546886043884\n",
      "Gradient Descent(918/999): loss=0.1071736210987098\n",
      "Gradient Descent(919/999): loss=0.10717177635072041\n",
      "Gradient Descent(920/999): loss=0.10716993460875833\n",
      "Gradient Descent(921/999): loss=0.1071680958651371\n",
      "Gradient Descent(922/999): loss=0.1071662601121957\n",
      "Gradient Descent(923/999): loss=0.10716442734229881\n",
      "Gradient Descent(924/999): loss=0.10716259754783639\n",
      "Gradient Descent(925/999): loss=0.10716077072122382\n",
      "Gradient Descent(926/999): loss=0.10715894685490158\n",
      "Gradient Descent(927/999): loss=0.10715712594133546\n",
      "Gradient Descent(928/999): loss=0.10715530797301612\n",
      "Gradient Descent(929/999): loss=0.10715349294245904\n",
      "Gradient Descent(930/999): loss=0.10715168084220478\n",
      "Gradient Descent(931/999): loss=0.10714987166481832\n",
      "Gradient Descent(932/999): loss=0.10714806540288949\n",
      "Gradient Descent(933/999): loss=0.10714626204903244\n",
      "Gradient Descent(934/999): loss=0.10714446159588585\n",
      "Gradient Descent(935/999): loss=0.10714266403611267\n",
      "Gradient Descent(936/999): loss=0.10714086936240008\n",
      "Gradient Descent(937/999): loss=0.10713907756745933\n",
      "Gradient Descent(938/999): loss=0.10713728864402579\n",
      "Gradient Descent(939/999): loss=0.10713550258485864\n",
      "Gradient Descent(940/999): loss=0.10713371938274097\n",
      "Gradient Descent(941/999): loss=0.10713193903047957\n",
      "Gradient Descent(942/999): loss=0.10713016152090485\n",
      "Gradient Descent(943/999): loss=0.1071283868468708\n",
      "Gradient Descent(944/999): loss=0.10712661500125487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(945/999): loss=0.10712484597695777\n",
      "Gradient Descent(946/999): loss=0.1071230797669036\n",
      "Gradient Descent(947/999): loss=0.10712131636403957\n",
      "Gradient Descent(948/999): loss=0.10711955576133594\n",
      "Gradient Descent(949/999): loss=0.10711779795178601\n",
      "Gradient Descent(950/999): loss=0.10711604292840593\n",
      "Gradient Descent(951/999): loss=0.10711429068423467\n",
      "Gradient Descent(952/999): loss=0.10711254121233388\n",
      "Gradient Descent(953/999): loss=0.10711079450578793\n",
      "Gradient Descent(954/999): loss=0.10710905055770364\n",
      "Gradient Descent(955/999): loss=0.10710730936121028\n",
      "Gradient Descent(956/999): loss=0.10710557090945948\n",
      "Gradient Descent(957/999): loss=0.10710383519562515\n",
      "Gradient Descent(958/999): loss=0.10710210221290337\n",
      "Gradient Descent(959/999): loss=0.1071003719545123\n",
      "Gradient Descent(960/999): loss=0.10709864441369212\n",
      "Gradient Descent(961/999): loss=0.10709691958370497\n",
      "Gradient Descent(962/999): loss=0.10709519745783473\n",
      "Gradient Descent(963/999): loss=0.10709347802938705\n",
      "Gradient Descent(964/999): loss=0.1070917612916893\n",
      "Gradient Descent(965/999): loss=0.10709004723809039\n",
      "Gradient Descent(966/999): loss=0.10708833586196069\n",
      "Gradient Descent(967/999): loss=0.10708662715669201\n",
      "Gradient Descent(968/999): loss=0.10708492111569748\n",
      "Gradient Descent(969/999): loss=0.10708321773241146\n",
      "Gradient Descent(970/999): loss=0.10708151700028944\n",
      "Gradient Descent(971/999): loss=0.10707981891280804\n",
      "Gradient Descent(972/999): loss=0.10707812346346483\n",
      "Gradient Descent(973/999): loss=0.10707643064577832\n",
      "Gradient Descent(974/999): loss=0.1070747404532878\n",
      "Gradient Descent(975/999): loss=0.10707305287955333\n",
      "Gradient Descent(976/999): loss=0.10707136791815568\n",
      "Gradient Descent(977/999): loss=0.10706968556269612\n",
      "Gradient Descent(978/999): loss=0.1070680058067965\n",
      "Gradient Descent(979/999): loss=0.10706632864409907\n",
      "Gradient Descent(980/999): loss=0.10706465406826637\n",
      "Gradient Descent(981/999): loss=0.10706298207298132\n",
      "Gradient Descent(982/999): loss=0.10706131265194696\n",
      "Gradient Descent(983/999): loss=0.10705964579888638\n",
      "Gradient Descent(984/999): loss=0.10705798150754288\n",
      "Gradient Descent(985/999): loss=0.10705631977167956\n",
      "Gradient Descent(986/999): loss=0.10705466058507945\n",
      "Gradient Descent(987/999): loss=0.10705300394154538\n",
      "Gradient Descent(988/999): loss=0.10705134983489992\n",
      "Gradient Descent(989/999): loss=0.10704969825898523\n",
      "Gradient Descent(990/999): loss=0.10704804920766313\n",
      "Gradient Descent(991/999): loss=0.10704640267481494\n",
      "Gradient Descent(992/999): loss=0.10704475865434127\n",
      "Gradient Descent(993/999): loss=0.10704311714016224\n",
      "Gradient Descent(994/999): loss=0.10704147812621717\n",
      "Gradient Descent(995/999): loss=0.10703984160646457\n",
      "Gradient Descent(996/999): loss=0.10703820757488215\n",
      "Gradient Descent(997/999): loss=0.10703657602546657\n",
      "Gradient Descent(998/999): loss=0.1070349469522335\n",
      "Gradient Descent(999/999): loss=0.10703332034921764\n",
      "Gradient Descent(0/999): loss=0.23409482421110328\n",
      "Gradient Descent(1/999): loss=0.12542420375261132\n",
      "Gradient Descent(2/999): loss=0.12447686373288312\n",
      "Gradient Descent(3/999): loss=0.12415244107103232\n",
      "Gradient Descent(4/999): loss=0.12384904361536689\n",
      "Gradient Descent(5/999): loss=0.12355964275515412\n",
      "Gradient Descent(6/999): loss=0.12328174142093244\n",
      "Gradient Descent(7/999): loss=0.12301365867420296\n",
      "Gradient Descent(8/999): loss=0.12275423200586219\n",
      "Gradient Descent(9/999): loss=0.12250263654429855\n",
      "Gradient Descent(10/999): loss=0.12225826817861489\n",
      "Gradient Descent(11/999): loss=0.12202066792932431\n",
      "Gradient Descent(12/999): loss=0.12178947298059584\n",
      "Gradient Descent(13/999): loss=0.12156438494728286\n",
      "Gradient Descent(14/999): loss=0.12134514928402106\n",
      "Gradient Descent(15/999): loss=0.12113154189851083\n",
      "Gradient Descent(16/999): loss=0.12092336042378782\n",
      "Gradient Descent(17/999): loss=0.12072041850439494\n",
      "Gradient Descent(18/999): loss=0.1205225420331204\n",
      "Gradient Descent(19/999): loss=0.12032956665096374\n",
      "Gradient Descent(20/999): loss=0.1201413360660015\n",
      "Gradient Descent(21/999): loss=0.11995770090389092\n",
      "Gradient Descent(22/999): loss=0.11977851790426362\n",
      "Gradient Descent(23/999): loss=0.11960364934287578\n",
      "Gradient Descent(24/999): loss=0.11943296260179007\n",
      "Gradient Descent(25/999): loss=0.11926632983727804\n",
      "Gradient Descent(26/999): loss=0.11910362771285081\n",
      "Gradient Descent(27/999): loss=0.11894473717628153\n",
      "Gradient Descent(28/999): loss=0.11878954326688967\n",
      "Gradient Descent(29/999): loss=0.11863793494414544\n",
      "Gradient Descent(30/999): loss=0.11848980493175187\n",
      "Gradient Descent(31/999): loss=0.11834504957336557\n",
      "Gradient Descent(32/999): loss=0.11820356869741548\n",
      "Gradient Descent(33/999): loss=0.11806526548931956\n",
      "Gradient Descent(34/999): loss=0.11793004636994564\n",
      "Gradient Descent(35/999): loss=0.11779782087951678\n",
      "Gradient Descent(36/999): loss=0.1176685015663928\n",
      "Gradient Descent(37/999): loss=0.11754200388031134\n",
      "Gradient Descent(38/999): loss=0.117418246069771\n",
      "Gradient Descent(39/999): loss=0.11729714908330557\n",
      "Gradient Descent(40/999): loss=0.11717863647444267\n",
      "Gradient Descent(41/999): loss=0.11706263431017037\n",
      "Gradient Descent(42/999): loss=0.11694907108275673\n",
      "Gradient Descent(43/999): loss=0.11683787762478245\n",
      "Gradient Descent(44/999): loss=0.1167289870272581\n",
      "Gradient Descent(45/999): loss=0.1166223345607053\n",
      "Gradient Descent(46/999): loss=0.11651785759909009\n",
      "Gradient Descent(47/999): loss=0.11641549554650014\n",
      "Gradient Descent(48/999): loss=0.1163151897664645\n",
      "Gradient Descent(49/999): loss=0.11621688351381775\n",
      "Gradient Descent(50/999): loss=0.11612052186901496\n",
      "Gradient Descent(51/999): loss=0.11602605167480778\n",
      "Gradient Descent(52/999): loss=0.11593342147519482\n",
      "Gradient Descent(53/999): loss=0.1158425814565635\n",
      "Gradient Descent(54/999): loss=0.11575348339094309\n",
      "Gradient Descent(55/999): loss=0.11566608058129213\n",
      "Gradient Descent(56/999): loss=0.11558032780874593\n",
      "Gradient Descent(57/999): loss=0.11549618128175265\n",
      "Gradient Descent(58/999): loss=0.11541359858702943\n",
      "Gradient Descent(59/999): loss=0.11533253864227185\n",
      "Gradient Descent(60/999): loss=0.1152529616505535\n",
      "Gradient Descent(61/999): loss=0.11517482905635361\n",
      "Gradient Descent(62/999): loss=0.11509810350315372\n",
      "Gradient Descent(63/999): loss=0.11502274879254652\n",
      "Gradient Descent(64/999): loss=0.11494872984480171\n",
      "Gradient Descent(65/999): loss=0.11487601266083568\n",
      "Gradient Descent(66/999): loss=0.11480456428553466\n",
      "Gradient Descent(67/999): loss=0.11473435277238167\n",
      "Gradient Descent(68/999): loss=0.11466534714934001\n",
      "Gradient Descent(69/999): loss=0.11459751738594781\n",
      "Gradient Descent(70/999): loss=0.11453083436157967\n",
      "Gradient Descent(71/999): loss=0.11446526983483321\n",
      "Gradient Descent(72/999): loss=0.11440079641399926\n",
      "Gradient Descent(73/999): loss=0.1143373875285771\n",
      "Gradient Descent(74/999): loss=0.11427501740179628\n",
      "Gradient Descent(75/999): loss=0.11421366102410926\n",
      "Gradient Descent(76/999): loss=0.1141532941276187\n",
      "Gradient Descent(77/999): loss=0.11409389316140678\n",
      "Gradient Descent(78/999): loss=0.11403543526773312\n",
      "Gradient Descent(79/999): loss=0.11397789825907008\n",
      "Gradient Descent(80/999): loss=0.11392126059594536\n",
      "Gradient Descent(81/999): loss=0.11386550136556252\n",
      "Gradient Descent(82/999): loss=0.11381060026117143\n",
      "Gradient Descent(83/999): loss=0.11375653756216182\n",
      "Gradient Descent(84/999): loss=0.11370329411485336\n",
      "Gradient Descent(85/999): loss=0.11365085131395734\n",
      "Gradient Descent(86/999): loss=0.11359919108468639\n",
      "Gradient Descent(87/999): loss=0.11354829586548751\n",
      "Gradient Descent(88/999): loss=0.11349814859137759\n",
      "Gradient Descent(89/999): loss=0.11344873267785853\n",
      "Gradient Descent(90/999): loss=0.11340003200539163\n",
      "Gradient Descent(91/999): loss=0.113352030904411\n",
      "Gradient Descent(92/999): loss=0.11330471414085704\n",
      "Gradient Descent(93/999): loss=0.11325806690221045\n",
      "Gradient Descent(94/999): loss=0.11321207478400978\n",
      "Gradient Descent(95/999): loss=0.11316672377683455\n",
      "Gradient Descent(96/999): loss=0.11312200025373748\n",
      "Gradient Descent(97/999): loss=0.11307789095810962\n",
      "Gradient Descent(98/999): loss=0.11303438299196325\n",
      "Gradient Descent(99/999): loss=0.11299146380461705\n",
      "Gradient Descent(100/999): loss=0.11294912118176954\n",
      "Gradient Descent(101/999): loss=0.11290734323494699\n",
      "Gradient Descent(102/999): loss=0.1128661183913119\n",
      "Gradient Descent(103/999): loss=0.1128254353838201\n",
      "Gradient Descent(104/999): loss=0.11278528324171293\n",
      "Gradient Descent(105/999): loss=0.11274565128133354\n",
      "Gradient Descent(106/999): loss=0.11270652909725523\n",
      "Gradient Descent(107/999): loss=0.11266790655371084\n",
      "Gradient Descent(108/999): loss=0.11262977377631261\n",
      "Gradient Descent(109/999): loss=0.11259212114405197\n",
      "Gradient Descent(110/999): loss=0.11255493928156968\n",
      "Gradient Descent(111/999): loss=0.11251821905168628\n",
      "Gradient Descent(112/999): loss=0.11248195154818408\n",
      "Gradient Descent(113/999): loss=0.11244612808883135\n",
      "Gradient Descent(114/999): loss=0.1124107402086406\n",
      "Gradient Descent(115/999): loss=0.11237577965335223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(116/999): loss=0.11234123837313603\n",
      "Gradient Descent(117/999): loss=0.11230710851650244\n",
      "Gradient Descent(118/999): loss=0.11227338242441638\n",
      "Gradient Descent(119/999): loss=0.1122400526246067\n",
      "Gradient Descent(120/999): loss=0.11220711182606401\n",
      "Gradient Descent(121/999): loss=0.11217455291372025\n",
      "Gradient Descent(122/999): loss=0.11214236894330429\n",
      "Gradient Descent(123/999): loss=0.11211055313636642\n",
      "Gradient Descent(124/999): loss=0.11207909887546666\n",
      "Gradient Descent(125/999): loss=0.11204799969952056\n",
      "Gradient Descent(126/999): loss=0.11201724929929728\n",
      "Gradient Descent(127/999): loss=0.11198684151306464\n",
      "Gradient Descent(128/999): loss=0.11195677032237587\n",
      "Gradient Descent(129/999): loss=0.11192702984799334\n",
      "Gradient Descent(130/999): loss=0.11189761434594407\n",
      "Gradient Descent(131/999): loss=0.1118685182037032\n",
      "Gradient Descent(132/999): loss=0.11183973593649998\n",
      "Gradient Descent(133/999): loss=0.11181126218374311\n",
      "Gradient Descent(134/999): loss=0.11178309170556022\n",
      "Gradient Descent(135/999): loss=0.11175521937944863\n",
      "Gradient Descent(136/999): loss=0.1117276401970325\n",
      "Gradient Descent(137/999): loss=0.11170034926092356\n",
      "Gradient Descent(138/999): loss=0.11167334178168137\n",
      "Gradient Descent(139/999): loss=0.11164661307486959\n",
      "Gradient Descent(140/999): loss=0.11162015855820571\n",
      "Gradient Descent(141/999): loss=0.11159397374879997\n",
      "Gradient Descent(142/999): loss=0.11156805426048123\n",
      "Gradient Descent(143/999): loss=0.11154239580120652\n",
      "Gradient Descent(144/999): loss=0.11151699417055137\n",
      "Gradient Descent(145/999): loss=0.11149184525727832\n",
      "Gradient Descent(146/999): loss=0.11146694503698072\n",
      "Gradient Descent(147/999): loss=0.11144228956979958\n",
      "Gradient Descent(148/999): loss=0.11141787499821064\n",
      "Gradient Descent(149/999): loss=0.11139369754487968\n",
      "Gradient Descent(150/999): loss=0.11136975351058327\n",
      "Gradient Descent(151/999): loss=0.11134603927219316\n",
      "Gradient Descent(152/999): loss=0.11132255128072212\n",
      "Gradient Descent(153/999): loss=0.11129928605942892\n",
      "Gradient Descent(154/999): loss=0.1112762402019807\n",
      "Gradient Descent(155/999): loss=0.11125341037067055\n",
      "Gradient Descent(156/999): loss=0.11123079329468909\n",
      "Gradient Descent(157/999): loss=0.11120838576844705\n",
      "Gradient Descent(158/999): loss=0.11118618464994866\n",
      "Gradient Descent(159/999): loss=0.11116418685921281\n",
      "Gradient Descent(160/999): loss=0.11114238937674123\n",
      "Gradient Descent(161/999): loss=0.11112078924203188\n",
      "Gradient Descent(162/999): loss=0.11109938355213592\n",
      "Gradient Descent(163/999): loss=0.11107816946025702\n",
      "Gradient Descent(164/999): loss=0.11105714417439164\n",
      "Gradient Descent(165/999): loss=0.11103630495600872\n",
      "Gradient Descent(166/999): loss=0.11101564911876757\n",
      "Gradient Descent(167/999): loss=0.11099517402727292\n",
      "Gradient Descent(168/999): loss=0.11097487709586558\n",
      "Gradient Descent(169/999): loss=0.11095475578744791\n",
      "Gradient Descent(170/999): loss=0.11093480761234255\n",
      "Gradient Descent(171/999): loss=0.1109150301271837\n",
      "Gradient Descent(172/999): loss=0.11089542093383992\n",
      "Gradient Descent(173/999): loss=0.11087597767836682\n",
      "Gradient Descent(174/999): loss=0.11085669804998965\n",
      "Gradient Descent(175/999): loss=0.11083757978011402\n",
      "Gradient Descent(176/999): loss=0.11081862064136411\n",
      "Gradient Descent(177/999): loss=0.11079981844664782\n",
      "Gradient Descent(178/999): loss=0.11078117104824738\n",
      "Gradient Descent(179/999): loss=0.11076267633693512\n",
      "Gradient Descent(180/999): loss=0.1107443322411135\n",
      "Gradient Descent(181/999): loss=0.11072613672597832\n",
      "Gradient Descent(182/999): loss=0.11070808779270484\n",
      "Gradient Descent(183/999): loss=0.11069018347765579\n",
      "Gradient Descent(184/999): loss=0.1106724218516107\n",
      "Gradient Descent(185/999): loss=0.11065480101901583\n",
      "Gradient Descent(186/999): loss=0.110637319117254\n",
      "Gradient Descent(187/999): loss=0.11061997431593405\n",
      "Gradient Descent(188/999): loss=0.11060276481619873\n",
      "Gradient Descent(189/999): loss=0.11058568885005103\n",
      "Gradient Descent(190/999): loss=0.110568744679698\n",
      "Gradient Descent(191/999): loss=0.11055193059691167\n",
      "Gradient Descent(192/999): loss=0.11053524492240661\n",
      "Gradient Descent(193/999): loss=0.11051868600523355\n",
      "Gradient Descent(194/999): loss=0.11050225222218842\n",
      "Gradient Descent(195/999): loss=0.1104859419772367\n",
      "Gradient Descent(196/999): loss=0.11046975370095229\n",
      "Gradient Descent(197/999): loss=0.11045368584997058\n",
      "Gradient Descent(198/999): loss=0.1104377369064554\n",
      "Gradient Descent(199/999): loss=0.11042190537757934\n",
      "Gradient Descent(200/999): loss=0.1104061897950168\n",
      "Gradient Descent(201/999): loss=0.11039058871445007\n",
      "Gradient Descent(202/999): loss=0.11037510071508716\n",
      "Gradient Descent(203/999): loss=0.11035972439919182\n",
      "Gradient Descent(204/999): loss=0.11034445839162499\n",
      "Gradient Descent(205/999): loss=0.11032930133939729\n",
      "Gradient Descent(206/999): loss=0.11031425191123255\n",
      "Gradient Descent(207/999): loss=0.11029930879714178\n",
      "Gradient Descent(208/999): loss=0.11028447070800737\n",
      "Gradient Descent(209/999): loss=0.11026973637517729\n",
      "Gradient Descent(210/999): loss=0.11025510455006882\n",
      "Gradient Descent(211/999): loss=0.11024057400378172\n",
      "Gradient Descent(212/999): loss=0.11022614352672072\n",
      "Gradient Descent(213/999): loss=0.11021181192822649\n",
      "Gradient Descent(214/999): loss=0.1101975780362154\n",
      "Gradient Descent(215/999): loss=0.11018344069682773\n",
      "Gradient Descent(216/999): loss=0.11016939877408381\n",
      "Gradient Descent(217/999): loss=0.11015545114954838\n",
      "Gradient Descent(218/999): loss=0.11014159672200206\n",
      "Gradient Descent(219/999): loss=0.11012783440712108\n",
      "Gradient Descent(220/999): loss=0.11011416313716356\n",
      "Gradient Descent(221/999): loss=0.11010058186066321\n",
      "Gradient Descent(222/999): loss=0.11008708954212992\n",
      "Gradient Descent(223/999): loss=0.11007368516175682\n",
      "Gradient Descent(224/999): loss=0.11006036771513421\n",
      "Gradient Descent(225/999): loss=0.11004713621296935\n",
      "Gradient Descent(226/999): loss=0.11003398968081282\n",
      "Gradient Descent(227/999): loss=0.11002092715879064\n",
      "Gradient Descent(228/999): loss=0.11000794770134237\n",
      "Gradient Descent(229/999): loss=0.10999505037696482\n",
      "Gradient Descent(230/999): loss=0.10998223426796128\n",
      "Gradient Descent(231/999): loss=0.10996949847019608\n",
      "Gradient Descent(232/999): loss=0.10995684209285475\n",
      "Gradient Descent(233/999): loss=0.1099442642582087\n",
      "Gradient Descent(234/999): loss=0.10993176410138544\n",
      "Gradient Descent(235/999): loss=0.10991934077014333\n",
      "Gradient Descent(236/999): loss=0.10990699342465117\n",
      "Gradient Descent(237/999): loss=0.10989472123727231\n",
      "Gradient Descent(238/999): loss=0.10988252339235346\n",
      "Gradient Descent(239/999): loss=0.10987039908601759\n",
      "Gradient Descent(240/999): loss=0.10985834752596127\n",
      "Gradient Descent(241/999): loss=0.10984636793125624\n",
      "Gradient Descent(242/999): loss=0.1098344595321548\n",
      "Gradient Descent(243/999): loss=0.10982262156989943\n",
      "Gradient Descent(244/999): loss=0.10981085329653603\n",
      "Gradient Descent(245/999): loss=0.10979915397473099\n",
      "Gradient Descent(246/999): loss=0.10978752287759215\n",
      "Gradient Descent(247/999): loss=0.109775959288493\n",
      "Gradient Descent(248/999): loss=0.10976446250090062\n",
      "Gradient Descent(249/999): loss=0.10975303181820682\n",
      "Gradient Descent(250/999): loss=0.10974166655356295\n",
      "Gradient Descent(251/999): loss=0.10973036602971761\n",
      "Gradient Descent(252/999): loss=0.1097191295788577\n",
      "Gradient Descent(253/999): loss=0.10970795654245248\n",
      "Gradient Descent(254/999): loss=0.10969684627110086\n",
      "Gradient Descent(255/999): loss=0.10968579812438142\n",
      "Gradient Descent(256/999): loss=0.1096748114707053\n",
      "Gradient Descent(257/999): loss=0.1096638856871723\n",
      "Gradient Descent(258/999): loss=0.10965302015942906\n",
      "Gradient Descent(259/999): loss=0.10964221428153058\n",
      "Gradient Descent(260/999): loss=0.10963146745580396\n",
      "Gradient Descent(261/999): loss=0.10962077909271502\n",
      "Gradient Descent(262/999): loss=0.10961014861073708\n",
      "Gradient Descent(263/999): loss=0.10959957543622237\n",
      "Gradient Descent(264/999): loss=0.10958905900327605\n",
      "Gradient Descent(265/999): loss=0.1095785987536322\n",
      "Gradient Descent(266/999): loss=0.10956819413653227\n",
      "Gradient Descent(267/999): loss=0.10955784460860579\n",
      "Gradient Descent(268/999): loss=0.10954754963375339\n",
      "Gradient Descent(269/999): loss=0.10953730868303173\n",
      "Gradient Descent(270/999): loss=0.1095271212345405\n",
      "Gradient Descent(271/999): loss=0.10951698677331195\n",
      "Gradient Descent(272/999): loss=0.10950690479120172\n",
      "Gradient Descent(273/999): loss=0.10949687478678229\n",
      "Gradient Descent(274/999): loss=0.10948689626523785\n",
      "Gradient Descent(275/999): loss=0.10947696873826143\n",
      "Gradient Descent(276/999): loss=0.10946709172395364\n",
      "Gradient Descent(277/999): loss=0.10945726474672327\n",
      "Gradient Descent(278/999): loss=0.10944748733718979\n",
      "Gradient Descent(279/999): loss=0.10943775903208716\n",
      "Gradient Descent(280/999): loss=0.10942807937417008\n",
      "Gradient Descent(281/999): loss=0.10941844791212106\n",
      "Gradient Descent(282/999): loss=0.10940886420045967\n",
      "Gradient Descent(283/999): loss=0.10939932779945323\n",
      "Gradient Descent(284/999): loss=0.10938983827502902\n",
      "Gradient Descent(285/999): loss=0.10938039519868796\n",
      "Gradient Descent(286/999): loss=0.10937099814742\n",
      "Gradient Descent(287/999): loss=0.10936164670362077\n",
      "Gradient Descent(288/999): loss=0.10935234045500963\n",
      "Gradient Descent(289/999): loss=0.1093430789945494\n",
      "Gradient Descent(290/999): loss=0.10933386192036719\n",
      "Gradient Descent(291/999): loss=0.10932468883567674\n",
      "Gradient Descent(292/999): loss=0.10931555934870199\n",
      "Gradient Descent(293/999): loss=0.10930647307260198\n",
      "Gradient Descent(294/999): loss=0.10929742962539712\n",
      "Gradient Descent(295/999): loss=0.1092884286298964\n",
      "Gradient Descent(296/999): loss=0.10927946971362637\n",
      "Gradient Descent(297/999): loss=0.10927055250876073\n",
      "Gradient Descent(298/999): loss=0.1092616766520514\n",
      "Gradient Descent(299/999): loss=0.10925284178476079\n",
      "Gradient Descent(300/999): loss=0.10924404755259504\n",
      "Gradient Descent(301/999): loss=0.10923529360563844\n",
      "Gradient Descent(302/999): loss=0.10922657959828899\n",
      "Gradient Descent(303/999): loss=0.10921790518919493\n",
      "Gradient Descent(304/999): loss=0.10920927004119238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(305/999): loss=0.10920067382124397\n",
      "Gradient Descent(306/999): loss=0.10919211620037858\n",
      "Gradient Descent(307/999): loss=0.10918359685363185\n",
      "Gradient Descent(308/999): loss=0.109175115459988\n",
      "Gradient Descent(309/999): loss=0.10916667170232218\n",
      "Gradient Descent(310/999): loss=0.10915826526734415\n",
      "Gradient Descent(311/999): loss=0.10914989584554267\n",
      "Gradient Descent(312/999): loss=0.10914156313113083\n",
      "Gradient Descent(313/999): loss=0.10913326682199215\n",
      "Gradient Descent(314/999): loss=0.10912500661962796\n",
      "Gradient Descent(315/999): loss=0.10911678222910491\n",
      "Gradient Descent(316/999): loss=0.10910859335900419\n",
      "Gradient Descent(317/999): loss=0.10910043972137079\n",
      "Gradient Descent(318/999): loss=0.10909232103166414\n",
      "Gradient Descent(319/999): loss=0.10908423700870915\n",
      "Gradient Descent(320/999): loss=0.10907618737464818\n",
      "Gradient Descent(321/999): loss=0.10906817185489404\n",
      "Gradient Descent(322/999): loss=0.10906019017808306\n",
      "Gradient Descent(323/999): loss=0.10905224207602976\n",
      "Gradient Descent(324/999): loss=0.10904432728368145\n",
      "Gradient Descent(325/999): loss=0.10903644553907411\n",
      "Gradient Descent(326/999): loss=0.10902859658328873\n",
      "Gradient Descent(327/999): loss=0.10902078016040827\n",
      "Gradient Descent(328/999): loss=0.10901299601747548\n",
      "Gradient Descent(329/999): loss=0.10900524390445127\n",
      "Gradient Descent(330/999): loss=0.10899752357417378\n",
      "Gradient Descent(331/999): loss=0.10898983478231802\n",
      "Gradient Descent(332/999): loss=0.10898217728735618\n",
      "Gradient Descent(333/999): loss=0.10897455085051863\n",
      "Gradient Descent(334/999): loss=0.10896695523575538\n",
      "Gradient Descent(335/999): loss=0.10895939020969837\n",
      "Gradient Descent(336/999): loss=0.10895185554162394\n",
      "Gradient Descent(337/999): loss=0.1089443510034164\n",
      "Gradient Descent(338/999): loss=0.10893687636953178\n",
      "Gradient Descent(339/999): loss=0.1089294314169622\n",
      "Gradient Descent(340/999): loss=0.108922015925201\n",
      "Gradient Descent(341/999): loss=0.10891462967620807\n",
      "Gradient Descent(342/999): loss=0.10890727245437613\n",
      "Gradient Descent(343/999): loss=0.1088999440464971\n",
      "Gradient Descent(344/999): loss=0.10889264424172923\n",
      "Gradient Descent(345/999): loss=0.10888537283156467\n",
      "Gradient Descent(346/999): loss=0.10887812960979767\n",
      "Gradient Descent(347/999): loss=0.10887091437249305\n",
      "Gradient Descent(348/999): loss=0.10886372691795519\n",
      "Gradient Descent(349/999): loss=0.10885656704669758\n",
      "Gradient Descent(350/999): loss=0.10884943456141291\n",
      "Gradient Descent(351/999): loss=0.10884232926694336\n",
      "Gradient Descent(352/999): loss=0.1088352509702515\n",
      "Gradient Descent(353/999): loss=0.10882819948039177\n",
      "Gradient Descent(354/999): loss=0.10882117460848195\n",
      "Gradient Descent(355/999): loss=0.1088141761676756\n",
      "Gradient Descent(356/999): loss=0.10880720397313455\n",
      "Gradient Descent(357/999): loss=0.10880025784200191\n",
      "Gradient Descent(358/999): loss=0.10879333759337549\n",
      "Gradient Descent(359/999): loss=0.1087864430482816\n",
      "Gradient Descent(360/999): loss=0.10877957402964934\n",
      "Gradient Descent(361/999): loss=0.1087727303622851\n",
      "Gradient Descent(362/999): loss=0.10876591187284758\n",
      "Gradient Descent(363/999): loss=0.10875911838982306\n",
      "Gradient Descent(364/999): loss=0.10875234974350129\n",
      "Gradient Descent(365/999): loss=0.1087456057659514\n",
      "Gradient Descent(366/999): loss=0.10873888629099833\n",
      "Gradient Descent(367/999): loss=0.10873219115419977\n",
      "Gradient Descent(368/999): loss=0.10872552019282311\n",
      "Gradient Descent(369/999): loss=0.10871887324582301\n",
      "Gradient Descent(370/999): loss=0.10871225015381916\n",
      "Gradient Descent(371/999): loss=0.10870565075907442\n",
      "Gradient Descent(372/999): loss=0.10869907490547327\n",
      "Gradient Descent(373/999): loss=0.10869252243850057\n",
      "Gradient Descent(374/999): loss=0.10868599320522056\n",
      "Gradient Descent(375/999): loss=0.10867948705425641\n",
      "Gradient Descent(376/999): loss=0.10867300383576989\n",
      "Gradient Descent(377/999): loss=0.10866654340144107\n",
      "Gradient Descent(378/999): loss=0.10866010560444907\n",
      "Gradient Descent(379/999): loss=0.10865369029945221\n",
      "Gradient Descent(380/999): loss=0.10864729734256914\n",
      "Gradient Descent(381/999): loss=0.10864092659135982\n",
      "Gradient Descent(382/999): loss=0.10863457790480704\n",
      "Gradient Descent(383/999): loss=0.10862825114329791\n",
      "Gradient Descent(384/999): loss=0.10862194616860611\n",
      "Gradient Descent(385/999): loss=0.10861566284387375\n",
      "Gradient Descent(386/999): loss=0.10860940103359415\n",
      "Gradient Descent(387/999): loss=0.10860316060359437\n",
      "Gradient Descent(388/999): loss=0.10859694142101821\n",
      "Gradient Descent(389/999): loss=0.10859074335430953\n",
      "Gradient Descent(390/999): loss=0.1085845662731955\n",
      "Gradient Descent(391/999): loss=0.1085784100486706\n",
      "Gradient Descent(392/999): loss=0.1085722745529802\n",
      "Gradient Descent(393/999): loss=0.10856615965960509\n",
      "Gradient Descent(394/999): loss=0.10856006524324557\n",
      "Gradient Descent(395/999): loss=0.10855399117980628\n",
      "Gradient Descent(396/999): loss=0.10854793734638091\n",
      "Gradient Descent(397/999): loss=0.10854190362123729\n",
      "Gradient Descent(398/999): loss=0.10853588988380275\n",
      "Gradient Descent(399/999): loss=0.10852989601464938\n",
      "Gradient Descent(400/999): loss=0.10852392189548002\n",
      "Gradient Descent(401/999): loss=0.10851796740911389\n",
      "Gradient Descent(402/999): loss=0.10851203243947281\n",
      "Gradient Descent(403/999): loss=0.10850611687156753\n",
      "Gradient Descent(404/999): loss=0.10850022059148406\n",
      "Gradient Descent(405/999): loss=0.10849434348637055\n",
      "Gradient Descent(406/999): loss=0.108488485444424\n",
      "Gradient Descent(407/999): loss=0.10848264635487741\n",
      "Gradient Descent(408/999): loss=0.10847682610798694\n",
      "Gradient Descent(409/999): loss=0.10847102459501944\n",
      "Gradient Descent(410/999): loss=0.1084652417082399\n",
      "Gradient Descent(411/999): loss=0.10845947734089936\n",
      "Gradient Descent(412/999): loss=0.1084537313872228\n",
      "Gradient Descent(413/999): loss=0.10844800374239721\n",
      "Gradient Descent(414/999): loss=0.10844229430255997\n",
      "Gradient Descent(415/999): loss=0.10843660296478722\n",
      "Gradient Descent(416/999): loss=0.10843092962708246\n",
      "Gradient Descent(417/999): loss=0.10842527418836537\n",
      "Gradient Descent(418/999): loss=0.1084196365484608\n",
      "Gradient Descent(419/999): loss=0.10841401660808765\n",
      "Gradient Descent(420/999): loss=0.1084084142688483\n",
      "Gradient Descent(421/999): loss=0.10840282943321783\n",
      "Gradient Descent(422/999): loss=0.10839726200453373\n",
      "Gradient Descent(423/999): loss=0.10839171188698553\n",
      "Gradient Descent(424/999): loss=0.1083861789856044\n",
      "Gradient Descent(425/999): loss=0.10838066320625353\n",
      "Gradient Descent(426/999): loss=0.1083751644556178\n",
      "Gradient Descent(427/999): loss=0.10836968264119433\n",
      "Gradient Descent(428/999): loss=0.10836421767128274\n",
      "Gradient Descent(429/999): loss=0.10835876945497559\n",
      "Gradient Descent(430/999): loss=0.10835333790214917\n",
      "Gradient Descent(431/999): loss=0.10834792292345422\n",
      "Gradient Descent(432/999): loss=0.10834252443030669\n",
      "Gradient Descent(433/999): loss=0.10833714233487898\n",
      "Gradient Descent(434/999): loss=0.10833177655009094\n",
      "Gradient Descent(435/999): loss=0.10832642698960117\n",
      "Gradient Descent(436/999): loss=0.10832109356779845\n",
      "Gradient Descent(437/999): loss=0.10831577619979314\n",
      "Gradient Descent(438/999): loss=0.10831047480140901\n",
      "Gradient Descent(439/999): loss=0.10830518928917465\n",
      "Gradient Descent(440/999): loss=0.10829991958031573\n",
      "Gradient Descent(441/999): loss=0.10829466559274653\n",
      "Gradient Descent(442/999): loss=0.1082894272450624\n",
      "Gradient Descent(443/999): loss=0.10828420445653161\n",
      "Gradient Descent(444/999): loss=0.1082789971470879\n",
      "Gradient Descent(445/999): loss=0.10827380523732277\n",
      "Gradient Descent(446/999): loss=0.10826862864847782\n",
      "Gradient Descent(447/999): loss=0.10826346730243765\n",
      "Gradient Descent(448/999): loss=0.10825832112172225\n",
      "Gradient Descent(449/999): loss=0.10825319002948003\n",
      "Gradient Descent(450/999): loss=0.10824807394948056\n",
      "Gradient Descent(451/999): loss=0.10824297280610773\n",
      "Gradient Descent(452/999): loss=0.1082378865243525\n",
      "Gradient Descent(453/999): loss=0.10823281502980651\n",
      "Gradient Descent(454/999): loss=0.10822775824865502\n",
      "Gradient Descent(455/999): loss=0.10822271610767038\n",
      "Gradient Descent(456/999): loss=0.10821768853420548\n",
      "Gradient Descent(457/999): loss=0.10821267545618726\n",
      "Gradient Descent(458/999): loss=0.10820767680211037\n",
      "Gradient Descent(459/999): loss=0.10820269250103087\n",
      "Gradient Descent(460/999): loss=0.10819772248255992\n",
      "Gradient Descent(461/999): loss=0.10819276667685786\n",
      "Gradient Descent(462/999): loss=0.10818782501462788\n",
      "Gradient Descent(463/999): loss=0.10818289742711035\n",
      "Gradient Descent(464/999): loss=0.10817798384607677\n",
      "Gradient Descent(465/999): loss=0.1081730842038239\n",
      "Gradient Descent(466/999): loss=0.10816819843316818\n",
      "Gradient Descent(467/999): loss=0.10816332646743998\n",
      "Gradient Descent(468/999): loss=0.10815846824047816\n",
      "Gradient Descent(469/999): loss=0.10815362368662426\n",
      "Gradient Descent(470/999): loss=0.10814879274071743\n",
      "Gradient Descent(471/999): loss=0.1081439753380888\n",
      "Gradient Descent(472/999): loss=0.1081391714145563\n",
      "Gradient Descent(473/999): loss=0.10813438090641939\n",
      "Gradient Descent(474/999): loss=0.10812960375045401\n",
      "Gradient Descent(475/999): loss=0.1081248398839073\n",
      "Gradient Descent(476/999): loss=0.10812008924449283\n",
      "Gradient Descent(477/999): loss=0.10811535177038532\n",
      "Gradient Descent(478/999): loss=0.10811062740021617\n",
      "Gradient Descent(479/999): loss=0.1081059160730682\n",
      "Gradient Descent(480/999): loss=0.10810121772847119\n",
      "Gradient Descent(481/999): loss=0.108096532306397\n",
      "Gradient Descent(482/999): loss=0.10809185974725509\n",
      "Gradient Descent(483/999): loss=0.10808719999188773\n",
      "Gradient Descent(484/999): loss=0.10808255298156574\n",
      "Gradient Descent(485/999): loss=0.10807791865798369\n",
      "Gradient Descent(486/999): loss=0.10807329696325592\n",
      "Gradient Descent(487/999): loss=0.10806868783991173\n",
      "Gradient Descent(488/999): loss=0.10806409123089142\n",
      "Gradient Descent(489/999): loss=0.10805950707954194\n",
      "Gradient Descent(490/999): loss=0.10805493532961266\n",
      "Gradient Descent(491/999): loss=0.10805037592525134\n",
      "Gradient Descent(492/999): loss=0.10804582881099994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(493/999): loss=0.10804129393179074\n",
      "Gradient Descent(494/999): loss=0.10803677123294206\n",
      "Gradient Descent(495/999): loss=0.10803226066015485\n",
      "Gradient Descent(496/999): loss=0.10802776215950827\n",
      "Gradient Descent(497/999): loss=0.10802327567745625\n",
      "Gradient Descent(498/999): loss=0.10801880116082346\n",
      "Gradient Descent(499/999): loss=0.10801433855680181\n",
      "Gradient Descent(500/999): loss=0.10800988781294656\n",
      "Gradient Descent(501/999): loss=0.10800544887717277\n",
      "Gradient Descent(502/999): loss=0.1080010216977518\n",
      "Gradient Descent(503/999): loss=0.1079966062233076\n",
      "Gradient Descent(504/999): loss=0.1079922024028133\n",
      "Gradient Descent(505/999): loss=0.10798781018558777\n",
      "Gradient Descent(506/999): loss=0.10798342952129215\n",
      "Gradient Descent(507/999): loss=0.1079790603599265\n",
      "Gradient Descent(508/999): loss=0.1079747026518265\n",
      "Gradient Descent(509/999): loss=0.10797035634766011\n",
      "Gradient Descent(510/999): loss=0.10796602139842441\n",
      "Gradient Descent(511/999): loss=0.10796169775544225\n",
      "Gradient Descent(512/999): loss=0.10795738537035925\n",
      "Gradient Descent(513/999): loss=0.1079530841951405\n",
      "Gradient Descent(514/999): loss=0.10794879418206774\n",
      "Gradient Descent(515/999): loss=0.10794451528373587\n",
      "Gradient Descent(516/999): loss=0.10794024745305045\n",
      "Gradient Descent(517/999): loss=0.10793599064322432\n",
      "Gradient Descent(518/999): loss=0.10793174480777484\n",
      "Gradient Descent(519/999): loss=0.10792750990052102\n",
      "Gradient Descent(520/999): loss=0.10792328587558052\n",
      "Gradient Descent(521/999): loss=0.10791907268736689\n",
      "Gradient Descent(522/999): loss=0.10791487029058688\n",
      "Gradient Descent(523/999): loss=0.10791067864023739\n",
      "Gradient Descent(524/999): loss=0.10790649769160304\n",
      "Gradient Descent(525/999): loss=0.1079023274002533\n",
      "Gradient Descent(526/999): loss=0.10789816772203989\n",
      "Gradient Descent(527/999): loss=0.10789401861309411\n",
      "Gradient Descent(528/999): loss=0.10788988002982425\n",
      "Gradient Descent(529/999): loss=0.10788575192891302\n",
      "Gradient Descent(530/999): loss=0.10788163426731509\n",
      "Gradient Descent(531/999): loss=0.10787752700225436\n",
      "Gradient Descent(532/999): loss=0.10787343009122179\n",
      "Gradient Descent(533/999): loss=0.10786934349197268\n",
      "Gradient Descent(534/999): loss=0.10786526716252438\n",
      "Gradient Descent(535/999): loss=0.10786120106115395\n",
      "Gradient Descent(536/999): loss=0.10785714514639563\n",
      "Gradient Descent(537/999): loss=0.10785309937703862\n",
      "Gradient Descent(538/999): loss=0.10784906371212483\n",
      "Gradient Descent(539/999): loss=0.10784503811094644\n",
      "Gradient Descent(540/999): loss=0.10784102253304373\n",
      "Gradient Descent(541/999): loss=0.107837016938203\n",
      "Gradient Descent(542/999): loss=0.10783302128645399\n",
      "Gradient Descent(543/999): loss=0.10782903553806812\n",
      "Gradient Descent(544/999): loss=0.10782505965355613\n",
      "Gradient Descent(545/999): loss=0.10782109359366591\n",
      "Gradient Descent(546/999): loss=0.10781713731938063\n",
      "Gradient Descent(547/999): loss=0.10781319079191636\n",
      "Gradient Descent(548/999): loss=0.10780925397272023\n",
      "Gradient Descent(549/999): loss=0.10780532682346841\n",
      "Gradient Descent(550/999): loss=0.107801409306064\n",
      "Gradient Descent(551/999): loss=0.10779750138263505\n",
      "Gradient Descent(552/999): loss=0.10779360301553276\n",
      "Gradient Descent(553/999): loss=0.10778971416732933\n",
      "Gradient Descent(554/999): loss=0.10778583480081623\n",
      "Gradient Descent(555/999): loss=0.10778196487900216\n",
      "Gradient Descent(556/999): loss=0.10777810436511132\n",
      "Gradient Descent(557/999): loss=0.10777425322258148\n",
      "Gradient Descent(558/999): loss=0.10777041141506216\n",
      "Gradient Descent(559/999): loss=0.10776657890641286\n",
      "Gradient Descent(560/999): loss=0.10776275566070122\n",
      "Gradient Descent(561/999): loss=0.10775894164220126\n",
      "Gradient Descent(562/999): loss=0.10775513681539177\n",
      "Gradient Descent(563/999): loss=0.10775134114495435\n",
      "Gradient Descent(564/999): loss=0.10774755459577194\n",
      "Gradient Descent(565/999): loss=0.10774377713292696\n",
      "Gradient Descent(566/999): loss=0.10774000872169971\n",
      "Gradient Descent(567/999): loss=0.10773624932756676\n",
      "Gradient Descent(568/999): loss=0.10773249891619924\n",
      "Gradient Descent(569/999): loss=0.10772875745346128\n",
      "Gradient Descent(570/999): loss=0.10772502490540839\n",
      "Gradient Descent(571/999): loss=0.10772130123828595\n",
      "Gradient Descent(572/999): loss=0.10771758641852752\n",
      "Gradient Descent(573/999): loss=0.10771388041275341\n",
      "Gradient Descent(574/999): loss=0.10771018318776919\n",
      "Gradient Descent(575/999): loss=0.10770649471056394\n",
      "Gradient Descent(576/999): loss=0.10770281494830913\n",
      "Gradient Descent(577/999): loss=0.10769914386835683\n",
      "Gradient Descent(578/999): loss=0.10769548143823836\n",
      "Gradient Descent(579/999): loss=0.10769182762566296\n",
      "Gradient Descent(580/999): loss=0.1076881823985161\n",
      "Gradient Descent(581/999): loss=0.10768454572485836\n",
      "Gradient Descent(582/999): loss=0.10768091757292388\n",
      "Gradient Descent(583/999): loss=0.10767729791111894\n",
      "Gradient Descent(584/999): loss=0.10767368670802066\n",
      "Gradient Descent(585/999): loss=0.10767008393237575\n",
      "Gradient Descent(586/999): loss=0.10766648955309889\n",
      "Gradient Descent(587/999): loss=0.10766290353927174\n",
      "Gradient Descent(588/999): loss=0.10765932586014135\n",
      "Gradient Descent(589/999): loss=0.10765575648511909\n",
      "Gradient Descent(590/999): loss=0.10765219538377918\n",
      "Gradient Descent(591/999): loss=0.10764864252585761\n",
      "Gradient Descent(592/999): loss=0.1076450978812507\n",
      "Gradient Descent(593/999): loss=0.10764156142001396\n",
      "Gradient Descent(594/999): loss=0.10763803311236089\n",
      "Gradient Descent(595/999): loss=0.1076345129286617\n",
      "Gradient Descent(596/999): loss=0.1076310008394422\n",
      "Gradient Descent(597/999): loss=0.10762749681538238\n",
      "Gradient Descent(598/999): loss=0.10762400082731555\n",
      "Gradient Descent(599/999): loss=0.10762051284622697\n",
      "Gradient Descent(600/999): loss=0.10761703284325279\n",
      "Gradient Descent(601/999): loss=0.10761356078967882\n",
      "Gradient Descent(602/999): loss=0.10761009665693948\n",
      "Gradient Descent(603/999): loss=0.10760664041661666\n",
      "Gradient Descent(604/999): loss=0.10760319204043864\n",
      "Gradient Descent(605/999): loss=0.10759975150027887\n",
      "Gradient Descent(606/999): loss=0.10759631876815515\n",
      "Gradient Descent(607/999): loss=0.10759289381622827\n",
      "Gradient Descent(608/999): loss=0.10758947661680107\n",
      "Gradient Descent(609/999): loss=0.10758606714231748\n",
      "Gradient Descent(610/999): loss=0.1075826653653614\n",
      "Gradient Descent(611/999): loss=0.10757927125865555\n",
      "Gradient Descent(612/999): loss=0.10757588479506071\n",
      "Gradient Descent(613/999): loss=0.10757250594757448\n",
      "Gradient Descent(614/999): loss=0.10756913468933044\n",
      "Gradient Descent(615/999): loss=0.10756577099359707\n",
      "Gradient Descent(616/999): loss=0.10756241483377685\n",
      "Gradient Descent(617/999): loss=0.10755906618340513\n",
      "Gradient Descent(618/999): loss=0.10755572501614942\n",
      "Gradient Descent(619/999): loss=0.10755239130580824\n",
      "Gradient Descent(620/999): loss=0.10754906502631027\n",
      "Gradient Descent(621/999): loss=0.10754574615171339\n",
      "Gradient Descent(622/999): loss=0.10754243465620383\n",
      "Gradient Descent(623/999): loss=0.10753913051409511\n",
      "Gradient Descent(624/999): loss=0.10753583369982726\n",
      "Gradient Descent(625/999): loss=0.10753254418796596\n",
      "Gradient Descent(626/999): loss=0.10752926195320146\n",
      "Gradient Descent(627/999): loss=0.10752598697034801\n",
      "Gradient Descent(628/999): loss=0.10752271921434263\n",
      "Gradient Descent(629/999): loss=0.10751945866024458\n",
      "Gradient Descent(630/999): loss=0.1075162052832343\n",
      "Gradient Descent(631/999): loss=0.10751295905861269\n",
      "Gradient Descent(632/999): loss=0.10750971996180012\n",
      "Gradient Descent(633/999): loss=0.10750648796833583\n",
      "Gradient Descent(634/999): loss=0.10750326305387689\n",
      "Gradient Descent(635/999): loss=0.10750004519419752\n",
      "Gradient Descent(636/999): loss=0.10749683436518828\n",
      "Gradient Descent(637/999): loss=0.10749363054285525\n",
      "Gradient Descent(638/999): loss=0.10749043370331919\n",
      "Gradient Descent(639/999): loss=0.10748724382281483\n",
      "Gradient Descent(640/999): loss=0.10748406087769015\n",
      "Gradient Descent(641/999): loss=0.10748088484440543\n",
      "Gradient Descent(642/999): loss=0.10747771569953268\n",
      "Gradient Descent(643/999): loss=0.10747455341975483\n",
      "Gradient Descent(644/999): loss=0.10747139798186493\n",
      "Gradient Descent(645/999): loss=0.10746824936276543\n",
      "Gradient Descent(646/999): loss=0.1074651075394676\n",
      "Gradient Descent(647/999): loss=0.10746197248909055\n",
      "Gradient Descent(648/999): loss=0.10745884418886066\n",
      "Gradient Descent(649/999): loss=0.10745572261611096\n",
      "Gradient Descent(650/999): loss=0.1074526077482802\n",
      "Gradient Descent(651/999): loss=0.10744949956291237\n",
      "Gradient Descent(652/999): loss=0.10744639803765581\n",
      "Gradient Descent(653/999): loss=0.10744330315026279\n",
      "Gradient Descent(654/999): loss=0.10744021487858853\n",
      "Gradient Descent(655/999): loss=0.10743713320059074\n",
      "Gradient Descent(656/999): loss=0.10743405809432888\n",
      "Gradient Descent(657/999): loss=0.10743098953796348\n",
      "Gradient Descent(658/999): loss=0.10742792750975555\n",
      "Gradient Descent(659/999): loss=0.1074248719880659\n",
      "Gradient Descent(660/999): loss=0.10742182295135447\n",
      "Gradient Descent(661/999): loss=0.10741878037817972\n",
      "Gradient Descent(662/999): loss=0.10741574424719802\n",
      "Gradient Descent(663/999): loss=0.10741271453716297\n",
      "Gradient Descent(664/999): loss=0.10740969122692487\n",
      "Gradient Descent(665/999): loss=0.10740667429543\n",
      "Gradient Descent(666/999): loss=0.10740366372172014\n",
      "Gradient Descent(667/999): loss=0.10740065948493174\n",
      "Gradient Descent(668/999): loss=0.10739766156429566\n",
      "Gradient Descent(669/999): loss=0.10739466993913628\n",
      "Gradient Descent(670/999): loss=0.10739168458887106\n",
      "Gradient Descent(671/999): loss=0.10738870549300987\n",
      "Gradient Descent(672/999): loss=0.10738573263115457\n",
      "Gradient Descent(673/999): loss=0.10738276598299824\n",
      "Gradient Descent(674/999): loss=0.10737980552832475\n",
      "Gradient Descent(675/999): loss=0.10737685124700819\n",
      "Gradient Descent(676/999): loss=0.10737390311901218\n",
      "Gradient Descent(677/999): loss=0.10737096112438951\n",
      "Gradient Descent(678/999): loss=0.10736802524328155\n",
      "Gradient Descent(679/999): loss=0.10736509545591742\n",
      "Gradient Descent(680/999): loss=0.10736217174261399\n",
      "Gradient Descent(681/999): loss=0.10735925408377484\n",
      "Gradient Descent(682/999): loss=0.10735634245989004\n",
      "Gradient Descent(683/999): loss=0.10735343685153544\n",
      "Gradient Descent(684/999): loss=0.10735053723937231\n",
      "Gradient Descent(685/999): loss=0.10734764360414674\n",
      "Gradient Descent(686/999): loss=0.10734475592668911\n",
      "Gradient Descent(687/999): loss=0.10734187418791356\n",
      "Gradient Descent(688/999): loss=0.10733899836881768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(689/999): loss=0.10733612845048172\n",
      "Gradient Descent(690/999): loss=0.10733326441406835\n",
      "Gradient Descent(691/999): loss=0.107330406240822\n",
      "Gradient Descent(692/999): loss=0.10732755391206843\n",
      "Gradient Descent(693/999): loss=0.10732470740921436\n",
      "Gradient Descent(694/999): loss=0.10732186671374676\n",
      "Gradient Descent(695/999): loss=0.10731903180723258\n",
      "Gradient Descent(696/999): loss=0.10731620267131817\n",
      "Gradient Descent(697/999): loss=0.10731337928772887\n",
      "Gradient Descent(698/999): loss=0.10731056163826849\n",
      "Gradient Descent(699/999): loss=0.10730774970481896\n",
      "Gradient Descent(700/999): loss=0.1073049434693397\n",
      "Gradient Descent(701/999): loss=0.10730214291386739\n",
      "Gradient Descent(702/999): loss=0.10729934802051525\n",
      "Gradient Descent(703/999): loss=0.10729655877147287\n",
      "Gradient Descent(704/999): loss=0.1072937751490056\n",
      "Gradient Descent(705/999): loss=0.10729099713545419\n",
      "Gradient Descent(706/999): loss=0.10728822471323428\n",
      "Gradient Descent(707/999): loss=0.10728545786483604\n",
      "Gradient Descent(708/999): loss=0.1072826965728237\n",
      "Gradient Descent(709/999): loss=0.10727994081983519\n",
      "Gradient Descent(710/999): loss=0.10727719058858168\n",
      "Gradient Descent(711/999): loss=0.10727444586184708\n",
      "Gradient Descent(712/999): loss=0.10727170662248776\n",
      "Gradient Descent(713/999): loss=0.10726897285343208\n",
      "Gradient Descent(714/999): loss=0.10726624453768005\n",
      "Gradient Descent(715/999): loss=0.10726352165830276\n",
      "Gradient Descent(716/999): loss=0.10726080419844214\n",
      "Gradient Descent(717/999): loss=0.1072580921413105\n",
      "Gradient Descent(718/999): loss=0.10725538547019016\n",
      "Gradient Descent(719/999): loss=0.107252684168433\n",
      "Gradient Descent(720/999): loss=0.10724998821946018\n",
      "Gradient Descent(721/999): loss=0.1072472976067616\n",
      "Gradient Descent(722/999): loss=0.10724461231389569\n",
      "Gradient Descent(723/999): loss=0.1072419323244889\n",
      "Gradient Descent(724/999): loss=0.10723925762223538\n",
      "Gradient Descent(725/999): loss=0.10723658819089665\n",
      "Gradient Descent(726/999): loss=0.10723392401430101\n",
      "Gradient Descent(727/999): loss=0.10723126507634355\n",
      "Gradient Descent(728/999): loss=0.10722861136098547\n",
      "Gradient Descent(729/999): loss=0.10722596285225379\n",
      "Gradient Descent(730/999): loss=0.10722331953424112\n",
      "Gradient Descent(731/999): loss=0.10722068139110506\n",
      "Gradient Descent(732/999): loss=0.10721804840706813\n",
      "Gradient Descent(733/999): loss=0.1072154205664172\n",
      "Gradient Descent(734/999): loss=0.10721279785350322\n",
      "Gradient Descent(735/999): loss=0.1072101802527409\n",
      "Gradient Descent(736/999): loss=0.10720756774860835\n",
      "Gradient Descent(737/999): loss=0.10720496032564665\n",
      "Gradient Descent(738/999): loss=0.10720235796845973\n",
      "Gradient Descent(739/999): loss=0.1071997606617137\n",
      "Gradient Descent(740/999): loss=0.10719716839013692\n",
      "Gradient Descent(741/999): loss=0.10719458113851926\n",
      "Gradient Descent(742/999): loss=0.10719199889171209\n",
      "Gradient Descent(743/999): loss=0.10718942163462784\n",
      "Gradient Descent(744/999): loss=0.10718684935223952\n",
      "Gradient Descent(745/999): loss=0.10718428202958072\n",
      "Gradient Descent(746/999): loss=0.10718171965174506\n",
      "Gradient Descent(747/999): loss=0.10717916220388582\n",
      "Gradient Descent(748/999): loss=0.10717660967121592\n",
      "Gradient Descent(749/999): loss=0.10717406203900733\n",
      "Gradient Descent(750/999): loss=0.10717151929259075\n",
      "Gradient Descent(751/999): loss=0.10716898141735554\n",
      "Gradient Descent(752/999): loss=0.10716644839874924\n",
      "Gradient Descent(753/999): loss=0.10716392022227728\n",
      "Gradient Descent(754/999): loss=0.1071613968735027\n",
      "Gradient Descent(755/999): loss=0.10715887833804592\n",
      "Gradient Descent(756/999): loss=0.10715636460158422\n",
      "Gradient Descent(757/999): loss=0.10715385564985173\n",
      "Gradient Descent(758/999): loss=0.107151351468639\n",
      "Gradient Descent(759/999): loss=0.10714885204379263\n",
      "Gradient Descent(760/999): loss=0.10714635736121511\n",
      "Gradient Descent(761/999): loss=0.10714386740686449\n",
      "Gradient Descent(762/999): loss=0.10714138216675412\n",
      "Gradient Descent(763/999): loss=0.10713890162695228\n",
      "Gradient Descent(764/999): loss=0.10713642577358196\n",
      "Gradient Descent(765/999): loss=0.10713395459282059\n",
      "Gradient Descent(766/999): loss=0.10713148807089977\n",
      "Gradient Descent(767/999): loss=0.10712902619410501\n",
      "Gradient Descent(768/999): loss=0.10712656894877529\n",
      "Gradient Descent(769/999): loss=0.10712411632130303\n",
      "Gradient Descent(770/999): loss=0.10712166829813366\n",
      "Gradient Descent(771/999): loss=0.10711922486576546\n",
      "Gradient Descent(772/999): loss=0.10711678601074917\n",
      "Gradient Descent(773/999): loss=0.10711435171968779\n",
      "Gradient Descent(774/999): loss=0.10711192197923637\n",
      "Gradient Descent(775/999): loss=0.10710949677610164\n",
      "Gradient Descent(776/999): loss=0.10710707609704181\n",
      "Gradient Descent(777/999): loss=0.1071046599288664\n",
      "Gradient Descent(778/999): loss=0.10710224825843578\n",
      "Gradient Descent(779/999): loss=0.10709984107266107\n",
      "Gradient Descent(780/999): loss=0.10709743835850385\n",
      "Gradient Descent(781/999): loss=0.10709504010297594\n",
      "Gradient Descent(782/999): loss=0.10709264629313907\n",
      "Gradient Descent(783/999): loss=0.10709025691610467\n",
      "Gradient Descent(784/999): loss=0.1070878719590337\n",
      "Gradient Descent(785/999): loss=0.1070854914091363\n",
      "Gradient Descent(786/999): loss=0.1070831152536716\n",
      "Gradient Descent(787/999): loss=0.10708074347994744\n",
      "Gradient Descent(788/999): loss=0.10707837607532018\n",
      "Gradient Descent(789/999): loss=0.10707601302719447\n",
      "Gradient Descent(790/999): loss=0.10707365432302292\n",
      "Gradient Descent(791/999): loss=0.107071299950306\n",
      "Gradient Descent(792/999): loss=0.10706894989659167\n",
      "Gradient Descent(793/999): loss=0.10706660414947526\n",
      "Gradient Descent(794/999): loss=0.10706426269659919\n",
      "Gradient Descent(795/999): loss=0.10706192552565275\n",
      "Gradient Descent(796/999): loss=0.10705959262437181\n",
      "Gradient Descent(797/999): loss=0.10705726398053876\n",
      "Gradient Descent(798/999): loss=0.1070549395819821\n",
      "Gradient Descent(799/999): loss=0.10705261941657632\n",
      "Gradient Descent(800/999): loss=0.10705030347224162\n",
      "Gradient Descent(801/999): loss=0.10704799173694379\n",
      "Gradient Descent(802/999): loss=0.10704568419869395\n",
      "Gradient Descent(803/999): loss=0.10704338084554815\n",
      "Gradient Descent(804/999): loss=0.1070410816656075\n",
      "Gradient Descent(805/999): loss=0.10703878664701764\n",
      "Gradient Descent(806/999): loss=0.1070364957779687\n",
      "Gradient Descent(807/999): loss=0.10703420904669514\n",
      "Gradient Descent(808/999): loss=0.10703192644147522\n",
      "Gradient Descent(809/999): loss=0.1070296479506312\n",
      "Gradient Descent(810/999): loss=0.10702737356252892\n",
      "Gradient Descent(811/999): loss=0.10702510326557757\n",
      "Gradient Descent(812/999): loss=0.10702283704822954\n",
      "Gradient Descent(813/999): loss=0.10702057489898027\n",
      "Gradient Descent(814/999): loss=0.10701831680636789\n",
      "Gradient Descent(815/999): loss=0.10701606275897318\n",
      "Gradient Descent(816/999): loss=0.10701381274541936\n",
      "Gradient Descent(817/999): loss=0.10701156675437169\n",
      "Gradient Descent(818/999): loss=0.10700932477453755\n",
      "Gradient Descent(819/999): loss=0.10700708679466607\n",
      "Gradient Descent(820/999): loss=0.10700485280354795\n",
      "Gradient Descent(821/999): loss=0.10700262279001532\n",
      "Gradient Descent(822/999): loss=0.10700039674294154\n",
      "Gradient Descent(823/999): loss=0.10699817465124094\n",
      "Gradient Descent(824/999): loss=0.10699595650386869\n",
      "Gradient Descent(825/999): loss=0.10699374228982066\n",
      "Gradient Descent(826/999): loss=0.10699153199813302\n",
      "Gradient Descent(827/999): loss=0.1069893256178824\n",
      "Gradient Descent(828/999): loss=0.10698712313818534\n",
      "Gradient Descent(829/999): loss=0.1069849245481984\n",
      "Gradient Descent(830/999): loss=0.1069827298371177\n",
      "Gradient Descent(831/999): loss=0.10698053899417904\n",
      "Gradient Descent(832/999): loss=0.10697835200865746\n",
      "Gradient Descent(833/999): loss=0.10697616886986717\n",
      "Gradient Descent(834/999): loss=0.10697398956716145\n",
      "Gradient Descent(835/999): loss=0.10697181408993224\n",
      "Gradient Descent(836/999): loss=0.1069696424276102\n",
      "Gradient Descent(837/999): loss=0.10696747456966443\n",
      "Gradient Descent(838/999): loss=0.1069653105056023\n",
      "Gradient Descent(839/999): loss=0.10696315022496927\n",
      "Gradient Descent(840/999): loss=0.10696099371734873\n",
      "Gradient Descent(841/999): loss=0.10695884097236184\n",
      "Gradient Descent(842/999): loss=0.10695669197966727\n",
      "Gradient Descent(843/999): loss=0.10695454672896124\n",
      "Gradient Descent(844/999): loss=0.10695240520997706\n",
      "Gradient Descent(845/999): loss=0.10695026741248526\n",
      "Gradient Descent(846/999): loss=0.10694813332629312\n",
      "Gradient Descent(847/999): loss=0.10694600294124483\n",
      "Gradient Descent(848/999): loss=0.10694387624722099\n",
      "Gradient Descent(849/999): loss=0.10694175323413874\n",
      "Gradient Descent(850/999): loss=0.1069396338919514\n",
      "Gradient Descent(851/999): loss=0.10693751821064838\n",
      "Gradient Descent(852/999): loss=0.10693540618025507\n",
      "Gradient Descent(853/999): loss=0.10693329779083252\n",
      "Gradient Descent(854/999): loss=0.10693119303247751\n",
      "Gradient Descent(855/999): loss=0.1069290918953221\n",
      "Gradient Descent(856/999): loss=0.1069269943695338\n",
      "Gradient Descent(857/999): loss=0.10692490044531512\n",
      "Gradient Descent(858/999): loss=0.10692281011290367\n",
      "Gradient Descent(859/999): loss=0.10692072336257172\n",
      "Gradient Descent(860/999): loss=0.10691864018462638\n",
      "Gradient Descent(861/999): loss=0.10691656056940908\n",
      "Gradient Descent(862/999): loss=0.10691448450729577\n",
      "Gradient Descent(863/999): loss=0.10691241198869647\n",
      "Gradient Descent(864/999): loss=0.10691034300405536\n",
      "Gradient Descent(865/999): loss=0.10690827754385047\n",
      "Gradient Descent(866/999): loss=0.10690621559859359\n",
      "Gradient Descent(867/999): loss=0.10690415715883013\n",
      "Gradient Descent(868/999): loss=0.1069021022151389\n",
      "Gradient Descent(869/999): loss=0.10690005075813212\n",
      "Gradient Descent(870/999): loss=0.10689800277845508\n",
      "Gradient Descent(871/999): loss=0.10689595826678615\n",
      "Gradient Descent(872/999): loss=0.10689391721383651\n",
      "Gradient Descent(873/999): loss=0.10689187961035014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(874/999): loss=0.10688984544710357\n",
      "Gradient Descent(875/999): loss=0.10688781471490579\n",
      "Gradient Descent(876/999): loss=0.10688578740459803\n",
      "Gradient Descent(877/999): loss=0.10688376350705386\n",
      "Gradient Descent(878/999): loss=0.10688174301317865\n",
      "Gradient Descent(879/999): loss=0.10687972591390978\n",
      "Gradient Descent(880/999): loss=0.10687771220021638\n",
      "Gradient Descent(881/999): loss=0.10687570186309918\n",
      "Gradient Descent(882/999): loss=0.1068736948935903\n",
      "Gradient Descent(883/999): loss=0.10687169128275334\n",
      "Gradient Descent(884/999): loss=0.10686969102168299\n",
      "Gradient Descent(885/999): loss=0.10686769410150501\n",
      "Gradient Descent(886/999): loss=0.1068657005133762\n",
      "Gradient Descent(887/999): loss=0.10686371024848401\n",
      "Gradient Descent(888/999): loss=0.10686172329804668\n",
      "Gradient Descent(889/999): loss=0.10685973965331291\n",
      "Gradient Descent(890/999): loss=0.10685775930556185\n",
      "Gradient Descent(891/999): loss=0.10685578224610291\n",
      "Gradient Descent(892/999): loss=0.1068538084662756\n",
      "Gradient Descent(893/999): loss=0.10685183795744954\n",
      "Gradient Descent(894/999): loss=0.10684987071102417\n",
      "Gradient Descent(895/999): loss=0.10684790671842866\n",
      "Gradient Descent(896/999): loss=0.10684594597112193\n",
      "Gradient Descent(897/999): loss=0.10684398846059227\n",
      "Gradient Descent(898/999): loss=0.10684203417835747\n",
      "Gradient Descent(899/999): loss=0.10684008311596452\n",
      "Gradient Descent(900/999): loss=0.10683813526498949\n",
      "Gradient Descent(901/999): loss=0.10683619061703757\n",
      "Gradient Descent(902/999): loss=0.10683424916374275\n",
      "Gradient Descent(903/999): loss=0.10683231089676787\n",
      "Gradient Descent(904/999): loss=0.10683037580780434\n",
      "Gradient Descent(905/999): loss=0.10682844388857206\n",
      "Gradient Descent(906/999): loss=0.10682651513081944\n",
      "Gradient Descent(907/999): loss=0.10682458952632308\n",
      "Gradient Descent(908/999): loss=0.10682266706688781\n",
      "Gradient Descent(909/999): loss=0.10682074774434648\n",
      "Gradient Descent(910/999): loss=0.10681883155055978\n",
      "Gradient Descent(911/999): loss=0.10681691847741638\n",
      "Gradient Descent(912/999): loss=0.1068150085168325\n",
      "Gradient Descent(913/999): loss=0.10681310166075202\n",
      "Gradient Descent(914/999): loss=0.1068111979011462\n",
      "Gradient Descent(915/999): loss=0.1068092972300137\n",
      "Gradient Descent(916/999): loss=0.10680739963938045\n",
      "Gradient Descent(917/999): loss=0.10680550512129941\n",
      "Gradient Descent(918/999): loss=0.10680361366785066\n",
      "Gradient Descent(919/999): loss=0.10680172527114103\n",
      "Gradient Descent(920/999): loss=0.10679983992330419\n",
      "Gradient Descent(921/999): loss=0.10679795761650057\n",
      "Gradient Descent(922/999): loss=0.10679607834291704\n",
      "Gradient Descent(923/999): loss=0.10679420209476699\n",
      "Gradient Descent(924/999): loss=0.10679232886429006\n",
      "Gradient Descent(925/999): loss=0.10679045864375225\n",
      "Gradient Descent(926/999): loss=0.10678859142544557\n",
      "Gradient Descent(927/999): loss=0.10678672720168814\n",
      "Gradient Descent(928/999): loss=0.10678486596482384\n",
      "Gradient Descent(929/999): loss=0.10678300770722249\n",
      "Gradient Descent(930/999): loss=0.10678115242127961\n",
      "Gradient Descent(931/999): loss=0.10677930009941618\n",
      "Gradient Descent(932/999): loss=0.1067774507340788\n",
      "Gradient Descent(933/999): loss=0.10677560431773932\n",
      "Gradient Descent(934/999): loss=0.10677376084289492\n",
      "Gradient Descent(935/999): loss=0.10677192030206799\n",
      "Gradient Descent(936/999): loss=0.106770082687806\n",
      "Gradient Descent(937/999): loss=0.10676824799268124\n",
      "Gradient Descent(938/999): loss=0.10676641620929096\n",
      "Gradient Descent(939/999): loss=0.10676458733025727\n",
      "Gradient Descent(940/999): loss=0.10676276134822672\n",
      "Gradient Descent(941/999): loss=0.1067609382558706\n",
      "Gradient Descent(942/999): loss=0.10675911804588459\n",
      "Gradient Descent(943/999): loss=0.10675730071098866\n",
      "Gradient Descent(944/999): loss=0.10675548624392722\n",
      "Gradient Descent(945/999): loss=0.10675367463746863\n",
      "Gradient Descent(946/999): loss=0.10675186588440544\n",
      "Gradient Descent(947/999): loss=0.10675005997755416\n",
      "Gradient Descent(948/999): loss=0.10674825690975512\n",
      "Gradient Descent(949/999): loss=0.10674645667387242\n",
      "Gradient Descent(950/999): loss=0.10674465926279393\n",
      "Gradient Descent(951/999): loss=0.10674286466943098\n",
      "Gradient Descent(952/999): loss=0.10674107288671841\n",
      "Gradient Descent(953/999): loss=0.10673928390761452\n",
      "Gradient Descent(954/999): loss=0.10673749772510081\n",
      "Gradient Descent(955/999): loss=0.10673571433218207\n",
      "Gradient Descent(956/999): loss=0.10673393372188611\n",
      "Gradient Descent(957/999): loss=0.1067321558872638\n",
      "Gradient Descent(958/999): loss=0.10673038082138894\n",
      "Gradient Descent(959/999): loss=0.10672860851735815\n",
      "Gradient Descent(960/999): loss=0.10672683896829077\n",
      "Gradient Descent(961/999): loss=0.10672507216732882\n",
      "Gradient Descent(962/999): loss=0.10672330810763682\n",
      "Gradient Descent(963/999): loss=0.10672154678240185\n",
      "Gradient Descent(964/999): loss=0.10671978818483324\n",
      "Gradient Descent(965/999): loss=0.10671803230816272\n",
      "Gradient Descent(966/999): loss=0.10671627914564415\n",
      "Gradient Descent(967/999): loss=0.10671452869055353\n",
      "Gradient Descent(968/999): loss=0.10671278093618884\n",
      "Gradient Descent(969/999): loss=0.10671103587587007\n",
      "Gradient Descent(970/999): loss=0.10670929350293895\n",
      "Gradient Descent(971/999): loss=0.10670755381075908\n",
      "Gradient Descent(972/999): loss=0.10670581679271565\n",
      "Gradient Descent(973/999): loss=0.10670408244221544\n",
      "Gradient Descent(974/999): loss=0.10670235075268682\n",
      "Gradient Descent(975/999): loss=0.10670062171757945\n",
      "Gradient Descent(976/999): loss=0.10669889533036436\n",
      "Gradient Descent(977/999): loss=0.10669717158453394\n",
      "Gradient Descent(978/999): loss=0.10669545047360154\n",
      "Gradient Descent(979/999): loss=0.10669373199110176\n",
      "Gradient Descent(980/999): loss=0.10669201613059012\n",
      "Gradient Descent(981/999): loss=0.10669030288564303\n",
      "Gradient Descent(982/999): loss=0.10668859224985781\n",
      "Gradient Descent(983/999): loss=0.10668688421685239\n",
      "Gradient Descent(984/999): loss=0.10668517878026554\n",
      "Gradient Descent(985/999): loss=0.10668347593375649\n",
      "Gradient Descent(986/999): loss=0.106681775671005\n",
      "Gradient Descent(987/999): loss=0.10668007798571129\n",
      "Gradient Descent(988/999): loss=0.1066783828715959\n",
      "Gradient Descent(989/999): loss=0.1066766903223996\n",
      "Gradient Descent(990/999): loss=0.10667500033188337\n",
      "Gradient Descent(991/999): loss=0.10667331289382836\n",
      "Gradient Descent(992/999): loss=0.1066716280020356\n",
      "Gradient Descent(993/999): loss=0.10666994565032617\n",
      "Gradient Descent(994/999): loss=0.10666826583254103\n",
      "Gradient Descent(995/999): loss=0.10666658854254091\n",
      "Gradient Descent(996/999): loss=0.10666491377420623\n",
      "Gradient Descent(997/999): loss=0.10666324152143702\n",
      "Gradient Descent(998/999): loss=0.10666157177815302\n",
      "Gradient Descent(999/999): loss=0.10665990453829326\n",
      "Gradient Descent(0/999): loss=0.23422206561741446\n",
      "Gradient Descent(1/999): loss=0.1255838471785101\n",
      "Gradient Descent(2/999): loss=0.12452814084855582\n",
      "Gradient Descent(3/999): loss=0.12419434874977872\n",
      "Gradient Descent(4/999): loss=0.12388445097235347\n",
      "Gradient Descent(5/999): loss=0.12358967463002533\n",
      "Gradient Descent(6/999): loss=0.12330713231492543\n",
      "Gradient Descent(7/999): loss=0.12303489810297794\n",
      "Gradient Descent(8/999): loss=0.12277165157277864\n",
      "Gradient Descent(9/999): loss=0.12251646600766018\n",
      "Gradient Descent(10/999): loss=0.12226867158591205\n",
      "Gradient Descent(11/999): loss=0.12202776690352446\n",
      "Gradient Descent(12/999): loss=0.12179336172506941\n",
      "Gradient Descent(13/999): loss=0.12156513991294113\n",
      "Gradient Descent(14/999): loss=0.1213428353960526\n",
      "Gradient Descent(15/999): loss=0.12112621656573778\n",
      "Gradient Descent(16/999): loss=0.12091507611896973\n",
      "Gradient Descent(17/999): loss=0.12070922442361283\n",
      "Gradient Descent(18/999): loss=0.1205084851617641\n",
      "Gradient Descent(19/999): loss=0.12031269244742716\n",
      "Gradient Descent(20/999): loss=0.1201216888991522\n",
      "Gradient Descent(21/999): loss=0.1199353243320108\n",
      "Gradient Descent(22/999): loss=0.11975345485198209\n",
      "Gradient Descent(23/999): loss=0.11957594221252044\n",
      "Gradient Descent(24/999): loss=0.1194026533426287\n",
      "Gradient Descent(25/999): loss=0.11923345998777606\n",
      "Gradient Descent(26/999): loss=0.11906823842568799\n",
      "Gradient Descent(27/999): loss=0.11890686923240382\n",
      "Gradient Descent(28/999): loss=0.11874923708263538\n",
      "Gradient Descent(29/999): loss=0.11859523057404635\n",
      "Gradient Descent(30/999): loss=0.1184447420686796\n",
      "Gradient Descent(31/999): loss=0.11829766754709592\n",
      "Gradient Descent(32/999): loss=0.11815390647229812\n",
      "Gradient Descent(33/999): loss=0.11801336166149139\n",
      "Gradient Descent(34/999): loss=0.11787593916436678\n",
      "Gradient Descent(35/999): loss=0.11774154814700453\n",
      "Gradient Descent(36/999): loss=0.11761010078076296\n",
      "Gradient Descent(37/999): loss=0.11748151213569355\n",
      "Gradient Descent(38/999): loss=0.11735570007813718\n",
      "Gradient Descent(39/999): loss=0.11723258517223305\n",
      "Gradient Descent(40/999): loss=0.1171120905851224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(41/999): loss=0.11699414199566352\n",
      "Gradient Descent(42/999): loss=0.11687866750649824\n",
      "Gradient Descent(43/999): loss=0.11676559755932733\n",
      "Gradient Descent(44/999): loss=0.11665486485326451\n",
      "Gradient Descent(45/999): loss=0.11654640426614787\n",
      "Gradient Descent(46/999): loss=0.11644015277869521\n",
      "Gradient Descent(47/999): loss=0.11633604940139615\n",
      "Gradient Descent(48/999): loss=0.11623403510403803\n",
      "Gradient Descent(49/999): loss=0.11613405274776889\n",
      "Gradient Descent(50/999): loss=0.11603604701960327\n",
      "Gradient Descent(51/999): loss=0.11593996436928092\n",
      "Gradient Descent(52/999): loss=0.11584575294839254\n",
      "Gradient Descent(53/999): loss=0.11575336255168993\n",
      "Gradient Descent(54/999): loss=0.11566274456049924\n",
      "Gradient Descent(55/999): loss=0.11557385188816222\n",
      "Gradient Descent(56/999): loss=0.11548663892743048\n",
      "Gradient Descent(57/999): loss=0.11540106149974154\n",
      "Gradient Descent(58/999): loss=0.11531707680630866\n",
      "Gradient Descent(59/999): loss=0.11523464338095757\n",
      "Gradient Descent(60/999): loss=0.11515372104464718\n",
      "Gradient Descent(61/999): loss=0.11507427086161252\n",
      "Gradient Descent(62/999): loss=0.11499625509707097\n",
      "Gradient Descent(63/999): loss=0.11491963717643501\n",
      "Gradient Descent(64/999): loss=0.11484438164597656\n",
      "Gradient Descent(65/999): loss=0.11477045413489027\n",
      "Gradient Descent(66/999): loss=0.11469782131870465\n",
      "Gradient Descent(67/999): loss=0.11462645088399243\n",
      "Gradient Descent(68/999): loss=0.11455631149433285\n",
      "Gradient Descent(69/999): loss=0.11448737275748003\n",
      "Gradient Descent(70/999): loss=0.11441960519369451\n",
      "Gradient Descent(71/999): loss=0.11435298020519477\n",
      "Gradient Descent(72/999): loss=0.11428747004668885\n",
      "Gradient Descent(73/999): loss=0.11422304779694668\n",
      "Gradient Descent(74/999): loss=0.11415968733137526\n",
      "Gradient Descent(75/999): loss=0.11409736329556065\n",
      "Gradient Descent(76/999): loss=0.11403605107974157\n",
      "Gradient Descent(77/999): loss=0.11397572679418114\n",
      "Gradient Descent(78/999): loss=0.11391636724540365\n",
      "Gradient Descent(79/999): loss=0.11385794991326618\n",
      "Gradient Descent(80/999): loss=0.11380045292883371\n",
      "Gradient Descent(81/999): loss=0.11374385505302968\n",
      "Gradient Descent(82/999): loss=0.11368813565603336\n",
      "Gradient Descent(83/999): loss=0.11363327469739726\n",
      "Gradient Descent(84/999): loss=0.11357925270685898\n",
      "Gradient Descent(85/999): loss=0.11352605076582185\n",
      "Gradient Descent(86/999): loss=0.11347365048948066\n",
      "Gradient Descent(87/999): loss=0.11342203400956935\n",
      "Gradient Descent(88/999): loss=0.11337118395770789\n",
      "Gradient Descent(89/999): loss=0.11332108344932713\n",
      "Gradient Descent(90/999): loss=0.1132717160681507\n",
      "Gradient Descent(91/999): loss=0.11322306585121404\n",
      "Gradient Descent(92/999): loss=0.11317511727440105\n",
      "Gradient Descent(93/999): loss=0.11312785523848032\n",
      "Gradient Descent(94/999): loss=0.11308126505562224\n",
      "Gradient Descent(95/999): loss=0.11303533243638066\n",
      "Gradient Descent(96/999): loss=0.11299004347712138\n",
      "Gradient Descent(97/999): loss=0.11294538464788287\n",
      "Gradient Descent(98/999): loss=0.11290134278065231\n",
      "Gradient Descent(99/999): loss=0.11285790505804318\n",
      "Gradient Descent(100/999): loss=0.11281505900235962\n",
      "Gradient Descent(101/999): loss=0.1127727924650337\n",
      "Gradient Descent(102/999): loss=0.1127310936164225\n",
      "Gradient Descent(103/999): loss=0.11268995093595216\n",
      "Gradient Descent(104/999): loss=0.11264935320259634\n",
      "Gradient Descent(105/999): loss=0.11260928948567742\n",
      "Gradient Descent(106/999): loss=0.11256974913597897\n",
      "Gradient Descent(107/999): loss=0.11253072177715821\n",
      "Gradient Descent(108/999): loss=0.11249219729744801\n",
      "Gradient Descent(109/999): loss=0.1124541658416382\n",
      "Gradient Descent(110/999): loss=0.11241661780332608\n",
      "Gradient Descent(111/999): loss=0.1123795438174268\n",
      "Gradient Descent(112/999): loss=0.11234293475293441\n",
      "Gradient Descent(113/999): loss=0.11230678170592442\n",
      "Gradient Descent(114/999): loss=0.11227107599279006\n",
      "Gradient Descent(115/999): loss=0.11223580914370293\n",
      "Gradient Descent(116/999): loss=0.11220097289629122\n",
      "Gradient Descent(117/999): loss=0.11216655918952721\n",
      "Gradient Descent(118/999): loss=0.11213256015781681\n",
      "Gradient Descent(119/999): loss=0.11209896812528396\n",
      "Gradient Descent(120/999): loss=0.11206577560024344\n",
      "Gradient Descent(121/999): loss=0.11203297526985496\n",
      "Gradient Descent(122/999): loss=0.11200055999495277\n",
      "Gradient Descent(123/999): loss=0.11196852280504375\n",
      "Gradient Descent(124/999): loss=0.11193685689346944\n",
      "Gradient Descent(125/999): loss=0.11190555561272482\n",
      "Gradient Descent(126/999): loss=0.11187461246992936\n",
      "Gradient Descent(127/999): loss=0.11184402112244497\n",
      "Gradient Descent(128/999): loss=0.11181377537363517\n",
      "Gradient Descent(129/999): loss=0.11178386916876105\n",
      "Gradient Descent(130/999): loss=0.11175429659100931\n",
      "Gradient Descent(131/999): loss=0.11172505185764756\n",
      "Gradient Descent(132/999): loss=0.11169612931630234\n",
      "Gradient Descent(133/999): loss=0.11166752344135648\n",
      "Gradient Descent(134/999): loss=0.1116392288304603\n",
      "Gradient Descent(135/999): loss=0.11161124020115407\n",
      "Gradient Descent(136/999): loss=0.11158355238759711\n",
      "Gradient Descent(137/999): loss=0.11155616033740001\n",
      "Gradient Descent(138/999): loss=0.11152905910855672\n",
      "Gradient Descent(139/999): loss=0.11150224386647262\n",
      "Gradient Descent(140/999): loss=0.11147570988108581\n",
      "Gradient Descent(141/999): loss=0.11144945252407792\n",
      "Gradient Descent(142/999): loss=0.11142346726617186\n",
      "Gradient Descent(143/999): loss=0.1113977496745132\n",
      "Gradient Descent(144/999): loss=0.11137229541013258\n",
      "Gradient Descent(145/999): loss=0.11134710022548609\n",
      "Gradient Descent(146/999): loss=0.11132215996207145\n",
      "Gradient Descent(147/999): loss=0.11129747054811709\n",
      "Gradient Descent(148/999): loss=0.11127302799634146\n",
      "Gradient Descent(149/999): loss=0.111248828401781\n",
      "Gradient Descent(150/999): loss=0.11122486793968349\n",
      "Gradient Descent(151/999): loss=0.11120114286346527\n",
      "Gradient Descent(152/999): loss=0.11117764950272999\n",
      "Gradient Descent(153/999): loss=0.11115438426134669\n",
      "Gradient Descent(154/999): loss=0.11113134361558533\n",
      "Gradient Descent(155/999): loss=0.11110852411230814\n",
      "Gradient Descent(156/999): loss=0.11108592236721433\n",
      "Gradient Descent(157/999): loss=0.11106353506313692\n",
      "Gradient Descent(158/999): loss=0.11104135894838978\n",
      "Gradient Descent(159/999): loss=0.11101939083516316\n",
      "Gradient Descent(160/999): loss=0.11099762759796619\n",
      "Gradient Descent(161/999): loss=0.1109760661721148\n",
      "Gradient Descent(162/999): loss=0.11095470355226361\n",
      "Gradient Descent(163/999): loss=0.11093353679098003\n",
      "Gradient Descent(164/999): loss=0.11091256299735984\n",
      "Gradient Descent(165/999): loss=0.11089177933568241\n",
      "Gradient Descent(166/999): loss=0.11087118302410404\n",
      "Gradient Descent(167/999): loss=0.11085077133338904\n",
      "Gradient Descent(168/999): loss=0.11083054158567636\n",
      "Gradient Descent(169/999): loss=0.11081049115328145\n",
      "Gradient Descent(170/999): loss=0.11079061745753147\n",
      "Gradient Descent(171/999): loss=0.11077091796763346\n",
      "Gradient Descent(172/999): loss=0.11075139019957395\n",
      "Gradient Descent(173/999): loss=0.11073203171504915\n",
      "Gradient Descent(174/999): loss=0.11071284012042475\n",
      "Gradient Descent(175/999): loss=0.11069381306572454\n",
      "Gradient Descent(176/999): loss=0.1106749482436466\n",
      "Gradient Descent(177/999): loss=0.11065624338860638\n",
      "Gradient Descent(178/999): loss=0.110637696275806\n",
      "Gradient Descent(179/999): loss=0.11061930472032862\n",
      "Gradient Descent(180/999): loss=0.11060106657625723\n",
      "Gradient Descent(181/999): loss=0.11058297973581716\n",
      "Gradient Descent(182/999): loss=0.11056504212854158\n",
      "Gradient Descent(183/999): loss=0.11054725172045904\n",
      "Gradient Descent(184/999): loss=0.11052960651330264\n",
      "Gradient Descent(185/999): loss=0.11051210454374023\n",
      "Gradient Descent(186/999): loss=0.11049474388262437\n",
      "Gradient Descent(187/999): loss=0.11047752263426248\n",
      "Gradient Descent(188/999): loss=0.11046043893570551\n",
      "Gradient Descent(189/999): loss=0.11044349095605523\n",
      "Gradient Descent(190/999): loss=0.11042667689578944\n",
      "Gradient Descent(191/999): loss=0.11040999498610445\n",
      "Gradient Descent(192/999): loss=0.11039344348827414\n",
      "Gradient Descent(193/999): loss=0.11037702069302585\n",
      "Gradient Descent(194/999): loss=0.11036072491993136\n",
      "Gradient Descent(195/999): loss=0.11034455451681387\n",
      "Gradient Descent(196/999): loss=0.11032850785916944\n",
      "Gradient Descent(197/999): loss=0.11031258334960295\n",
      "Gradient Descent(198/999): loss=0.11029677941727825\n",
      "Gradient Descent(199/999): loss=0.1102810945173816\n",
      "Gradient Descent(200/999): loss=0.11026552713059862\n",
      "Gradient Descent(201/999): loss=0.11025007576260377\n",
      "Gradient Descent(202/999): loss=0.11023473894356246\n",
      "Gradient Descent(203/999): loss=0.11021951522764523\n",
      "Gradient Descent(204/999): loss=0.1102044031925535\n",
      "Gradient Descent(205/999): loss=0.11018940143905684\n",
      "Gradient Descent(206/999): loss=0.11017450859054133\n",
      "Gradient Descent(207/999): loss=0.11015972329256858\n",
      "Gradient Descent(208/999): loss=0.11014504421244514\n",
      "Gradient Descent(209/999): loss=0.11013047003880234\n",
      "Gradient Descent(210/999): loss=0.11011599948118554\n",
      "Gradient Descent(211/999): loss=0.11010163126965337\n",
      "Gradient Descent(212/999): loss=0.11008736415438607\n",
      "Gradient Descent(213/999): loss=0.110073196905303\n",
      "Gradient Descent(214/999): loss=0.11005912831168865\n",
      "Gradient Descent(215/999): loss=0.11004515718182784\n",
      "Gradient Descent(216/999): loss=0.11003128234264846\n",
      "Gradient Descent(217/999): loss=0.11001750263937284\n",
      "Gradient Descent(218/999): loss=0.11000381693517693\n",
      "Gradient Descent(219/999): loss=0.10999022411085688\n",
      "Gradient Descent(220/999): loss=0.10997672306450336\n",
      "Gradient Descent(221/999): loss=0.10996331271118295\n",
      "Gradient Descent(222/999): loss=0.10994999198262664\n",
      "Gradient Descent(223/999): loss=0.10993675982692531\n",
      "Gradient Descent(224/999): loss=0.10992361520823157\n",
      "Gradient Descent(225/999): loss=0.10991055710646853\n",
      "Gradient Descent(226/999): loss=0.10989758451704461\n",
      "Gradient Descent(227/999): loss=0.1098846964505746\n",
      "Gradient Descent(228/999): loss=0.10987189193260671\n",
      "Gradient Descent(229/999): loss=0.1098591700033556\n",
      "Gradient Descent(230/999): loss=0.1098465297174409\n",
      "Gradient Descent(231/999): loss=0.10983397014363136\n",
      "Gradient Descent(232/999): loss=0.10982149036459463\n",
      "Gradient Descent(233/999): loss=0.10980908947665186\n",
      "Gradient Descent(234/999): loss=0.10979676658953785\n",
      "Gradient Descent(235/999): loss=0.10978452082616606\n",
      "Gradient Descent(236/999): loss=0.10977235132239846\n",
      "Gradient Descent(237/999): loss=0.10976025722682009\n",
      "Gradient Descent(238/999): loss=0.1097482377005185\n",
      "Gradient Descent(239/999): loss=0.1097362919168675\n",
      "Gradient Descent(240/999): loss=0.10972441906131525\n",
      "Gradient Descent(241/999): loss=0.109712618331177\n",
      "Gradient Descent(242/999): loss=0.10970088893543166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(243/999): loss=0.10968923009452268\n",
      "Gradient Descent(244/999): loss=0.10967764104016285\n",
      "Gradient Descent(245/999): loss=0.10966612101514299\n",
      "Gradient Descent(246/999): loss=0.10965466927314456\n",
      "Gradient Descent(247/999): loss=0.1096432850785558\n",
      "Gradient Descent(248/999): loss=0.10963196770629159\n",
      "Gradient Descent(249/999): loss=0.10962071644161696\n",
      "Gradient Descent(250/999): loss=0.10960953057997383\n",
      "Gradient Descent(251/999): loss=0.10959840942681136\n",
      "Gradient Descent(252/999): loss=0.10958735229741932\n",
      "Gradient Descent(253/999): loss=0.10957635851676509\n",
      "Gradient Descent(254/999): loss=0.1095654274193333\n",
      "Gradient Descent(255/999): loss=0.1095545583489689\n",
      "Gradient Descent(256/999): loss=0.10954375065872334\n",
      "Gradient Descent(257/999): loss=0.10953300371070304\n",
      "Gradient Descent(258/999): loss=0.10952231687592162\n",
      "Gradient Descent(259/999): loss=0.10951168953415415\n",
      "Gradient Descent(260/999): loss=0.10950112107379464\n",
      "Gradient Descent(261/999): loss=0.10949061089171602\n",
      "Gradient Descent(262/999): loss=0.10948015839313266\n",
      "Gradient Descent(263/999): loss=0.10946976299146563\n",
      "Gradient Descent(264/999): loss=0.10945942410821036\n",
      "Gradient Descent(265/999): loss=0.10944914117280671\n",
      "Gradient Descent(266/999): loss=0.10943891362251165\n",
      "Gradient Descent(267/999): loss=0.10942874090227389\n",
      "Gradient Descent(268/999): loss=0.10941862246461125\n",
      "Gradient Descent(269/999): loss=0.10940855776948981\n",
      "Gradient Descent(270/999): loss=0.1093985462842058\n",
      "Gradient Descent(271/999): loss=0.10938858748326905\n",
      "Gradient Descent(272/999): loss=0.10937868084828913\n",
      "Gradient Descent(273/999): loss=0.10936882586786287\n",
      "Gradient Descent(274/999): loss=0.1093590220374646\n",
      "Gradient Descent(275/999): loss=0.10934926885933785\n",
      "Gradient Descent(276/999): loss=0.10933956584238921\n",
      "Gradient Descent(277/999): loss=0.10932991250208404\n",
      "Gradient Descent(278/999): loss=0.10932030836034391\n",
      "Gradient Descent(279/999): loss=0.1093107529454461\n",
      "Gradient Descent(280/999): loss=0.10930124579192467\n",
      "Gradient Descent(281/999): loss=0.10929178644047327\n",
      "Gradient Descent(282/999): loss=0.10928237443784983\n",
      "Gradient Descent(283/999): loss=0.10927300933678268\n",
      "Gradient Descent(284/999): loss=0.1092636906958785\n",
      "Gradient Descent(285/999): loss=0.10925441807953178\n",
      "Gradient Descent(286/999): loss=0.1092451910578358\n",
      "Gradient Descent(287/999): loss=0.10923600920649532\n",
      "Gradient Descent(288/999): loss=0.10922687210674059\n",
      "Gradient Descent(289/999): loss=0.1092177793452429\n",
      "Gradient Descent(290/999): loss=0.10920873051403163\n",
      "Gradient Descent(291/999): loss=0.10919972521041267\n",
      "Gradient Descent(292/999): loss=0.10919076303688822\n",
      "Gradient Descent(293/999): loss=0.10918184360107804\n",
      "Gradient Descent(294/999): loss=0.10917296651564191\n",
      "Gradient Descent(295/999): loss=0.10916413139820347\n",
      "Gradient Descent(296/999): loss=0.10915533787127531\n",
      "Gradient Descent(297/999): loss=0.1091465855621855\n",
      "Gradient Descent(298/999): loss=0.10913787410300495\n",
      "Gradient Descent(299/999): loss=0.10912920313047653\n",
      "Gradient Descent(300/999): loss=0.10912057228594498\n",
      "Gradient Descent(301/999): loss=0.1091119812152879\n",
      "Gradient Descent(302/999): loss=0.10910342956884837\n",
      "Gradient Descent(303/999): loss=0.1090949170013684\n",
      "Gradient Descent(304/999): loss=0.10908644317192326\n",
      "Gradient Descent(305/999): loss=0.10907800774385734\n",
      "Gradient Descent(306/999): loss=0.10906961038472078\n",
      "Gradient Descent(307/999): loss=0.10906125076620721\n",
      "Gradient Descent(308/999): loss=0.10905292856409254\n",
      "Gradient Descent(309/999): loss=0.10904464345817481\n",
      "Gradient Descent(310/999): loss=0.10903639513221486\n",
      "Gradient Descent(311/999): loss=0.109028183273878\n",
      "Gradient Descent(312/999): loss=0.10902000757467693\n",
      "Gradient Descent(313/999): loss=0.1090118677299151\n",
      "Gradient Descent(314/999): loss=0.1090037634386315\n",
      "Gradient Descent(315/999): loss=0.10899569440354576\n",
      "Gradient Descent(316/999): loss=0.10898766033100477\n",
      "Gradient Descent(317/999): loss=0.10897966093092974\n",
      "Gradient Descent(318/999): loss=0.10897169591676413\n",
      "Gradient Descent(319/999): loss=0.10896376500542279\n",
      "Gradient Descent(320/999): loss=0.10895586791724132\n",
      "Gradient Descent(321/999): loss=0.10894800437592683\n",
      "Gradient Descent(322/999): loss=0.10894017410850905\n",
      "Gradient Descent(323/999): loss=0.10893237684529254\n",
      "Gradient Descent(324/999): loss=0.10892461231980946\n",
      "Gradient Descent(325/999): loss=0.1089168802687731\n",
      "Gradient Descent(326/999): loss=0.10890918043203222\n",
      "Gradient Descent(327/999): loss=0.10890151255252625\n",
      "Gradient Descent(328/999): loss=0.1088938763762408\n",
      "Gradient Descent(329/999): loss=0.10888627165216433\n",
      "Gradient Descent(330/999): loss=0.10887869813224509\n",
      "Gradient Descent(331/999): loss=0.10887115557134919\n",
      "Gradient Descent(332/999): loss=0.10886364372721888\n",
      "Gradient Descent(333/999): loss=0.10885616236043176\n",
      "Gradient Descent(334/999): loss=0.10884871123436056\n",
      "Gradient Descent(335/999): loss=0.10884129011513362\n",
      "Gradient Descent(336/999): loss=0.10883389877159574\n",
      "Gradient Descent(337/999): loss=0.10882653697526999\n",
      "Gradient Descent(338/999): loss=0.10881920450031989\n",
      "Gradient Descent(339/999): loss=0.1088119011235122\n",
      "Gradient Descent(340/999): loss=0.10880462662418033\n",
      "Gradient Descent(341/999): loss=0.10879738078418844\n",
      "Gradient Descent(342/999): loss=0.10879016338789574\n",
      "Gradient Descent(343/999): loss=0.10878297422212178\n",
      "Gradient Descent(344/999): loss=0.10877581307611199\n",
      "Gradient Descent(345/999): loss=0.10876867974150375\n",
      "Gradient Descent(346/999): loss=0.1087615740122932\n",
      "Gradient Descent(347/999): loss=0.10875449568480243\n",
      "Gradient Descent(348/999): loss=0.10874744455764698\n",
      "Gradient Descent(349/999): loss=0.10874042043170429\n",
      "Gradient Descent(350/999): loss=0.10873342311008222\n",
      "Gradient Descent(351/999): loss=0.10872645239808815\n",
      "Gradient Descent(352/999): loss=0.10871950810319886\n",
      "Gradient Descent(353/999): loss=0.10871259003503031\n",
      "Gradient Descent(354/999): loss=0.10870569800530852\n",
      "Gradient Descent(355/999): loss=0.10869883182784018\n",
      "Gradient Descent(356/999): loss=0.10869199131848455\n",
      "Gradient Descent(357/999): loss=0.10868517629512488\n",
      "Gradient Descent(358/999): loss=0.10867838657764108\n",
      "Gradient Descent(359/999): loss=0.10867162198788222\n",
      "Gradient Descent(360/999): loss=0.10866488234963975\n",
      "Gradient Descent(361/999): loss=0.10865816748862114\n",
      "Gradient Descent(362/999): loss=0.10865147723242356\n",
      "Gradient Descent(363/999): loss=0.10864481141050852\n",
      "Gradient Descent(364/999): loss=0.10863816985417635\n",
      "Gradient Descent(365/999): loss=0.10863155239654151\n",
      "Gradient Descent(366/999): loss=0.10862495887250793\n",
      "Gradient Descent(367/999): loss=0.10861838911874493\n",
      "Gradient Descent(368/999): loss=0.10861184297366336\n",
      "Gradient Descent(369/999): loss=0.1086053202773923\n",
      "Gradient Descent(370/999): loss=0.10859882087175583\n",
      "Gradient Descent(371/999): loss=0.10859234460025041\n",
      "Gradient Descent(372/999): loss=0.10858589130802244\n",
      "Gradient Descent(373/999): loss=0.10857946084184623\n",
      "Gradient Descent(374/999): loss=0.1085730530501022\n",
      "Gradient Descent(375/999): loss=0.10856666778275556\n",
      "Gradient Descent(376/999): loss=0.10856030489133514\n",
      "Gradient Descent(377/999): loss=0.10855396422891272\n",
      "Gradient Descent(378/999): loss=0.1085476456500825\n",
      "Gradient Descent(379/999): loss=0.1085413490109409\n",
      "Gradient Descent(380/999): loss=0.10853507416906678\n",
      "Gradient Descent(381/999): loss=0.10852882098350182\n",
      "Gradient Descent(382/999): loss=0.10852258931473129\n",
      "Gradient Descent(383/999): loss=0.10851637902466502\n",
      "Gradient Descent(384/999): loss=0.10851018997661864\n",
      "Gradient Descent(385/999): loss=0.10850402203529527\n",
      "Gradient Descent(386/999): loss=0.10849787506676724\n",
      "Gradient Descent(387/999): loss=0.10849174893845821\n",
      "Gradient Descent(388/999): loss=0.10848564351912562\n",
      "Gradient Descent(389/999): loss=0.10847955867884318\n",
      "Gradient Descent(390/999): loss=0.10847349428898391\n",
      "Gradient Descent(391/999): loss=0.1084674502222031\n",
      "Gradient Descent(392/999): loss=0.1084614263524218\n",
      "Gradient Descent(393/999): loss=0.10845542255481042\n",
      "Gradient Descent(394/999): loss=0.10844943870577253\n",
      "Gradient Descent(395/999): loss=0.10844347468292909\n",
      "Gradient Descent(396/999): loss=0.10843753036510258\n",
      "Gradient Descent(397/999): loss=0.10843160563230178\n",
      "Gradient Descent(398/999): loss=0.10842570036570638\n",
      "Gradient Descent(399/999): loss=0.10841981444765197\n",
      "Gradient Descent(400/999): loss=0.10841394776161548\n",
      "Gradient Descent(401/999): loss=0.10840810019220033\n",
      "Gradient Descent(402/999): loss=0.1084022716251223\n",
      "Gradient Descent(403/999): loss=0.10839646194719514\n",
      "Gradient Descent(404/999): loss=0.10839067104631699\n",
      "Gradient Descent(405/999): loss=0.10838489881145627\n",
      "Gradient Descent(406/999): loss=0.10837914513263831\n",
      "Gradient Descent(407/999): loss=0.10837340990093201\n",
      "Gradient Descent(408/999): loss=0.10836769300843668\n",
      "Gradient Descent(409/999): loss=0.10836199434826907\n",
      "Gradient Descent(410/999): loss=0.10835631381455042\n",
      "Gradient Descent(411/999): loss=0.10835065130239423\n",
      "Gradient Descent(412/999): loss=0.10834500670789347\n",
      "Gradient Descent(413/999): loss=0.10833937992810862\n",
      "Gradient Descent(414/999): loss=0.10833377086105538\n",
      "Gradient Descent(415/999): loss=0.10832817940569299\n",
      "Gradient Descent(416/999): loss=0.10832260546191237\n",
      "Gradient Descent(417/999): loss=0.10831704893052459\n",
      "Gradient Descent(418/999): loss=0.10831150971324954\n",
      "Gradient Descent(419/999): loss=0.1083059877127047\n",
      "Gradient Descent(420/999): loss=0.10830048283239392\n",
      "Gradient Descent(421/999): loss=0.10829499497669677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(422/999): loss=0.10828952405085757\n",
      "Gradient Descent(423/999): loss=0.10828406996097487\n",
      "Gradient Descent(424/999): loss=0.10827863261399104\n",
      "Gradient Descent(425/999): loss=0.10827321191768188\n",
      "Gradient Descent(426/999): loss=0.10826780778064655\n",
      "Gradient Descent(427/999): loss=0.10826242011229747\n",
      "Gradient Descent(428/999): loss=0.10825704882285052\n",
      "Gradient Descent(429/999): loss=0.10825169382331526\n",
      "Gradient Descent(430/999): loss=0.10824635502548535\n",
      "Gradient Descent(431/999): loss=0.10824103234192914\n",
      "Gradient Descent(432/999): loss=0.10823572568598022\n",
      "Gradient Descent(433/999): loss=0.10823043497172838\n",
      "Gradient Descent(434/999): loss=0.10822516011401036\n",
      "Gradient Descent(435/999): loss=0.10821990102840102\n",
      "Gradient Descent(436/999): loss=0.10821465763120461\n",
      "Gradient Descent(437/999): loss=0.10820942983944587\n",
      "Gradient Descent(438/999): loss=0.10820421757086161\n",
      "Gradient Descent(439/999): loss=0.10819902074389216\n",
      "Gradient Descent(440/999): loss=0.1081938392776732\n",
      "Gradient Descent(441/999): loss=0.10818867309202729\n",
      "Gradient Descent(442/999): loss=0.10818352210745602\n",
      "Gradient Descent(443/999): loss=0.10817838624513183\n",
      "Gradient Descent(444/999): loss=0.10817326542689022\n",
      "Gradient Descent(445/999): loss=0.10816815957522202\n",
      "Gradient Descent(446/999): loss=0.10816306861326552\n",
      "Gradient Descent(447/999): loss=0.10815799246479928\n",
      "Gradient Descent(448/999): loss=0.10815293105423421\n",
      "Gradient Descent(449/999): loss=0.10814788430660664\n",
      "Gradient Descent(450/999): loss=0.1081428521475708\n",
      "Gradient Descent(451/999): loss=0.10813783450339184\n",
      "Gradient Descent(452/999): loss=0.10813283130093862\n",
      "Gradient Descent(453/999): loss=0.1081278424676769\n",
      "Gradient Descent(454/999): loss=0.10812286793166234\n",
      "Gradient Descent(455/999): loss=0.10811790762153396\n",
      "Gradient Descent(456/999): loss=0.10811296146650713\n",
      "Gradient Descent(457/999): loss=0.1081080293963673\n",
      "Gradient Descent(458/999): loss=0.10810311134146335\n",
      "Gradient Descent(459/999): loss=0.10809820723270123\n",
      "Gradient Descent(460/999): loss=0.10809331700153763\n",
      "Gradient Descent(461/999): loss=0.10808844057997367\n",
      "Gradient Descent(462/999): loss=0.10808357790054901\n",
      "Gradient Descent(463/999): loss=0.10807872889633549\n",
      "Gradient Descent(464/999): loss=0.10807389350093131\n",
      "Gradient Descent(465/999): loss=0.10806907164845506\n",
      "Gradient Descent(466/999): loss=0.10806426327353998\n",
      "Gradient Descent(467/999): loss=0.10805946831132815\n",
      "Gradient Descent(468/999): loss=0.10805468669746487\n",
      "Gradient Descent(469/999): loss=0.10804991836809304\n",
      "Gradient Descent(470/999): loss=0.10804516325984764\n",
      "Gradient Descent(471/999): loss=0.10804042130985037\n",
      "Gradient Descent(472/999): loss=0.10803569245570412\n",
      "Gradient Descent(473/999): loss=0.10803097663548789\n",
      "Gradient Descent(474/999): loss=0.10802627378775145\n",
      "Gradient Descent(475/999): loss=0.10802158385151012\n",
      "Gradient Descent(476/999): loss=0.10801690676623979\n",
      "Gradient Descent(477/999): loss=0.10801224247187192\n",
      "Gradient Descent(478/999): loss=0.10800759090878856\n",
      "Gradient Descent(479/999): loss=0.10800295201781736\n",
      "Gradient Descent(480/999): loss=0.1079983257402269\n",
      "Gradient Descent(481/999): loss=0.10799371201772195\n",
      "Gradient Descent(482/999): loss=0.10798911079243854\n",
      "Gradient Descent(483/999): loss=0.10798452200693967\n",
      "Gradient Descent(484/999): loss=0.10797994560421043\n",
      "Gradient Descent(485/999): loss=0.10797538152765371\n",
      "Gradient Descent(486/999): loss=0.10797082972108564\n",
      "Gradient Descent(487/999): loss=0.10796629012873125\n",
      "Gradient Descent(488/999): loss=0.1079617626952201\n",
      "Gradient Descent(489/999): loss=0.10795724736558202\n",
      "Gradient Descent(490/999): loss=0.10795274408524286\n",
      "Gradient Descent(491/999): loss=0.10794825280002045\n",
      "Gradient Descent(492/999): loss=0.10794377345612025\n",
      "Gradient Descent(493/999): loss=0.10793930600013159\n",
      "Gradient Descent(494/999): loss=0.10793485037902338\n",
      "Gradient Descent(495/999): loss=0.10793040654014036\n",
      "Gradient Descent(496/999): loss=0.1079259744311991\n",
      "Gradient Descent(497/999): loss=0.10792155400028426\n",
      "Gradient Descent(498/999): loss=0.10791714519584455\n",
      "Gradient Descent(499/999): loss=0.10791274796668929\n",
      "Gradient Descent(500/999): loss=0.10790836226198447\n",
      "Gradient Descent(501/999): loss=0.10790398803124919\n",
      "Gradient Descent(502/999): loss=0.10789962522435208\n",
      "Gradient Descent(503/999): loss=0.10789527379150764\n",
      "Gradient Descent(504/999): loss=0.10789093368327278\n",
      "Gradient Descent(505/999): loss=0.10788660485054344\n",
      "Gradient Descent(506/999): loss=0.10788228724455101\n",
      "Gradient Descent(507/999): loss=0.10787798081685898\n",
      "Gradient Descent(508/999): loss=0.10787368551935973\n",
      "Gradient Descent(509/999): loss=0.10786940130427103\n",
      "Gradient Descent(510/999): loss=0.10786512812413293\n",
      "Gradient Descent(511/999): loss=0.10786086593180458\n",
      "Gradient Descent(512/999): loss=0.10785661468046083\n",
      "Gradient Descent(513/999): loss=0.1078523743235894\n",
      "Gradient Descent(514/999): loss=0.10784814481498757\n",
      "Gradient Descent(515/999): loss=0.10784392610875917\n",
      "Gradient Descent(516/999): loss=0.10783971815931162\n",
      "Gradient Descent(517/999): loss=0.10783552092135293\n",
      "Gradient Descent(518/999): loss=0.10783133434988874\n",
      "Gradient Descent(519/999): loss=0.10782715840021936\n",
      "Gradient Descent(520/999): loss=0.10782299302793706\n",
      "Gradient Descent(521/999): loss=0.10781883818892311\n",
      "Gradient Descent(522/999): loss=0.10781469383934503\n",
      "Gradient Descent(523/999): loss=0.10781055993565374\n",
      "Gradient Descent(524/999): loss=0.10780643643458104\n",
      "Gradient Descent(525/999): loss=0.10780232329313669\n",
      "Gradient Descent(526/999): loss=0.10779822046860589\n",
      "Gradient Descent(527/999): loss=0.10779412791854659\n",
      "Gradient Descent(528/999): loss=0.10779004560078698\n",
      "Gradient Descent(529/999): loss=0.10778597347342272\n",
      "Gradient Descent(530/999): loss=0.10778191149481468\n",
      "Gradient Descent(531/999): loss=0.10777785962358633\n",
      "Gradient Descent(532/999): loss=0.10777381781862112\n",
      "Gradient Descent(533/999): loss=0.1077697860390603\n",
      "Gradient Descent(534/999): loss=0.10776576424430033\n",
      "Gradient Descent(535/999): loss=0.10776175239399056\n",
      "Gradient Descent(536/999): loss=0.10775775044803085\n",
      "Gradient Descent(537/999): loss=0.10775375836656935\n",
      "Gradient Descent(538/999): loss=0.10774977611000006\n",
      "Gradient Descent(539/999): loss=0.10774580363896062\n",
      "Gradient Descent(540/999): loss=0.10774184091433017\n",
      "Gradient Descent(541/999): loss=0.10773788789722696\n",
      "Gradient Descent(542/999): loss=0.1077339445490063\n",
      "Gradient Descent(543/999): loss=0.1077300108312583\n",
      "Gradient Descent(544/999): loss=0.10772608670580591\n",
      "Gradient Descent(545/999): loss=0.1077221721347026\n",
      "Gradient Descent(546/999): loss=0.10771826708023041\n",
      "Gradient Descent(547/999): loss=0.10771437150489782\n",
      "Gradient Descent(548/999): loss=0.1077104853714378\n",
      "Gradient Descent(549/999): loss=0.10770660864280576\n",
      "Gradient Descent(550/999): loss=0.10770274128217754\n",
      "Gradient Descent(551/999): loss=0.1076988832529474\n",
      "Gradient Descent(552/999): loss=0.10769503451872625\n",
      "Gradient Descent(553/999): loss=0.10769119504333961\n",
      "Gradient Descent(554/999): loss=0.10768736479082573\n",
      "Gradient Descent(555/999): loss=0.10768354372543376\n",
      "Gradient Descent(556/999): loss=0.1076797318116218\n",
      "Gradient Descent(557/999): loss=0.10767592901405522\n",
      "Gradient Descent(558/999): loss=0.10767213529760476\n",
      "Gradient Descent(559/999): loss=0.10766835062734476\n",
      "Gradient Descent(560/999): loss=0.10766457496855138\n",
      "Gradient Descent(561/999): loss=0.10766080828670095\n",
      "Gradient Descent(562/999): loss=0.10765705054746809\n",
      "Gradient Descent(563/999): loss=0.10765330171672412\n",
      "Gradient Descent(564/999): loss=0.1076495617605354\n",
      "Gradient Descent(565/999): loss=0.10764583064516159\n",
      "Gradient Descent(566/999): loss=0.10764210833705397\n",
      "Gradient Descent(567/999): loss=0.10763839480285395\n",
      "Gradient Descent(568/999): loss=0.10763469000939138\n",
      "Gradient Descent(569/999): loss=0.10763099392368289\n",
      "Gradient Descent(570/999): loss=0.10762730651293051\n",
      "Gradient Descent(571/999): loss=0.10762362774451992\n",
      "Gradient Descent(572/999): loss=0.10761995758601908\n",
      "Gradient Descent(573/999): loss=0.1076162960051765\n",
      "Gradient Descent(574/999): loss=0.10761264296992003\n",
      "Gradient Descent(575/999): loss=0.10760899844835507\n",
      "Gradient Descent(576/999): loss=0.10760536240876338\n",
      "Gradient Descent(577/999): loss=0.10760173481960138\n",
      "Gradient Descent(578/999): loss=0.10759811564949895\n",
      "Gradient Descent(579/999): loss=0.10759450486725777\n",
      "Gradient Descent(580/999): loss=0.10759090244185014\n",
      "Gradient Descent(581/999): loss=0.10758730834241749\n",
      "Gradient Descent(582/999): loss=0.10758372253826896\n",
      "Gradient Descent(583/999): loss=0.10758014499888012\n",
      "Gradient Descent(584/999): loss=0.10757657569389165\n",
      "Gradient Descent(585/999): loss=0.10757301459310789\n",
      "Gradient Descent(586/999): loss=0.10756946166649574\n",
      "Gradient Descent(587/999): loss=0.10756591688418309\n",
      "Gradient Descent(588/999): loss=0.10756238021645773\n",
      "Gradient Descent(589/999): loss=0.10755885163376609\n",
      "Gradient Descent(590/999): loss=0.10755533110671182\n",
      "Gradient Descent(591/999): loss=0.10755181860605477\n",
      "Gradient Descent(592/999): loss=0.10754831410270954\n",
      "Gradient Descent(593/999): loss=0.10754481756774446\n",
      "Gradient Descent(594/999): loss=0.10754132897238018\n",
      "Gradient Descent(595/999): loss=0.10753784828798872\n",
      "Gradient Descent(596/999): loss=0.10753437548609215\n",
      "Gradient Descent(597/999): loss=0.10753091053836143\n",
      "Gradient Descent(598/999): loss=0.10752745341661525\n",
      "Gradient Descent(599/999): loss=0.10752400409281895\n",
      "Gradient Descent(600/999): loss=0.10752056253908342\n",
      "Gradient Descent(601/999): loss=0.1075171287276639\n",
      "Gradient Descent(602/999): loss=0.10751370263095884\n",
      "Gradient Descent(603/999): loss=0.10751028422150895\n",
      "Gradient Descent(604/999): loss=0.10750687347199608\n",
      "Gradient Descent(605/999): loss=0.10750347035524208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(606/999): loss=0.10750007484420772\n",
      "Gradient Descent(607/999): loss=0.10749668691199184\n",
      "Gradient Descent(608/999): loss=0.10749330653183012\n",
      "Gradient Descent(609/999): loss=0.10748993367709415\n",
      "Gradient Descent(610/999): loss=0.10748656832129035\n",
      "Gradient Descent(611/999): loss=0.10748321043805903\n",
      "Gradient Descent(612/999): loss=0.10747986000117338\n",
      "Gradient Descent(613/999): loss=0.10747651698453851\n",
      "Gradient Descent(614/999): loss=0.10747318136219038\n",
      "Gradient Descent(615/999): loss=0.10746985310829497\n",
      "Gradient Descent(616/999): loss=0.10746653219714734\n",
      "Gradient Descent(617/999): loss=0.10746321860317042\n",
      "Gradient Descent(618/999): loss=0.10745991230091446\n",
      "Gradient Descent(619/999): loss=0.10745661326505586\n",
      "Gradient Descent(620/999): loss=0.10745332147039635\n",
      "Gradient Descent(621/999): loss=0.1074500368918619\n",
      "Gradient Descent(622/999): loss=0.10744675950450226\n",
      "Gradient Descent(623/999): loss=0.10744348928348953\n",
      "Gradient Descent(624/999): loss=0.10744022620411768\n",
      "Gradient Descent(625/999): loss=0.10743697024180163\n",
      "Gradient Descent(626/999): loss=0.10743372137207607\n",
      "Gradient Descent(627/999): loss=0.10743047957059511\n",
      "Gradient Descent(628/999): loss=0.10742724481313101\n",
      "Gradient Descent(629/999): loss=0.10742401707557354\n",
      "Gradient Descent(630/999): loss=0.10742079633392923\n",
      "Gradient Descent(631/999): loss=0.10741758256432031\n",
      "Gradient Descent(632/999): loss=0.10741437574298415\n",
      "Gradient Descent(633/999): loss=0.1074111758462723\n",
      "Gradient Descent(634/999): loss=0.10740798285064973\n",
      "Gradient Descent(635/999): loss=0.10740479673269414\n",
      "Gradient Descent(636/999): loss=0.10740161746909503\n",
      "Gradient Descent(637/999): loss=0.107398445036653\n",
      "Gradient Descent(638/999): loss=0.10739527941227907\n",
      "Gradient Descent(639/999): loss=0.10739212057299374\n",
      "Gradient Descent(640/999): loss=0.10738896849592641\n",
      "Gradient Descent(641/999): loss=0.10738582315831459\n",
      "Gradient Descent(642/999): loss=0.10738268453750312\n",
      "Gradient Descent(643/999): loss=0.10737955261094345\n",
      "Gradient Descent(644/999): loss=0.10737642735619297\n",
      "Gradient Descent(645/999): loss=0.1073733087509143\n",
      "Gradient Descent(646/999): loss=0.10737019677287458\n",
      "Gradient Descent(647/999): loss=0.10736709139994462\n",
      "Gradient Descent(648/999): loss=0.10736399261009844\n",
      "Gradient Descent(649/999): loss=0.10736090038141248\n",
      "Gradient Descent(650/999): loss=0.10735781469206482\n",
      "Gradient Descent(651/999): loss=0.10735473552033478\n",
      "Gradient Descent(652/999): loss=0.10735166284460186\n",
      "Gradient Descent(653/999): loss=0.10734859664334549\n",
      "Gradient Descent(654/999): loss=0.10734553689514403\n",
      "Gradient Descent(655/999): loss=0.10734248357867437\n",
      "Gradient Descent(656/999): loss=0.10733943667271117\n",
      "Gradient Descent(657/999): loss=0.10733639615612628\n",
      "Gradient Descent(658/999): loss=0.10733336200788794\n",
      "Gradient Descent(659/999): loss=0.10733033420706042\n",
      "Gradient Descent(660/999): loss=0.10732731273280322\n",
      "Gradient Descent(661/999): loss=0.10732429756437058\n",
      "Gradient Descent(662/999): loss=0.1073212886811106\n",
      "Gradient Descent(663/999): loss=0.10731828606246507\n",
      "Gradient Descent(664/999): loss=0.1073152896879685\n",
      "Gradient Descent(665/999): loss=0.10731229953724773\n",
      "Gradient Descent(666/999): loss=0.10730931559002123\n",
      "Gradient Descent(667/999): loss=0.10730633782609861\n",
      "Gradient Descent(668/999): loss=0.10730336622537996\n",
      "Gradient Descent(669/999): loss=0.1073004007678554\n",
      "Gradient Descent(670/999): loss=0.10729744143360435\n",
      "Gradient Descent(671/999): loss=0.10729448820279515\n",
      "Gradient Descent(672/999): loss=0.10729154105568435\n",
      "Gradient Descent(673/999): loss=0.1072885999726162\n",
      "Gradient Descent(674/999): loss=0.10728566493402222\n",
      "Gradient Descent(675/999): loss=0.10728273592042051\n",
      "Gradient Descent(676/999): loss=0.10727981291241526\n",
      "Gradient Descent(677/999): loss=0.10727689589069624\n",
      "Gradient Descent(678/999): loss=0.10727398483603826\n",
      "Gradient Descent(679/999): loss=0.10727107972930072\n",
      "Gradient Descent(680/999): loss=0.10726818055142688\n",
      "Gradient Descent(681/999): loss=0.10726528728344364\n",
      "Gradient Descent(682/999): loss=0.1072623999064608\n",
      "Gradient Descent(683/999): loss=0.1072595184016707\n",
      "Gradient Descent(684/999): loss=0.10725664275034755\n",
      "Gradient Descent(685/999): loss=0.10725377293384711\n",
      "Gradient Descent(686/999): loss=0.10725090893360616\n",
      "Gradient Descent(687/999): loss=0.10724805073114196\n",
      "Gradient Descent(688/999): loss=0.10724519830805182\n",
      "Gradient Descent(689/999): loss=0.10724235164601248\n",
      "Gradient Descent(690/999): loss=0.10723951072677988\n",
      "Gradient Descent(691/999): loss=0.10723667553218852\n",
      "Gradient Descent(692/999): loss=0.107233846044151\n",
      "Gradient Descent(693/999): loss=0.1072310222446576\n",
      "Gradient Descent(694/999): loss=0.10722820411577583\n",
      "Gradient Descent(695/999): loss=0.10722539163964998\n",
      "Gradient Descent(696/999): loss=0.10722258479850055\n",
      "Gradient Descent(697/999): loss=0.10721978357462396\n",
      "Gradient Descent(698/999): loss=0.10721698795039207\n",
      "Gradient Descent(699/999): loss=0.10721419790825168\n",
      "Gradient Descent(700/999): loss=0.10721141343072406\n",
      "Gradient Descent(701/999): loss=0.10720863450040469\n",
      "Gradient Descent(702/999): loss=0.10720586109996262\n",
      "Gradient Descent(703/999): loss=0.10720309321214028\n",
      "Gradient Descent(704/999): loss=0.10720033081975282\n",
      "Gradient Descent(705/999): loss=0.10719757390568777\n",
      "Gradient Descent(706/999): loss=0.10719482245290479\n",
      "Gradient Descent(707/999): loss=0.10719207644443494\n",
      "Gradient Descent(708/999): loss=0.1071893358633806\n",
      "Gradient Descent(709/999): loss=0.10718660069291486\n",
      "Gradient Descent(710/999): loss=0.1071838709162811\n",
      "Gradient Descent(711/999): loss=0.10718114651679275\n",
      "Gradient Descent(712/999): loss=0.10717842747783281\n",
      "Gradient Descent(713/999): loss=0.1071757137828534\n",
      "Gradient Descent(714/999): loss=0.10717300541537543\n",
      "Gradient Descent(715/999): loss=0.10717030235898829\n",
      "Gradient Descent(716/999): loss=0.10716760459734924\n",
      "Gradient Descent(717/999): loss=0.10716491211418326\n",
      "Gradient Descent(718/999): loss=0.10716222489328259\n",
      "Gradient Descent(719/999): loss=0.10715954291850634\n",
      "Gradient Descent(720/999): loss=0.10715686617378013\n",
      "Gradient Descent(721/999): loss=0.1071541946430957\n",
      "Gradient Descent(722/999): loss=0.10715152831051056\n",
      "Gradient Descent(723/999): loss=0.10714886716014765\n",
      "Gradient Descent(724/999): loss=0.10714621117619502\n",
      "Gradient Descent(725/999): loss=0.10714356034290529\n",
      "Gradient Descent(726/999): loss=0.1071409146445955\n",
      "Gradient Descent(727/999): loss=0.10713827406564665\n",
      "Gradient Descent(728/999): loss=0.10713563859050339\n",
      "Gradient Descent(729/999): loss=0.1071330082036737\n",
      "Gradient Descent(730/999): loss=0.10713038288972841\n",
      "Gradient Descent(731/999): loss=0.10712776263330102\n",
      "Gradient Descent(732/999): loss=0.10712514741908731\n",
      "Gradient Descent(733/999): loss=0.10712253723184495\n",
      "Gradient Descent(734/999): loss=0.10711993205639321\n",
      "Gradient Descent(735/999): loss=0.10711733187761263\n",
      "Gradient Descent(736/999): loss=0.10711473668044476\n",
      "Gradient Descent(737/999): loss=0.10711214644989163\n",
      "Gradient Descent(738/999): loss=0.10710956117101565\n",
      "Gradient Descent(739/999): loss=0.10710698082893912\n",
      "Gradient Descent(740/999): loss=0.1071044054088441\n",
      "Gradient Descent(741/999): loss=0.10710183489597186\n",
      "Gradient Descent(742/999): loss=0.10709926927562274\n",
      "Gradient Descent(743/999): loss=0.10709670853315581\n",
      "Gradient Descent(744/999): loss=0.10709415265398847\n",
      "Gradient Descent(745/999): loss=0.10709160162359622\n",
      "Gradient Descent(746/999): loss=0.1070890554275124\n",
      "Gradient Descent(747/999): loss=0.10708651405132777\n",
      "Gradient Descent(748/999): loss=0.10708397748069025\n",
      "Gradient Descent(749/999): loss=0.10708144570130469\n",
      "Gradient Descent(750/999): loss=0.10707891869893248\n",
      "Gradient Descent(751/999): loss=0.10707639645939128\n",
      "Gradient Descent(752/999): loss=0.10707387896855484\n",
      "Gradient Descent(753/999): loss=0.10707136621235251\n",
      "Gradient Descent(754/999): loss=0.1070688581767691\n",
      "Gradient Descent(755/999): loss=0.10706635484784453\n",
      "Gradient Descent(756/999): loss=0.1070638562116736\n",
      "Gradient Descent(757/999): loss=0.10706136225440559\n",
      "Gradient Descent(758/999): loss=0.10705887296224423\n",
      "Gradient Descent(759/999): loss=0.10705638832144702\n",
      "Gradient Descent(760/999): loss=0.10705390831832541\n",
      "Gradient Descent(761/999): loss=0.10705143293924418\n",
      "Gradient Descent(762/999): loss=0.10704896217062129\n",
      "Gradient Descent(763/999): loss=0.10704649599892771\n",
      "Gradient Descent(764/999): loss=0.10704403441068695\n",
      "Gradient Descent(765/999): loss=0.10704157739247491\n",
      "Gradient Descent(766/999): loss=0.10703912493091963\n",
      "Gradient Descent(767/999): loss=0.10703667701270103\n",
      "Gradient Descent(768/999): loss=0.1070342336245506\n",
      "Gradient Descent(769/999): loss=0.10703179475325103\n",
      "Gradient Descent(770/999): loss=0.10702936038563629\n",
      "Gradient Descent(771/999): loss=0.10702693050859104\n",
      "Gradient Descent(772/999): loss=0.10702450510905047\n",
      "Gradient Descent(773/999): loss=0.10702208417400022\n",
      "Gradient Descent(774/999): loss=0.10701966769047587\n",
      "Gradient Descent(775/999): loss=0.10701725564556276\n",
      "Gradient Descent(776/999): loss=0.10701484802639595\n",
      "Gradient Descent(777/999): loss=0.10701244482015976\n",
      "Gradient Descent(778/999): loss=0.10701004601408745\n",
      "Gradient Descent(779/999): loss=0.10700765159546131\n",
      "Gradient Descent(780/999): loss=0.10700526155161207\n",
      "Gradient Descent(781/999): loss=0.10700287586991887\n",
      "Gradient Descent(782/999): loss=0.10700049453780895\n",
      "Gradient Descent(783/999): loss=0.10699811754275748\n",
      "Gradient Descent(784/999): loss=0.10699574487228714\n",
      "Gradient Descent(785/999): loss=0.10699337651396815\n",
      "Gradient Descent(786/999): loss=0.10699101245541785\n",
      "Gradient Descent(787/999): loss=0.10698865268430052\n",
      "Gradient Descent(788/999): loss=0.10698629718832714\n",
      "Gradient Descent(789/999): loss=0.10698394595525527\n",
      "Gradient Descent(790/999): loss=0.1069815989728887\n",
      "Gradient Descent(791/999): loss=0.10697925622907714\n",
      "Gradient Descent(792/999): loss=0.10697691771171633\n",
      "Gradient Descent(793/999): loss=0.10697458340874746\n",
      "Gradient Descent(794/999): loss=0.10697225330815711\n",
      "Gradient Descent(795/999): loss=0.10696992739797706\n",
      "Gradient Descent(796/999): loss=0.1069676056662841\n",
      "Gradient Descent(797/999): loss=0.10696528810119964\n",
      "Gradient Descent(798/999): loss=0.10696297469088963\n",
      "Gradient Descent(799/999): loss=0.10696066542356433\n",
      "Gradient Descent(800/999): loss=0.10695836028747813\n",
      "Gradient Descent(801/999): loss=0.10695605927092926\n",
      "Gradient Descent(802/999): loss=0.10695376236225962\n",
      "Gradient Descent(803/999): loss=0.10695146954985461\n",
      "Gradient Descent(804/999): loss=0.10694918082214287\n",
      "Gradient Descent(805/999): loss=0.10694689616759609\n",
      "Gradient Descent(806/999): loss=0.10694461557472883\n",
      "Gradient Descent(807/999): loss=0.10694233903209828\n",
      "Gradient Descent(808/999): loss=0.10694006652830408\n",
      "Gradient Descent(809/999): loss=0.10693779805198815\n",
      "Gradient Descent(810/999): loss=0.1069355335918344\n",
      "Gradient Descent(811/999): loss=0.10693327313656868\n",
      "Gradient Descent(812/999): loss=0.10693101667495838\n",
      "Gradient Descent(813/999): loss=0.10692876419581253\n",
      "Gradient Descent(814/999): loss=0.10692651568798123\n",
      "Gradient Descent(815/999): loss=0.10692427114035581\n",
      "Gradient Descent(816/999): loss=0.10692203054186837\n",
      "Gradient Descent(817/999): loss=0.10691979388149184\n",
      "Gradient Descent(818/999): loss=0.10691756114823951\n",
      "Gradient Descent(819/999): loss=0.10691533233116514\n",
      "Gradient Descent(820/999): loss=0.10691310741936245\n",
      "Gradient Descent(821/999): loss=0.1069108864019653\n",
      "Gradient Descent(822/999): loss=0.10690866926814718\n",
      "Gradient Descent(823/999): loss=0.10690645600712118\n",
      "Gradient Descent(824/999): loss=0.10690424660813985\n",
      "Gradient Descent(825/999): loss=0.10690204106049488\n",
      "Gradient Descent(826/999): loss=0.10689983935351706\n",
      "Gradient Descent(827/999): loss=0.10689764147657597\n",
      "Gradient Descent(828/999): loss=0.10689544741907998\n",
      "Gradient Descent(829/999): loss=0.1068932571704759\n",
      "Gradient Descent(830/999): loss=0.10689107072024882\n",
      "Gradient Descent(831/999): loss=0.10688888805792204\n",
      "Gradient Descent(832/999): loss=0.1068867091730569\n",
      "Gradient Descent(833/999): loss=0.10688453405525244\n",
      "Gradient Descent(834/999): loss=0.10688236269414537\n",
      "Gradient Descent(835/999): loss=0.10688019507940995\n",
      "Gradient Descent(836/999): loss=0.10687803120075759\n",
      "Gradient Descent(837/999): loss=0.10687587104793694\n",
      "Gradient Descent(838/999): loss=0.1068737146107336\n",
      "Gradient Descent(839/999): loss=0.10687156187896993\n",
      "Gradient Descent(840/999): loss=0.1068694128425049\n",
      "Gradient Descent(841/999): loss=0.10686726749123401\n",
      "Gradient Descent(842/999): loss=0.10686512581508904\n",
      "Gradient Descent(843/999): loss=0.10686298780403794\n",
      "Gradient Descent(844/999): loss=0.10686085344808453\n",
      "Gradient Descent(845/999): loss=0.10685872273726851\n",
      "Gradient Descent(846/999): loss=0.10685659566166528\n",
      "Gradient Descent(847/999): loss=0.1068544722113857\n",
      "Gradient Descent(848/999): loss=0.10685235237657596\n",
      "Gradient Descent(849/999): loss=0.10685023614741739\n",
      "Gradient Descent(850/999): loss=0.1068481235141265\n",
      "Gradient Descent(851/999): loss=0.10684601446695445\n",
      "Gradient Descent(852/999): loss=0.10684390899618733\n",
      "Gradient Descent(853/999): loss=0.10684180709214564\n",
      "Gradient Descent(854/999): loss=0.10683970874518436\n",
      "Gradient Descent(855/999): loss=0.10683761394569273\n",
      "Gradient Descent(856/999): loss=0.10683552268409409\n",
      "Gradient Descent(857/999): loss=0.1068334349508457\n",
      "Gradient Descent(858/999): loss=0.10683135073643865\n",
      "Gradient Descent(859/999): loss=0.10682927003139776\n",
      "Gradient Descent(860/999): loss=0.10682719282628118\n",
      "Gradient Descent(861/999): loss=0.10682511911168067\n",
      "Gradient Descent(862/999): loss=0.10682304887822103\n",
      "Gradient Descent(863/999): loss=0.10682098211656019\n",
      "Gradient Descent(864/999): loss=0.10681891881738902\n",
      "Gradient Descent(865/999): loss=0.10681685897143114\n",
      "Gradient Descent(866/999): loss=0.10681480256944284\n",
      "Gradient Descent(867/999): loss=0.1068127496022129\n",
      "Gradient Descent(868/999): loss=0.10681070006056247\n",
      "Gradient Descent(869/999): loss=0.10680865393534497\n",
      "Gradient Descent(870/999): loss=0.10680661121744577\n",
      "Gradient Descent(871/999): loss=0.10680457189778231\n",
      "Gradient Descent(872/999): loss=0.10680253596730375\n",
      "Gradient Descent(873/999): loss=0.10680050341699096\n",
      "Gradient Descent(874/999): loss=0.10679847423785635\n",
      "Gradient Descent(875/999): loss=0.10679644842094366\n",
      "Gradient Descent(876/999): loss=0.10679442595732792\n",
      "Gradient Descent(877/999): loss=0.10679240683811535\n",
      "Gradient Descent(878/999): loss=0.10679039105444305\n",
      "Gradient Descent(879/999): loss=0.10678837859747906\n",
      "Gradient Descent(880/999): loss=0.1067863694584221\n",
      "Gradient Descent(881/999): loss=0.10678436362850148\n",
      "Gradient Descent(882/999): loss=0.106782361098977\n",
      "Gradient Descent(883/999): loss=0.10678036186113883\n",
      "Gradient Descent(884/999): loss=0.10677836590630722\n",
      "Gradient Descent(885/999): loss=0.10677637322583264\n",
      "Gradient Descent(886/999): loss=0.10677438381109536\n",
      "Gradient Descent(887/999): loss=0.10677239765350562\n",
      "Gradient Descent(888/999): loss=0.10677041474450323\n",
      "Gradient Descent(889/999): loss=0.10676843507555764\n",
      "Gradient Descent(890/999): loss=0.10676645863816772\n",
      "Gradient Descent(891/999): loss=0.10676448542386163\n",
      "Gradient Descent(892/999): loss=0.10676251542419675\n",
      "Gradient Descent(893/999): loss=0.10676054863075952\n",
      "Gradient Descent(894/999): loss=0.10675858503516537\n",
      "Gradient Descent(895/999): loss=0.10675662462905847\n",
      "Gradient Descent(896/999): loss=0.10675466740411176\n",
      "Gradient Descent(897/999): loss=0.10675271335202671\n",
      "Gradient Descent(898/999): loss=0.10675076246453336\n",
      "Gradient Descent(899/999): loss=0.10674881473338992\n",
      "Gradient Descent(900/999): loss=0.10674687015038298\n",
      "Gradient Descent(901/999): loss=0.10674492870732714\n",
      "Gradient Descent(902/999): loss=0.10674299039606508\n",
      "Gradient Descent(903/999): loss=0.10674105520846724\n",
      "Gradient Descent(904/999): loss=0.10673912313643187\n",
      "Gradient Descent(905/999): loss=0.10673719417188492\n",
      "Gradient Descent(906/999): loss=0.10673526830677972\n",
      "Gradient Descent(907/999): loss=0.10673334553309713\n",
      "Gradient Descent(908/999): loss=0.10673142584284524\n",
      "Gradient Descent(909/999): loss=0.10672950922805939\n",
      "Gradient Descent(910/999): loss=0.1067275956808019\n",
      "Gradient Descent(911/999): loss=0.1067256851931621\n",
      "Gradient Descent(912/999): loss=0.10672377775725614\n",
      "Gradient Descent(913/999): loss=0.106721873365227\n",
      "Gradient Descent(914/999): loss=0.1067199720092441\n",
      "Gradient Descent(915/999): loss=0.10671807368150352\n",
      "Gradient Descent(916/999): loss=0.1067161783742277\n",
      "Gradient Descent(917/999): loss=0.10671428607966535\n",
      "Gradient Descent(918/999): loss=0.10671239679009141\n",
      "Gradient Descent(919/999): loss=0.10671051049780685\n",
      "Gradient Descent(920/999): loss=0.10670862719513866\n",
      "Gradient Descent(921/999): loss=0.10670674687443964\n",
      "Gradient Descent(922/999): loss=0.10670486952808841\n",
      "Gradient Descent(923/999): loss=0.10670299514848922\n",
      "Gradient Descent(924/999): loss=0.10670112372807183\n",
      "Gradient Descent(925/999): loss=0.10669925525929147\n",
      "Gradient Descent(926/999): loss=0.10669738973462874\n",
      "Gradient Descent(927/999): loss=0.10669552714658942\n",
      "Gradient Descent(928/999): loss=0.10669366748770444\n",
      "Gradient Descent(929/999): loss=0.10669181075052983\n",
      "Gradient Descent(930/999): loss=0.10668995692764639\n",
      "Gradient Descent(931/999): loss=0.10668810601165991\n",
      "Gradient Descent(932/999): loss=0.1066862579952008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(933/999): loss=0.10668441287092419\n",
      "Gradient Descent(934/999): loss=0.1066825706315096\n",
      "Gradient Descent(935/999): loss=0.10668073126966113\n",
      "Gradient Descent(936/999): loss=0.10667889477810705\n",
      "Gradient Descent(937/999): loss=0.10667706114959997\n",
      "Gradient Descent(938/999): loss=0.10667523037691665\n",
      "Gradient Descent(939/999): loss=0.10667340245285774\n",
      "Gradient Descent(940/999): loss=0.10667157737024795\n",
      "Gradient Descent(941/999): loss=0.10666975512193581\n",
      "Gradient Descent(942/999): loss=0.10666793570079355\n",
      "Gradient Descent(943/999): loss=0.1066661190997171\n",
      "Gradient Descent(944/999): loss=0.1066643053116259\n",
      "Gradient Descent(945/999): loss=0.10666249432946286\n",
      "Gradient Descent(946/999): loss=0.10666068614619426\n",
      "Gradient Descent(947/999): loss=0.10665888075480962\n",
      "Gradient Descent(948/999): loss=0.10665707814832168\n",
      "Gradient Descent(949/999): loss=0.10665527831976628\n",
      "Gradient Descent(950/999): loss=0.10665348126220213\n",
      "Gradient Descent(951/999): loss=0.10665168696871101\n",
      "Gradient Descent(952/999): loss=0.10664989543239733\n",
      "Gradient Descent(953/999): loss=0.1066481066463884\n",
      "Gradient Descent(954/999): loss=0.10664632060383397\n",
      "Gradient Descent(955/999): loss=0.10664453729790646\n",
      "Gradient Descent(956/999): loss=0.1066427567218007\n",
      "Gradient Descent(957/999): loss=0.10664097886873386\n",
      "Gradient Descent(958/999): loss=0.10663920373194537\n",
      "Gradient Descent(959/999): loss=0.10663743130469684\n",
      "Gradient Descent(960/999): loss=0.10663566158027203\n",
      "Gradient Descent(961/999): loss=0.10663389455197665\n",
      "Gradient Descent(962/999): loss=0.10663213021313826\n",
      "Gradient Descent(963/999): loss=0.10663036855710642\n",
      "Gradient Descent(964/999): loss=0.10662860957725226\n",
      "Gradient Descent(965/999): loss=0.10662685326696866\n",
      "Gradient Descent(966/999): loss=0.10662509961967007\n",
      "Gradient Descent(967/999): loss=0.10662334862879239\n",
      "Gradient Descent(968/999): loss=0.10662160028779295\n",
      "Gradient Descent(969/999): loss=0.10661985459015036\n",
      "Gradient Descent(970/999): loss=0.10661811152936451\n",
      "Gradient Descent(971/999): loss=0.10661637109895643\n",
      "Gradient Descent(972/999): loss=0.10661463329246815\n",
      "Gradient Descent(973/999): loss=0.10661289810346279\n",
      "Gradient Descent(974/999): loss=0.1066111655255243\n",
      "Gradient Descent(975/999): loss=0.10660943555225745\n",
      "Gradient Descent(976/999): loss=0.10660770817728778\n",
      "Gradient Descent(977/999): loss=0.10660598339426144\n",
      "Gradient Descent(978/999): loss=0.10660426119684524\n",
      "Gradient Descent(979/999): loss=0.10660254157872635\n",
      "Gradient Descent(980/999): loss=0.10660082453361254\n",
      "Gradient Descent(981/999): loss=0.1065991100552317\n",
      "Gradient Descent(982/999): loss=0.10659739813733214\n",
      "Gradient Descent(983/999): loss=0.10659568877368229\n",
      "Gradient Descent(984/999): loss=0.10659398195807065\n",
      "Gradient Descent(985/999): loss=0.10659227768430583\n",
      "Gradient Descent(986/999): loss=0.10659057594621624\n",
      "Gradient Descent(987/999): loss=0.10658887673765027\n",
      "Gradient Descent(988/999): loss=0.10658718005247608\n",
      "Gradient Descent(989/999): loss=0.10658548588458151\n",
      "Gradient Descent(990/999): loss=0.10658379422787405\n",
      "Gradient Descent(991/999): loss=0.10658210507628071\n",
      "Gradient Descent(992/999): loss=0.10658041842374805\n",
      "Gradient Descent(993/999): loss=0.10657873426424196\n",
      "Gradient Descent(994/999): loss=0.10657705259174778\n",
      "Gradient Descent(995/999): loss=0.10657537340026998\n",
      "Gradient Descent(996/999): loss=0.10657369668383226\n",
      "Gradient Descent(997/999): loss=0.10657202243647745\n",
      "Gradient Descent(998/999): loss=0.10657035065226737\n",
      "Gradient Descent(999/999): loss=0.10656868132528284\n",
      "Gradient Descent(0/999): loss=0.23372288779265524\n",
      "Gradient Descent(1/999): loss=0.12563579853095694\n",
      "Gradient Descent(2/999): loss=0.12467744710985239\n",
      "Gradient Descent(3/999): loss=0.12434647980196692\n",
      "Gradient Descent(4/999): loss=0.1240375447491014\n",
      "Gradient Descent(5/999): loss=0.12374323179689267\n",
      "Gradient Descent(6/999): loss=0.12346083402303897\n",
      "Gradient Descent(7/999): loss=0.1231885376373575\n",
      "Gradient Descent(8/999): loss=0.12292509586303632\n",
      "Gradient Descent(9/999): loss=0.12266963038646993\n",
      "Gradient Descent(10/999): loss=0.12242150320111572\n",
      "Gradient Descent(11/999): loss=0.12218023381498555\n",
      "Gradient Descent(12/999): loss=0.12194544573554669\n",
      "Gradient Descent(13/999): loss=0.12171683185135419\n",
      "Gradient Descent(14/999): loss=0.12149413201133454\n",
      "Gradient Descent(15/999): loss=0.12127711847848884\n",
      "Gradient Descent(16/999): loss=0.12106558646799644\n",
      "Gradient Descent(17/999): loss=0.12085934796913189\n",
      "Gradient Descent(18/999): loss=0.12065822768892427\n",
      "Gradient Descent(19/999): loss=0.12046206036754403\n",
      "Gradient Descent(20/999): loss=0.1202706889813185\n",
      "Gradient Descent(21/999): loss=0.12008396352088543\n",
      "Gradient Descent(22/999): loss=0.11990174014273997\n",
      "Gradient Descent(23/999): loss=0.11972388056390186\n",
      "Gradient Descent(24/999): loss=0.11955025161555514\n",
      "Gradient Descent(25/999): loss=0.11938072490128215\n",
      "Gradient Descent(26/999): loss=0.11921517652472638\n",
      "Gradient Descent(27/999): loss=0.1190534868639218\n",
      "Gradient Descent(28/999): loss=0.11889554037753103\n",
      "Gradient Descent(29/999): loss=0.11874122543340392\n",
      "Gradient Descent(30/999): loss=0.1185904341532066\n",
      "Gradient Descent(31/999): loss=0.11844306226902628\n",
      "Gradient Descent(32/999): loss=0.11829900898925276\n",
      "Gradient Descent(33/999): loss=0.11815817687193673\n",
      "Gradient Descent(34/999): loss=0.118020471704411\n",
      "Gradient Descent(35/999): loss=0.1178858023883386\n",
      "Gradient Descent(36/999): loss=0.1177540808295974\n",
      "Gradient Descent(37/999): loss=0.11762522183257318\n",
      "Gradient Descent(38/999): loss=0.11749914299853718\n",
      "Gradient Descent(39/999): loss=0.11737576462785486\n",
      "Gradient Descent(40/999): loss=0.11725500962581868\n",
      "Gradient Descent(41/999): loss=0.11713680341192978\n",
      "Gradient Descent(42/999): loss=0.11702107383247551\n",
      "Gradient Descent(43/999): loss=0.1169077510762649\n",
      "Gradient Descent(44/999): loss=0.11679676759339569\n",
      "Gradient Descent(45/999): loss=0.11668805801693662\n",
      "Gradient Descent(46/999): loss=0.11658155908741334\n",
      "Gradient Descent(47/999): loss=0.11647720957999419\n",
      "Gradient Descent(48/999): loss=0.11637495023427573\n",
      "Gradient Descent(49/999): loss=0.11627472368657307\n",
      "Gradient Descent(50/999): loss=0.11617647440462328\n",
      "Gradient Descent(51/999): loss=0.11608014862461423\n",
      "Gradient Descent(52/999): loss=0.11598569429045467\n",
      "Gradient Descent(53/999): loss=0.11589306099520402\n",
      "Gradient Descent(54/999): loss=0.11580219992458408\n",
      "Gradient Descent(55/999): loss=0.11571306380249714\n",
      "Gradient Descent(56/999): loss=0.11562560683847808\n",
      "Gradient Descent(57/999): loss=0.11553978467701087\n",
      "Gradient Descent(58/999): loss=0.11545555434864171\n",
      "Gradient Descent(59/999): loss=0.11537287422282447\n",
      "Gradient Descent(60/999): loss=0.11529170396243567\n",
      "Gradient Descent(61/999): loss=0.11521200447989904\n",
      "Gradient Descent(62/999): loss=0.1151337378948613\n",
      "Gradient Descent(63/999): loss=0.11505686749336365\n",
      "Gradient Descent(64/999): loss=0.1149813576884551\n",
      "Gradient Descent(65/999): loss=0.11490717398219527\n",
      "Gradient Descent(66/999): loss=0.1148342829289972\n",
      "Gradient Descent(67/999): loss=0.1147626521002617\n",
      "Gradient Descent(68/999): loss=0.1146922500502568\n",
      "Gradient Descent(69/999): loss=0.11462304628319746\n",
      "Gradient Descent(70/999): loss=0.11455501122148277\n",
      "Gradient Descent(71/999): loss=0.11448811617504848\n",
      "Gradient Descent(72/999): loss=0.11442233331179522\n",
      "Gradient Descent(73/999): loss=0.11435763562905366\n",
      "Gradient Descent(74/999): loss=0.11429399692604951\n",
      "Gradient Descent(75/999): loss=0.1142313917773322\n",
      "Gradient Descent(76/999): loss=0.114169795507133\n",
      "Gradient Descent(77/999): loss=0.11410918416461913\n",
      "Gradient Descent(78/999): loss=0.11404953450001147\n",
      "Gradient Descent(79/999): loss=0.11399082394153587\n",
      "Gradient Descent(80/999): loss=0.11393303057317677\n",
      "Gradient Descent(81/999): loss=0.11387613311320571\n",
      "Gradient Descent(82/999): loss=0.11382011089345655\n",
      "Gradient Descent(83/999): loss=0.11376494383932015\n",
      "Gradient Descent(84/999): loss=0.11371061245043405\n",
      "Gradient Descent(85/999): loss=0.11365709778204092\n",
      "Gradient Descent(86/999): loss=0.11360438142699295\n",
      "Gradient Descent(87/999): loss=0.11355244549837853\n",
      "Gradient Descent(88/999): loss=0.11350127261274948\n",
      "Gradient Descent(89/999): loss=0.11345084587392686\n",
      "Gradient Descent(90/999): loss=0.11340114885736527\n",
      "Gradient Descent(91/999): loss=0.11335216559505552\n",
      "Gradient Descent(92/999): loss=0.11330388056094637\n",
      "Gradient Descent(93/999): loss=0.11325627865686723\n",
      "Gradient Descent(94/999): loss=0.11320934519893366\n",
      "Gradient Descent(95/999): loss=0.1131630659044186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(96/999): loss=0.11311742687907307\n",
      "Gradient Descent(97/999): loss=0.11307241460488007\n",
      "Gradient Descent(98/999): loss=0.11302801592822609\n",
      "Gradient Descent(99/999): loss=0.11298421804847629\n",
      "Gradient Descent(100/999): loss=0.11294100850693804\n",
      "Gradient Descent(101/999): loss=0.11289837517619984\n",
      "Gradient Descent(102/999): loss=0.11285630624983173\n",
      "Gradient Descent(103/999): loss=0.11281479023243508\n",
      "Gradient Descent(104/999): loss=0.11277381593002851\n",
      "Gradient Descent(105/999): loss=0.11273337244075927\n",
      "Gradient Descent(106/999): loss=0.11269344914592731\n",
      "Gradient Descent(107/999): loss=0.11265403570131208\n",
      "Gradient Descent(108/999): loss=0.11261512202879062\n",
      "Gradient Descent(109/999): loss=0.11257669830823723\n",
      "Gradient Descent(110/999): loss=0.11253875496969457\n",
      "Gradient Descent(111/999): loss=0.11250128268580657\n",
      "Gradient Descent(112/999): loss=0.11246427236450408\n",
      "Gradient Descent(113/999): loss=0.1124277151419346\n",
      "Gradient Descent(114/999): loss=0.11239160237562697\n",
      "Gradient Descent(115/999): loss=0.11235592563788326\n",
      "Gradient Descent(116/999): loss=0.11232067670938992\n",
      "Gradient Descent(117/999): loss=0.11228584757304008\n",
      "Gradient Descent(118/999): loss=0.11225143040796029\n",
      "Gradient Descent(119/999): loss=0.11221741758373367\n",
      "Gradient Descent(120/999): loss=0.11218380165481372\n",
      "Gradient Descent(121/999): loss=0.11215057535512113\n",
      "Gradient Descent(122/999): loss=0.11211773159281786\n",
      "Gradient Descent(123/999): loss=0.11208526344525205\n",
      "Gradient Descent(124/999): loss=0.11205316415406778\n",
      "Gradient Descent(125/999): loss=0.11202142712047461\n",
      "Gradient Descent(126/999): loss=0.11199004590066991\n",
      "Gradient Descent(127/999): loss=0.1119590142014105\n",
      "Gradient Descent(128/999): loss=0.1119283258757269\n",
      "Gradient Descent(129/999): loss=0.11189797491877648\n",
      "Gradient Descent(130/999): loss=0.11186795546382947\n",
      "Gradient Descent(131/999): loss=0.11183826177838449\n",
      "Gradient Descent(132/999): loss=0.11180888826040832\n",
      "Gradient Descent(133/999): loss=0.11177982943469583\n",
      "Gradient Descent(134/999): loss=0.11175107994934622\n",
      "Gradient Descent(135/999): loss=0.1117226345723511\n",
      "Gradient Descent(136/999): loss=0.11169448818829103\n",
      "Gradient Descent(137/999): loss=0.11166663579513655\n",
      "Gradient Descent(138/999): loss=0.1116390725011502\n",
      "Gradient Descent(139/999): loss=0.11161179352188595\n",
      "Gradient Descent(140/999): loss=0.11158479417728323\n",
      "Gradient Descent(141/999): loss=0.11155806988885136\n",
      "Gradient Descent(142/999): loss=0.11153161617694257\n",
      "Gradient Descent(143/999): loss=0.11150542865810936\n",
      "Gradient Descent(144/999): loss=0.11147950304254435\n",
      "Gradient Descent(145/999): loss=0.11145383513159914\n",
      "Gradient Descent(146/999): loss=0.11142842081537986\n",
      "Gradient Descent(147/999): loss=0.11140325607041673\n",
      "Gradient Descent(148/999): loss=0.11137833695740529\n",
      "Gradient Descent(149/999): loss=0.11135365961901629\n",
      "Gradient Descent(150/999): loss=0.111329220277773\n",
      "Gradient Descent(151/999): loss=0.11130501523399262\n",
      "Gradient Descent(152/999): loss=0.11128104086379022\n",
      "Gradient Descent(153/999): loss=0.11125729361714302\n",
      "Gradient Descent(154/999): loss=0.11123377001601302\n",
      "Gradient Descent(155/999): loss=0.11121046665252582\n",
      "Gradient Descent(156/999): loss=0.11118738018720416\n",
      "Gradient Descent(157/999): loss=0.11116450734725389\n",
      "Gradient Descent(158/999): loss=0.11114184492490116\n",
      "Gradient Descent(159/999): loss=0.11111938977577857\n",
      "Gradient Descent(160/999): loss=0.11109713881735915\n",
      "Gradient Descent(161/999): loss=0.11107508902743643\n",
      "Gradient Descent(162/999): loss=0.11105323744264878\n",
      "Gradient Descent(163/999): loss=0.11103158115704728\n",
      "Gradient Descent(164/999): loss=0.1110101173207049\n",
      "Gradient Descent(165/999): loss=0.1109888431383661\n",
      "Gradient Descent(166/999): loss=0.1109677558681355\n",
      "Gradient Descent(167/999): loss=0.1109468528202043\n",
      "Gradient Descent(168/999): loss=0.11092613135561322\n",
      "Gradient Descent(169/999): loss=0.1109055888850505\n",
      "Gradient Descent(170/999): loss=0.11088522286768455\n",
      "Gradient Descent(171/999): loss=0.11086503081002932\n",
      "Gradient Descent(172/999): loss=0.11084501026484199\n",
      "Gradient Descent(173/999): loss=0.11082515883005128\n",
      "Gradient Descent(174/999): loss=0.11080547414771624\n",
      "Gradient Descent(175/999): loss=0.1107859539030137\n",
      "Gradient Descent(176/999): loss=0.11076659582325395\n",
      "Gradient Descent(177/999): loss=0.1107473976769239\n",
      "Gradient Descent(178/999): loss=0.11072835727275616\n",
      "Gradient Descent(179/999): loss=0.11070947245882427\n",
      "Gradient Descent(180/999): loss=0.11069074112166181\n",
      "Gradient Descent(181/999): loss=0.11067216118540643\n",
      "Gradient Descent(182/999): loss=0.11065373061096634\n",
      "Gradient Descent(183/999): loss=0.11063544739520965\n",
      "Gradient Descent(184/999): loss=0.11061730957017545\n",
      "Gradient Descent(185/999): loss=0.11059931520230586\n",
      "Gradient Descent(186/999): loss=0.1105814623916986\n",
      "Gradient Descent(187/999): loss=0.11056374927137959\n",
      "Gradient Descent(188/999): loss=0.11054617400659426\n",
      "Gradient Descent(189/999): loss=0.11052873479411805\n",
      "Gradient Descent(190/999): loss=0.1105114298615845\n",
      "Gradient Descent(191/999): loss=0.11049425746683121\n",
      "Gradient Descent(192/999): loss=0.11047721589726238\n",
      "Gradient Descent(193/999): loss=0.11046030346922814\n",
      "Gradient Descent(194/999): loss=0.11044351852741963\n",
      "Gradient Descent(195/999): loss=0.11042685944427957\n",
      "Gradient Descent(196/999): loss=0.1104103246194281\n",
      "Gradient Descent(197/999): loss=0.11039391247910262\n",
      "Gradient Descent(198/999): loss=0.11037762147561231\n",
      "Gradient Descent(199/999): loss=0.11036145008680615\n",
      "Gradient Descent(200/999): loss=0.11034539681555401\n",
      "Gradient Descent(201/999): loss=0.1103294601892411\n",
      "Gradient Descent(202/999): loss=0.11031363875927447\n",
      "Gradient Descent(203/999): loss=0.1102979311006021\n",
      "Gradient Descent(204/999): loss=0.11028233581124329\n",
      "Gradient Descent(205/999): loss=0.11026685151183116\n",
      "Gradient Descent(206/999): loss=0.11025147684516551\n",
      "Gradient Descent(207/999): loss=0.11023621047577722\n",
      "Gradient Descent(208/999): loss=0.11022105108950249\n",
      "Gradient Descent(209/999): loss=0.11020599739306795\n",
      "Gradient Descent(210/999): loss=0.11019104811368481\n",
      "Gradient Descent(211/999): loss=0.11017620199865355\n",
      "Gradient Descent(212/999): loss=0.1101614578149773\n",
      "Gradient Descent(213/999): loss=0.1101468143489844\n",
      "Gradient Descent(214/999): loss=0.11013227040596024\n",
      "Gradient Descent(215/999): loss=0.11011782480978698\n",
      "Gradient Descent(216/999): loss=0.11010347640259203\n",
      "Gradient Descent(217/999): loss=0.11008922404440477\n",
      "Gradient Descent(218/999): loss=0.11007506661282061\n",
      "Gradient Descent(219/999): loss=0.11006100300267321\n",
      "Gradient Descent(220/999): loss=0.11004703212571408\n",
      "Gradient Descent(221/999): loss=0.11003315291029905\n",
      "Gradient Descent(222/999): loss=0.11001936430108226\n",
      "Gradient Descent(223/999): loss=0.11000566525871673\n",
      "Gradient Descent(224/999): loss=0.10999205475956175\n",
      "Gradient Descent(225/999): loss=0.10997853179539654\n",
      "Gradient Descent(226/999): loss=0.1099650953731405\n",
      "Gradient Descent(227/999): loss=0.10995174451457952\n",
      "Gradient Descent(228/999): loss=0.10993847825609805\n",
      "Gradient Descent(229/999): loss=0.1099252956484175\n",
      "Gradient Descent(230/999): loss=0.10991219575633963\n",
      "Gradient Descent(231/999): loss=0.10989917765849613\n",
      "Gradient Descent(232/999): loss=0.10988624044710317\n",
      "Gradient Descent(233/999): loss=0.10987338322772132\n",
      "Gradient Descent(234/999): loss=0.10986060511902056\n",
      "Gradient Descent(235/999): loss=0.10984790525255032\n",
      "Gradient Descent(236/999): loss=0.10983528277251417\n",
      "Gradient Descent(237/999): loss=0.1098227368355496\n",
      "Gradient Descent(238/999): loss=0.10981026661051196\n",
      "Gradient Descent(239/999): loss=0.10979787127826322\n",
      "Gradient Descent(240/999): loss=0.10978555003146497\n",
      "Gradient Descent(241/999): loss=0.10977330207437563\n",
      "Gradient Descent(242/999): loss=0.10976112662265203\n",
      "Gradient Descent(243/999): loss=0.10974902290315465\n",
      "Gradient Descent(244/999): loss=0.10973699015375742\n",
      "Gradient Descent(245/999): loss=0.10972502762316061\n",
      "Gradient Descent(246/999): loss=0.10971313457070815\n",
      "Gradient Descent(247/999): loss=0.10970131026620834\n",
      "Gradient Descent(248/999): loss=0.10968955398975805\n",
      "Gradient Descent(249/999): loss=0.10967786503157047\n",
      "Gradient Descent(250/999): loss=0.10966624269180655\n",
      "Gradient Descent(251/999): loss=0.10965468628040939\n",
      "Gradient Descent(252/999): loss=0.1096431951169419\n",
      "Gradient Descent(253/999): loss=0.10963176853042798\n",
      "Gradient Descent(254/999): loss=0.1096204058591965\n",
      "Gradient Descent(255/999): loss=0.10960910645072823\n",
      "Gradient Descent(256/999): loss=0.10959786966150616\n",
      "Gradient Descent(257/999): loss=0.109586694856868\n",
      "Gradient Descent(258/999): loss=0.10957558141086245\n",
      "Gradient Descent(259/999): loss=0.10956452870610711\n",
      "Gradient Descent(260/999): loss=0.1095535361336501\n",
      "Gradient Descent(261/999): loss=0.10954260309283359\n",
      "Gradient Descent(262/999): loss=0.10953172899116034\n",
      "Gradient Descent(263/999): loss=0.10952091324416241\n",
      "Gradient Descent(264/999): loss=0.10951015527527269\n",
      "Gradient Descent(265/999): loss=0.10949945451569845\n",
      "Gradient Descent(266/999): loss=0.10948881040429756\n",
      "Gradient Descent(267/999): loss=0.10947822238745672\n",
      "Gradient Descent(268/999): loss=0.10946768991897221\n",
      "Gradient Descent(269/999): loss=0.10945721245993256\n",
      "Gradient Descent(270/999): loss=0.10944678947860363\n",
      "Gradient Descent(271/999): loss=0.1094364204503156\n",
      "Gradient Descent(272/999): loss=0.10942610485735205\n",
      "Gradient Descent(273/999): loss=0.10941584218884104\n",
      "Gradient Descent(274/999): loss=0.10940563194064831\n",
      "Gradient Descent(275/999): loss=0.10939547361527213\n",
      "Gradient Descent(276/999): loss=0.10938536672174033\n",
      "Gradient Descent(277/999): loss=0.1093753107755088\n",
      "Gradient Descent(278/999): loss=0.1093653052983623\n",
      "Gradient Descent(279/999): loss=0.10935534981831653\n",
      "Gradient Descent(280/999): loss=0.10934544386952219\n",
      "Gradient Descent(281/999): loss=0.1093355869921708\n",
      "Gradient Descent(282/999): loss=0.10932577873240194\n",
      "Gradient Descent(283/999): loss=0.10931601864221224\n",
      "Gradient Descent(284/999): loss=0.10930630627936609\n",
      "Gradient Descent(285/999): loss=0.10929664120730768\n",
      "Gradient Descent(286/999): loss=0.10928702299507465\n",
      "Gradient Descent(287/999): loss=0.10927745121721322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(288/999): loss=0.10926792545369501\n",
      "Gradient Descent(289/999): loss=0.1092584452898348\n",
      "Gradient Descent(290/999): loss=0.10924901031621019\n",
      "Gradient Descent(291/999): loss=0.10923962012858243\n",
      "Gradient Descent(292/999): loss=0.10923027432781851\n",
      "Gradient Descent(293/999): loss=0.10922097251981475\n",
      "Gradient Descent(294/999): loss=0.10921171431542157\n",
      "Gradient Descent(295/999): loss=0.10920249933036962\n",
      "Gradient Descent(296/999): loss=0.10919332718519707\n",
      "Gradient Descent(297/999): loss=0.10918419750517815\n",
      "Gradient Descent(298/999): loss=0.1091751099202529\n",
      "Gradient Descent(299/999): loss=0.10916606406495816\n",
      "Gradient Descent(300/999): loss=0.10915705957835958\n",
      "Gradient Descent(301/999): loss=0.10914809610398488\n",
      "Gradient Descent(302/999): loss=0.10913917328975814\n",
      "Gradient Descent(303/999): loss=0.10913029078793511\n",
      "Gradient Descent(304/999): loss=0.10912144825504\n",
      "Gradient Descent(305/999): loss=0.10911264535180254\n",
      "Gradient Descent(306/999): loss=0.109103881743097\n",
      "Gradient Descent(307/999): loss=0.1090951570978814\n",
      "Gradient Descent(308/999): loss=0.1090864710891382\n",
      "Gradient Descent(309/999): loss=0.10907782339381586\n",
      "Gradient Descent(310/999): loss=0.10906921369277126\n",
      "Gradient Descent(311/999): loss=0.10906064167071298\n",
      "Gradient Descent(312/999): loss=0.10905210701614598\n",
      "Gradient Descent(313/999): loss=0.10904360942131633\n",
      "Gradient Descent(314/999): loss=0.10903514858215786\n",
      "Gradient Descent(315/999): loss=0.10902672419823868\n",
      "Gradient Descent(316/999): loss=0.1090183359727093\n",
      "Gradient Descent(317/999): loss=0.1090099836122512\n",
      "Gradient Descent(318/999): loss=0.10900166682702628\n",
      "Gradient Descent(319/999): loss=0.10899338533062726\n",
      "Gradient Descent(320/999): loss=0.1089851388400287\n",
      "Gradient Descent(321/999): loss=0.10897692707553892\n",
      "Gradient Descent(322/999): loss=0.10896874976075259\n",
      "Gradient Descent(323/999): loss=0.10896060662250412\n",
      "Gradient Descent(324/999): loss=0.10895249739082191\n",
      "Gradient Descent(325/999): loss=0.10894442179888293\n",
      "Gradient Descent(326/999): loss=0.10893637958296855\n",
      "Gradient Descent(327/999): loss=0.10892837048242072\n",
      "Gradient Descent(328/999): loss=0.10892039423959879\n",
      "Gradient Descent(329/999): loss=0.1089124505998374\n",
      "Gradient Descent(330/999): loss=0.10890453931140442\n",
      "Gradient Descent(331/999): loss=0.10889666012546029\n",
      "Gradient Descent(332/999): loss=0.10888881279601717\n",
      "Gradient Descent(333/999): loss=0.10888099707989951\n",
      "Gradient Descent(334/999): loss=0.10887321273670468\n",
      "Gradient Descent(335/999): loss=0.10886545952876445\n",
      "Gradient Descent(336/999): loss=0.10885773722110711\n",
      "Gradient Descent(337/999): loss=0.1088500455814199\n",
      "Gradient Descent(338/999): loss=0.1088423843800124\n",
      "Gradient Descent(339/999): loss=0.10883475338978016\n",
      "Gradient Descent(340/999): loss=0.10882715238616911\n",
      "Gradient Descent(341/999): loss=0.10881958114714041\n",
      "Gradient Descent(342/999): loss=0.10881203945313582\n",
      "Gradient Descent(343/999): loss=0.10880452708704362\n",
      "Gradient Descent(344/999): loss=0.10879704383416526\n",
      "Gradient Descent(345/999): loss=0.10878958948218206\n",
      "Gradient Descent(346/999): loss=0.10878216382112285\n",
      "Gradient Descent(347/999): loss=0.108774766643332\n",
      "Gradient Descent(348/999): loss=0.10876739774343758\n",
      "Gradient Descent(349/999): loss=0.10876005691832076\n",
      "Gradient Descent(350/999): loss=0.10875274396708476\n",
      "Gradient Descent(351/999): loss=0.10874545869102507\n",
      "Gradient Descent(352/999): loss=0.10873820089359958\n",
      "Gradient Descent(353/999): loss=0.1087309703803994\n",
      "Gradient Descent(354/999): loss=0.10872376695912009\n",
      "Gradient Descent(355/999): loss=0.10871659043953331\n",
      "Gradient Descent(356/999): loss=0.10870944063345894\n",
      "Gradient Descent(357/999): loss=0.10870231735473745\n",
      "Gradient Descent(358/999): loss=0.10869522041920297\n",
      "Gradient Descent(359/999): loss=0.10868814964465642\n",
      "Gradient Descent(360/999): loss=0.10868110485083941\n",
      "Gradient Descent(361/999): loss=0.10867408585940823\n",
      "Gradient Descent(362/999): loss=0.10866709249390831\n",
      "Gradient Descent(363/999): loss=0.1086601245797492\n",
      "Gradient Descent(364/999): loss=0.10865318194417979\n",
      "Gradient Descent(365/999): loss=0.10864626441626382\n",
      "Gradient Descent(366/999): loss=0.10863937182685598\n",
      "Gradient Descent(367/999): loss=0.10863250400857824\n",
      "Gradient Descent(368/999): loss=0.10862566079579632\n",
      "Gradient Descent(369/999): loss=0.108618842024597\n",
      "Gradient Descent(370/999): loss=0.10861204753276536\n",
      "Gradient Descent(371/999): loss=0.10860527715976244\n",
      "Gradient Descent(372/999): loss=0.10859853074670328\n",
      "Gradient Descent(373/999): loss=0.1085918081363354\n",
      "Gradient Descent(374/999): loss=0.10858510917301725\n",
      "Gradient Descent(375/999): loss=0.10857843370269737\n",
      "Gradient Descent(376/999): loss=0.10857178157289364\n",
      "Gradient Descent(377/999): loss=0.10856515263267273\n",
      "Gradient Descent(378/999): loss=0.10855854673263027\n",
      "Gradient Descent(379/999): loss=0.10855196372487079\n",
      "Gradient Descent(380/999): loss=0.1085454034629882\n",
      "Gradient Descent(381/999): loss=0.10853886580204677\n",
      "Gradient Descent(382/999): loss=0.10853235059856198\n",
      "Gradient Descent(383/999): loss=0.10852585771048187\n",
      "Gradient Descent(384/999): loss=0.10851938699716861\n",
      "Gradient Descent(385/999): loss=0.10851293831938037\n",
      "Gradient Descent(386/999): loss=0.10850651153925356\n",
      "Gradient Descent(387/999): loss=0.10850010652028495\n",
      "Gradient Descent(388/999): loss=0.1084937231273145\n",
      "Gradient Descent(389/999): loss=0.10848736122650825\n",
      "Gradient Descent(390/999): loss=0.1084810206853412\n",
      "Gradient Descent(391/999): loss=0.108474701372581\n",
      "Gradient Descent(392/999): loss=0.10846840315827136\n",
      "Gradient Descent(393/999): loss=0.10846212591371601\n",
      "Gradient Descent(394/999): loss=0.10845586951146266\n",
      "Gradient Descent(395/999): loss=0.10844963382528733\n",
      "Gradient Descent(396/999): loss=0.10844341873017904\n",
      "Gradient Descent(397/999): loss=0.10843722410232434\n",
      "Gradient Descent(398/999): loss=0.10843104981909236\n",
      "Gradient Descent(399/999): loss=0.1084248957590201\n",
      "Gradient Descent(400/999): loss=0.10841876180179771\n",
      "Gradient Descent(401/999): loss=0.10841264782825406\n",
      "Gradient Descent(402/999): loss=0.10840655372034269\n",
      "Gradient Descent(403/999): loss=0.10840047936112765\n",
      "Gradient Descent(404/999): loss=0.10839442463476992\n",
      "Gradient Descent(405/999): loss=0.10838838942651365\n",
      "Gradient Descent(406/999): loss=0.10838237362267272\n",
      "Gradient Descent(407/999): loss=0.10837637711061776\n",
      "Gradient Descent(408/999): loss=0.10837039977876288\n",
      "Gradient Descent(409/999): loss=0.1083644415165529\n",
      "Gradient Descent(410/999): loss=0.10835850221445081\n",
      "Gradient Descent(411/999): loss=0.10835258176392519\n",
      "Gradient Descent(412/999): loss=0.10834668005743775\n",
      "Gradient Descent(413/999): loss=0.10834079698843156\n",
      "Gradient Descent(414/999): loss=0.10833493245131878\n",
      "Gradient Descent(415/999): loss=0.10832908634146897\n",
      "Gradient Descent(416/999): loss=0.10832325855519745\n",
      "Gradient Descent(417/999): loss=0.10831744898975389\n",
      "Gradient Descent(418/999): loss=0.1083116575433109\n",
      "Gradient Descent(419/999): loss=0.10830588411495301\n",
      "Gradient Descent(420/999): loss=0.1083001286046655\n",
      "Gradient Descent(421/999): loss=0.10829439091332377\n",
      "Gradient Descent(422/999): loss=0.1082886709426825\n",
      "Gradient Descent(423/999): loss=0.10828296859536517\n",
      "Gradient Descent(424/999): loss=0.10827728377485366\n",
      "Gradient Descent(425/999): loss=0.10827161638547794\n",
      "Gradient Descent(426/999): loss=0.10826596633240612\n",
      "Gradient Descent(427/999): loss=0.1082603335216343\n",
      "Gradient Descent(428/999): loss=0.10825471785997694\n",
      "Gradient Descent(429/999): loss=0.1082491192550569\n",
      "Gradient Descent(430/999): loss=0.10824353761529612\n",
      "Gradient Descent(431/999): loss=0.10823797284990615\n",
      "Gradient Descent(432/999): loss=0.10823242486887871\n",
      "Gradient Descent(433/999): loss=0.10822689358297666\n",
      "Gradient Descent(434/999): loss=0.10822137890372499\n",
      "Gradient Descent(435/999): loss=0.10821588074340174\n",
      "Gradient Descent(436/999): loss=0.10821039901502928\n",
      "Gradient Descent(437/999): loss=0.10820493363236572\n",
      "Gradient Descent(438/999): loss=0.10819948450989625\n",
      "Gradient Descent(439/999): loss=0.10819405156282469\n",
      "Gradient Descent(440/999): loss=0.10818863470706529\n",
      "Gradient Descent(441/999): loss=0.10818323385923428\n",
      "Gradient Descent(442/999): loss=0.10817784893664208\n",
      "Gradient Descent(443/999): loss=0.10817247985728505\n",
      "Gradient Descent(444/999): loss=0.10816712653983776\n",
      "Gradient Descent(445/999): loss=0.10816178890364514\n",
      "Gradient Descent(446/999): loss=0.10815646686871482\n",
      "Gradient Descent(447/999): loss=0.10815116035570962\n",
      "Gradient Descent(448/999): loss=0.10814586928594008\n",
      "Gradient Descent(449/999): loss=0.10814059358135691\n",
      "Gradient Descent(450/999): loss=0.10813533316454406\n",
      "Gradient Descent(451/999): loss=0.10813008795871129\n",
      "Gradient Descent(452/999): loss=0.10812485788768725\n",
      "Gradient Descent(453/999): loss=0.10811964287591232\n",
      "Gradient Descent(454/999): loss=0.10811444284843201\n",
      "Gradient Descent(455/999): loss=0.10810925773088995\n",
      "Gradient Descent(456/999): loss=0.10810408744952121\n",
      "Gradient Descent(457/999): loss=0.1080989319311459\n",
      "Gradient Descent(458/999): loss=0.10809379110316238\n",
      "Gradient Descent(459/999): loss=0.10808866489354095\n",
      "Gradient Descent(460/999): loss=0.10808355323081754\n",
      "Gradient Descent(461/999): loss=0.1080784560440874\n",
      "Gradient Descent(462/999): loss=0.10807337326299893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(463/999): loss=0.10806830481774762\n",
      "Gradient Descent(464/999): loss=0.10806325063906995\n",
      "Gradient Descent(465/999): loss=0.10805821065823751\n",
      "Gradient Descent(466/999): loss=0.10805318480705123\n",
      "Gradient Descent(467/999): loss=0.10804817301783536\n",
      "Gradient Descent(468/999): loss=0.10804317522343208\n",
      "Gradient Descent(469/999): loss=0.10803819135719558\n",
      "Gradient Descent(470/999): loss=0.10803322135298675\n",
      "Gradient Descent(471/999): loss=0.10802826514516749\n",
      "Gradient Descent(472/999): loss=0.10802332266859543\n",
      "Gradient Descent(473/999): loss=0.10801839385861862\n",
      "Gradient Descent(474/999): loss=0.1080134786510701\n",
      "Gradient Descent(475/999): loss=0.10800857698226289\n",
      "Gradient Descent(476/999): loss=0.10800368878898467\n",
      "Gradient Descent(477/999): loss=0.10799881400849297\n",
      "Gradient Descent(478/999): loss=0.10799395257850991\n",
      "Gradient Descent(479/999): loss=0.10798910443721738\n",
      "Gradient Descent(480/999): loss=0.10798426952325221\n",
      "Gradient Descent(481/999): loss=0.1079794477757013\n",
      "Gradient Descent(482/999): loss=0.10797463913409693\n",
      "Gradient Descent(483/999): loss=0.10796984353841199\n",
      "Gradient Descent(484/999): loss=0.1079650609290555\n",
      "Gradient Descent(485/999): loss=0.1079602912468678\n",
      "Gradient Descent(486/999): loss=0.10795553443311644\n",
      "Gradient Descent(487/999): loss=0.10795079042949136\n",
      "Gradient Descent(488/999): loss=0.10794605917810066\n",
      "Gradient Descent(489/999): loss=0.10794134062146626\n",
      "Gradient Descent(490/999): loss=0.10793663470251967\n",
      "Gradient Descent(491/999): loss=0.10793194136459766\n",
      "Gradient Descent(492/999): loss=0.1079272605514382\n",
      "Gradient Descent(493/999): loss=0.10792259220717622\n",
      "Gradient Descent(494/999): loss=0.10791793627633971\n",
      "Gradient Descent(495/999): loss=0.10791329270384556\n",
      "Gradient Descent(496/999): loss=0.10790866143499575\n",
      "Gradient Descent(497/999): loss=0.10790404241547327\n",
      "Gradient Descent(498/999): loss=0.10789943559133837\n",
      "Gradient Descent(499/999): loss=0.10789484090902474\n",
      "Gradient Descent(500/999): loss=0.10789025831533573\n",
      "Gradient Descent(501/999): loss=0.10788568775744069\n",
      "Gradient Descent(502/999): loss=0.10788112918287111\n",
      "Gradient Descent(503/999): loss=0.10787658253951735\n",
      "Gradient Descent(504/999): loss=0.1078720477756246\n",
      "Gradient Descent(505/999): loss=0.10786752483978986\n",
      "Gradient Descent(506/999): loss=0.10786301368095798\n",
      "Gradient Descent(507/999): loss=0.10785851424841854\n",
      "Gradient Descent(508/999): loss=0.10785402649180234\n",
      "Gradient Descent(509/999): loss=0.10784955036107804\n",
      "Gradient Descent(510/999): loss=0.10784508580654874\n",
      "Gradient Descent(511/999): loss=0.10784063277884898\n",
      "Gradient Descent(512/999): loss=0.10783619122894117\n",
      "Gradient Descent(513/999): loss=0.10783176110811266\n",
      "Gradient Descent(514/999): loss=0.10782734236797245\n",
      "Gradient Descent(515/999): loss=0.10782293496044813\n",
      "Gradient Descent(516/999): loss=0.10781853883778274\n",
      "Gradient Descent(517/999): loss=0.10781415395253183\n",
      "Gradient Descent(518/999): loss=0.10780978025756029\n",
      "Gradient Descent(519/999): loss=0.10780541770603962\n",
      "Gradient Descent(520/999): loss=0.10780106625144478\n",
      "Gradient Descent(521/999): loss=0.10779672584755143\n",
      "Gradient Descent(522/999): loss=0.107792396448433\n",
      "Gradient Descent(523/999): loss=0.1077880780084579\n",
      "Gradient Descent(524/999): loss=0.10778377048228675\n",
      "Gradient Descent(525/999): loss=0.10777947382486962\n",
      "Gradient Descent(526/999): loss=0.10777518799144321\n",
      "Gradient Descent(527/999): loss=0.10777091293752829\n",
      "Gradient Descent(528/999): loss=0.10776664861892706\n",
      "Gradient Descent(529/999): loss=0.1077623949917203\n",
      "Gradient Descent(530/999): loss=0.10775815201226509\n",
      "Gradient Descent(531/999): loss=0.10775391963719201\n",
      "Gradient Descent(532/999): loss=0.10774969782340273\n",
      "Gradient Descent(533/999): loss=0.10774548652806745\n",
      "Gradient Descent(534/999): loss=0.10774128570862246\n",
      "Gradient Descent(535/999): loss=0.10773709532276776\n",
      "Gradient Descent(536/999): loss=0.10773291532846445\n",
      "Gradient Descent(537/999): loss=0.10772874568393258\n",
      "Gradient Descent(538/999): loss=0.10772458634764864\n",
      "Gradient Descent(539/999): loss=0.10772043727834334\n",
      "Gradient Descent(540/999): loss=0.1077162984349992\n",
      "Gradient Descent(541/999): loss=0.10771216977684835\n",
      "Gradient Descent(542/999): loss=0.10770805126337028\n",
      "Gradient Descent(543/999): loss=0.10770394285428964\n",
      "Gradient Descent(544/999): loss=0.10769984450957387\n",
      "Gradient Descent(545/999): loss=0.1076957561894314\n",
      "Gradient Descent(546/999): loss=0.10769167785430915\n",
      "Gradient Descent(547/999): loss=0.10768760946489056\n",
      "Gradient Descent(548/999): loss=0.1076835509820935\n",
      "Gradient Descent(549/999): loss=0.10767950236706822\n",
      "Gradient Descent(550/999): loss=0.10767546358119522\n",
      "Gradient Descent(551/999): loss=0.10767143458608334\n",
      "Gradient Descent(552/999): loss=0.10766741534356768\n",
      "Gradient Descent(553/999): loss=0.10766340581570764\n",
      "Gradient Descent(554/999): loss=0.10765940596478506\n",
      "Gradient Descent(555/999): loss=0.10765541575330213\n",
      "Gradient Descent(556/999): loss=0.10765143514397962\n",
      "Gradient Descent(557/999): loss=0.10764746409975497\n",
      "Gradient Descent(558/999): loss=0.10764350258378033\n",
      "Gradient Descent(559/999): loss=0.10763955055942083\n",
      "Gradient Descent(560/999): loss=0.10763560799025275\n",
      "Gradient Descent(561/999): loss=0.10763167484006167\n",
      "Gradient Descent(562/999): loss=0.10762775107284069\n",
      "Gradient Descent(563/999): loss=0.10762383665278875\n",
      "Gradient Descent(564/999): loss=0.10761993154430877\n",
      "Gradient Descent(565/999): loss=0.10761603571200606\n",
      "Gradient Descent(566/999): loss=0.10761214912068649\n",
      "Gradient Descent(567/999): loss=0.10760827173535494\n",
      "Gradient Descent(568/999): loss=0.1076044035212135\n",
      "Gradient Descent(569/999): loss=0.10760054444366003\n",
      "Gradient Descent(570/999): loss=0.10759669446828622\n",
      "Gradient Descent(571/999): loss=0.10759285356087626\n",
      "Gradient Descent(572/999): loss=0.10758902168740525\n",
      "Gradient Descent(573/999): loss=0.10758519881403737\n",
      "Gradient Descent(574/999): loss=0.10758138490712461\n",
      "Gradient Descent(575/999): loss=0.10757757993320499\n",
      "Gradient Descent(576/999): loss=0.10757378385900138\n",
      "Gradient Descent(577/999): loss=0.10756999665141953\n",
      "Gradient Descent(578/999): loss=0.10756621827754702\n",
      "Gradient Descent(579/999): loss=0.10756244870465154\n",
      "Gradient Descent(580/999): loss=0.10755868790017949\n",
      "Gradient Descent(581/999): loss=0.10755493583175456\n",
      "Gradient Descent(582/999): loss=0.1075511924671763\n",
      "Gradient Descent(583/999): loss=0.10754745777441875\n",
      "Gradient Descent(584/999): loss=0.10754373172162897\n",
      "Gradient Descent(585/999): loss=0.1075400142771257\n",
      "Gradient Descent(586/999): loss=0.10753630540939804\n",
      "Gradient Descent(587/999): loss=0.10753260508710405\n",
      "Gradient Descent(588/999): loss=0.10752891327906944\n",
      "Gradient Descent(589/999): loss=0.10752522995428619\n",
      "Gradient Descent(590/999): loss=0.10752155508191137\n",
      "Gradient Descent(591/999): loss=0.10751788863126577\n",
      "Gradient Descent(592/999): loss=0.1075142305718326\n",
      "Gradient Descent(593/999): loss=0.10751058087325631\n",
      "Gradient Descent(594/999): loss=0.10750693950534122\n",
      "Gradient Descent(595/999): loss=0.10750330643805045\n",
      "Gradient Descent(596/999): loss=0.10749968164150453\n",
      "Gradient Descent(597/999): loss=0.10749606508598027\n",
      "Gradient Descent(598/999): loss=0.10749245674190958\n",
      "Gradient Descent(599/999): loss=0.10748885657987828\n",
      "Gradient Descent(600/999): loss=0.10748526457062484\n",
      "Gradient Descent(601/999): loss=0.10748168068503933\n",
      "Gradient Descent(602/999): loss=0.10747810489416225\n",
      "Gradient Descent(603/999): loss=0.10747453716918333\n",
      "Gradient Descent(604/999): loss=0.10747097748144045\n",
      "Gradient Descent(605/999): loss=0.1074674258024186\n",
      "Gradient Descent(606/999): loss=0.1074638821037486\n",
      "Gradient Descent(607/999): loss=0.10746034635720622\n",
      "Gradient Descent(608/999): loss=0.1074568185347109\n",
      "Gradient Descent(609/999): loss=0.10745329860832484\n",
      "Gradient Descent(610/999): loss=0.10744978655025186\n",
      "Gradient Descent(611/999): loss=0.10744628233283637\n",
      "Gradient Descent(612/999): loss=0.10744278592856243\n",
      "Gradient Descent(613/999): loss=0.1074392973100525\n",
      "Gradient Descent(614/999): loss=0.10743581645006663\n",
      "Gradient Descent(615/999): loss=0.10743234332150142\n",
      "Gradient Descent(616/999): loss=0.107428877897389\n",
      "Gradient Descent(617/999): loss=0.10742542015089594\n",
      "Gradient Descent(618/999): loss=0.10742197005532252\n",
      "Gradient Descent(619/999): loss=0.10741852758410152\n",
      "Gradient Descent(620/999): loss=0.10741509271079738\n",
      "Gradient Descent(621/999): loss=0.10741166540910536\n",
      "Gradient Descent(622/999): loss=0.10740824565285036\n",
      "Gradient Descent(623/999): loss=0.10740483341598617\n",
      "Gradient Descent(624/999): loss=0.10740142867259451\n",
      "Gradient Descent(625/999): loss=0.10739803139688418\n",
      "Gradient Descent(626/999): loss=0.10739464156318995\n",
      "Gradient Descent(627/999): loss=0.107391259145972\n",
      "Gradient Descent(628/999): loss=0.10738788411981474\n",
      "Gradient Descent(629/999): loss=0.10738451645942614\n",
      "Gradient Descent(630/999): loss=0.10738115613963664\n",
      "Gradient Descent(631/999): loss=0.10737780313539867\n",
      "Gradient Descent(632/999): loss=0.10737445742178538\n",
      "Gradient Descent(633/999): loss=0.10737111897399003\n",
      "Gradient Descent(634/999): loss=0.10736778776732515\n",
      "Gradient Descent(635/999): loss=0.10736446377722177\n",
      "Gradient Descent(636/999): loss=0.10736114697922831\n",
      "Gradient Descent(637/999): loss=0.10735783734901024\n",
      "Gradient Descent(638/999): loss=0.10735453486234885\n",
      "Gradient Descent(639/999): loss=0.10735123949514064\n",
      "Gradient Descent(640/999): loss=0.10734795122339666\n",
      "Gradient Descent(641/999): loss=0.10734467002324148\n",
      "Gradient Descent(642/999): loss=0.1073413958709126\n",
      "Gradient Descent(643/999): loss=0.10733812874275961\n",
      "Gradient Descent(644/999): loss=0.1073348686152435\n",
      "Gradient Descent(645/999): loss=0.10733161546493585\n",
      "Gradient Descent(646/999): loss=0.10732836926851805\n",
      "Gradient Descent(647/999): loss=0.10732513000278074\n",
      "Gradient Descent(648/999): loss=0.10732189764462285\n",
      "Gradient Descent(649/999): loss=0.1073186721710511\n",
      "Gradient Descent(650/999): loss=0.10731545355917912\n",
      "Gradient Descent(651/999): loss=0.10731224178622684\n",
      "Gradient Descent(652/999): loss=0.1073090368295198\n",
      "Gradient Descent(653/999): loss=0.10730583866648827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(654/999): loss=0.10730264727466689\n",
      "Gradient Descent(655/999): loss=0.1072994626316937\n",
      "Gradient Descent(656/999): loss=0.10729628471530953\n",
      "Gradient Descent(657/999): loss=0.10729311350335752\n",
      "Gradient Descent(658/999): loss=0.1072899489737822\n",
      "Gradient Descent(659/999): loss=0.10728679110462896\n",
      "Gradient Descent(660/999): loss=0.10728363987404341\n",
      "Gradient Descent(661/999): loss=0.10728049526027067\n",
      "Gradient Descent(662/999): loss=0.10727735724165482\n",
      "Gradient Descent(663/999): loss=0.10727422579663819\n",
      "Gradient Descent(664/999): loss=0.10727110090376075\n",
      "Gradient Descent(665/999): loss=0.1072679825416595\n",
      "Gradient Descent(666/999): loss=0.10726487068906791\n",
      "Gradient Descent(667/999): loss=0.10726176532481517\n",
      "Gradient Descent(668/999): loss=0.10725866642782565\n",
      "Gradient Descent(669/999): loss=0.10725557397711842\n",
      "Gradient Descent(670/999): loss=0.10725248795180652\n",
      "Gradient Descent(671/999): loss=0.10724940833109628\n",
      "Gradient Descent(672/999): loss=0.10724633509428705\n",
      "Gradient Descent(673/999): loss=0.1072432682207703\n",
      "Gradient Descent(674/999): loss=0.10724020769002918\n",
      "Gradient Descent(675/999): loss=0.10723715348163805\n",
      "Gradient Descent(676/999): loss=0.10723410557526164\n",
      "Gradient Descent(677/999): loss=0.1072310639506548\n",
      "Gradient Descent(678/999): loss=0.10722802858766178\n",
      "Gradient Descent(679/999): loss=0.10722499946621566\n",
      "Gradient Descent(680/999): loss=0.10722197656633795\n",
      "Gradient Descent(681/999): loss=0.10721895986813781\n",
      "Gradient Descent(682/999): loss=0.10721594935181182\n",
      "Gradient Descent(683/999): loss=0.10721294499764315\n",
      "Gradient Descent(684/999): loss=0.10720994678600126\n",
      "Gradient Descent(685/999): loss=0.10720695469734126\n",
      "Gradient Descent(686/999): loss=0.10720396871220342\n",
      "Gradient Descent(687/999): loss=0.10720098881121262\n",
      "Gradient Descent(688/999): loss=0.10719801497507801\n",
      "Gradient Descent(689/999): loss=0.10719504718459222\n",
      "Gradient Descent(690/999): loss=0.10719208542063109\n",
      "Gradient Descent(691/999): loss=0.10718912966415313\n",
      "Gradient Descent(692/999): loss=0.1071861798961989\n",
      "Gradient Descent(693/999): loss=0.10718323609789075\n",
      "Gradient Descent(694/999): loss=0.10718029825043213\n",
      "Gradient Descent(695/999): loss=0.10717736633510713\n",
      "Gradient Descent(696/999): loss=0.10717444033328023\n",
      "Gradient Descent(697/999): loss=0.10717152022639549\n",
      "Gradient Descent(698/999): loss=0.10716860599597633\n",
      "Gradient Descent(699/999): loss=0.10716569762362498\n",
      "Gradient Descent(700/999): loss=0.10716279509102204\n",
      "Gradient Descent(701/999): loss=0.10715989837992598\n",
      "Gradient Descent(702/999): loss=0.10715700747217274\n",
      "Gradient Descent(703/999): loss=0.10715412234967527\n",
      "Gradient Descent(704/999): loss=0.10715124299442304\n",
      "Gradient Descent(705/999): loss=0.10714836938848163\n",
      "Gradient Descent(706/999): loss=0.10714550151399234\n",
      "Gradient Descent(707/999): loss=0.10714263935317167\n",
      "Gradient Descent(708/999): loss=0.10713978288831096\n",
      "Gradient Descent(709/999): loss=0.10713693210177586\n",
      "Gradient Descent(710/999): loss=0.10713408697600604\n",
      "Gradient Descent(711/999): loss=0.10713124749351476\n",
      "Gradient Descent(712/999): loss=0.10712841363688821\n",
      "Gradient Descent(713/999): loss=0.10712558538878547\n",
      "Gradient Descent(714/999): loss=0.10712276273193783\n",
      "Gradient Descent(715/999): loss=0.10711994564914845\n",
      "Gradient Descent(716/999): loss=0.107117134123292\n",
      "Gradient Descent(717/999): loss=0.10711432813731417\n",
      "Gradient Descent(718/999): loss=0.10711152767423149\n",
      "Gradient Descent(719/999): loss=0.10710873271713052\n",
      "Gradient Descent(720/999): loss=0.10710594324916792\n",
      "Gradient Descent(721/999): loss=0.10710315925356975\n",
      "Gradient Descent(722/999): loss=0.10710038071363125\n",
      "Gradient Descent(723/999): loss=0.10709760761271633\n",
      "Gradient Descent(724/999): loss=0.1070948399342573\n",
      "Gradient Descent(725/999): loss=0.1070920776617544\n",
      "Gradient Descent(726/999): loss=0.10708932077877549\n",
      "Gradient Descent(727/999): loss=0.1070865692689557\n",
      "Gradient Descent(728/999): loss=0.10708382311599693\n",
      "Gradient Descent(729/999): loss=0.10708108230366767\n",
      "Gradient Descent(730/999): loss=0.10707834681580247\n",
      "Gradient Descent(731/999): loss=0.1070756166363017\n",
      "Gradient Descent(732/999): loss=0.10707289174913112\n",
      "Gradient Descent(733/999): loss=0.1070701721383215\n",
      "Gradient Descent(734/999): loss=0.10706745778796843\n",
      "Gradient Descent(735/999): loss=0.10706474868223174\n",
      "Gradient Descent(736/999): loss=0.10706204480533535\n",
      "Gradient Descent(737/999): loss=0.1070593461415668\n",
      "Gradient Descent(738/999): loss=0.10705665267527698\n",
      "Gradient Descent(739/999): loss=0.10705396439087973\n",
      "Gradient Descent(740/999): loss=0.10705128127285153\n",
      "Gradient Descent(741/999): loss=0.10704860330573124\n",
      "Gradient Descent(742/999): loss=0.1070459304741196\n",
      "Gradient Descent(743/999): loss=0.10704326276267911\n",
      "Gradient Descent(744/999): loss=0.10704060015613347\n",
      "Gradient Descent(745/999): loss=0.1070379426392675\n",
      "Gradient Descent(746/999): loss=0.10703529019692658\n",
      "Gradient Descent(747/999): loss=0.10703264281401656\n",
      "Gradient Descent(748/999): loss=0.10703000047550323\n",
      "Gradient Descent(749/999): loss=0.10702736316641215\n",
      "Gradient Descent(750/999): loss=0.10702473087182827\n",
      "Gradient Descent(751/999): loss=0.10702210357689569\n",
      "Gradient Descent(752/999): loss=0.10701948126681722\n",
      "Gradient Descent(753/999): loss=0.1070168639268542\n",
      "Gradient Descent(754/999): loss=0.10701425154232613\n",
      "Gradient Descent(755/999): loss=0.10701164409861036\n",
      "Gradient Descent(756/999): loss=0.1070090415811419\n",
      "Gradient Descent(757/999): loss=0.10700644397541298\n",
      "Gradient Descent(758/999): loss=0.10700385126697279\n",
      "Gradient Descent(759/999): loss=0.10700126344142727\n",
      "Gradient Descent(760/999): loss=0.1069986804844387\n",
      "Gradient Descent(761/999): loss=0.1069961023817255\n",
      "Gradient Descent(762/999): loss=0.10699352911906194\n",
      "Gradient Descent(763/999): loss=0.1069909606822777\n",
      "Gradient Descent(764/999): loss=0.10698839705725789\n",
      "Gradient Descent(765/999): loss=0.1069858382299424\n",
      "Gradient Descent(766/999): loss=0.10698328418632598\n",
      "Gradient Descent(767/999): loss=0.10698073491245769\n",
      "Gradient Descent(768/999): loss=0.10697819039444077\n",
      "Gradient Descent(769/999): loss=0.10697565061843224\n",
      "Gradient Descent(770/999): loss=0.10697311557064285\n",
      "Gradient Descent(771/999): loss=0.10697058523733656\n",
      "Gradient Descent(772/999): loss=0.10696805960483038\n",
      "Gradient Descent(773/999): loss=0.10696553865949422\n",
      "Gradient Descent(774/999): loss=0.10696302238775032\n",
      "Gradient Descent(775/999): loss=0.10696051077607338\n",
      "Gradient Descent(776/999): loss=0.10695800381098997\n",
      "Gradient Descent(777/999): loss=0.1069555014790784\n",
      "Gradient Descent(778/999): loss=0.10695300376696851\n",
      "Gradient Descent(779/999): loss=0.10695051066134133\n",
      "Gradient Descent(780/999): loss=0.10694802214892887\n",
      "Gradient Descent(781/999): loss=0.10694553821651384\n",
      "Gradient Descent(782/999): loss=0.10694305885092946\n",
      "Gradient Descent(783/999): loss=0.10694058403905911\n",
      "Gradient Descent(784/999): loss=0.10693811376783617\n",
      "Gradient Descent(785/999): loss=0.10693564802424374\n",
      "Gradient Descent(786/999): loss=0.10693318679531436\n",
      "Gradient Descent(787/999): loss=0.10693073006812989\n",
      "Gradient Descent(788/999): loss=0.10692827782982109\n",
      "Gradient Descent(789/999): loss=0.1069258300675676\n",
      "Gradient Descent(790/999): loss=0.10692338676859739\n",
      "Gradient Descent(791/999): loss=0.10692094792018686\n",
      "Gradient Descent(792/999): loss=0.10691851350966042\n",
      "Gradient Descent(793/999): loss=0.10691608352439033\n",
      "Gradient Descent(794/999): loss=0.10691365795179633\n",
      "Gradient Descent(795/999): loss=0.10691123677934557\n",
      "Gradient Descent(796/999): loss=0.10690881999455236\n",
      "Gradient Descent(797/999): loss=0.10690640758497785\n",
      "Gradient Descent(798/999): loss=0.10690399953822986\n",
      "Gradient Descent(799/999): loss=0.10690159584196272\n",
      "Gradient Descent(800/999): loss=0.10689919648387683\n",
      "Gradient Descent(801/999): loss=0.10689680145171881\n",
      "Gradient Descent(802/999): loss=0.10689441073328086\n",
      "Gradient Descent(803/999): loss=0.1068920243164009\n",
      "Gradient Descent(804/999): loss=0.10688964218896202\n",
      "Gradient Descent(805/999): loss=0.10688726433889256\n",
      "Gradient Descent(806/999): loss=0.10688489075416584\n",
      "Gradient Descent(807/999): loss=0.10688252142279968\n",
      "Gradient Descent(808/999): loss=0.1068801563328565\n",
      "Gradient Descent(809/999): loss=0.10687779547244304\n",
      "Gradient Descent(810/999): loss=0.10687543882971005\n",
      "Gradient Descent(811/999): loss=0.10687308639285215\n",
      "Gradient Descent(812/999): loss=0.10687073815010761\n",
      "Gradient Descent(813/999): loss=0.10686839408975812\n",
      "Gradient Descent(814/999): loss=0.10686605420012868\n",
      "Gradient Descent(815/999): loss=0.10686371846958728\n",
      "Gradient Descent(816/999): loss=0.10686138688654478\n",
      "Gradient Descent(817/999): loss=0.10685905943945466\n",
      "Gradient Descent(818/999): loss=0.10685673611681283\n",
      "Gradient Descent(819/999): loss=0.10685441690715747\n",
      "Gradient Descent(820/999): loss=0.10685210179906875\n",
      "Gradient Descent(821/999): loss=0.10684979078116878\n",
      "Gradient Descent(822/999): loss=0.10684748384212127\n",
      "Gradient Descent(823/999): loss=0.10684518097063132\n",
      "Gradient Descent(824/999): loss=0.1068428821554455\n",
      "Gradient Descent(825/999): loss=0.10684058738535127\n",
      "Gradient Descent(826/999): loss=0.10683829664917703\n",
      "Gradient Descent(827/999): loss=0.10683600993579193\n",
      "Gradient Descent(828/999): loss=0.10683372723410554\n",
      "Gradient Descent(829/999): loss=0.10683144853306786\n",
      "Gradient Descent(830/999): loss=0.10682917382166901\n",
      "Gradient Descent(831/999): loss=0.10682690308893902\n",
      "Gradient Descent(832/999): loss=0.1068246363239477\n",
      "Gradient Descent(833/999): loss=0.10682237351580454\n",
      "Gradient Descent(834/999): loss=0.10682011465365832\n",
      "Gradient Descent(835/999): loss=0.10681785972669712\n",
      "Gradient Descent(836/999): loss=0.10681560872414808\n",
      "Gradient Descent(837/999): loss=0.10681336163527719\n",
      "Gradient Descent(838/999): loss=0.10681111844938913\n",
      "Gradient Descent(839/999): loss=0.10680887915582717\n",
      "Gradient Descent(840/999): loss=0.10680664374397285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(841/999): loss=0.10680441220324596\n",
      "Gradient Descent(842/999): loss=0.10680218452310423\n",
      "Gradient Descent(843/999): loss=0.10679996069304323\n",
      "Gradient Descent(844/999): loss=0.10679774070259625\n",
      "Gradient Descent(845/999): loss=0.10679552454133401\n",
      "Gradient Descent(846/999): loss=0.10679331219886459\n",
      "Gradient Descent(847/999): loss=0.10679110366483321\n",
      "Gradient Descent(848/999): loss=0.10678889892892211\n",
      "Gradient Descent(849/999): loss=0.10678669798085026\n",
      "Gradient Descent(850/999): loss=0.10678450081037344\n",
      "Gradient Descent(851/999): loss=0.10678230740728381\n",
      "Gradient Descent(852/999): loss=0.10678011776140994\n",
      "Gradient Descent(853/999): loss=0.10677793186261647\n",
      "Gradient Descent(854/999): loss=0.10677574970080415\n",
      "Gradient Descent(855/999): loss=0.10677357126590956\n",
      "Gradient Descent(856/999): loss=0.10677139654790489\n",
      "Gradient Descent(857/999): loss=0.106769225536798\n",
      "Gradient Descent(858/999): loss=0.10676705822263208\n",
      "Gradient Descent(859/999): loss=0.10676489459548547\n",
      "Gradient Descent(860/999): loss=0.10676273464547158\n",
      "Gradient Descent(861/999): loss=0.10676057836273888\n",
      "Gradient Descent(862/999): loss=0.10675842573747041\n",
      "Gradient Descent(863/999): loss=0.10675627675988392\n",
      "Gradient Descent(864/999): loss=0.10675413142023162\n",
      "Gradient Descent(865/999): loss=0.10675198970879994\n",
      "Gradient Descent(866/999): loss=0.10674985161590954\n",
      "Gradient Descent(867/999): loss=0.10674771713191504\n",
      "Gradient Descent(868/999): loss=0.1067455862472049\n",
      "Gradient Descent(869/999): loss=0.10674345895220136\n",
      "Gradient Descent(870/999): loss=0.10674133523736007\n",
      "Gradient Descent(871/999): loss=0.10673921509317026\n",
      "Gradient Descent(872/999): loss=0.10673709851015435\n",
      "Gradient Descent(873/999): loss=0.10673498547886785\n",
      "Gradient Descent(874/999): loss=0.10673287598989925\n",
      "Gradient Descent(875/999): loss=0.10673077003387\n",
      "Gradient Descent(876/999): loss=0.10672866760143407\n",
      "Gradient Descent(877/999): loss=0.10672656868327807\n",
      "Gradient Descent(878/999): loss=0.106724473270121\n",
      "Gradient Descent(879/999): loss=0.1067223813527142\n",
      "Gradient Descent(880/999): loss=0.10672029292184097\n",
      "Gradient Descent(881/999): loss=0.10671820796831682\n",
      "Gradient Descent(882/999): loss=0.10671612648298893\n",
      "Gradient Descent(883/999): loss=0.10671404845673634\n",
      "Gradient Descent(884/999): loss=0.10671197388046952\n",
      "Gradient Descent(885/999): loss=0.10670990274513058\n",
      "Gradient Descent(886/999): loss=0.10670783504169273\n",
      "Gradient Descent(887/999): loss=0.10670577076116057\n",
      "Gradient Descent(888/999): loss=0.10670370989456954\n",
      "Gradient Descent(889/999): loss=0.10670165243298617\n",
      "Gradient Descent(890/999): loss=0.10669959836750767\n",
      "Gradient Descent(891/999): loss=0.10669754768926192\n",
      "Gradient Descent(892/999): loss=0.10669550038940732\n",
      "Gradient Descent(893/999): loss=0.10669345645913267\n",
      "Gradient Descent(894/999): loss=0.10669141588965703\n",
      "Gradient Descent(895/999): loss=0.10668937867222955\n",
      "Gradient Descent(896/999): loss=0.10668734479812948\n",
      "Gradient Descent(897/999): loss=0.10668531425866583\n",
      "Gradient Descent(898/999): loss=0.10668328704517742\n",
      "Gradient Descent(899/999): loss=0.10668126314903274\n",
      "Gradient Descent(900/999): loss=0.10667924256162974\n",
      "Gradient Descent(901/999): loss=0.10667722527439569\n",
      "Gradient Descent(902/999): loss=0.10667521127878721\n",
      "Gradient Descent(903/999): loss=0.10667320056628997\n",
      "Gradient Descent(904/999): loss=0.1066711931284187\n",
      "Gradient Descent(905/999): loss=0.10666918895671701\n",
      "Gradient Descent(906/999): loss=0.10666718804275727\n",
      "Gradient Descent(907/999): loss=0.10666519037814046\n",
      "Gradient Descent(908/999): loss=0.10666319595449614\n",
      "Gradient Descent(909/999): loss=0.10666120476348222\n",
      "Gradient Descent(910/999): loss=0.10665921679678493\n",
      "Gradient Descent(911/999): loss=0.10665723204611868\n",
      "Gradient Descent(912/999): loss=0.1066552505032259\n",
      "Gradient Descent(913/999): loss=0.10665327215987698\n",
      "Gradient Descent(914/999): loss=0.10665129700787011\n",
      "Gradient Descent(915/999): loss=0.10664932503903118\n",
      "Gradient Descent(916/999): loss=0.1066473562452137\n",
      "Gradient Descent(917/999): loss=0.10664539061829861\n",
      "Gradient Descent(918/999): loss=0.10664342815019426\n",
      "Gradient Descent(919/999): loss=0.10664146883283618\n",
      "Gradient Descent(920/999): loss=0.10663951265818712\n",
      "Gradient Descent(921/999): loss=0.10663755961823672\n",
      "Gradient Descent(922/999): loss=0.10663560970500174\n",
      "Gradient Descent(923/999): loss=0.10663366291052548\n",
      "Gradient Descent(924/999): loss=0.10663171922687818\n",
      "Gradient Descent(925/999): loss=0.10662977864615644\n",
      "Gradient Descent(926/999): loss=0.10662784116048352\n",
      "Gradient Descent(927/999): loss=0.10662590676200887\n",
      "Gradient Descent(928/999): loss=0.10662397544290833\n",
      "Gradient Descent(929/999): loss=0.10662204719538382\n",
      "Gradient Descent(930/999): loss=0.1066201220116633\n",
      "Gradient Descent(931/999): loss=0.10661819988400072\n",
      "Gradient Descent(932/999): loss=0.10661628080467575\n",
      "Gradient Descent(933/999): loss=0.10661436476599387\n",
      "Gradient Descent(934/999): loss=0.10661245176028612\n",
      "Gradient Descent(935/999): loss=0.10661054177990913\n",
      "Gradient Descent(936/999): loss=0.10660863481724481\n",
      "Gradient Descent(937/999): loss=0.1066067308647005\n",
      "Gradient Descent(938/999): loss=0.10660482991470865\n",
      "Gradient Descent(939/999): loss=0.10660293195972692\n",
      "Gradient Descent(940/999): loss=0.1066010369922378\n",
      "Gradient Descent(941/999): loss=0.10659914500474886\n",
      "Gradient Descent(942/999): loss=0.10659725598979232\n",
      "Gradient Descent(943/999): loss=0.10659536993992519\n",
      "Gradient Descent(944/999): loss=0.10659348684772897\n",
      "Gradient Descent(945/999): loss=0.10659160670580982\n",
      "Gradient Descent(946/999): loss=0.10658972950679811\n",
      "Gradient Descent(947/999): loss=0.10658785524334863\n",
      "Gradient Descent(948/999): loss=0.10658598390814035\n",
      "Gradient Descent(949/999): loss=0.10658411549387631\n",
      "Gradient Descent(950/999): loss=0.10658224999328361\n",
      "Gradient Descent(951/999): loss=0.10658038739911317\n",
      "Gradient Descent(952/999): loss=0.10657852770413984\n",
      "Gradient Descent(953/999): loss=0.10657667090116207\n",
      "Gradient Descent(954/999): loss=0.10657481698300204\n",
      "Gradient Descent(955/999): loss=0.10657296594250544\n",
      "Gradient Descent(956/999): loss=0.10657111777254127\n",
      "Gradient Descent(957/999): loss=0.10656927246600202\n",
      "Gradient Descent(958/999): loss=0.10656743001580338\n",
      "Gradient Descent(959/999): loss=0.10656559041488413\n",
      "Gradient Descent(960/999): loss=0.1065637536562063\n",
      "Gradient Descent(961/999): loss=0.10656191973275463\n",
      "Gradient Descent(962/999): loss=0.10656008863753691\n",
      "Gradient Descent(963/999): loss=0.10655826036358373\n",
      "Gradient Descent(964/999): loss=0.10655643490394828\n",
      "Gradient Descent(965/999): loss=0.1065546122517064\n",
      "Gradient Descent(966/999): loss=0.10655279239995648\n",
      "Gradient Descent(967/999): loss=0.10655097534181932\n",
      "Gradient Descent(968/999): loss=0.10654916107043803\n",
      "Gradient Descent(969/999): loss=0.10654734957897798\n",
      "Gradient Descent(970/999): loss=0.10654554086062677\n",
      "Gradient Descent(971/999): loss=0.10654373490859399\n",
      "Gradient Descent(972/999): loss=0.10654193171611127\n",
      "Gradient Descent(973/999): loss=0.10654013127643211\n",
      "Gradient Descent(974/999): loss=0.10653833358283182\n",
      "Gradient Descent(975/999): loss=0.10653653862860753\n",
      "Gradient Descent(976/999): loss=0.1065347464070779\n",
      "Gradient Descent(977/999): loss=0.1065329569115832\n",
      "Gradient Descent(978/999): loss=0.10653117013548519\n",
      "Gradient Descent(979/999): loss=0.10652938607216698\n",
      "Gradient Descent(980/999): loss=0.10652760471503303\n",
      "Gradient Descent(981/999): loss=0.106525826057509\n",
      "Gradient Descent(982/999): loss=0.10652405009304165\n",
      "Gradient Descent(983/999): loss=0.10652227681509893\n",
      "Gradient Descent(984/999): loss=0.1065205062171696\n",
      "Gradient Descent(985/999): loss=0.10651873829276344\n",
      "Gradient Descent(986/999): loss=0.10651697303541094\n",
      "Gradient Descent(987/999): loss=0.10651521043866342\n",
      "Gradient Descent(988/999): loss=0.10651345049609279\n",
      "Gradient Descent(989/999): loss=0.1065116932012915\n",
      "Gradient Descent(990/999): loss=0.10650993854787259\n",
      "Gradient Descent(991/999): loss=0.1065081865294694\n",
      "Gradient Descent(992/999): loss=0.10650643713973572\n",
      "Gradient Descent(993/999): loss=0.10650469037234543\n",
      "Gradient Descent(994/999): loss=0.10650294622099278\n",
      "Gradient Descent(995/999): loss=0.10650120467939193\n",
      "Gradient Descent(996/999): loss=0.10649946574127717\n",
      "Gradient Descent(997/999): loss=0.10649772940040268\n",
      "Gradient Descent(998/999): loss=0.10649599565054255\n",
      "Gradient Descent(999/999): loss=0.10649426448549063\n",
      "Gradient Descent(0/999): loss=0.14802841371423545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1/999): loss=0.13520156577666093\n",
      "Gradient Descent(2/999): loss=0.12617133474016917\n",
      "Gradient Descent(3/999): loss=0.11980817246015243\n",
      "Gradient Descent(4/999): loss=0.11531862134887258\n",
      "Gradient Descent(5/999): loss=0.11214531233136474\n",
      "Gradient Descent(6/999): loss=0.1098967179053675\n",
      "Gradient Descent(7/999): loss=0.10829780682407136\n",
      "Gradient Descent(8/999): loss=0.10715538122918253\n",
      "Gradient Descent(9/999): loss=0.10633372754522481\n",
      "Gradient Descent(10/999): loss=0.1057375123274167\n",
      "Gradient Descent(11/999): loss=0.10529976736371024\n",
      "Gradient Descent(12/999): loss=0.10497344974907173\n",
      "Gradient Descent(13/999): loss=0.10472551321703684\n",
      "Gradient Descent(14/999): loss=0.1045327435165986\n",
      "Gradient Descent(15/999): loss=0.10437883295163332\n",
      "Gradient Descent(16/999): loss=0.10425232537629948\n",
      "Gradient Descent(17/999): loss=0.10414517264662367\n",
      "Gradient Descent(18/999): loss=0.10405172059256956\n",
      "Gradient Descent(19/999): loss=0.10396799670893586\n",
      "Gradient Descent(20/999): loss=0.10389120979016846\n",
      "Gradient Descent(21/999): loss=0.10381939844624477\n",
      "Gradient Descent(22/999): loss=0.10375118420081847\n",
      "Gradient Descent(23/999): loss=0.10368559805370224\n",
      "Gradient Descent(24/999): loss=0.10362195864874532\n",
      "Gradient Descent(25/999): loss=0.10355978669218291\n",
      "Gradient Descent(26/999): loss=0.10349874483531246\n",
      "Gradient Descent(27/999): loss=0.10343859544471656\n",
      "Gradient Descent(28/999): loss=0.10337917093768388\n",
      "Gradient Descent(29/999): loss=0.1033203529441176\n",
      "Gradient Descent(30/999): loss=0.10326205766865612\n",
      "Gradient Descent(31/999): loss=0.10320422560816456\n",
      "Gradient Descent(32/999): loss=0.10314681432867973\n",
      "Gradient Descent(33/999): loss=0.10308979339148451\n",
      "Gradient Descent(34/999): loss=0.10303314078884968\n",
      "Gradient Descent(35/999): loss=0.10297684044025072\n",
      "Gradient Descent(36/999): loss=0.10292088043352081\n",
      "Gradient Descent(37/999): loss=0.10286525178928964\n",
      "Gradient Descent(38/999): loss=0.10280994759300698\n",
      "Gradient Descent(39/999): loss=0.10275496238517959\n",
      "Gradient Descent(40/999): loss=0.1027002917329918\n",
      "Gradient Descent(41/999): loss=0.10264593192934095\n",
      "Gradient Descent(42/999): loss=0.10259187978137646\n",
      "Gradient Descent(43/999): loss=0.10253813246191251\n",
      "Gradient Descent(44/999): loss=0.10248468740500688\n",
      "Gradient Descent(45/999): loss=0.10243154223256529\n",
      "Gradient Descent(46/999): loss=0.10237869470274094\n",
      "Gradient Descent(47/999): loss=0.10232614267364401\n",
      "Gradient Descent(48/999): loss=0.10227388407780742\n",
      "Gradient Descent(49/999): loss=0.10222191690420827\n",
      "Gradient Descent(50/999): loss=0.10217023918559763\n",
      "Gradient Descent(51/999): loss=0.10211884898956027\n",
      "Gradient Descent(52/999): loss=0.1020677444121945\n",
      "Gradient Descent(53/999): loss=0.10201692357363341\n",
      "Gradient Descent(54/999): loss=0.10196638461486046\n",
      "Gradient Descent(55/999): loss=0.10191612569543436\n",
      "Gradient Descent(56/999): loss=0.10186614499185355\n",
      "Gradient Descent(57/999): loss=0.10181644069637065\n",
      "Gradient Descent(58/999): loss=0.10176701101612318\n",
      "Gradient Descent(59/999): loss=0.10171785417248733\n",
      "Gradient Descent(60/999): loss=0.10166896840058856\n",
      "Gradient Descent(61/999): loss=0.10162035194892341\n",
      "Gradient Descent(62/999): loss=0.10157200307905938\n",
      "Gradient Descent(63/999): loss=0.10152392006539034\n",
      "Gradient Descent(64/999): loss=0.10147610119493157\n",
      "Gradient Descent(65/999): loss=0.10142854476714307\n",
      "Gradient Descent(66/999): loss=0.1013812490937729\n",
      "Gradient Descent(67/999): loss=0.10133421249871538\n",
      "Gradient Descent(68/999): loss=0.10128743331788004\n",
      "Gradient Descent(69/999): loss=0.10124090989906849\n",
      "Gradient Descent(70/999): loss=0.1011946406018574\n",
      "Gradient Descent(71/999): loss=0.10114862379748608\n",
      "Gradient Descent(72/999): loss=0.10110285786874781\n",
      "Gradient Descent(73/999): loss=0.10105734120988404\n",
      "Gradient Descent(74/999): loss=0.10101207222648116\n",
      "Gradient Descent(75/999): loss=0.10096704933536928\n",
      "Gradient Descent(76/999): loss=0.10092227096452304\n",
      "Gradient Descent(77/999): loss=0.10087773555296409\n",
      "Gradient Descent(78/999): loss=0.10083344155066479\n",
      "Gradient Descent(79/999): loss=0.10078938741845388\n",
      "Gradient Descent(80/999): loss=0.10074557162792297\n",
      "Gradient Descent(81/999): loss=0.10070199266133477\n",
      "Gradient Descent(82/999): loss=0.10065864901153213\n",
      "Gradient Descent(83/999): loss=0.10061553918184862\n",
      "Gradient Descent(84/999): loss=0.10057266168602015\n",
      "Gradient Descent(85/999): loss=0.10053001504809768\n",
      "Gradient Descent(86/999): loss=0.10048759780236105\n",
      "Gradient Descent(87/999): loss=0.10044540849323393\n",
      "Gradient Descent(88/999): loss=0.10040344567519967\n",
      "Gradient Descent(89/999): loss=0.1003617079127185\n",
      "Gradient Descent(90/999): loss=0.10032019378014542\n",
      "Gradient Descent(91/999): loss=0.10027890186164914\n",
      "Gradient Descent(92/999): loss=0.10023783075113205\n",
      "Gradient Descent(93/999): loss=0.10019697905215132\n",
      "Gradient Descent(94/999): loss=0.10015634537784052\n",
      "Gradient Descent(95/999): loss=0.10011592835083241\n",
      "Gradient Descent(96/999): loss=0.10007572660318279\n",
      "Gradient Descent(97/999): loss=0.10003573877629483\n",
      "Gradient Descent(98/999): loss=0.09999596352084458\n",
      "Gradient Descent(99/999): loss=0.09995639949670715\n",
      "Gradient Descent(100/999): loss=0.09991704537288397\n",
      "Gradient Descent(101/999): loss=0.09987789982743048\n",
      "Gradient Descent(102/999): loss=0.09983896154738509\n",
      "Gradient Descent(103/999): loss=0.0998002292286985\n",
      "Gradient Descent(104/999): loss=0.09976170157616404\n",
      "Gradient Descent(105/999): loss=0.09972337730334878\n",
      "Gradient Descent(106/999): loss=0.09968525513252537\n",
      "Gradient Descent(107/999): loss=0.09964733379460446\n",
      "Gradient Descent(108/999): loss=0.09960961202906807\n",
      "Gradient Descent(109/999): loss=0.09957208858390351\n",
      "Gradient Descent(110/999): loss=0.0995347622155382\n",
      "Gradient Descent(111/999): loss=0.09949763168877479\n",
      "Gradient Descent(112/999): loss=0.0994606957767276\n",
      "Gradient Descent(113/999): loss=0.09942395326075895\n",
      "Gradient Descent(114/999): loss=0.09938740293041677\n",
      "Gradient Descent(115/999): loss=0.09935104358337266\n",
      "Gradient Descent(116/999): loss=0.09931487402536031\n",
      "Gradient Descent(117/999): loss=0.09927889307011513\n",
      "Gradient Descent(118/999): loss=0.09924309953931382\n",
      "Gradient Descent(119/999): loss=0.09920749226251509\n",
      "Gradient Descent(120/999): loss=0.09917207007710065\n",
      "Gradient Descent(121/999): loss=0.09913683182821699\n",
      "Gradient Descent(122/999): loss=0.09910177636871764\n",
      "Gradient Descent(123/999): loss=0.09906690255910586\n",
      "Gradient Descent(124/999): loss=0.09903220926747822\n",
      "Gradient Descent(125/999): loss=0.09899769536946838\n",
      "Gradient Descent(126/999): loss=0.09896335974819176\n",
      "Gradient Descent(127/999): loss=0.09892920129419042\n",
      "Gradient Descent(128/999): loss=0.09889521890537857\n",
      "Gradient Descent(129/999): loss=0.09886141148698877\n",
      "Gradient Descent(130/999): loss=0.09882777795151848\n",
      "Gradient Descent(131/999): loss=0.09879431721867703\n",
      "Gradient Descent(132/999): loss=0.09876102821533327\n",
      "Gradient Descent(133/999): loss=0.09872790987546369\n",
      "Gradient Descent(134/999): loss=0.09869496114010085\n",
      "Gradient Descent(135/999): loss=0.09866218095728241\n",
      "Gradient Descent(136/999): loss=0.09862956828200072\n",
      "Gradient Descent(137/999): loss=0.09859712207615266\n",
      "Gradient Descent(138/999): loss=0.09856484130849011\n",
      "Gradient Descent(139/999): loss=0.0985327249545707\n",
      "Gradient Descent(140/999): loss=0.09850077199670922\n",
      "Gradient Descent(141/999): loss=0.09846898142392932\n",
      "Gradient Descent(142/999): loss=0.09843735223191556\n",
      "Gradient Descent(143/999): loss=0.09840588342296618\n",
      "Gradient Descent(144/999): loss=0.09837457400594583\n",
      "Gradient Descent(145/999): loss=0.09834342299623937\n",
      "Gradient Descent(146/999): loss=0.09831242941570525\n",
      "Gradient Descent(147/999): loss=0.09828159229263009\n",
      "Gradient Descent(148/999): loss=0.09825091066168302\n",
      "Gradient Descent(149/999): loss=0.0982203835638709\n",
      "Gradient Descent(150/999): loss=0.09819001004649369\n",
      "Gradient Descent(151/999): loss=0.09815978916310003\n",
      "Gradient Descent(152/999): loss=0.09812971997344365\n",
      "Gradient Descent(153/999): loss=0.09809980154343971\n",
      "Gradient Descent(154/999): loss=0.09807003294512179\n",
      "Gradient Descent(155/999): loss=0.0980404132565991\n",
      "Gradient Descent(156/999): loss=0.09801094156201408\n",
      "Gradient Descent(157/999): loss=0.09798161695150044\n",
      "Gradient Descent(158/999): loss=0.09795243852114134\n",
      "Gradient Descent(159/999): loss=0.09792340537292826\n",
      "Gradient Descent(160/999): loss=0.09789451661471973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(161/999): loss=0.09786577136020087\n",
      "Gradient Descent(162/999): loss=0.097837168728843\n",
      "Gradient Descent(163/999): loss=0.09780870784586355\n",
      "Gradient Descent(164/999): loss=0.0977803878421866\n",
      "Gradient Descent(165/999): loss=0.09775220785440324\n",
      "Gradient Descent(166/999): loss=0.09772416702473274\n",
      "Gradient Descent(167/999): loss=0.09769626450098372\n",
      "Gradient Descent(168/999): loss=0.09766849943651582\n",
      "Gradient Descent(169/999): loss=0.09764087099020147\n",
      "Gradient Descent(170/999): loss=0.09761337832638811\n",
      "Gradient Descent(171/999): loss=0.09758602061486085\n",
      "Gradient Descent(172/999): loss=0.09755879703080499\n",
      "Gradient Descent(173/999): loss=0.09753170675476928\n",
      "Gradient Descent(174/999): loss=0.09750474897262926\n",
      "Gradient Descent(175/999): loss=0.09747792287555077\n",
      "Gradient Descent(176/999): loss=0.09745122765995425\n",
      "Gradient Descent(177/999): loss=0.09742466252747842\n",
      "Gradient Descent(178/999): loss=0.09739822668494526\n",
      "Gradient Descent(179/999): loss=0.09737191934432464\n",
      "Gradient Descent(180/999): loss=0.09734573972269911\n",
      "Gradient Descent(181/999): loss=0.09731968704222961\n",
      "Gradient Descent(182/999): loss=0.09729376053012068\n",
      "Gradient Descent(183/999): loss=0.09726795941858662\n",
      "Gradient Descent(184/999): loss=0.09724228294481732\n",
      "Gradient Descent(185/999): loss=0.09721673035094473\n",
      "Gradient Descent(186/999): loss=0.09719130088400957\n",
      "Gradient Descent(187/999): loss=0.09716599379592791\n",
      "Gradient Descent(188/999): loss=0.09714080834345858\n",
      "Gradient Descent(189/999): loss=0.09711574378817035\n",
      "Gradient Descent(190/999): loss=0.09709079939640959\n",
      "Gradient Descent(191/999): loss=0.09706597443926815\n",
      "Gradient Descent(192/999): loss=0.09704126819255132\n",
      "Gradient Descent(193/999): loss=0.09701667993674624\n",
      "Gradient Descent(194/999): loss=0.09699220895699055\n",
      "Gradient Descent(195/999): loss=0.09696785454304092\n",
      "Gradient Descent(196/999): loss=0.09694361598924241\n",
      "Gradient Descent(197/999): loss=0.0969194925944973\n",
      "Gradient Descent(198/999): loss=0.09689548366223494\n",
      "Gradient Descent(199/999): loss=0.09687158850038129\n",
      "Gradient Descent(200/999): loss=0.09684780642132884\n",
      "Gradient Descent(201/999): loss=0.09682413674190671\n",
      "Gradient Descent(202/999): loss=0.09680057878335112\n",
      "Gradient Descent(203/999): loss=0.09677713187127579\n",
      "Gradient Descent(204/999): loss=0.096753795335643\n",
      "Gradient Descent(205/999): loss=0.09673056851073425\n",
      "Gradient Descent(206/999): loss=0.09670745073512167\n",
      "Gradient Descent(207/999): loss=0.09668444135163944\n",
      "Gradient Descent(208/999): loss=0.09666153970735536\n",
      "Gradient Descent(209/999): loss=0.0966387451535426\n",
      "Gradient Descent(210/999): loss=0.09661605704565188\n",
      "Gradient Descent(211/999): loss=0.0965934747432836\n",
      "Gradient Descent(212/999): loss=0.09657099761016029\n",
      "Gradient Descent(213/999): loss=0.09654862501409918\n",
      "Gradient Descent(214/999): loss=0.09652635632698513\n",
      "Gradient Descent(215/999): loss=0.09650419092474347\n",
      "Gradient Descent(216/999): loss=0.09648212818731342\n",
      "Gradient Descent(217/999): loss=0.0964601674986213\n",
      "Gradient Descent(218/999): loss=0.09643830824655415\n",
      "Gradient Descent(219/999): loss=0.09641654982293357\n",
      "Gradient Descent(220/999): loss=0.09639489162348958\n",
      "Gradient Descent(221/999): loss=0.0963733330478348\n",
      "Gradient Descent(222/999): loss=0.09635187349943879\n",
      "Gradient Descent(223/999): loss=0.09633051238560242\n",
      "Gradient Descent(224/999): loss=0.09630924911743285\n",
      "Gradient Descent(225/999): loss=0.09628808310981789\n",
      "Gradient Descent(226/999): loss=0.09626701378140154\n",
      "Gradient Descent(227/999): loss=0.09624604055455888\n",
      "Gradient Descent(228/999): loss=0.09622516285537158\n",
      "Gradient Descent(229/999): loss=0.09620438011360331\n",
      "Gradient Descent(230/999): loss=0.09618369176267563\n",
      "Gradient Descent(231/999): loss=0.09616309723964377\n",
      "Gradient Descent(232/999): loss=0.09614259598517273\n",
      "Gradient Descent(233/999): loss=0.09612218744351342\n",
      "Gradient Descent(234/999): loss=0.0961018710624792\n",
      "Gradient Descent(235/999): loss=0.09608164629342222\n",
      "Gradient Descent(236/999): loss=0.09606151259121032\n",
      "Gradient Descent(237/999): loss=0.09604146941420379\n",
      "Gradient Descent(238/999): loss=0.09602151622423247\n",
      "Gradient Descent(239/999): loss=0.0960016524865728\n",
      "Gradient Descent(240/999): loss=0.09598187766992539\n",
      "Gradient Descent(241/999): loss=0.09596219124639231\n",
      "Gradient Descent(242/999): loss=0.09594259269145484\n",
      "Gradient Descent(243/999): loss=0.09592308148395128\n",
      "Gradient Descent(244/999): loss=0.09590365710605485\n",
      "Gradient Descent(245/999): loss=0.09588431904325193\n",
      "Gradient Descent(246/999): loss=0.09586506678432014\n",
      "Gradient Descent(247/999): loss=0.09584589982130692\n",
      "Gradient Descent(248/999): loss=0.09582681764950797\n",
      "Gradient Descent(249/999): loss=0.0958078197674459\n",
      "Gradient Descent(250/999): loss=0.09578890567684939\n",
      "Gradient Descent(251/999): loss=0.09577007488263174\n",
      "Gradient Descent(252/999): loss=0.09575132689287032\n",
      "Gradient Descent(253/999): loss=0.0957326612187857\n",
      "Gradient Descent(254/999): loss=0.09571407737472118\n",
      "Gradient Descent(255/999): loss=0.09569557487812218\n",
      "Gradient Descent(256/999): loss=0.09567715324951606\n",
      "Gradient Descent(257/999): loss=0.09565881201249199\n",
      "Gradient Descent(258/999): loss=0.09564055069368073\n",
      "Gradient Descent(259/999): loss=0.09562236882273495\n",
      "Gradient Descent(260/999): loss=0.09560426593230929\n",
      "Gradient Descent(261/999): loss=0.09558624155804082\n",
      "Gradient Descent(262/999): loss=0.09556829523852958\n",
      "Gradient Descent(263/999): loss=0.0955504265153191\n",
      "Gradient Descent(264/999): loss=0.09553263493287728\n",
      "Gradient Descent(265/999): loss=0.09551492003857717\n",
      "Gradient Descent(266/999): loss=0.09549728138267805\n",
      "Gradient Descent(267/999): loss=0.09547971851830672\n",
      "Gradient Descent(268/999): loss=0.09546223100143844\n",
      "Gradient Descent(269/999): loss=0.09544481839087869\n",
      "Gradient Descent(270/999): loss=0.09542748024824442\n",
      "Gradient Descent(271/999): loss=0.09541021613794595\n",
      "Gradient Descent(272/999): loss=0.09539302562716845\n",
      "Gradient Descent(273/999): loss=0.09537590828585417\n",
      "Gradient Descent(274/999): loss=0.0953588636866841\n",
      "Gradient Descent(275/999): loss=0.0953418914050605\n",
      "Gradient Descent(276/999): loss=0.09532499101908877\n",
      "Gradient Descent(277/999): loss=0.09530816210956009\n",
      "Gradient Descent(278/999): loss=0.09529140425993382\n",
      "Gradient Descent(279/999): loss=0.09527471705632004\n",
      "Gradient Descent(280/999): loss=0.0952581000874625\n",
      "Gradient Descent(281/999): loss=0.09524155294472118\n",
      "Gradient Descent(282/999): loss=0.09522507522205541\n",
      "Gradient Descent(283/999): loss=0.09520866651600697\n",
      "Gradient Descent(284/999): loss=0.09519232642568305\n",
      "Gradient Descent(285/999): loss=0.09517605455273993\n",
      "Gradient Descent(286/999): loss=0.09515985050136594\n",
      "Gradient Descent(287/999): loss=0.09514371387826524\n",
      "Gradient Descent(288/999): loss=0.0951276442926415\n",
      "Gradient Descent(289/999): loss=0.09511164135618141\n",
      "Gradient Descent(290/999): loss=0.09509570468303861\n",
      "Gradient Descent(291/999): loss=0.09507983388981783\n",
      "Gradient Descent(292/999): loss=0.0950640285955586\n",
      "Gradient Descent(293/999): loss=0.09504828842171968\n",
      "Gradient Descent(294/999): loss=0.09503261299216317\n",
      "Gradient Descent(295/999): loss=0.09501700193313901\n",
      "Gradient Descent(296/999): loss=0.09500145487326936\n",
      "Gradient Descent(297/999): loss=0.0949859714435332\n",
      "Gradient Descent(298/999): loss=0.09497055127725096\n",
      "Gradient Descent(299/999): loss=0.09495519401006941\n",
      "Gradient Descent(300/999): loss=0.0949398992799465\n",
      "Gradient Descent(301/999): loss=0.09492466672713627\n",
      "Gradient Descent(302/999): loss=0.09490949599417392\n",
      "Gradient Descent(303/999): loss=0.09489438672586116\n",
      "Gradient Descent(304/999): loss=0.09487933856925132\n",
      "Gradient Descent(305/999): loss=0.09486435117363479\n",
      "Gradient Descent(306/999): loss=0.09484942419052436\n",
      "Gradient Descent(307/999): loss=0.09483455727364111\n",
      "Gradient Descent(308/999): loss=0.09481975007889967\n",
      "Gradient Descent(309/999): loss=0.09480500226439423\n",
      "Gradient Descent(310/999): loss=0.09479031349038436\n",
      "Gradient Descent(311/999): loss=0.09477568341928085\n",
      "Gradient Descent(312/999): loss=0.09476111171563183\n",
      "Gradient Descent(313/999): loss=0.09474659804610892\n",
      "Gradient Descent(314/999): loss=0.09473214207949333\n",
      "Gradient Descent(315/999): loss=0.0947177434866623\n",
      "Gradient Descent(316/999): loss=0.09470340194057543\n",
      "Gradient Descent(317/999): loss=0.09468911711626113\n",
      "Gradient Descent(318/999): loss=0.09467488869080334\n",
      "Gradient Descent(319/999): loss=0.09466071634332804\n",
      "Gradient Descent(320/999): loss=0.09464659975499005\n",
      "Gradient Descent(321/999): loss=0.09463253860895995\n",
      "Gradient Descent(322/999): loss=0.09461853259041095\n",
      "Gradient Descent(323/999): loss=0.09460458138650579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(324/999): loss=0.09459068468638406\n",
      "Gradient Descent(325/999): loss=0.09457684218114926\n",
      "Gradient Descent(326/999): loss=0.09456305356385594\n",
      "Gradient Descent(327/999): loss=0.09454931852949741\n",
      "Gradient Descent(328/999): loss=0.09453563677499276\n",
      "Gradient Descent(329/999): loss=0.09452200799917466\n",
      "Gradient Descent(330/999): loss=0.09450843190277686\n",
      "Gradient Descent(331/999): loss=0.09449490818842182\n",
      "Gradient Descent(332/999): loss=0.09448143656060858\n",
      "Gradient Descent(333/999): loss=0.09446801672570054\n",
      "Gradient Descent(334/999): loss=0.09445464839191338\n",
      "Gradient Descent(335/999): loss=0.09444133126930304\n",
      "Gradient Descent(336/999): loss=0.09442806506975383\n",
      "Gradient Descent(337/999): loss=0.09441484950696664\n",
      "Gradient Descent(338/999): loss=0.09440168429644705\n",
      "Gradient Descent(339/999): loss=0.09438856915549369\n",
      "Gradient Descent(340/999): loss=0.09437550380318672\n",
      "Gradient Descent(341/999): loss=0.09436248796037618\n",
      "Gradient Descent(342/999): loss=0.09434952134967049\n",
      "Gradient Descent(343/999): loss=0.09433660369542526\n",
      "Gradient Descent(344/999): loss=0.09432373472373179\n",
      "Gradient Descent(345/999): loss=0.09431091416240589\n",
      "Gradient Descent(346/999): loss=0.09429814174097681\n",
      "Gradient Descent(347/999): loss=0.09428541719067596\n",
      "Gradient Descent(348/999): loss=0.09427274024442603\n",
      "Gradient Descent(349/999): loss=0.0942601106368301\n",
      "Gradient Descent(350/999): loss=0.09424752810416052\n",
      "Gradient Descent(351/999): loss=0.0942349923843484\n",
      "Gradient Descent(352/999): loss=0.0942225032169727\n",
      "Gradient Descent(353/999): loss=0.09421006034324957\n",
      "Gradient Descent(354/999): loss=0.09419766350602182\n",
      "Gradient Descent(355/999): loss=0.09418531244974843\n",
      "Gradient Descent(356/999): loss=0.09417300692049392\n",
      "Gradient Descent(357/999): loss=0.09416074666591819\n",
      "Gradient Descent(358/999): loss=0.09414853143526604\n",
      "Gradient Descent(359/999): loss=0.09413636097935697\n",
      "Gradient Descent(360/999): loss=0.09412423505057506\n",
      "Gradient Descent(361/999): loss=0.0941121534028587\n",
      "Gradient Descent(362/999): loss=0.09410011579169064\n",
      "Gradient Descent(363/999): loss=0.09408812197408806\n",
      "Gradient Descent(364/999): loss=0.09407617170859249\n",
      "Gradient Descent(365/999): loss=0.09406426475526014\n",
      "Gradient Descent(366/999): loss=0.09405240087565195\n",
      "Gradient Descent(367/999): loss=0.09404057983282399\n",
      "Gradient Descent(368/999): loss=0.09402880139131757\n",
      "Gradient Descent(369/999): loss=0.09401706531714998\n",
      "Gradient Descent(370/999): loss=0.09400537137780472\n",
      "Gradient Descent(371/999): loss=0.093993719342222\n",
      "Gradient Descent(372/999): loss=0.09398210898078954\n",
      "Gradient Descent(373/999): loss=0.09397054006533294\n",
      "Gradient Descent(374/999): loss=0.09395901236910655\n",
      "Gradient Descent(375/999): loss=0.0939475256667843\n",
      "Gradient Descent(376/999): loss=0.09393607973445034\n",
      "Gradient Descent(377/999): loss=0.0939246743495901\n",
      "Gradient Descent(378/999): loss=0.09391330929108117\n",
      "Gradient Descent(379/999): loss=0.09390198433918423\n",
      "Gradient Descent(380/999): loss=0.09389069927553427\n",
      "Gradient Descent(381/999): loss=0.09387945388313164\n",
      "Gradient Descent(382/999): loss=0.09386824794633308\n",
      "Gradient Descent(383/999): loss=0.09385708125084337\n",
      "Gradient Descent(384/999): loss=0.09384595358370608\n",
      "Gradient Descent(385/999): loss=0.09383486473329541\n",
      "Gradient Descent(386/999): loss=0.09382381448930732\n",
      "Gradient Descent(387/999): loss=0.09381280264275105\n",
      "Gradient Descent(388/999): loss=0.09380182898594072\n",
      "Gradient Descent(389/999): loss=0.09379089331248674\n",
      "Gradient Descent(390/999): loss=0.09377999541728765\n",
      "Gradient Descent(391/999): loss=0.09376913509652172\n",
      "Gradient Descent(392/999): loss=0.09375831214763863\n",
      "Gradient Descent(393/999): loss=0.09374752636935126\n",
      "Gradient Descent(394/999): loss=0.09373677756162761\n",
      "Gradient Descent(395/999): loss=0.09372606552568276\n",
      "Gradient Descent(396/999): loss=0.09371539006397064\n",
      "Gradient Descent(397/999): loss=0.09370475098017615\n",
      "Gradient Descent(398/999): loss=0.09369414807920728\n",
      "Gradient Descent(399/999): loss=0.093683581167187\n",
      "Gradient Descent(400/999): loss=0.09367305005144572\n",
      "Gradient Descent(401/999): loss=0.09366255454051317\n",
      "Gradient Descent(402/999): loss=0.09365209444411099\n",
      "Gradient Descent(403/999): loss=0.0936416695731448\n",
      "Gradient Descent(404/999): loss=0.09363127973969666\n",
      "Gradient Descent(405/999): loss=0.09362092475701747\n",
      "Gradient Descent(406/999): loss=0.09361060443951949\n",
      "Gradient Descent(407/999): loss=0.0936003186027687\n",
      "Gradient Descent(408/999): loss=0.09359006706347754\n",
      "Gradient Descent(409/999): loss=0.09357984963949738\n",
      "Gradient Descent(410/999): loss=0.09356966614981121\n",
      "Gradient Descent(411/999): loss=0.09355951641452641\n",
      "Gradient Descent(412/999): loss=0.09354940025486748\n",
      "Gradient Descent(413/999): loss=0.09353931749316871\n",
      "Gradient Descent(414/999): loss=0.09352926795286723\n",
      "Gradient Descent(415/999): loss=0.09351925145849573\n",
      "Gradient Descent(416/999): loss=0.0935092678356755\n",
      "Gradient Descent(417/999): loss=0.0934993169111094\n",
      "Gradient Descent(418/999): loss=0.09348939851257479\n",
      "Gradient Descent(419/999): loss=0.09347951246891675\n",
      "Gradient Descent(420/999): loss=0.09346965861004115\n",
      "Gradient Descent(421/999): loss=0.09345983676690782\n",
      "Gradient Descent(422/999): loss=0.09345004677152363\n",
      "Gradient Descent(423/999): loss=0.0934402884569359\n",
      "Gradient Descent(424/999): loss=0.09343056165722577\n",
      "Gradient Descent(425/999): loss=0.0934208662075013\n",
      "Gradient Descent(426/999): loss=0.09341120194389094\n",
      "Gradient Descent(427/999): loss=0.09340156870353711\n",
      "Gradient Descent(428/999): loss=0.09339196632458953\n",
      "Gradient Descent(429/999): loss=0.09338239464619867\n",
      "Gradient Descent(430/999): loss=0.09337285350850952\n",
      "Gradient Descent(431/999): loss=0.09336334275265495\n",
      "Gradient Descent(432/999): loss=0.09335386222074946\n",
      "Gradient Descent(433/999): loss=0.09334441175588297\n",
      "Gradient Descent(434/999): loss=0.09333499120211435\n",
      "Gradient Descent(435/999): loss=0.09332560040446519\n",
      "Gradient Descent(436/999): loss=0.09331623920891378\n",
      "Gradient Descent(437/999): loss=0.09330690746238883\n",
      "Gradient Descent(438/999): loss=0.09329760501276327\n",
      "Gradient Descent(439/999): loss=0.0932883317088484\n",
      "Gradient Descent(440/999): loss=0.09327908740038762\n",
      "Gradient Descent(441/999): loss=0.09326987193805066\n",
      "Gradient Descent(442/999): loss=0.0932606851734272\n",
      "Gradient Descent(443/999): loss=0.09325152695902163\n",
      "Gradient Descent(444/999): loss=0.09324239714824646\n",
      "Gradient Descent(445/999): loss=0.09323329559541696\n",
      "Gradient Descent(446/999): loss=0.09322422215574515\n",
      "Gradient Descent(447/999): loss=0.09321517668533402\n",
      "Gradient Descent(448/999): loss=0.0932061590411719\n",
      "Gradient Descent(449/999): loss=0.09319716908112666\n",
      "Gradient Descent(450/999): loss=0.09318820666394018\n",
      "Gradient Descent(451/999): loss=0.09317927164922253\n",
      "Gradient Descent(452/999): loss=0.09317036389744668\n",
      "Gradient Descent(453/999): loss=0.09316148326994257\n",
      "Gradient Descent(454/999): loss=0.09315262962889198\n",
      "Gradient Descent(455/999): loss=0.09314380283732286\n",
      "Gradient Descent(456/999): loss=0.09313500275910384\n",
      "Gradient Descent(457/999): loss=0.09312622925893894\n",
      "Gradient Descent(458/999): loss=0.09311748220236214\n",
      "Gradient Descent(459/999): loss=0.09310876145573213\n",
      "Gradient Descent(460/999): loss=0.09310006688622684\n",
      "Gradient Descent(461/999): loss=0.09309139836183836\n",
      "Gradient Descent(462/999): loss=0.09308275575136764\n",
      "Gradient Descent(463/999): loss=0.09307413892441917\n",
      "Gradient Descent(464/999): loss=0.09306554775139605\n",
      "Gradient Descent(465/999): loss=0.09305698210349465\n",
      "Gradient Descent(466/999): loss=0.09304844185269963\n",
      "Gradient Descent(467/999): loss=0.09303992687177884\n",
      "Gradient Descent(468/999): loss=0.09303143703427832\n",
      "Gradient Descent(469/999): loss=0.0930229722145172\n",
      "Gradient Descent(470/999): loss=0.09301453228758293\n",
      "Gradient Descent(471/999): loss=0.09300611712932612\n",
      "Gradient Descent(472/999): loss=0.09299772661635575\n",
      "Gradient Descent(473/999): loss=0.09298936062603437\n",
      "Gradient Descent(474/999): loss=0.0929810190364731\n",
      "Gradient Descent(475/999): loss=0.09297270172652697\n",
      "Gradient Descent(476/999): loss=0.0929644085757901\n",
      "Gradient Descent(477/999): loss=0.0929561394645908\n",
      "Gradient Descent(478/999): loss=0.0929478942739872\n",
      "Gradient Descent(479/999): loss=0.09293967288576221\n",
      "Gradient Descent(480/999): loss=0.09293147518241912\n",
      "Gradient Descent(481/999): loss=0.09292330104717672\n",
      "Gradient Descent(482/999): loss=0.0929151503639651\n",
      "Gradient Descent(483/999): loss=0.09290702301742065\n",
      "Gradient Descent(484/999): loss=0.09289891889288188\n",
      "Gradient Descent(485/999): loss=0.0928908378763846\n",
      "Gradient Descent(486/999): loss=0.09288277985465776\n",
      "Gradient Descent(487/999): loss=0.09287474471511878\n",
      "Gradient Descent(488/999): loss=0.09286673234586924\n",
      "Gradient Descent(489/999): loss=0.0928587426356905\n",
      "Gradient Descent(490/999): loss=0.09285077547403917\n",
      "Gradient Descent(491/999): loss=0.09284283075104306\n",
      "Gradient Descent(492/999): loss=0.09283490835749666\n",
      "Gradient Descent(493/999): loss=0.09282700818485694\n",
      "Gradient Descent(494/999): loss=0.09281913012523908\n",
      "Gradient Descent(495/999): loss=0.0928112740714123\n",
      "Gradient Descent(496/999): loss=0.09280343991679563\n",
      "Gradient Descent(497/999): loss=0.09279562755545377\n",
      "Gradient Descent(498/999): loss=0.09278783688209284\n",
      "Gradient Descent(499/999): loss=0.09278006779205646\n",
      "Gradient Descent(500/999): loss=0.09277232018132155\n",
      "Gradient Descent(501/999): loss=0.09276459394649426\n",
      "Gradient Descent(502/999): loss=0.09275688898480597\n",
      "Gradient Descent(503/999): loss=0.09274920519410929\n",
      "Gradient Descent(504/999): loss=0.09274154247287407\n",
      "Gradient Descent(505/999): loss=0.09273390072018348\n",
      "Gradient Descent(506/999): loss=0.09272627983572997\n",
      "Gradient Descent(507/999): loss=0.09271867971981153\n",
      "Gradient Descent(508/999): loss=0.09271110027332771\n",
      "Gradient Descent(509/999): loss=0.09270354139777577\n",
      "Gradient Descent(510/999): loss=0.09269600299524686\n",
      "Gradient Descent(511/999): loss=0.09268848496842225\n",
      "Gradient Descent(512/999): loss=0.09268098722056956\n",
      "Gradient Descent(513/999): loss=0.0926735096555389\n",
      "Gradient Descent(514/999): loss=0.09266605217775935\n",
      "Gradient Descent(515/999): loss=0.09265861469223496\n",
      "Gradient Descent(516/999): loss=0.09265119710454134\n",
      "Gradient Descent(517/999): loss=0.09264379932082191\n",
      "Gradient Descent(518/999): loss=0.09263642124778423\n",
      "Gradient Descent(519/999): loss=0.09262906279269637\n",
      "Gradient Descent(520/999): loss=0.0926217238633834\n",
      "Gradient Descent(521/999): loss=0.09261440436822384\n",
      "Gradient Descent(522/999): loss=0.09260710421614606\n",
      "Gradient Descent(523/999): loss=0.09259982331662471\n",
      "Gradient Descent(524/999): loss=0.09259256157967745\n",
      "Gradient Descent(525/999): loss=0.0925853189158611\n",
      "Gradient Descent(526/999): loss=0.09257809523626866\n",
      "Gradient Descent(527/999): loss=0.09257089045252546\n",
      "Gradient Descent(528/999): loss=0.09256370447678608\n",
      "Gradient Descent(529/999): loss=0.09255653722173066\n",
      "Gradient Descent(530/999): loss=0.0925493886005619\n",
      "Gradient Descent(531/999): loss=0.09254225852700136\n",
      "Gradient Descent(532/999): loss=0.09253514691528641\n",
      "Gradient Descent(533/999): loss=0.09252805368016678\n",
      "Gradient Descent(534/999): loss=0.09252097873690139\n",
      "Gradient Descent(535/999): loss=0.09251392200125501\n",
      "Gradient Descent(536/999): loss=0.09250688338949511\n",
      "Gradient Descent(537/999): loss=0.09249986281838855\n",
      "Gradient Descent(538/999): loss=0.09249286020519851\n",
      "Gradient Descent(539/999): loss=0.09248587546768122\n",
      "Gradient Descent(540/999): loss=0.09247890852408283\n",
      "Gradient Descent(541/999): loss=0.09247195929313638\n",
      "Gradient Descent(542/999): loss=0.09246502769405858\n",
      "Gradient Descent(543/999): loss=0.09245811364654669\n",
      "Gradient Descent(544/999): loss=0.09245121707077562\n",
      "Gradient Descent(545/999): loss=0.09244433788739469\n",
      "Gradient Descent(546/999): loss=0.09243747601752479\n",
      "Gradient Descent(547/999): loss=0.09243063138275524\n",
      "Gradient Descent(548/999): loss=0.09242380390514078\n",
      "Gradient Descent(549/999): loss=0.09241699350719877\n",
      "Gradient Descent(550/999): loss=0.09241020011190604\n",
      "Gradient Descent(551/999): loss=0.092403423642696\n",
      "Gradient Descent(552/999): loss=0.09239666402345591\n",
      "Gradient Descent(553/999): loss=0.0923899211785238\n",
      "Gradient Descent(554/999): loss=0.09238319503268563\n",
      "Gradient Descent(555/999): loss=0.09237648551117245\n",
      "Gradient Descent(556/999): loss=0.09236979253965752\n",
      "Gradient Descent(557/999): loss=0.09236311604425367\n",
      "Gradient Descent(558/999): loss=0.09235645595151022\n",
      "Gradient Descent(559/999): loss=0.09234981218841044\n",
      "Gradient Descent(560/999): loss=0.09234318468236859\n",
      "Gradient Descent(561/999): loss=0.09233657336122743\n",
      "Gradient Descent(562/999): loss=0.09232997815325514\n",
      "Gradient Descent(563/999): loss=0.09232339898714295\n",
      "Gradient Descent(564/999): loss=0.09231683579200224\n",
      "Gradient Descent(565/999): loss=0.0923102884973619\n",
      "Gradient Descent(566/999): loss=0.09230375703316579\n",
      "Gradient Descent(567/999): loss=0.09229724132976984\n",
      "Gradient Descent(568/999): loss=0.09229074131793975\n",
      "Gradient Descent(569/999): loss=0.09228425692884809\n",
      "Gradient Descent(570/999): loss=0.09227778809407194\n",
      "Gradient Descent(571/999): loss=0.09227133474559016\n",
      "Gradient Descent(572/999): loss=0.09226489681578093\n",
      "Gradient Descent(573/999): loss=0.0922584742374191\n",
      "Gradient Descent(574/999): loss=0.09225206694367383\n",
      "Gradient Descent(575/999): loss=0.0922456748681059\n",
      "Gradient Descent(576/999): loss=0.09223929794466534\n",
      "Gradient Descent(577/999): loss=0.09223293610768891\n",
      "Gradient Descent(578/999): loss=0.0922265892918977\n",
      "Gradient Descent(579/999): loss=0.09222025743239459\n",
      "Gradient Descent(580/999): loss=0.09221394046466184\n",
      "Gradient Descent(581/999): loss=0.09220763832455882\n",
      "Gradient Descent(582/999): loss=0.09220135094831937\n",
      "Gradient Descent(583/999): loss=0.09219507827254962\n",
      "Gradient Descent(584/999): loss=0.09218882023422562\n",
      "Gradient Descent(585/999): loss=0.09218257677069079\n",
      "Gradient Descent(586/999): loss=0.09217634781965375\n",
      "Gradient Descent(587/999): loss=0.09217013331918612\n",
      "Gradient Descent(588/999): loss=0.0921639332077199\n",
      "Gradient Descent(589/999): loss=0.09215774742404542\n",
      "Gradient Descent(590/999): loss=0.09215157590730898\n",
      "Gradient Descent(591/999): loss=0.09214541859701061\n",
      "Gradient Descent(592/999): loss=0.0921392754330018\n",
      "Gradient Descent(593/999): loss=0.0921331463554833\n",
      "Gradient Descent(594/999): loss=0.09212703130500294\n",
      "Gradient Descent(595/999): loss=0.09212093022245331\n",
      "Gradient Descent(596/999): loss=0.09211484304906961\n",
      "Gradient Descent(597/999): loss=0.09210876972642763\n",
      "Gradient Descent(598/999): loss=0.0921027101964414\n",
      "Gradient Descent(599/999): loss=0.09209666440136108\n",
      "Gradient Descent(600/999): loss=0.0920906322837709\n",
      "Gradient Descent(601/999): loss=0.09208461378658697\n",
      "Gradient Descent(602/999): loss=0.09207860885305529\n",
      "Gradient Descent(603/999): loss=0.09207261742674949\n",
      "Gradient Descent(604/999): loss=0.09206663945156889\n",
      "Gradient Descent(605/999): loss=0.09206067487173632\n",
      "Gradient Descent(606/999): loss=0.09205472363179623\n",
      "Gradient Descent(607/999): loss=0.09204878567661254\n",
      "Gradient Descent(608/999): loss=0.09204286095136657\n",
      "Gradient Descent(609/999): loss=0.09203694940155512\n",
      "Gradient Descent(610/999): loss=0.09203105097298854\n",
      "Gradient Descent(611/999): loss=0.09202516561178847\n",
      "Gradient Descent(612/999): loss=0.09201929326438625\n",
      "Gradient Descent(613/999): loss=0.09201343387752067\n",
      "Gradient Descent(614/999): loss=0.09200758739823607\n",
      "Gradient Descent(615/999): loss=0.09200175377388053\n",
      "Gradient Descent(616/999): loss=0.09199593295210381\n",
      "Gradient Descent(617/999): loss=0.09199012488085558\n",
      "Gradient Descent(618/999): loss=0.09198432950838334\n",
      "Gradient Descent(619/999): loss=0.09197854678323071\n",
      "Gradient Descent(620/999): loss=0.09197277665423548\n",
      "Gradient Descent(621/999): loss=0.09196701907052776\n",
      "Gradient Descent(622/999): loss=0.09196127398152812\n",
      "Gradient Descent(623/999): loss=0.09195554133694572\n",
      "Gradient Descent(624/999): loss=0.09194982108677663\n",
      "Gradient Descent(625/999): loss=0.09194411318130183\n",
      "Gradient Descent(626/999): loss=0.09193841757108553\n",
      "Gradient Descent(627/999): loss=0.09193273420697336\n",
      "Gradient Descent(628/999): loss=0.09192706304009055\n",
      "Gradient Descent(629/999): loss=0.09192140402184021\n",
      "Gradient Descent(630/999): loss=0.09191575710390155\n",
      "Gradient Descent(631/999): loss=0.0919101222382282\n",
      "Gradient Descent(632/999): loss=0.09190449937704635\n",
      "Gradient Descent(633/999): loss=0.09189888847285312\n",
      "Gradient Descent(634/999): loss=0.09189328947841487\n",
      "Gradient Descent(635/999): loss=0.09188770234676542\n",
      "Gradient Descent(636/999): loss=0.09188212703120452\n",
      "Gradient Descent(637/999): loss=0.09187656348529584\n",
      "Gradient Descent(638/999): loss=0.09187101166286575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(639/999): loss=0.09186547151800131\n",
      "Gradient Descent(640/999): loss=0.09185994300504886\n",
      "Gradient Descent(641/999): loss=0.09185442607861216\n",
      "Gradient Descent(642/999): loss=0.091848920693551\n",
      "Gradient Descent(643/999): loss=0.09184342680497933\n",
      "Gradient Descent(644/999): loss=0.09183794436826401\n",
      "Gradient Descent(645/999): loss=0.09183247333902286\n",
      "Gradient Descent(646/999): loss=0.09182701367312329\n",
      "Gradient Descent(647/999): loss=0.09182156532668065\n",
      "Gradient Descent(648/999): loss=0.0918161282560567\n",
      "Gradient Descent(649/999): loss=0.09181070241785803\n",
      "Gradient Descent(650/999): loss=0.09180528776893455\n",
      "Gradient Descent(651/999): loss=0.09179988426637797\n",
      "Gradient Descent(652/999): loss=0.0917944918675202\n",
      "Gradient Descent(653/999): loss=0.09178911052993197\n",
      "Gradient Descent(654/999): loss=0.09178374021142112\n",
      "Gradient Descent(655/999): loss=0.0917783808700314\n",
      "Gradient Descent(656/999): loss=0.09177303246404073\n",
      "Gradient Descent(657/999): loss=0.09176769495195984\n",
      "Gradient Descent(658/999): loss=0.09176236829253077\n",
      "Gradient Descent(659/999): loss=0.0917570524447255\n",
      "Gradient Descent(660/999): loss=0.0917517473677444\n",
      "Gradient Descent(661/999): loss=0.0917464530210148\n",
      "Gradient Descent(662/999): loss=0.09174116936418972\n",
      "Gradient Descent(663/999): loss=0.09173589635714621\n",
      "Gradient Descent(664/999): loss=0.09173063395998414\n",
      "Gradient Descent(665/999): loss=0.09172538213302468\n",
      "Gradient Descent(666/999): loss=0.09172014083680903\n",
      "Gradient Descent(667/999): loss=0.09171491003209692\n",
      "Gradient Descent(668/999): loss=0.09170968967986527\n",
      "Gradient Descent(669/999): loss=0.0917044797413069\n",
      "Gradient Descent(670/999): loss=0.09169928017782912\n",
      "Gradient Descent(671/999): loss=0.09169409095105233\n",
      "Gradient Descent(672/999): loss=0.09168891202280877\n",
      "Gradient Descent(673/999): loss=0.0916837433551412\n",
      "Gradient Descent(674/999): loss=0.09167858491030152\n",
      "Gradient Descent(675/999): loss=0.09167343665074947\n",
      "Gradient Descent(676/999): loss=0.09166829853915132\n",
      "Gradient Descent(677/999): loss=0.09166317053837866\n",
      "Gradient Descent(678/999): loss=0.09165805261150702\n",
      "Gradient Descent(679/999): loss=0.09165294472181462\n",
      "Gradient Descent(680/999): loss=0.09164784683278109\n",
      "Gradient Descent(681/999): loss=0.0916427589080863\n",
      "Gradient Descent(682/999): loss=0.09163768091160897\n",
      "Gradient Descent(683/999): loss=0.09163261280742549\n",
      "Gradient Descent(684/999): loss=0.09162755455980867\n",
      "Gradient Descent(685/999): loss=0.09162250613322659\n",
      "Gradient Descent(686/999): loss=0.09161746749234125\n",
      "Gradient Descent(687/999): loss=0.09161243860200743\n",
      "Gradient Descent(688/999): loss=0.09160741942727157\n",
      "Gradient Descent(689/999): loss=0.0916024099333703\n",
      "Gradient Descent(690/999): loss=0.09159741008572962\n",
      "Gradient Descent(691/999): loss=0.09159241984996336\n",
      "Gradient Descent(692/999): loss=0.09158743919187227\n",
      "Gradient Descent(693/999): loss=0.09158246807744276\n",
      "Gradient Descent(694/999): loss=0.09157750647284572\n",
      "Gradient Descent(695/999): loss=0.09157255434443538\n",
      "Gradient Descent(696/999): loss=0.0915676116587482\n",
      "Gradient Descent(697/999): loss=0.09156267838250165\n",
      "Gradient Descent(698/999): loss=0.0915577544825932\n",
      "Gradient Descent(699/999): loss=0.0915528399260991\n",
      "Gradient Descent(700/999): loss=0.09154793468027331\n",
      "Gradient Descent(701/999): loss=0.09154303871254642\n",
      "Gradient Descent(702/999): loss=0.09153815199052444\n",
      "Gradient Descent(703/999): loss=0.09153327448198781\n",
      "Gradient Descent(704/999): loss=0.09152840615489033\n",
      "Gradient Descent(705/999): loss=0.09152354697735794\n",
      "Gradient Descent(706/999): loss=0.09151869691768787\n",
      "Gradient Descent(707/999): loss=0.09151385594434727\n",
      "Gradient Descent(708/999): loss=0.09150902402597255\n",
      "Gradient Descent(709/999): loss=0.0915042011313679\n",
      "Gradient Descent(710/999): loss=0.09149938722950454\n",
      "Gradient Descent(711/999): loss=0.09149458228951968\n",
      "Gradient Descent(712/999): loss=0.09148978628071518\n",
      "Gradient Descent(713/999): loss=0.09148499917255692\n",
      "Gradient Descent(714/999): loss=0.09148022093467365\n",
      "Gradient Descent(715/999): loss=0.09147545153685573\n",
      "Gradient Descent(716/999): loss=0.09147069094905451\n",
      "Gradient Descent(717/999): loss=0.09146593914138108\n",
      "Gradient Descent(718/999): loss=0.09146119608410536\n",
      "Gradient Descent(719/999): loss=0.09145646174765515\n",
      "Gradient Descent(720/999): loss=0.09145173610261498\n",
      "Gradient Descent(721/999): loss=0.09144701911972543\n",
      "Gradient Descent(722/999): loss=0.09144231076988184\n",
      "Gradient Descent(723/999): loss=0.09143761102413361\n",
      "Gradient Descent(724/999): loss=0.09143291985368308\n",
      "Gradient Descent(725/999): loss=0.09142823722988469\n",
      "Gradient Descent(726/999): loss=0.09142356312424392\n",
      "Gradient Descent(727/999): loss=0.09141889750841646\n",
      "Gradient Descent(728/999): loss=0.0914142403542073\n",
      "Gradient Descent(729/999): loss=0.09140959163356961\n",
      "Gradient Descent(730/999): loss=0.09140495131860411\n",
      "Gradient Descent(731/999): loss=0.0914003193815579\n",
      "Gradient Descent(732/999): loss=0.0913956957948237\n",
      "Gradient Descent(733/999): loss=0.09139108053093895\n",
      "Gradient Descent(734/999): loss=0.09138647356258478\n",
      "Gradient Descent(735/999): loss=0.09138187486258527\n",
      "Gradient Descent(736/999): loss=0.09137728440390656\n",
      "Gradient Descent(737/999): loss=0.09137270215965582\n",
      "Gradient Descent(738/999): loss=0.09136812810308059\n",
      "Gradient Descent(739/999): loss=0.09136356220756775\n",
      "Gradient Descent(740/999): loss=0.09135900444664265\n",
      "Gradient Descent(741/999): loss=0.09135445479396849\n",
      "Gradient Descent(742/999): loss=0.09134991322334515\n",
      "Gradient Descent(743/999): loss=0.09134537970870858\n",
      "Gradient Descent(744/999): loss=0.09134085422412984\n",
      "Gradient Descent(745/999): loss=0.09133633674381428\n",
      "Gradient Descent(746/999): loss=0.0913318272421008\n",
      "Gradient Descent(747/999): loss=0.09132732569346089\n",
      "Gradient Descent(748/999): loss=0.09132283207249799\n",
      "Gradient Descent(749/999): loss=0.0913183463539465\n",
      "Gradient Descent(750/999): loss=0.091313868512671\n",
      "Gradient Descent(751/999): loss=0.09130939852366561\n",
      "Gradient Descent(752/999): loss=0.09130493636205295\n",
      "Gradient Descent(753/999): loss=0.09130048200308363\n",
      "Gradient Descent(754/999): loss=0.09129603542213514\n",
      "Gradient Descent(755/999): loss=0.09129159659471141\n",
      "Gradient Descent(756/999): loss=0.0912871654964417\n",
      "Gradient Descent(757/999): loss=0.0912827421030801\n",
      "Gradient Descent(758/999): loss=0.09127832639050461\n",
      "Gradient Descent(759/999): loss=0.09127391833471642\n",
      "Gradient Descent(760/999): loss=0.09126951791183918\n",
      "Gradient Descent(761/999): loss=0.09126512509811821\n",
      "Gradient Descent(762/999): loss=0.0912607398699198\n",
      "Gradient Descent(763/999): loss=0.09125636220373037\n",
      "Gradient Descent(764/999): loss=0.0912519920761558\n",
      "Gradient Descent(765/999): loss=0.09124762946392079\n",
      "Gradient Descent(766/999): loss=0.09124327434386793\n",
      "Gradient Descent(767/999): loss=0.09123892669295715\n",
      "Gradient Descent(768/999): loss=0.0912345864882649\n",
      "Gradient Descent(769/999): loss=0.0912302537069835\n",
      "Gradient Descent(770/999): loss=0.09122592832642037\n",
      "Gradient Descent(771/999): loss=0.09122161032399742\n",
      "Gradient Descent(772/999): loss=0.09121729967725017\n",
      "Gradient Descent(773/999): loss=0.09121299636382728\n",
      "Gradient Descent(774/999): loss=0.0912087003614897\n",
      "Gradient Descent(775/999): loss=0.09120441164811008\n",
      "Gradient Descent(776/999): loss=0.09120013020167196\n",
      "Gradient Descent(777/999): loss=0.09119585600026918\n",
      "Gradient Descent(778/999): loss=0.09119158902210539\n",
      "Gradient Descent(779/999): loss=0.09118732924549287\n",
      "Gradient Descent(780/999): loss=0.09118307664885242\n",
      "Gradient Descent(781/999): loss=0.09117883121071238\n",
      "Gradient Descent(782/999): loss=0.09117459290970807\n",
      "Gradient Descent(783/999): loss=0.09117036172458115\n",
      "Gradient Descent(784/999): loss=0.0911661376341789\n",
      "Gradient Descent(785/999): loss=0.09116192061745355\n",
      "Gradient Descent(786/999): loss=0.09115771065346195\n",
      "Gradient Descent(787/999): loss=0.09115350772136444\n",
      "Gradient Descent(788/999): loss=0.09114931180042463\n",
      "Gradient Descent(789/999): loss=0.09114512287000856\n",
      "Gradient Descent(790/999): loss=0.09114094090958412\n",
      "Gradient Descent(791/999): loss=0.09113676589872048\n",
      "Gradient Descent(792/999): loss=0.09113259781708741\n",
      "Gradient Descent(793/999): loss=0.09112843664445473\n",
      "Gradient Descent(794/999): loss=0.09112428236069166\n",
      "Gradient Descent(795/999): loss=0.09112013494576612\n",
      "Gradient Descent(796/999): loss=0.09111599437974441\n",
      "Gradient Descent(797/999): loss=0.0911118606427903\n",
      "Gradient Descent(798/999): loss=0.0911077337151647\n",
      "Gradient Descent(799/999): loss=0.09110361357722478\n",
      "Gradient Descent(800/999): loss=0.09109950020942366\n",
      "Gradient Descent(801/999): loss=0.09109539359230971\n",
      "Gradient Descent(802/999): loss=0.09109129370652602\n",
      "Gradient Descent(803/999): loss=0.09108720053280969\n",
      "Gradient Descent(804/999): loss=0.09108311405199149\n",
      "Gradient Descent(805/999): loss=0.09107903424499499\n",
      "Gradient Descent(806/999): loss=0.09107496109283632\n",
      "Gradient Descent(807/999): loss=0.09107089457662343\n",
      "Gradient Descent(808/999): loss=0.09106683467755558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(809/999): loss=0.09106278137692264\n",
      "Gradient Descent(810/999): loss=0.09105873465610484\n",
      "Gradient Descent(811/999): loss=0.09105469449657204\n",
      "Gradient Descent(812/999): loss=0.09105066087988312\n",
      "Gradient Descent(813/999): loss=0.09104663378768557\n",
      "Gradient Descent(814/999): loss=0.0910426132017149\n",
      "Gradient Descent(815/999): loss=0.09103859910379428\n",
      "Gradient Descent(816/999): loss=0.09103459147583357\n",
      "Gradient Descent(817/999): loss=0.09103059029982935\n",
      "Gradient Descent(818/999): loss=0.09102659555786402\n",
      "Gradient Descent(819/999): loss=0.09102260723210535\n",
      "Gradient Descent(820/999): loss=0.09101862530480613\n",
      "Gradient Descent(821/999): loss=0.09101464975830344\n",
      "Gradient Descent(822/999): loss=0.09101068057501825\n",
      "Gradient Descent(823/999): loss=0.091006717737455\n",
      "Gradient Descent(824/999): loss=0.09100276122820092\n",
      "Gradient Descent(825/999): loss=0.09099881102992557\n",
      "Gradient Descent(826/999): loss=0.09099486712538056\n",
      "Gradient Descent(827/999): loss=0.09099092949739875\n",
      "Gradient Descent(828/999): loss=0.09098699812889392\n",
      "Gradient Descent(829/999): loss=0.09098307300286036\n",
      "Gradient Descent(830/999): loss=0.09097915410237217\n",
      "Gradient Descent(831/999): loss=0.09097524141058305\n",
      "Gradient Descent(832/999): loss=0.0909713349107255\n",
      "Gradient Descent(833/999): loss=0.09096743458611063\n",
      "Gradient Descent(834/999): loss=0.09096354042012768\n",
      "Gradient Descent(835/999): loss=0.09095965239624326\n",
      "Gradient Descent(836/999): loss=0.09095577049800124\n",
      "Gradient Descent(837/999): loss=0.09095189470902205\n",
      "Gradient Descent(838/999): loss=0.09094802501300235\n",
      "Gradient Descent(839/999): loss=0.0909441613937145\n",
      "Gradient Descent(840/999): loss=0.09094030383500619\n",
      "Gradient Descent(841/999): loss=0.09093645232079985\n",
      "Gradient Descent(842/999): loss=0.09093260683509241\n",
      "Gradient Descent(843/999): loss=0.0909287673619546\n",
      "Gradient Descent(844/999): loss=0.09092493388553077\n",
      "Gradient Descent(845/999): loss=0.0909211063900383\n",
      "Gradient Descent(846/999): loss=0.09091728485976712\n",
      "Gradient Descent(847/999): loss=0.09091346927907944\n",
      "Gradient Descent(848/999): loss=0.09090965963240917\n",
      "Gradient Descent(849/999): loss=0.09090585590426159\n",
      "Gradient Descent(850/999): loss=0.0909020580792129\n",
      "Gradient Descent(851/999): loss=0.09089826614190974\n",
      "Gradient Descent(852/999): loss=0.09089448007706888\n",
      "Gradient Descent(853/999): loss=0.0908906998694767\n",
      "Gradient Descent(854/999): loss=0.09088692550398883\n",
      "Gradient Descent(855/999): loss=0.09088315696552972\n",
      "Gradient Descent(856/999): loss=0.09087939423909228\n",
      "Gradient Descent(857/999): loss=0.0908756373097374\n",
      "Gradient Descent(858/999): loss=0.09087188616259352\n",
      "Gradient Descent(859/999): loss=0.09086814078285645\n",
      "Gradient Descent(860/999): loss=0.09086440115578868\n",
      "Gradient Descent(861/999): loss=0.09086066726671918\n",
      "Gradient Descent(862/999): loss=0.09085693910104288\n",
      "Gradient Descent(863/999): loss=0.09085321664422039\n",
      "Gradient Descent(864/999): loss=0.09084949988177761\n",
      "Gradient Descent(865/999): loss=0.09084578879930517\n",
      "Gradient Descent(866/999): loss=0.09084208338245837\n",
      "Gradient Descent(867/999): loss=0.09083838361695643\n",
      "Gradient Descent(868/999): loss=0.09083468948858231\n",
      "Gradient Descent(869/999): loss=0.09083100098318247\n",
      "Gradient Descent(870/999): loss=0.09082731808666612\n",
      "Gradient Descent(871/999): loss=0.09082364078500524\n",
      "Gradient Descent(872/999): loss=0.09081996906423394\n",
      "Gradient Descent(873/999): loss=0.09081630291044823\n",
      "Gradient Descent(874/999): loss=0.09081264230980565\n",
      "Gradient Descent(875/999): loss=0.0908089872485248\n",
      "Gradient Descent(876/999): loss=0.0908053377128851\n",
      "Gradient Descent(877/999): loss=0.09080169368922643\n",
      "Gradient Descent(878/999): loss=0.09079805516394866\n",
      "Gradient Descent(879/999): loss=0.09079442212351146\n",
      "Gradient Descent(880/999): loss=0.09079079455443377\n",
      "Gradient Descent(881/999): loss=0.09078717244329361\n",
      "Gradient Descent(882/999): loss=0.09078355577672768\n",
      "Gradient Descent(883/999): loss=0.09077994454143094\n",
      "Gradient Descent(884/999): loss=0.09077633872415633\n",
      "Gradient Descent(885/999): loss=0.09077273831171456\n",
      "Gradient Descent(886/999): loss=0.09076914329097353\n",
      "Gradient Descent(887/999): loss=0.09076555364885816\n",
      "Gradient Descent(888/999): loss=0.09076196937234997\n",
      "Gradient Descent(889/999): loss=0.09075839044848683\n",
      "Gradient Descent(890/999): loss=0.09075481686436258\n",
      "Gradient Descent(891/999): loss=0.09075124860712667\n",
      "Gradient Descent(892/999): loss=0.090747685663984\n",
      "Gradient Descent(893/999): loss=0.09074412802219436\n",
      "Gradient Descent(894/999): loss=0.09074057566907223\n",
      "Gradient Descent(895/999): loss=0.09073702859198651\n",
      "Gradient Descent(896/999): loss=0.09073348677836018\n",
      "Gradient Descent(897/999): loss=0.09072995021566986\n",
      "Gradient Descent(898/999): loss=0.09072641889144564\n",
      "Gradient Descent(899/999): loss=0.09072289279327078\n",
      "Gradient Descent(900/999): loss=0.09071937190878125\n",
      "Gradient Descent(901/999): loss=0.09071585622566557\n",
      "Gradient Descent(902/999): loss=0.09071234573166448\n",
      "Gradient Descent(903/999): loss=0.0907088404145706\n",
      "Gradient Descent(904/999): loss=0.0907053402622281\n",
      "Gradient Descent(905/999): loss=0.09070184526253243\n",
      "Gradient Descent(906/999): loss=0.09069835540343014\n",
      "Gradient Descent(907/999): loss=0.09069487067291843\n",
      "Gradient Descent(908/999): loss=0.09069139105904483\n",
      "Gradient Descent(909/999): loss=0.09068791654990713\n",
      "Gradient Descent(910/999): loss=0.09068444713365284\n",
      "Gradient Descent(911/999): loss=0.09068098279847905\n",
      "Gradient Descent(912/999): loss=0.09067752353263209\n",
      "Gradient Descent(913/999): loss=0.09067406932440729\n",
      "Gradient Descent(914/999): loss=0.09067062016214861\n",
      "Gradient Descent(915/999): loss=0.09066717603424851\n",
      "Gradient Descent(916/999): loss=0.09066373692914746\n",
      "Gradient Descent(917/999): loss=0.09066030283533391\n",
      "Gradient Descent(918/999): loss=0.09065687374134382\n",
      "Gradient Descent(919/999): loss=0.09065344963576047\n",
      "Gradient Descent(920/999): loss=0.09065003050721411\n",
      "Gradient Descent(921/999): loss=0.09064661634438183\n",
      "Gradient Descent(922/999): loss=0.09064320713598728\n",
      "Gradient Descent(923/999): loss=0.09063980287080012\n",
      "Gradient Descent(924/999): loss=0.09063640353763618\n",
      "Gradient Descent(925/999): loss=0.09063300912535695\n",
      "Gradient Descent(926/999): loss=0.09062961962286924\n",
      "Gradient Descent(927/999): loss=0.09062623501912517\n",
      "Gradient Descent(928/999): loss=0.09062285530312175\n",
      "Gradient Descent(929/999): loss=0.0906194804639006\n",
      "Gradient Descent(930/999): loss=0.09061611049054782\n",
      "Gradient Descent(931/999): loss=0.09061274537219366\n",
      "Gradient Descent(932/999): loss=0.0906093850980122\n",
      "Gradient Descent(933/999): loss=0.09060602965722127\n",
      "Gradient Descent(934/999): loss=0.09060267903908202\n",
      "Gradient Descent(935/999): loss=0.09059933323289887\n",
      "Gradient Descent(936/999): loss=0.09059599222801903\n",
      "Gradient Descent(937/999): loss=0.09059265601383248\n",
      "Gradient Descent(938/999): loss=0.09058932457977155\n",
      "Gradient Descent(939/999): loss=0.09058599791531077\n",
      "Gradient Descent(940/999): loss=0.09058267600996671\n",
      "Gradient Descent(941/999): loss=0.09057935885329757\n",
      "Gradient Descent(942/999): loss=0.090576046434903\n",
      "Gradient Descent(943/999): loss=0.09057273874442393\n",
      "Gradient Descent(944/999): loss=0.09056943577154228\n",
      "Gradient Descent(945/999): loss=0.09056613750598078\n",
      "Gradient Descent(946/999): loss=0.09056284393750265\n",
      "Gradient Descent(947/999): loss=0.09055955505591147\n",
      "Gradient Descent(948/999): loss=0.0905562708510509\n",
      "Gradient Descent(949/999): loss=0.09055299131280442\n",
      "Gradient Descent(950/999): loss=0.0905497164310952\n",
      "Gradient Descent(951/999): loss=0.09054644619588578\n",
      "Gradient Descent(952/999): loss=0.09054318059717795\n",
      "Gradient Descent(953/999): loss=0.0905399196250124\n",
      "Gradient Descent(954/999): loss=0.0905366632694686\n",
      "Gradient Descent(955/999): loss=0.09053341152066458\n",
      "Gradient Descent(956/999): loss=0.09053016436875666\n",
      "Gradient Descent(957/999): loss=0.09052692180393931\n",
      "Gradient Descent(958/999): loss=0.09052368381644482\n",
      "Gradient Descent(959/999): loss=0.09052045039654316\n",
      "Gradient Descent(960/999): loss=0.09051722153454182\n",
      "Gradient Descent(961/999): loss=0.09051399722078557\n",
      "Gradient Descent(962/999): loss=0.09051077744565615\n",
      "Gradient Descent(963/999): loss=0.0905075621995722\n",
      "Gradient Descent(964/999): loss=0.09050435147298894\n",
      "Gradient Descent(965/999): loss=0.09050114525639807\n",
      "Gradient Descent(966/999): loss=0.09049794354032753\n",
      "Gradient Descent(967/999): loss=0.09049474631534121\n",
      "Gradient Descent(968/999): loss=0.09049155357203892\n",
      "Gradient Descent(969/999): loss=0.0904883653010561\n",
      "Gradient Descent(970/999): loss=0.09048518149306344\n",
      "Gradient Descent(971/999): loss=0.09048200213876714\n",
      "Gradient Descent(972/999): loss=0.0904788272289083\n",
      "Gradient Descent(973/999): loss=0.0904756567542628\n",
      "Gradient Descent(974/999): loss=0.09047249070564124\n",
      "Gradient Descent(975/999): loss=0.09046932907388867\n",
      "Gradient Descent(976/999): loss=0.09046617184988451\n",
      "Gradient Descent(977/999): loss=0.09046301902454203\n",
      "Gradient Descent(978/999): loss=0.09045987058880857\n",
      "Gradient Descent(979/999): loss=0.09045672653366514\n",
      "Gradient Descent(980/999): loss=0.09045358685012624\n",
      "Gradient Descent(981/999): loss=0.09045045152923971\n",
      "Gradient Descent(982/999): loss=0.09044732056208656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(983/999): loss=0.0904441939397807\n",
      "Gradient Descent(984/999): loss=0.09044107165346894\n",
      "Gradient Descent(985/999): loss=0.09043795369433055\n",
      "Gradient Descent(986/999): loss=0.09043484005357742\n",
      "Gradient Descent(987/999): loss=0.09043173072245347\n",
      "Gradient Descent(988/999): loss=0.09042862569223481\n",
      "Gradient Descent(989/999): loss=0.09042552495422948\n",
      "Gradient Descent(990/999): loss=0.09042242849977718\n",
      "Gradient Descent(991/999): loss=0.0904193363202491\n",
      "Gradient Descent(992/999): loss=0.09041624840704797\n",
      "Gradient Descent(993/999): loss=0.09041316475160754\n",
      "Gradient Descent(994/999): loss=0.09041008534539277\n",
      "Gradient Descent(995/999): loss=0.09040701017989934\n",
      "Gradient Descent(996/999): loss=0.09040393924665371\n",
      "Gradient Descent(997/999): loss=0.0904008725372128\n",
      "Gradient Descent(998/999): loss=0.09039781004316395\n",
      "Gradient Descent(999/999): loss=0.09039475175612473\n",
      "Gradient Descent(0/999): loss=0.1474419593440508\n",
      "Gradient Descent(1/999): loss=0.1347117309955451\n",
      "Gradient Descent(2/999): loss=0.12575214277546684\n",
      "Gradient Descent(3/999): loss=0.1194405140744494\n",
      "Gradient Descent(4/999): loss=0.11498846789822538\n",
      "Gradient Descent(5/999): loss=0.11184238415727998\n",
      "Gradient Descent(6/999): loss=0.10961349531669533\n",
      "Gradient Descent(7/999): loss=0.10802879770044027\n",
      "Gradient Descent(8/999): loss=0.10689658010235517\n",
      "Gradient Descent(9/999): loss=0.10608221705903037\n",
      "Gradient Descent(10/999): loss=0.10549117024152765\n",
      "Gradient Descent(11/999): loss=0.10505705158004433\n",
      "Gradient Descent(12/999): loss=0.10473324087220054\n",
      "Gradient Descent(13/999): loss=0.10448699944435451\n",
      "Gradient Descent(14/999): loss=0.1042953366078024\n",
      "Gradient Descent(15/999): loss=0.10414210697421018\n",
      "Gradient Descent(16/999): loss=0.10401597211316602\n",
      "Gradient Descent(17/999): loss=0.10390896917378148\n",
      "Gradient Descent(18/999): loss=0.10381550573260692\n",
      "Gradient Descent(19/999): loss=0.10373165394901272\n",
      "Gradient Descent(20/999): loss=0.10365465490224238\n",
      "Gradient Descent(21/999): loss=0.10358257052363182\n",
      "Gradient Descent(22/999): loss=0.10351403917408528\n",
      "Gradient Descent(23/999): loss=0.10344810400400986\n",
      "Gradient Descent(24/999): loss=0.10338409242303209\n",
      "Gradient Descent(25/999): loss=0.1033215314603644\n",
      "Gradient Descent(26/999): loss=0.10326008832854314\n",
      "Gradient Descent(27/999): loss=0.1031995286856454\n",
      "Gradient Descent(28/999): loss=0.10313968732585041\n",
      "Gradient Descent(29/999): loss=0.10308044759751608\n",
      "Gradient Descent(30/999): loss=0.10302172694994967\n",
      "Gradient Descent(31/999): loss=0.10296346678391245\n",
      "Gradient Descent(32/999): loss=0.10290562532432077\n",
      "Gradient Descent(33/999): loss=0.1028481726152157\n",
      "Gradient Descent(34/999): loss=0.10279108700504648\n",
      "Gradient Descent(35/999): loss=0.10273435267849323\n",
      "Gradient Descent(36/999): loss=0.10267795792319798\n",
      "Gradient Descent(37/999): loss=0.10262189391256858\n",
      "Gradient Descent(38/999): loss=0.10256615385098428\n",
      "Gradient Descent(39/999): loss=0.102510732373489\n",
      "Gradient Descent(40/999): loss=0.10245562512419452\n",
      "Gradient Descent(41/999): loss=0.10240082846017867\n",
      "Gradient Descent(42/999): loss=0.1023463392435103\n",
      "Gradient Descent(43/999): loss=0.10229215469515991\n",
      "Gradient Descent(44/999): loss=0.10223827229236891\n",
      "Gradient Descent(45/999): loss=0.10218468969653731\n",
      "Gradient Descent(46/999): loss=0.10213140470254259\n",
      "Gradient Descent(47/999): loss=0.10207841520310941\n",
      "Gradient Descent(48/999): loss=0.10202571916374843\n",
      "Gradient Descent(49/999): loss=0.10197331460511823\n",
      "Gradient Descent(50/999): loss=0.10192119959059992\n",
      "Gradient Descent(51/999): loss=0.10186937221753348\n",
      "Gradient Descent(52/999): loss=0.10181783061102566\n",
      "Gradient Descent(53/999): loss=0.10176657291956413\n",
      "Gradient Descent(54/999): loss=0.10171559731190129\n",
      "Gradient Descent(55/999): loss=0.10166490197482937\n",
      "Gradient Descent(56/999): loss=0.10161448511158255\n",
      "Gradient Descent(57/999): loss=0.1015643449406793\n",
      "Gradient Descent(58/999): loss=0.10151447969507547\n",
      "Gradient Descent(59/999): loss=0.10146488762153479\n",
      "Gradient Descent(60/999): loss=0.10141556698015354\n",
      "Gradient Descent(61/999): loss=0.10136651604399309\n",
      "Gradient Descent(62/999): loss=0.10131773309878986\n",
      "Gradient Descent(63/999): loss=0.10126921644271875\n",
      "Gradient Descent(64/999): loss=0.10122096438619546\n",
      "Gradient Descent(65/999): loss=0.10117297525170649\n",
      "Gradient Descent(66/999): loss=0.10112524737365848\n",
      "Gradient Descent(67/999): loss=0.10107777909824238\n",
      "Gradient Descent(68/999): loss=0.1010305687833074\n",
      "Gradient Descent(69/999): loss=0.10098361479824354\n",
      "Gradient Descent(70/999): loss=0.10093691552386905\n",
      "Gradient Descent(71/999): loss=0.10089046935232336\n",
      "Gradient Descent(72/999): loss=0.10084427468696266\n",
      "Gradient Descent(73/999): loss=0.10079832994225893\n",
      "Gradient Descent(74/999): loss=0.10075263354370119\n",
      "Gradient Descent(75/999): loss=0.10070718392769859\n",
      "Gradient Descent(76/999): loss=0.1006619795414857\n",
      "Gradient Descent(77/999): loss=0.1006170188430289\n",
      "Gradient Descent(78/999): loss=0.10057230030093459\n",
      "Gradient Descent(79/999): loss=0.10052782239435876\n",
      "Gradient Descent(80/999): loss=0.10048358361291758\n",
      "Gradient Descent(81/999): loss=0.10043958245659944\n",
      "Gradient Descent(82/999): loss=0.10039581743567814\n",
      "Gradient Descent(83/999): loss=0.10035228707062718\n",
      "Gradient Descent(84/999): loss=0.10030898989203503\n",
      "Gradient Descent(85/999): loss=0.10026592444052174\n",
      "Gradient Descent(86/999): loss=0.10022308926665635\n",
      "Gradient Descent(87/999): loss=0.10018048293087542\n",
      "Gradient Descent(88/999): loss=0.10013810400340256\n",
      "Gradient Descent(89/999): loss=0.100095951064169\n",
      "Gradient Descent(90/999): loss=0.10005402270273485\n",
      "Gradient Descent(91/999): loss=0.10001231751821164\n",
      "Gradient Descent(92/999): loss=0.09997083411918564\n",
      "Gradient Descent(93/999): loss=0.09992957112364201\n",
      "Gradient Descent(94/999): loss=0.09988852715888982\n",
      "Gradient Descent(95/999): loss=0.0998477008614882\n",
      "Gradient Descent(96/999): loss=0.09980709087717296\n",
      "Gradient Descent(97/999): loss=0.09976669586078439\n",
      "Gradient Descent(98/999): loss=0.09972651447619568\n",
      "Gradient Descent(99/999): loss=0.09968654539624218\n",
      "Gradient Descent(100/999): loss=0.09964678730265145\n",
      "Gradient Descent(101/999): loss=0.09960723888597425\n",
      "Gradient Descent(102/999): loss=0.09956789884551602\n",
      "Gradient Descent(103/999): loss=0.09952876588926926\n",
      "Gradient Descent(104/999): loss=0.0994898387338467\n",
      "Gradient Descent(105/999): loss=0.09945111610441516\n",
      "Gradient Descent(106/999): loss=0.09941259673463004\n",
      "Gradient Descent(107/999): loss=0.09937427936657045\n",
      "Gradient Descent(108/999): loss=0.09933616275067546\n",
      "Gradient Descent(109/999): loss=0.09929824564568036\n",
      "Gradient Descent(110/999): loss=0.09926052681855431\n",
      "Gradient Descent(111/999): loss=0.09922300504443796\n",
      "Gradient Descent(112/999): loss=0.09918567910658214\n",
      "Gradient Descent(113/999): loss=0.09914854779628712\n",
      "Gradient Descent(114/999): loss=0.09911160991284255\n",
      "Gradient Descent(115/999): loss=0.09907486426346762\n",
      "Gradient Descent(116/999): loss=0.09903830966325236\n",
      "Gradient Descent(117/999): loss=0.0990019449350992\n",
      "Gradient Descent(118/999): loss=0.09896576890966524\n",
      "Gradient Descent(119/999): loss=0.09892978042530506\n",
      "Gradient Descent(120/999): loss=0.09889397832801403\n",
      "Gradient Descent(121/999): loss=0.09885836147137228\n",
      "Gradient Descent(122/999): loss=0.09882292871648925\n",
      "Gradient Descent(123/999): loss=0.09878767893194862\n",
      "Gradient Descent(124/999): loss=0.09875261099375386\n",
      "Gradient Descent(125/999): loss=0.09871772378527444\n",
      "Gradient Descent(126/999): loss=0.0986830161971922\n",
      "Gradient Descent(127/999): loss=0.09864848712744859\n",
      "Gradient Descent(128/999): loss=0.09861413548119231\n",
      "Gradient Descent(129/999): loss=0.0985799601707273\n",
      "Gradient Descent(130/999): loss=0.09854596011546132\n",
      "Gradient Descent(131/999): loss=0.09851213424185513\n",
      "Gradient Descent(132/999): loss=0.09847848148337182\n",
      "Gradient Descent(133/999): loss=0.09844500078042698\n",
      "Gradient Descent(134/999): loss=0.09841169108033906\n",
      "Gradient Descent(135/999): loss=0.09837855133728027\n",
      "Gradient Descent(136/999): loss=0.09834558051222796\n",
      "Gradient Descent(137/999): loss=0.0983127775729163\n",
      "Gradient Descent(138/999): loss=0.09828014149378875\n",
      "Gradient Descent(139/999): loss=0.09824767125595035\n",
      "Gradient Descent(140/999): loss=0.09821536584712112\n",
      "Gradient Descent(141/999): loss=0.09818322426158922\n",
      "Gradient Descent(142/999): loss=0.09815124550016512\n",
      "Gradient Descent(143/999): loss=0.09811942857013575\n",
      "Gradient Descent(144/999): loss=0.09808777248521915\n",
      "Gradient Descent(145/999): loss=0.09805627626551967\n",
      "Gradient Descent(146/999): loss=0.09802493893748332\n",
      "Gradient Descent(147/999): loss=0.09799375953385366\n",
      "Gradient Descent(148/999): loss=0.09796273709362814\n",
      "Gradient Descent(149/999): loss=0.09793187066201453\n",
      "Gradient Descent(150/999): loss=0.09790115929038797\n",
      "Gradient Descent(151/999): loss=0.09787060203624828\n",
      "Gradient Descent(152/999): loss=0.09784019796317772\n",
      "Gradient Descent(153/999): loss=0.09780994614079891\n",
      "Gradient Descent(154/999): loss=0.0977798456447334\n",
      "Gradient Descent(155/999): loss=0.09774989555656013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(156/999): loss=0.09772009496377483\n",
      "Gradient Descent(157/999): loss=0.0976904429597492\n",
      "Gradient Descent(158/999): loss=0.09766093864369084\n",
      "Gradient Descent(159/999): loss=0.09763158112060313\n",
      "Gradient Descent(160/999): loss=0.09760236950124579\n",
      "Gradient Descent(161/999): loss=0.09757330290209551\n",
      "Gradient Descent(162/999): loss=0.09754438044530706\n",
      "Gradient Descent(163/999): loss=0.09751560125867456\n",
      "Gradient Descent(164/999): loss=0.09748696447559328\n",
      "Gradient Descent(165/999): loss=0.09745846923502148\n",
      "Gradient Descent(166/999): loss=0.09743011468144257\n",
      "Gradient Descent(167/999): loss=0.09740189996482805\n",
      "Gradient Descent(168/999): loss=0.09737382424060001\n",
      "Gradient Descent(169/999): loss=0.09734588666959444\n",
      "Gradient Descent(170/999): loss=0.0973180864180247\n",
      "Gradient Descent(171/999): loss=0.09729042265744516\n",
      "Gradient Descent(172/999): loss=0.09726289456471532\n",
      "Gradient Descent(173/999): loss=0.09723550132196393\n",
      "Gradient Descent(174/999): loss=0.09720824211655384\n",
      "Gradient Descent(175/999): loss=0.09718111614104648\n",
      "Gradient Descent(176/999): loss=0.09715412259316727\n",
      "Gradient Descent(177/999): loss=0.09712726067577092\n",
      "Gradient Descent(178/999): loss=0.09710052959680691\n",
      "Gradient Descent(179/999): loss=0.09707392856928566\n",
      "Gradient Descent(180/999): loss=0.09704745681124449\n",
      "Gradient Descent(181/999): loss=0.09702111354571412\n",
      "Gradient Descent(182/999): loss=0.0969948980006853\n",
      "Gradient Descent(183/999): loss=0.09696880940907579\n",
      "Gradient Descent(184/999): loss=0.09694284700869753\n",
      "Gradient Descent(185/999): loss=0.09691701004222394\n",
      "Gradient Descent(186/999): loss=0.09689129775715778\n",
      "Gradient Descent(187/999): loss=0.09686570940579889\n",
      "Gradient Descent(188/999): loss=0.09684024424521233\n",
      "Gradient Descent(189/999): loss=0.0968149015371969\n",
      "Gradient Descent(190/999): loss=0.09678968054825357\n",
      "Gradient Descent(191/999): loss=0.09676458054955445\n",
      "Gradient Descent(192/999): loss=0.09673960081691177\n",
      "Gradient Descent(193/999): loss=0.09671474063074725\n",
      "Gradient Descent(194/999): loss=0.09668999927606159\n",
      "Gradient Descent(195/999): loss=0.09666537604240417\n",
      "Gradient Descent(196/999): loss=0.0966408702238431\n",
      "Gradient Descent(197/999): loss=0.09661648111893525\n",
      "Gradient Descent(198/999): loss=0.0965922080306969\n",
      "Gradient Descent(199/999): loss=0.09656805026657408\n",
      "Gradient Descent(200/999): loss=0.09654400713841349\n",
      "Gradient Descent(201/999): loss=0.0965200779624337\n",
      "Gradient Descent(202/999): loss=0.09649626205919608\n",
      "Gradient Descent(203/999): loss=0.09647255875357644\n",
      "Gradient Descent(204/999): loss=0.09644896737473675\n",
      "Gradient Descent(205/999): loss=0.09642548725609675\n",
      "Gradient Descent(206/999): loss=0.09640211773530624\n",
      "Gradient Descent(207/999): loss=0.09637885815421728\n",
      "Gradient Descent(208/999): loss=0.09635570785885643\n",
      "Gradient Descent(209/999): loss=0.09633266619939772\n",
      "Gradient Descent(210/999): loss=0.0963097325301352\n",
      "Gradient Descent(211/999): loss=0.0962869062094562\n",
      "Gradient Descent(212/999): loss=0.09626418659981442\n",
      "Gradient Descent(213/999): loss=0.09624157306770335\n",
      "Gradient Descent(214/999): loss=0.09621906498362985\n",
      "Gradient Descent(215/999): loss=0.09619666172208806\n",
      "Gradient Descent(216/999): loss=0.0961743626615331\n",
      "Gradient Descent(217/999): loss=0.09615216718435554\n",
      "Gradient Descent(218/999): loss=0.09613007467685544\n",
      "Gradient Descent(219/999): loss=0.09610808452921704\n",
      "Gradient Descent(220/999): loss=0.09608619613548335\n",
      "Gradient Descent(221/999): loss=0.09606440889353103\n",
      "Gradient Descent(222/999): loss=0.09604272220504545\n",
      "Gradient Descent(223/999): loss=0.09602113547549579\n",
      "Gradient Descent(224/999): loss=0.09599964811411062\n",
      "Gradient Descent(225/999): loss=0.09597825953385318\n",
      "Gradient Descent(226/999): loss=0.09595696915139733\n",
      "Gradient Descent(227/999): loss=0.09593577638710327\n",
      "Gradient Descent(228/999): loss=0.09591468066499369\n",
      "Gradient Descent(229/999): loss=0.09589368141272987\n",
      "Gradient Descent(230/999): loss=0.09587277806158824\n",
      "Gradient Descent(231/999): loss=0.09585197004643665\n",
      "Gradient Descent(232/999): loss=0.09583125680571132\n",
      "Gradient Descent(233/999): loss=0.09581063778139354\n",
      "Gradient Descent(234/999): loss=0.09579011241898674\n",
      "Gradient Descent(235/999): loss=0.0957696801674937\n",
      "Gradient Descent(236/999): loss=0.09574934047939368\n",
      "Gradient Descent(237/999): loss=0.09572909281062021\n",
      "Gradient Descent(238/999): loss=0.09570893662053845\n",
      "Gradient Descent(239/999): loss=0.09568887137192317\n",
      "Gradient Descent(240/999): loss=0.09566889653093644\n",
      "Gradient Descent(241/999): loss=0.09564901156710603\n",
      "Gradient Descent(242/999): loss=0.09562921595330334\n",
      "Gradient Descent(243/999): loss=0.09560950916572206\n",
      "Gradient Descent(244/999): loss=0.0955898906838564\n",
      "Gradient Descent(245/999): loss=0.09557035999048002\n",
      "Gradient Descent(246/999): loss=0.09555091657162464\n",
      "Gradient Descent(247/999): loss=0.09553155991655918\n",
      "Gradient Descent(248/999): loss=0.09551228951776865\n",
      "Gradient Descent(249/999): loss=0.09549310487093357\n",
      "Gradient Descent(250/999): loss=0.09547400547490927\n",
      "Gradient Descent(251/999): loss=0.09545499083170543\n",
      "Gradient Descent(252/999): loss=0.09543606044646558\n",
      "Gradient Descent(253/999): loss=0.09541721382744717\n",
      "Gradient Descent(254/999): loss=0.09539845048600128\n",
      "Gradient Descent(255/999): loss=0.0953797699365527\n",
      "Gradient Descent(256/999): loss=0.09536117169658018\n",
      "Gradient Descent(257/999): loss=0.09534265528659674\n",
      "Gradient Descent(258/999): loss=0.09532422023013008\n",
      "Gradient Descent(259/999): loss=0.09530586605370324\n",
      "Gradient Descent(260/999): loss=0.0952875922868152\n",
      "Gradient Descent(261/999): loss=0.09526939846192176\n",
      "Gradient Descent(262/999): loss=0.09525128411441666\n",
      "Gradient Descent(263/999): loss=0.0952332487826124\n",
      "Gradient Descent(264/999): loss=0.09521529200772168\n",
      "Gradient Descent(265/999): loss=0.09519741333383862\n",
      "Gradient Descent(266/999): loss=0.09517961230792035\n",
      "Gradient Descent(267/999): loss=0.0951618884797684\n",
      "Gradient Descent(268/999): loss=0.09514424140201068\n",
      "Gradient Descent(269/999): loss=0.09512667063008304\n",
      "Gradient Descent(270/999): loss=0.09510917572221142\n",
      "Gradient Descent(271/999): loss=0.09509175623939385\n",
      "Gradient Descent(272/999): loss=0.09507441174538257\n",
      "Gradient Descent(273/999): loss=0.09505714180666651\n",
      "Gradient Descent(274/999): loss=0.09503994599245354\n",
      "Gradient Descent(275/999): loss=0.09502282387465308\n",
      "Gradient Descent(276/999): loss=0.09500577502785881\n",
      "Gradient Descent(277/999): loss=0.09498879902933131\n",
      "Gradient Descent(278/999): loss=0.09497189545898116\n",
      "Gradient Descent(279/999): loss=0.0949550638993516\n",
      "Gradient Descent(280/999): loss=0.09493830393560193\n",
      "Gradient Descent(281/999): loss=0.09492161515549066\n",
      "Gradient Descent(282/999): loss=0.09490499714935871\n",
      "Gradient Descent(283/999): loss=0.09488844951011303\n",
      "Gradient Descent(284/999): loss=0.09487197183321001\n",
      "Gradient Descent(285/999): loss=0.09485556371663918\n",
      "Gradient Descent(286/999): loss=0.09483922476090698\n",
      "Gradient Descent(287/999): loss=0.0948229545690206\n",
      "Gradient Descent(288/999): loss=0.09480675274647198\n",
      "Gradient Descent(289/999): loss=0.0947906189012219\n",
      "Gradient Descent(290/999): loss=0.09477455264368405\n",
      "Gradient Descent(291/999): loss=0.09475855358670951\n",
      "Gradient Descent(292/999): loss=0.09474262134557097\n",
      "Gradient Descent(293/999): loss=0.09472675553794728\n",
      "Gradient Descent(294/999): loss=0.09471095578390801\n",
      "Gradient Descent(295/999): loss=0.09469522170589827\n",
      "Gradient Descent(296/999): loss=0.09467955292872329\n",
      "Gradient Descent(297/999): loss=0.09466394907953349\n",
      "Gradient Descent(298/999): loss=0.09464840978780936\n",
      "Gradient Descent(299/999): loss=0.09463293468534659\n",
      "Gradient Descent(300/999): loss=0.09461752340624126\n",
      "Gradient Descent(301/999): loss=0.09460217558687513\n",
      "Gradient Descent(302/999): loss=0.09458689086590093\n",
      "Gradient Descent(303/999): loss=0.09457166888422797\n",
      "Gradient Descent(304/999): loss=0.09455650928500757\n",
      "Gradient Descent(305/999): loss=0.09454141171361884\n",
      "Gradient Descent(306/999): loss=0.09452637581765436\n",
      "Gradient Descent(307/999): loss=0.09451140124690592\n",
      "Gradient Descent(308/999): loss=0.09449648765335081\n",
      "Gradient Descent(309/999): loss=0.09448163469113739\n",
      "Gradient Descent(310/999): loss=0.0944668420165716\n",
      "Gradient Descent(311/999): loss=0.09445210928810284\n",
      "Gradient Descent(312/999): loss=0.09443743616631056\n",
      "Gradient Descent(313/999): loss=0.09442282231389053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(314/999): loss=0.09440826739564119\n",
      "Gradient Descent(315/999): loss=0.09439377107845044\n",
      "Gradient Descent(316/999): loss=0.09437933303128211\n",
      "Gradient Descent(317/999): loss=0.09436495292516281\n",
      "Gradient Descent(318/999): loss=0.09435063043316871\n",
      "Gradient Descent(319/999): loss=0.0943363652304124\n",
      "Gradient Descent(320/999): loss=0.09432215699402999\n",
      "Gradient Descent(321/999): loss=0.09430800540316817\n",
      "Gradient Descent(322/999): loss=0.09429391013897118\n",
      "Gradient Descent(323/999): loss=0.09427987088456834\n",
      "Gradient Descent(324/999): loss=0.09426588732506122\n",
      "Gradient Descent(325/999): loss=0.09425195914751104\n",
      "Gradient Descent(326/999): loss=0.09423808604092618\n",
      "Gradient Descent(327/999): loss=0.09422426769624986\n",
      "Gradient Descent(328/999): loss=0.09421050380634761\n",
      "Gradient Descent(329/999): loss=0.09419679406599507\n",
      "Gradient Descent(330/999): loss=0.0941831381718659\n",
      "Gradient Descent(331/999): loss=0.09416953582251954\n",
      "Gradient Descent(332/999): loss=0.09415598671838919\n",
      "Gradient Descent(333/999): loss=0.09414249056176996\n",
      "Gradient Descent(334/999): loss=0.0941290470568069\n",
      "Gradient Descent(335/999): loss=0.09411565590948319\n",
      "Gradient Descent(336/999): loss=0.09410231682760851\n",
      "Gradient Descent(337/999): loss=0.09408902952080725\n",
      "Gradient Descent(338/999): loss=0.09407579370050707\n",
      "Gradient Descent(339/999): loss=0.09406260907992733\n",
      "Gradient Descent(340/999): loss=0.09404947537406773\n",
      "Gradient Descent(341/999): loss=0.09403639229969679\n",
      "Gradient Descent(342/999): loss=0.0940233595753408\n",
      "Gradient Descent(343/999): loss=0.0940103769212725\n",
      "Gradient Descent(344/999): loss=0.09399744405949988\n",
      "Gradient Descent(345/999): loss=0.09398456071375529\n",
      "Gradient Descent(346/999): loss=0.09397172660948426\n",
      "Gradient Descent(347/999): loss=0.09395894147383471\n",
      "Gradient Descent(348/999): loss=0.09394620503564614\n",
      "Gradient Descent(349/999): loss=0.09393351702543865\n",
      "Gradient Descent(350/999): loss=0.09392087717540239\n",
      "Gradient Descent(351/999): loss=0.09390828521938696\n",
      "Gradient Descent(352/999): loss=0.09389574089289071\n",
      "Gradient Descent(353/999): loss=0.0938832439330503\n",
      "Gradient Descent(354/999): loss=0.0938707940786302\n",
      "Gradient Descent(355/999): loss=0.09385839107001243\n",
      "Gradient Descent(356/999): loss=0.0938460346491862\n",
      "Gradient Descent(357/999): loss=0.09383372455973768\n",
      "Gradient Descent(358/999): loss=0.09382146054683985\n",
      "Gradient Descent(359/999): loss=0.09380924235724224\n",
      "Gradient Descent(360/999): loss=0.09379706973926119\n",
      "Gradient Descent(361/999): loss=0.09378494244276957\n",
      "Gradient Descent(362/999): loss=0.09377286021918714\n",
      "Gradient Descent(363/999): loss=0.09376082282147043\n",
      "Gradient Descent(364/999): loss=0.09374883000410324\n",
      "Gradient Descent(365/999): loss=0.09373688152308658\n",
      "Gradient Descent(366/999): loss=0.09372497713592938\n",
      "Gradient Descent(367/999): loss=0.09371311660163863\n",
      "Gradient Descent(368/999): loss=0.09370129968070996\n",
      "Gradient Descent(369/999): loss=0.09368952613511815\n",
      "Gradient Descent(370/999): loss=0.09367779572830762\n",
      "Gradient Descent(371/999): loss=0.09366610822518333\n",
      "Gradient Descent(372/999): loss=0.09365446339210114\n",
      "Gradient Descent(373/999): loss=0.09364286099685887\n",
      "Gradient Descent(374/999): loss=0.09363130080868702\n",
      "Gradient Descent(375/999): loss=0.09361978259823958\n",
      "Gradient Descent(376/999): loss=0.09360830613758513\n",
      "Gradient Descent(377/999): loss=0.09359687120019765\n",
      "Gradient Descent(378/999): loss=0.09358547756094784\n",
      "Gradient Descent(379/999): loss=0.09357412499609395\n",
      "Gradient Descent(380/999): loss=0.09356281328327318\n",
      "Gradient Descent(381/999): loss=0.09355154220149274\n",
      "Gradient Descent(382/999): loss=0.09354031153112136\n",
      "Gradient Descent(383/999): loss=0.09352912105388028\n",
      "Gradient Descent(384/999): loss=0.09351797055283512\n",
      "Gradient Descent(385/999): loss=0.09350685981238688\n",
      "Gradient Descent(386/999): loss=0.0934957886182637\n",
      "Gradient Descent(387/999): loss=0.09348475675751246\n",
      "Gradient Descent(388/999): loss=0.0934737640184902\n",
      "Gradient Descent(389/999): loss=0.09346281019085594\n",
      "Gradient Descent(390/999): loss=0.09345189506556235\n",
      "Gradient Descent(391/999): loss=0.09344101843484756\n",
      "Gradient Descent(392/999): loss=0.09343018009222705\n",
      "Gradient Descent(393/999): loss=0.09341937983248527\n",
      "Gradient Descent(394/999): loss=0.09340861745166797\n",
      "Gradient Descent(395/999): loss=0.09339789274707383\n",
      "Gradient Descent(396/999): loss=0.09338720551724673\n",
      "Gradient Descent(397/999): loss=0.09337655556196776\n",
      "Gradient Descent(398/999): loss=0.09336594268224732\n",
      "Gradient Descent(399/999): loss=0.09335536668031741\n",
      "Gradient Descent(400/999): loss=0.09334482735962366\n",
      "Gradient Descent(401/999): loss=0.093334324524818\n",
      "Gradient Descent(402/999): loss=0.09332385798175064\n",
      "Gradient Descent(403/999): loss=0.09331342753746254\n",
      "Gradient Descent(404/999): loss=0.09330303300017799\n",
      "Gradient Descent(405/999): loss=0.09329267417929701\n",
      "Gradient Descent(406/999): loss=0.09328235088538782\n",
      "Gradient Descent(407/999): loss=0.09327206293017952\n",
      "Gradient Descent(408/999): loss=0.09326181012655468\n",
      "Gradient Descent(409/999): loss=0.09325159228854198\n",
      "Gradient Descent(410/999): loss=0.09324140923130902\n",
      "Gradient Descent(411/999): loss=0.09323126077115505\n",
      "Gradient Descent(412/999): loss=0.09322114672550373\n",
      "Gradient Descent(413/999): loss=0.09321106691289612\n",
      "Gradient Descent(414/999): loss=0.0932010211529834\n",
      "Gradient Descent(415/999): loss=0.09319100926652002\n",
      "Gradient Descent(416/999): loss=0.09318103107535665\n",
      "Gradient Descent(417/999): loss=0.09317108640243307\n",
      "Gradient Descent(418/999): loss=0.09316117507177149\n",
      "Gradient Descent(419/999): loss=0.09315129690846953\n",
      "Gradient Descent(420/999): loss=0.09314145173869344\n",
      "Gradient Descent(421/999): loss=0.09313163938967128\n",
      "Gradient Descent(422/999): loss=0.09312185968968631\n",
      "Gradient Descent(423/999): loss=0.09311211246807014\n",
      "Gradient Descent(424/999): loss=0.09310239755519617\n",
      "Gradient Descent(425/999): loss=0.093092714782473\n",
      "Gradient Descent(426/999): loss=0.09308306398233776\n",
      "Gradient Descent(427/999): loss=0.09307344498824974\n",
      "Gradient Descent(428/999): loss=0.09306385763468378\n",
      "Gradient Descent(429/999): loss=0.09305430175712381\n",
      "Gradient Descent(430/999): loss=0.09304477719205667\n",
      "Gradient Descent(431/999): loss=0.0930352837769655\n",
      "Gradient Descent(432/999): loss=0.09302582135032349\n",
      "Gradient Descent(433/999): loss=0.09301638975158776\n",
      "Gradient Descent(434/999): loss=0.09300698882119282\n",
      "Gradient Descent(435/999): loss=0.09299761840054478\n",
      "Gradient Descent(436/999): loss=0.09298827833201477\n",
      "Gradient Descent(437/999): loss=0.0929789684589331\n",
      "Gradient Descent(438/999): loss=0.09296968862558315\n",
      "Gradient Descent(439/999): loss=0.0929604386771952\n",
      "Gradient Descent(440/999): loss=0.0929512184599406\n",
      "Gradient Descent(441/999): loss=0.09294202782092559\n",
      "Gradient Descent(442/999): loss=0.09293286660818571\n",
      "Gradient Descent(443/999): loss=0.09292373467067948\n",
      "Gradient Descent(444/999): loss=0.09291463185828293\n",
      "Gradient Descent(445/999): loss=0.09290555802178362\n",
      "Gradient Descent(446/999): loss=0.0928965130128749\n",
      "Gradient Descent(447/999): loss=0.0928874966841501\n",
      "Gradient Descent(448/999): loss=0.09287850888909696\n",
      "Gradient Descent(449/999): loss=0.09286954948209188\n",
      "Gradient Descent(450/999): loss=0.09286061831839432\n",
      "Gradient Descent(451/999): loss=0.0928517152541411\n",
      "Gradient Descent(452/999): loss=0.09284284014634114\n",
      "Gradient Descent(453/999): loss=0.09283399285286957\n",
      "Gradient Descent(454/999): loss=0.09282517323246253\n",
      "Gradient Descent(455/999): loss=0.09281638114471155\n",
      "Gradient Descent(456/999): loss=0.09280761645005817\n",
      "Gradient Descent(457/999): loss=0.09279887900978873\n",
      "Gradient Descent(458/999): loss=0.09279016868602871\n",
      "Gradient Descent(459/999): loss=0.09278148534173772\n",
      "Gradient Descent(460/999): loss=0.09277282884070397\n",
      "Gradient Descent(461/999): loss=0.09276419904753923\n",
      "Gradient Descent(462/999): loss=0.09275559582767355\n",
      "Gradient Descent(463/999): loss=0.09274701904735001\n",
      "Gradient Descent(464/999): loss=0.09273846857361973\n",
      "Gradient Descent(465/999): loss=0.09272994427433655\n",
      "Gradient Descent(466/999): loss=0.0927214460181521\n",
      "Gradient Descent(467/999): loss=0.09271297367451083\n",
      "Gradient Descent(468/999): loss=0.09270452711364477\n",
      "Gradient Descent(469/999): loss=0.09269610620656875\n",
      "Gradient Descent(470/999): loss=0.09268771082507525\n",
      "Gradient Descent(471/999): loss=0.09267934084172973\n",
      "Gradient Descent(472/999): loss=0.09267099612986554\n",
      "Gradient Descent(473/999): loss=0.0926626765635792\n",
      "Gradient Descent(474/999): loss=0.09265438201772544\n",
      "Gradient Descent(475/999): loss=0.09264611236791252\n",
      "Gradient Descent(476/999): loss=0.09263786749049749\n",
      "Gradient Descent(477/999): loss=0.09262964726258129\n",
      "Gradient Descent(478/999): loss=0.09262145156200419\n",
      "Gradient Descent(479/999): loss=0.09261328026734116\n",
      "Gradient Descent(480/999): loss=0.09260513325789703\n",
      "Gradient Descent(481/999): loss=0.09259701041370212\n",
      "Gradient Descent(482/999): loss=0.09258891161550739\n",
      "Gradient Descent(483/999): loss=0.09258083674478022\n",
      "Gradient Descent(484/999): loss=0.09257278568369955\n",
      "Gradient Descent(485/999): loss=0.09256475831515162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(486/999): loss=0.09255675452272535\n",
      "Gradient Descent(487/999): loss=0.092548774190708\n",
      "Gradient Descent(488/999): loss=0.09254081720408076\n",
      "Gradient Descent(489/999): loss=0.09253288344851432\n",
      "Gradient Descent(490/999): loss=0.09252497281036448\n",
      "Gradient Descent(491/999): loss=0.09251708517666794\n",
      "Gradient Descent(492/999): loss=0.09250922043513789\n",
      "Gradient Descent(493/999): loss=0.09250137847415989\n",
      "Gradient Descent(494/999): loss=0.0924935591827874\n",
      "Gradient Descent(495/999): loss=0.09248576245073774\n",
      "Gradient Descent(496/999): loss=0.09247798816838793\n",
      "Gradient Descent(497/999): loss=0.0924702362267703\n",
      "Gradient Descent(498/999): loss=0.09246250651756859\n",
      "Gradient Descent(499/999): loss=0.09245479893311377\n",
      "Gradient Descent(500/999): loss=0.0924471133663799\n",
      "Gradient Descent(501/999): loss=0.0924394497109801\n",
      "Gradient Descent(502/999): loss=0.09243180786116258\n",
      "Gradient Descent(503/999): loss=0.09242418771180652\n",
      "Gradient Descent(504/999): loss=0.09241658915841823\n",
      "Gradient Descent(505/999): loss=0.09240901209712704\n",
      "Gradient Descent(506/999): loss=0.09240145642468153\n",
      "Gradient Descent(507/999): loss=0.09239392203844553\n",
      "Gradient Descent(508/999): loss=0.09238640883639422\n",
      "Gradient Descent(509/999): loss=0.0923789167171103\n",
      "Gradient Descent(510/999): loss=0.09237144557978029\n",
      "Gradient Descent(511/999): loss=0.09236399532419047\n",
      "Gradient Descent(512/999): loss=0.09235656585072331\n",
      "Gradient Descent(513/999): loss=0.09234915706035367\n",
      "Gradient Descent(514/999): loss=0.0923417688546449\n",
      "Gradient Descent(515/999): loss=0.09233440113574548\n",
      "Gradient Descent(516/999): loss=0.09232705380638496\n",
      "Gradient Descent(517/999): loss=0.09231972676987052\n",
      "Gradient Descent(518/999): loss=0.09231241993008328\n",
      "Gradient Descent(519/999): loss=0.09230513319147465\n",
      "Gradient Descent(520/999): loss=0.0922978664590628\n",
      "Gradient Descent(521/999): loss=0.09229061963842909\n",
      "Gradient Descent(522/999): loss=0.09228339263571435\n",
      "Gradient Descent(523/999): loss=0.09227618535761566\n",
      "Gradient Descent(524/999): loss=0.09226899771138261\n",
      "Gradient Descent(525/999): loss=0.09226182960481391\n",
      "Gradient Descent(526/999): loss=0.09225468094625393\n",
      "Gradient Descent(527/999): loss=0.09224755164458924\n",
      "Gradient Descent(528/999): loss=0.09224044160924524\n",
      "Gradient Descent(529/999): loss=0.09223335075018273\n",
      "Gradient Descent(530/999): loss=0.09222627897789452\n",
      "Gradient Descent(531/999): loss=0.09221922620340217\n",
      "Gradient Descent(532/999): loss=0.09221219233825259\n",
      "Gradient Descent(533/999): loss=0.09220517729451476\n",
      "Gradient Descent(534/999): loss=0.09219818098477643\n",
      "Gradient Descent(535/999): loss=0.09219120332214094\n",
      "Gradient Descent(536/999): loss=0.09218424422022374\n",
      "Gradient Descent(537/999): loss=0.09217730359314952\n",
      "Gradient Descent(538/999): loss=0.09217038135554871\n",
      "Gradient Descent(539/999): loss=0.09216347742255446\n",
      "Gradient Descent(540/999): loss=0.09215659170979947\n",
      "Gradient Descent(541/999): loss=0.09214972413341269\n",
      "Gradient Descent(542/999): loss=0.09214287461001645\n",
      "Gradient Descent(543/999): loss=0.09213604305672321\n",
      "Gradient Descent(544/999): loss=0.09212922939113244\n",
      "Gradient Descent(545/999): loss=0.0921224335313277\n",
      "Gradient Descent(546/999): loss=0.09211565539587344\n",
      "Gradient Descent(547/999): loss=0.09210889490381216\n",
      "Gradient Descent(548/999): loss=0.09210215197466125\n",
      "Gradient Descent(549/999): loss=0.09209542652841007\n",
      "Gradient Descent(550/999): loss=0.09208871848551706\n",
      "Gradient Descent(551/999): loss=0.09208202776690651\n",
      "Gradient Descent(552/999): loss=0.09207535429396604\n",
      "Gradient Descent(553/999): loss=0.09206869798854345\n",
      "Gradient Descent(554/999): loss=0.09206205877294385\n",
      "Gradient Descent(555/999): loss=0.09205543656992679\n",
      "Gradient Descent(556/999): loss=0.09204883130270346\n",
      "Gradient Descent(557/999): loss=0.09204224289493382\n",
      "Gradient Descent(558/999): loss=0.09203567127072383\n",
      "Gradient Descent(559/999): loss=0.0920291163546225\n",
      "Gradient Descent(560/999): loss=0.09202257807161936\n",
      "Gradient Descent(561/999): loss=0.09201605634714144\n",
      "Gradient Descent(562/999): loss=0.09200955110705074\n",
      "Gradient Descent(563/999): loss=0.09200306227764138\n",
      "Gradient Descent(564/999): loss=0.09199658978563684\n",
      "Gradient Descent(565/999): loss=0.09199013355818747\n",
      "Gradient Descent(566/999): loss=0.09198369352286756\n",
      "Gradient Descent(567/999): loss=0.0919772696076729\n",
      "Gradient Descent(568/999): loss=0.09197086174101807\n",
      "Gradient Descent(569/999): loss=0.09196446985173368\n",
      "Gradient Descent(570/999): loss=0.09195809386906395\n",
      "Gradient Descent(571/999): loss=0.0919517337226641\n",
      "Gradient Descent(572/999): loss=0.09194538934259758\n",
      "Gradient Descent(573/999): loss=0.09193906065933377\n",
      "Gradient Descent(574/999): loss=0.09193274760374526\n",
      "Gradient Descent(575/999): loss=0.0919264501071054\n",
      "Gradient Descent(576/999): loss=0.0919201681010858\n",
      "Gradient Descent(577/999): loss=0.09191390151775379\n",
      "Gradient Descent(578/999): loss=0.09190765028956989\n",
      "Gradient Descent(579/999): loss=0.09190141434938554\n",
      "Gradient Descent(580/999): loss=0.09189519363044052\n",
      "Gradient Descent(581/999): loss=0.09188898806636042\n",
      "Gradient Descent(582/999): loss=0.09188279759115446\n",
      "Gradient Descent(583/999): loss=0.09187662213921291\n",
      "Gradient Descent(584/999): loss=0.09187046164530477\n",
      "Gradient Descent(585/999): loss=0.09186431604457543\n",
      "Gradient Descent(586/999): loss=0.0918581852725442\n",
      "Gradient Descent(587/999): loss=0.09185206926510218\n",
      "Gradient Descent(588/999): loss=0.09184596795850965\n",
      "Gradient Descent(589/999): loss=0.09183988128939406\n",
      "Gradient Descent(590/999): loss=0.0918338091947475\n",
      "Gradient Descent(591/999): loss=0.09182775161192455\n",
      "Gradient Descent(592/999): loss=0.09182170847863999\n",
      "Gradient Descent(593/999): loss=0.09181567973296646\n",
      "Gradient Descent(594/999): loss=0.09180966531333243\n",
      "Gradient Descent(595/999): loss=0.09180366515851972\n",
      "Gradient Descent(596/999): loss=0.09179767920766149\n",
      "Gradient Descent(597/999): loss=0.09179170740023998\n",
      "Gradient Descent(598/999): loss=0.09178574967608426\n",
      "Gradient Descent(599/999): loss=0.0917798059753682\n",
      "Gradient Descent(600/999): loss=0.0917738762386082\n",
      "Gradient Descent(601/999): loss=0.09176796040666112\n",
      "Gradient Descent(602/999): loss=0.09176205842072208\n",
      "Gradient Descent(603/999): loss=0.0917561702223225\n",
      "Gradient Descent(604/999): loss=0.09175029575332781\n",
      "Gradient Descent(605/999): loss=0.09174443495593548\n",
      "Gradient Descent(606/999): loss=0.09173858777267295\n",
      "Gradient Descent(607/999): loss=0.09173275414639549\n",
      "Gradient Descent(608/999): loss=0.0917269340202843\n",
      "Gradient Descent(609/999): loss=0.09172112733784431\n",
      "Gradient Descent(610/999): loss=0.09171533404290226\n",
      "Gradient Descent(611/999): loss=0.09170955407960471\n",
      "Gradient Descent(612/999): loss=0.09170378739241591\n",
      "Gradient Descent(613/999): loss=0.09169803392611606\n",
      "Gradient Descent(614/999): loss=0.0916922936257991\n",
      "Gradient Descent(615/999): loss=0.0916865664368709\n",
      "Gradient Descent(616/999): loss=0.09168085230504722\n",
      "Gradient Descent(617/999): loss=0.09167515117635185\n",
      "Gradient Descent(618/999): loss=0.09166946299711473\n",
      "Gradient Descent(619/999): loss=0.09166378771396994\n",
      "Gradient Descent(620/999): loss=0.09165812527385378\n",
      "Gradient Descent(621/999): loss=0.09165247562400314\n",
      "Gradient Descent(622/999): loss=0.09164683871195323\n",
      "Gradient Descent(623/999): loss=0.09164121448553611\n",
      "Gradient Descent(624/999): loss=0.09163560289287856\n",
      "Gradient Descent(625/999): loss=0.09163000388240046\n",
      "Gradient Descent(626/999): loss=0.09162441740281276\n",
      "Gradient Descent(627/999): loss=0.09161884340311588\n",
      "Gradient Descent(628/999): loss=0.09161328183259773\n",
      "Gradient Descent(629/999): loss=0.09160773264083202\n",
      "Gradient Descent(630/999): loss=0.09160219577767643\n",
      "Gradient Descent(631/999): loss=0.09159667119327096\n",
      "Gradient Descent(632/999): loss=0.09159115883803597\n",
      "Gradient Descent(633/999): loss=0.09158565866267061\n",
      "Gradient Descent(634/999): loss=0.09158017061815109\n",
      "Gradient Descent(635/999): loss=0.0915746946557288\n",
      "Gradient Descent(636/999): loss=0.09156923072692882\n",
      "Gradient Descent(637/999): loss=0.09156377878354792\n",
      "Gradient Descent(638/999): loss=0.09155833877765325\n",
      "Gradient Descent(639/999): loss=0.09155291066158032\n",
      "Gradient Descent(640/999): loss=0.09154749438793164\n",
      "Gradient Descent(641/999): loss=0.0915420899095747\n",
      "Gradient Descent(642/999): loss=0.09153669717964068\n",
      "Gradient Descent(643/999): loss=0.09153131615152256\n",
      "Gradient Descent(644/999): loss=0.09152594677887371\n",
      "Gradient Descent(645/999): loss=0.09152058901560607\n",
      "Gradient Descent(646/999): loss=0.09151524281588864\n",
      "Gradient Descent(647/999): loss=0.09150990813414592\n",
      "Gradient Descent(648/999): loss=0.09150458492505628\n",
      "Gradient Descent(649/999): loss=0.09149927314355034\n",
      "Gradient Descent(650/999): loss=0.09149397274480957\n",
      "Gradient Descent(651/999): loss=0.09148868368426452\n",
      "Gradient Descent(652/999): loss=0.09148340591759346\n",
      "Gradient Descent(653/999): loss=0.09147813940072066\n",
      "Gradient Descent(654/999): loss=0.09147288408981509\n",
      "Gradient Descent(655/999): loss=0.09146763994128879\n",
      "Gradient Descent(656/999): loss=0.09146240691179516\n",
      "Gradient Descent(657/999): loss=0.091457184958228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(658/999): loss=0.09145197403771937\n",
      "Gradient Descent(659/999): loss=0.09144677410763866\n",
      "Gradient Descent(660/999): loss=0.09144158512559077\n",
      "Gradient Descent(661/999): loss=0.09143640704941483\n",
      "Gradient Descent(662/999): loss=0.09143123983718268\n",
      "Gradient Descent(663/999): loss=0.09142608344719745\n",
      "Gradient Descent(664/999): loss=0.09142093783799217\n",
      "Gradient Descent(665/999): loss=0.09141580296832823\n",
      "Gradient Descent(666/999): loss=0.09141067879719414\n",
      "Gradient Descent(667/999): loss=0.091405565283804\n",
      "Gradient Descent(668/999): loss=0.09140046238759604\n",
      "Gradient Descent(669/999): loss=0.0913953700682315\n",
      "Gradient Descent(670/999): loss=0.09139028828559294\n",
      "Gradient Descent(671/999): loss=0.09138521699978312\n",
      "Gradient Descent(672/999): loss=0.0913801561711235\n",
      "Gradient Descent(673/999): loss=0.09137510576015294\n",
      "Gradient Descent(674/999): loss=0.0913700657276263\n",
      "Gradient Descent(675/999): loss=0.09136503603451325\n",
      "Gradient Descent(676/999): loss=0.09136001664199671\n",
      "Gradient Descent(677/999): loss=0.09135500751147183\n",
      "Gradient Descent(678/999): loss=0.0913500086045444\n",
      "Gradient Descent(679/999): loss=0.09134501988302972\n",
      "Gradient Descent(680/999): loss=0.09134004130895126\n",
      "Gradient Descent(681/999): loss=0.0913350728445394\n",
      "Gradient Descent(682/999): loss=0.09133011445223008\n",
      "Gradient Descent(683/999): loss=0.09132516609466362\n",
      "Gradient Descent(684/999): loss=0.09132022773468344\n",
      "Gradient Descent(685/999): loss=0.09131529933533479\n",
      "Gradient Descent(686/999): loss=0.09131038085986347\n",
      "Gradient Descent(687/999): loss=0.09130547227171472\n",
      "Gradient Descent(688/999): loss=0.09130057353453179\n",
      "Gradient Descent(689/999): loss=0.09129568461215497\n",
      "Gradient Descent(690/999): loss=0.09129080546862015\n",
      "Gradient Descent(691/999): loss=0.0912859360681578\n",
      "Gradient Descent(692/999): loss=0.09128107637519157\n",
      "Gradient Descent(693/999): loss=0.09127622635433733\n",
      "Gradient Descent(694/999): loss=0.09127138597040182\n",
      "Gradient Descent(695/999): loss=0.0912665551883816\n",
      "Gradient Descent(696/999): loss=0.09126173397346171\n",
      "Gradient Descent(697/999): loss=0.0912569222910147\n",
      "Gradient Descent(698/999): loss=0.09125212010659935\n",
      "Gradient Descent(699/999): loss=0.09124732738595956\n",
      "Gradient Descent(700/999): loss=0.09124254409502339\n",
      "Gradient Descent(701/999): loss=0.0912377701999015\n",
      "Gradient Descent(702/999): loss=0.09123300566688645\n",
      "Gradient Descent(703/999): loss=0.09122825046245141\n",
      "Gradient Descent(704/999): loss=0.09122350455324907\n",
      "Gradient Descent(705/999): loss=0.09121876790611053\n",
      "Gradient Descent(706/999): loss=0.09121404048804416\n",
      "Gradient Descent(707/999): loss=0.09120932226623467\n",
      "Gradient Descent(708/999): loss=0.09120461320804184\n",
      "Gradient Descent(709/999): loss=0.09119991328099951\n",
      "Gradient Descent(710/999): loss=0.09119522245281468\n",
      "Gradient Descent(711/999): loss=0.09119054069136609\n",
      "Gradient Descent(712/999): loss=0.09118586796470358\n",
      "Gradient Descent(713/999): loss=0.09118120424104667\n",
      "Gradient Descent(714/999): loss=0.09117654948878386\n",
      "Gradient Descent(715/999): loss=0.09117190367647131\n",
      "Gradient Descent(716/999): loss=0.09116726677283196\n",
      "Gradient Descent(717/999): loss=0.09116263874675445\n",
      "Gradient Descent(718/999): loss=0.09115801956729228\n",
      "Gradient Descent(719/999): loss=0.09115340920366242\n",
      "Gradient Descent(720/999): loss=0.09114880762524478\n",
      "Gradient Descent(721/999): loss=0.09114421480158087\n",
      "Gradient Descent(722/999): loss=0.09113963070237292\n",
      "Gradient Descent(723/999): loss=0.09113505529748298\n",
      "Gradient Descent(724/999): loss=0.09113048855693172\n",
      "Gradient Descent(725/999): loss=0.09112593045089779\n",
      "Gradient Descent(726/999): loss=0.09112138094971653\n",
      "Gradient Descent(727/999): loss=0.09111684002387925\n",
      "Gradient Descent(728/999): loss=0.09111230764403214\n",
      "Gradient Descent(729/999): loss=0.09110778378097534\n",
      "Gradient Descent(730/999): loss=0.09110326840566206\n",
      "Gradient Descent(731/999): loss=0.09109876148919764\n",
      "Gradient Descent(732/999): loss=0.09109426300283857\n",
      "Gradient Descent(733/999): loss=0.09108977291799165\n",
      "Gradient Descent(734/999): loss=0.09108529120621289\n",
      "Gradient Descent(735/999): loss=0.09108081783920693\n",
      "Gradient Descent(736/999): loss=0.09107635278882567\n",
      "Gradient Descent(737/999): loss=0.09107189602706794\n",
      "Gradient Descent(738/999): loss=0.09106744752607805\n",
      "Gradient Descent(739/999): loss=0.09106300725814528\n",
      "Gradient Descent(740/999): loss=0.0910585751957029\n",
      "Gradient Descent(741/999): loss=0.0910541513113272\n",
      "Gradient Descent(742/999): loss=0.09104973557773671\n",
      "Gradient Descent(743/999): loss=0.09104532796779134\n",
      "Gradient Descent(744/999): loss=0.09104092845449142\n",
      "Gradient Descent(745/999): loss=0.09103653701097704\n",
      "Gradient Descent(746/999): loss=0.091032153610527\n",
      "Gradient Descent(747/999): loss=0.09102777822655804\n",
      "Gradient Descent(748/999): loss=0.09102341083262407\n",
      "Gradient Descent(749/999): loss=0.09101905140241523\n",
      "Gradient Descent(750/999): loss=0.09101469990975714\n",
      "Gradient Descent(751/999): loss=0.09101035632861001\n",
      "Gradient Descent(752/999): loss=0.09100602063306791\n",
      "Gradient Descent(753/999): loss=0.09100169279735787\n",
      "Gradient Descent(754/999): loss=0.09099737279583915\n",
      "Gradient Descent(755/999): loss=0.09099306060300236\n",
      "Gradient Descent(756/999): loss=0.09098875619346879\n",
      "Gradient Descent(757/999): loss=0.09098445954198936\n",
      "Gradient Descent(758/999): loss=0.09098017062344418\n",
      "Gradient Descent(759/999): loss=0.09097588941284157\n",
      "Gradient Descent(760/999): loss=0.09097161588531724\n",
      "Gradient Descent(761/999): loss=0.09096735001613362\n",
      "Gradient Descent(762/999): loss=0.09096309178067909\n",
      "Gradient Descent(763/999): loss=0.09095884115446712\n",
      "Gradient Descent(764/999): loss=0.09095459811313567\n",
      "Gradient Descent(765/999): loss=0.09095036263244627\n",
      "Gradient Descent(766/999): loss=0.09094613468828341\n",
      "Gradient Descent(767/999): loss=0.09094191425665377\n",
      "Gradient Descent(768/999): loss=0.09093770131368532\n",
      "Gradient Descent(769/999): loss=0.09093349583562685\n",
      "Gradient Descent(770/999): loss=0.09092929779884705\n",
      "Gradient Descent(771/999): loss=0.09092510717983389\n",
      "Gradient Descent(772/999): loss=0.09092092395519377\n",
      "Gradient Descent(773/999): loss=0.09091674810165098\n",
      "Gradient Descent(774/999): loss=0.09091257959604686\n",
      "Gradient Descent(775/999): loss=0.09090841841533913\n",
      "Gradient Descent(776/999): loss=0.09090426453660125\n",
      "Gradient Descent(777/999): loss=0.09090011793702159\n",
      "Gradient Descent(778/999): loss=0.0908959785939028\n",
      "Gradient Descent(779/999): loss=0.09089184648466128\n",
      "Gradient Descent(780/999): loss=0.09088772158682615\n",
      "Gradient Descent(781/999): loss=0.09088360387803897\n",
      "Gradient Descent(782/999): loss=0.09087949333605273\n",
      "Gradient Descent(783/999): loss=0.09087538993873132\n",
      "Gradient Descent(784/999): loss=0.09087129366404892\n",
      "Gradient Descent(785/999): loss=0.09086720449008925\n",
      "Gradient Descent(786/999): loss=0.09086312239504492\n",
      "Gradient Descent(787/999): loss=0.09085904735721685\n",
      "Gradient Descent(788/999): loss=0.09085497935501352\n",
      "Gradient Descent(789/999): loss=0.09085091836695033\n",
      "Gradient Descent(790/999): loss=0.09084686437164904\n",
      "Gradient Descent(791/999): loss=0.0908428173478371\n",
      "Gradient Descent(792/999): loss=0.090838777274347\n",
      "Gradient Descent(793/999): loss=0.0908347441301156\n",
      "Gradient Descent(794/999): loss=0.09083071789418357\n",
      "Gradient Descent(795/999): loss=0.09082669854569472\n",
      "Gradient Descent(796/999): loss=0.09082268606389549\n",
      "Gradient Descent(797/999): loss=0.0908186804281342\n",
      "Gradient Descent(798/999): loss=0.09081468161786048\n",
      "Gradient Descent(799/999): loss=0.09081068961262466\n",
      "Gradient Descent(800/999): loss=0.09080670439207723\n",
      "Gradient Descent(801/999): loss=0.09080272593596819\n",
      "Gradient Descent(802/999): loss=0.09079875422414645\n",
      "Gradient Descent(803/999): loss=0.09079478923655919\n",
      "Gradient Descent(804/999): loss=0.09079083095325141\n",
      "Gradient Descent(805/999): loss=0.09078687935436527\n",
      "Gradient Descent(806/999): loss=0.09078293442013947\n",
      "Gradient Descent(807/999): loss=0.09077899613090874\n",
      "Gradient Descent(808/999): loss=0.09077506446710318\n",
      "Gradient Descent(809/999): loss=0.09077113940924779\n",
      "Gradient Descent(810/999): loss=0.09076722093796188\n",
      "Gradient Descent(811/999): loss=0.09076330903395848\n",
      "Gradient Descent(812/999): loss=0.09075940367804368\n",
      "Gradient Descent(813/999): loss=0.09075550485111643\n",
      "Gradient Descent(814/999): loss=0.09075161253416755\n",
      "Gradient Descent(815/999): loss=0.09074772670827941\n",
      "Gradient Descent(816/999): loss=0.09074384735462539\n",
      "Gradient Descent(817/999): loss=0.09073997445446928\n",
      "Gradient Descent(818/999): loss=0.09073610798916476\n",
      "Gradient Descent(819/999): loss=0.09073224794015491\n",
      "Gradient Descent(820/999): loss=0.0907283942889716\n",
      "Gradient Descent(821/999): loss=0.09072454701723498\n",
      "Gradient Descent(822/999): loss=0.09072070610665307\n",
      "Gradient Descent(823/999): loss=0.090716871539021\n",
      "Gradient Descent(824/999): loss=0.09071304329622079\n",
      "Gradient Descent(825/999): loss=0.09070922136022057\n",
      "Gradient Descent(826/999): loss=0.09070540571307424\n",
      "Gradient Descent(827/999): loss=0.09070159633692086\n",
      "Gradient Descent(828/999): loss=0.09069779321398426\n",
      "Gradient Descent(829/999): loss=0.09069399632657241\n",
      "Gradient Descent(830/999): loss=0.09069020565707696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(831/999): loss=0.09068642118797285\n",
      "Gradient Descent(832/999): loss=0.09068264290181764\n",
      "Gradient Descent(833/999): loss=0.09067887078125113\n",
      "Gradient Descent(834/999): loss=0.09067510480899495\n",
      "Gradient Descent(835/999): loss=0.09067134496785188\n",
      "Gradient Descent(836/999): loss=0.09066759124070542\n",
      "Gradient Descent(837/999): loss=0.09066384361051955\n",
      "Gradient Descent(838/999): loss=0.09066010206033792\n",
      "Gradient Descent(839/999): loss=0.09065636657328362\n",
      "Gradient Descent(840/999): loss=0.09065263713255851\n",
      "Gradient Descent(841/999): loss=0.09064891372144303\n",
      "Gradient Descent(842/999): loss=0.09064519632329543\n",
      "Gradient Descent(843/999): loss=0.09064148492155151\n",
      "Gradient Descent(844/999): loss=0.09063777949972411\n",
      "Gradient Descent(845/999): loss=0.0906340800414027\n",
      "Gradient Descent(846/999): loss=0.09063038653025282\n",
      "Gradient Descent(847/999): loss=0.09062669895001574\n",
      "Gradient Descent(848/999): loss=0.09062301728450793\n",
      "Gradient Descent(849/999): loss=0.0906193415176207\n",
      "Gradient Descent(850/999): loss=0.09061567163331971\n",
      "Gradient Descent(851/999): loss=0.0906120076156445\n",
      "Gradient Descent(852/999): loss=0.09060834944870823\n",
      "Gradient Descent(853/999): loss=0.09060469711669698\n",
      "Gradient Descent(854/999): loss=0.09060105060386948\n",
      "Gradient Descent(855/999): loss=0.09059740989455678\n",
      "Gradient Descent(856/999): loss=0.09059377497316157\n",
      "Gradient Descent(857/999): loss=0.090590145824158\n",
      "Gradient Descent(858/999): loss=0.09058652243209105\n",
      "Gradient Descent(859/999): loss=0.09058290478157638\n",
      "Gradient Descent(860/999): loss=0.09057929285729968\n",
      "Gradient Descent(861/999): loss=0.09057568664401636\n",
      "Gradient Descent(862/999): loss=0.09057208612655107\n",
      "Gradient Descent(863/999): loss=0.09056849128979748\n",
      "Gradient Descent(864/999): loss=0.09056490211871761\n",
      "Gradient Descent(865/999): loss=0.09056131859834168\n",
      "Gradient Descent(866/999): loss=0.09055774071376757\n",
      "Gradient Descent(867/999): loss=0.0905541684501604\n",
      "Gradient Descent(868/999): loss=0.09055060179275227\n",
      "Gradient Descent(869/999): loss=0.09054704072684185\n",
      "Gradient Descent(870/999): loss=0.09054348523779378\n",
      "Gradient Descent(871/999): loss=0.09053993531103853\n",
      "Gradient Descent(872/999): loss=0.09053639093207194\n",
      "Gradient Descent(873/999): loss=0.09053285208645485\n",
      "Gradient Descent(874/999): loss=0.09052931875981263\n",
      "Gradient Descent(875/999): loss=0.09052579093783497\n",
      "Gradient Descent(876/999): loss=0.09052226860627535\n",
      "Gradient Descent(877/999): loss=0.0905187517509507\n",
      "Gradient Descent(878/999): loss=0.09051524035774115\n",
      "Gradient Descent(879/999): loss=0.0905117344125896\n",
      "Gradient Descent(880/999): loss=0.09050823390150119\n",
      "Gradient Descent(881/999): loss=0.09050473881054322\n",
      "Gradient Descent(882/999): loss=0.09050124912584462\n",
      "Gradient Descent(883/999): loss=0.09049776483359559\n",
      "Gradient Descent(884/999): loss=0.09049428592004728\n",
      "Gradient Descent(885/999): loss=0.09049081237151156\n",
      "Gradient Descent(886/999): loss=0.09048734417436036\n",
      "Gradient Descent(887/999): loss=0.0904838813150257\n",
      "Gradient Descent(888/999): loss=0.09048042377999901\n",
      "Gradient Descent(889/999): loss=0.09047697155583095\n",
      "Gradient Descent(890/999): loss=0.09047352462913116\n",
      "Gradient Descent(891/999): loss=0.09047008298656768\n",
      "Gradient Descent(892/999): loss=0.09046664661486684\n",
      "Gradient Descent(893/999): loss=0.09046321550081275\n",
      "Gradient Descent(894/999): loss=0.09045978963124708\n",
      "Gradient Descent(895/999): loss=0.09045636899306872\n",
      "Gradient Descent(896/999): loss=0.09045295357323337\n",
      "Gradient Descent(897/999): loss=0.09044954335875333\n",
      "Gradient Descent(898/999): loss=0.09044613833669697\n",
      "Gradient Descent(899/999): loss=0.09044273849418875\n",
      "Gradient Descent(900/999): loss=0.0904393438184086\n",
      "Gradient Descent(901/999): loss=0.09043595429659163\n",
      "Gradient Descent(902/999): loss=0.090432569916028\n",
      "Gradient Descent(903/999): loss=0.09042919066406238\n",
      "Gradient Descent(904/999): loss=0.09042581652809385\n",
      "Gradient Descent(905/999): loss=0.09042244749557542\n",
      "Gradient Descent(906/999): loss=0.09041908355401378\n",
      "Gradient Descent(907/999): loss=0.09041572469096901\n",
      "Gradient Descent(908/999): loss=0.09041237089405425\n",
      "Gradient Descent(909/999): loss=0.09040902215093544\n",
      "Gradient Descent(910/999): loss=0.09040567844933094\n",
      "Gradient Descent(911/999): loss=0.09040233977701133\n",
      "Gradient Descent(912/999): loss=0.09039900612179898\n",
      "Gradient Descent(913/999): loss=0.09039567747156793\n",
      "Gradient Descent(914/999): loss=0.0903923538142434\n",
      "Gradient Descent(915/999): loss=0.09038903513780168\n",
      "Gradient Descent(916/999): loss=0.0903857214302697\n",
      "Gradient Descent(917/999): loss=0.09038241267972483\n",
      "Gradient Descent(918/999): loss=0.09037910887429454\n",
      "Gradient Descent(919/999): loss=0.09037581000215612\n",
      "Gradient Descent(920/999): loss=0.09037251605153647\n",
      "Gradient Descent(921/999): loss=0.09036922701071175\n",
      "Gradient Descent(922/999): loss=0.09036594286800699\n",
      "Gradient Descent(923/999): loss=0.09036266361179612\n",
      "Gradient Descent(924/999): loss=0.09035938923050141\n",
      "Gradient Descent(925/999): loss=0.09035611971259325\n",
      "Gradient Descent(926/999): loss=0.09035285504658999\n",
      "Gradient Descent(927/999): loss=0.0903495952210576\n",
      "Gradient Descent(928/999): loss=0.09034634022460931\n",
      "Gradient Descent(929/999): loss=0.09034309004590559\n",
      "Gradient Descent(930/999): loss=0.09033984467365354\n",
      "Gradient Descent(931/999): loss=0.09033660409660697\n",
      "Gradient Descent(932/999): loss=0.09033336830356589\n",
      "Gradient Descent(933/999): loss=0.09033013728337635\n",
      "Gradient Descent(934/999): loss=0.09032691102493016\n",
      "Gradient Descent(935/999): loss=0.09032368951716468\n",
      "Gradient Descent(936/999): loss=0.09032047274906248\n",
      "Gradient Descent(937/999): loss=0.09031726070965114\n",
      "Gradient Descent(938/999): loss=0.09031405338800301\n",
      "Gradient Descent(939/999): loss=0.09031085077323492\n",
      "Gradient Descent(940/999): loss=0.09030765285450788\n",
      "Gradient Descent(941/999): loss=0.09030445962102703\n",
      "Gradient Descent(942/999): loss=0.09030127106204108\n",
      "Gradient Descent(943/999): loss=0.09029808716684246\n",
      "Gradient Descent(944/999): loss=0.09029490792476663\n",
      "Gradient Descent(945/999): loss=0.09029173332519226\n",
      "Gradient Descent(946/999): loss=0.09028856335754067\n",
      "Gradient Descent(947/999): loss=0.09028539801127576\n",
      "Gradient Descent(948/999): loss=0.09028223727590375\n",
      "Gradient Descent(949/999): loss=0.09027908114097287\n",
      "Gradient Descent(950/999): loss=0.09027592959607324\n",
      "Gradient Descent(951/999): loss=0.09027278263083656\n",
      "Gradient Descent(952/999): loss=0.09026964023493578\n",
      "Gradient Descent(953/999): loss=0.09026650239808515\n",
      "Gradient Descent(954/999): loss=0.09026336911003974\n",
      "Gradient Descent(955/999): loss=0.09026024036059528\n",
      "Gradient Descent(956/999): loss=0.09025711613958796\n",
      "Gradient Descent(957/999): loss=0.09025399643689422\n",
      "Gradient Descent(958/999): loss=0.09025088124243048\n",
      "Gradient Descent(959/999): loss=0.09024777054615292\n",
      "Gradient Descent(960/999): loss=0.09024466433805733\n",
      "Gradient Descent(961/999): loss=0.09024156260817875\n",
      "Gradient Descent(962/999): loss=0.09023846534659143\n",
      "Gradient Descent(963/999): loss=0.09023537254340847\n",
      "Gradient Descent(964/999): loss=0.09023228418878167\n",
      "Gradient Descent(965/999): loss=0.09022920027290131\n",
      "Gradient Descent(966/999): loss=0.09022612078599589\n",
      "Gradient Descent(967/999): loss=0.09022304571833199\n",
      "Gradient Descent(968/999): loss=0.0902199750602141\n",
      "Gradient Descent(969/999): loss=0.09021690880198423\n",
      "Gradient Descent(970/999): loss=0.09021384693402183\n",
      "Gradient Descent(971/999): loss=0.09021078944674363\n",
      "Gradient Descent(972/999): loss=0.09020773633060329\n",
      "Gradient Descent(973/999): loss=0.09020468757609133\n",
      "Gradient Descent(974/999): loss=0.09020164317373489\n",
      "Gradient Descent(975/999): loss=0.09019860311409747\n",
      "Gradient Descent(976/999): loss=0.09019556738777879\n",
      "Gradient Descent(977/999): loss=0.09019253598541459\n",
      "Gradient Descent(978/999): loss=0.09018950889767642\n",
      "Gradient Descent(979/999): loss=0.09018648611527137\n",
      "Gradient Descent(980/999): loss=0.09018346762894208\n",
      "Gradient Descent(981/999): loss=0.09018045342946626\n",
      "Gradient Descent(982/999): loss=0.0901774435076568\n",
      "Gradient Descent(983/999): loss=0.09017443785436133\n",
      "Gradient Descent(984/999): loss=0.09017143646046216\n",
      "Gradient Descent(985/999): loss=0.09016843931687603\n",
      "Gradient Descent(986/999): loss=0.090165446414554\n",
      "Gradient Descent(987/999): loss=0.09016245774448121\n",
      "Gradient Descent(988/999): loss=0.09015947329767666\n",
      "Gradient Descent(989/999): loss=0.09015649306519309\n",
      "Gradient Descent(990/999): loss=0.09015351703811678\n",
      "Gradient Descent(991/999): loss=0.09015054520756732\n",
      "Gradient Descent(992/999): loss=0.0901475775646975\n",
      "Gradient Descent(993/999): loss=0.09014461410069316\n",
      "Gradient Descent(994/999): loss=0.0901416548067728\n",
      "Gradient Descent(995/999): loss=0.09013869967418765\n",
      "Gradient Descent(996/999): loss=0.09013574869422146\n",
      "Gradient Descent(997/999): loss=0.09013280185819006\n",
      "Gradient Descent(998/999): loss=0.09012985915744157\n",
      "Gradient Descent(999/999): loss=0.09012692058335599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999): loss=0.14738697924684596\n",
      "Gradient Descent(1/999): loss=0.1346629104511156\n",
      "Gradient Descent(2/999): loss=0.1257064250805438\n",
      "Gradient Descent(3/999): loss=0.11939615502227299\n",
      "Gradient Descent(4/999): loss=0.11494452468250674\n",
      "Gradient Descent(5/999): loss=0.11179839227269665\n",
      "Gradient Descent(6/999): loss=0.10956926681731245\n",
      "Gradient Descent(7/999): loss=0.10798429719325478\n",
      "Gradient Descent(8/999): loss=0.10685185007109234\n",
      "Gradient Descent(9/999): loss=0.10603733413649513\n",
      "Gradient Descent(10/999): loss=0.10544622062151542\n",
      "Gradient Descent(11/999): loss=0.10501211804817087\n",
      "Gradient Descent(12/999): loss=0.10468839671662615\n",
      "Gradient Descent(13/999): loss=0.10444230629931313\n",
      "Gradient Descent(14/999): loss=0.10425084442784954\n",
      "Gradient Descent(15/999): loss=0.1040978550615452\n",
      "Gradient Descent(16/999): loss=0.10397199057319773\n",
      "Gradient Descent(17/999): loss=0.10386528045281493\n",
      "Gradient Descent(18/999): loss=0.10377212605966744\n",
      "Gradient Descent(19/999): loss=0.10368859460252461\n",
      "Gradient Descent(20/999): loss=0.10361192327798521\n",
      "Gradient Descent(21/999): loss=0.10354017100996314\n",
      "Gradient Descent(22/999): loss=0.10347197385446628\n",
      "Gradient Descent(23/999): loss=0.10340637321202645\n",
      "Gradient Descent(24/999): loss=0.10334269517541028\n",
      "Gradient Descent(25/999): loss=0.10328046579137287\n",
      "Gradient Descent(26/999): loss=0.10321935154606092\n",
      "Gradient Descent(27/999): loss=0.10315911756584138\n",
      "Gradient Descent(28/999): loss=0.10309959826027441\n",
      "Gradient Descent(29/999): loss=0.10304067670362699\n",
      "Gradient Descent(30/999): loss=0.10298227015376071\n",
      "Gradient Descent(31/999): loss=0.10292431988150591\n",
      "Gradient Descent(32/999): loss=0.10286678402743814\n",
      "Gradient Descent(33/999): loss=0.1028096325849016\n",
      "Gradient Descent(34/999): loss=0.10275284387636978\n",
      "Gradient Descent(35/999): loss=0.10269640207862724\n",
      "Gradient Descent(36/999): loss=0.10264029548457546\n",
      "Gradient Descent(37/999): loss=0.10258451528239515\n",
      "Gradient Descent(38/999): loss=0.10252905469806652\n",
      "Gradient Descent(39/999): loss=0.1024739083930883\n",
      "Gradient Descent(40/999): loss=0.10241907204143309\n",
      "Gradient Descent(41/999): loss=0.10236454203238629\n",
      "Gradient Descent(42/999): loss=0.10231031526179823\n",
      "Gradient Descent(43/999): loss=0.1022563889854332\n",
      "Gradient Descent(44/999): loss=0.1022027607159306\n",
      "Gradient Descent(45/999): loss=0.10214942815039822\n",
      "Gradient Descent(46/999): loss=0.10209638911951928\n",
      "Gradient Descent(47/999): loss=0.10204364155177041\n",
      "Gradient Descent(48/999): loss=0.10199118344825277\n",
      "Gradient Descent(49/999): loss=0.10193901286497786\n",
      "Gradient Descent(50/999): loss=0.10188712790038977\n",
      "Gradient Descent(51/999): loss=0.10183552668656527\n",
      "Gradient Descent(52/999): loss=0.10178420738299786\n",
      "Gradient Descent(53/999): loss=0.10173316817219702\n",
      "Gradient Descent(54/999): loss=0.10168240725656273\n",
      "Gradient Descent(55/999): loss=0.10163192285615652\n",
      "Gradient Descent(56/999): loss=0.10158171320710216\n",
      "Gradient Descent(57/999): loss=0.10153177656042933\n",
      "Gradient Descent(58/999): loss=0.10148211118122911\n",
      "Gradient Descent(59/999): loss=0.10143271534802842\n",
      "Gradient Descent(60/999): loss=0.10138358735231896\n",
      "Gradient Descent(61/999): loss=0.10133472549819521\n",
      "Gradient Descent(62/999): loss=0.10128612810206923\n",
      "Gradient Descent(63/999): loss=0.10123779349243991\n",
      "Gradient Descent(64/999): loss=0.10118972000970099\n",
      "Gradient Descent(65/999): loss=0.10114190600597633\n",
      "Gradient Descent(66/999): loss=0.10109434984497548\n",
      "Gradient Descent(67/999): loss=0.10104704990186285\n",
      "Gradient Descent(68/999): loss=0.10100000456313775\n",
      "Gradient Descent(69/999): loss=0.1009532122265219\n",
      "Gradient Descent(70/999): loss=0.10090667130085271\n",
      "Gradient Descent(71/999): loss=0.10086038020598105\n",
      "Gradient Descent(72/999): loss=0.10081433737267238\n",
      "Gradient Descent(73/999): loss=0.10076854124251067\n",
      "Gradient Descent(74/999): loss=0.10072299026780447\n",
      "Gradient Descent(75/999): loss=0.10067768291149515\n",
      "Gradient Descent(76/999): loss=0.10063261764706673\n",
      "Gradient Descent(77/999): loss=0.10058779295845698\n",
      "Gradient Descent(78/999): loss=0.10054320733997038\n",
      "Gradient Descent(79/999): loss=0.10049885929619166\n",
      "Gradient Descent(80/999): loss=0.10045474734190131\n",
      "Gradient Descent(81/999): loss=0.10041087000199152\n",
      "Gradient Descent(82/999): loss=0.10036722581138363\n",
      "Gradient Descent(83/999): loss=0.10032381331494655\n",
      "Gradient Descent(84/999): loss=0.10028063106741597\n",
      "Gradient Descent(85/999): loss=0.10023767763331495\n",
      "Gradient Descent(86/999): loss=0.10019495158687508\n",
      "Gradient Descent(87/999): loss=0.10015245151195871\n",
      "Gradient Descent(88/999): loss=0.10011017600198233\n",
      "Gradient Descent(89/999): loss=0.10006812365984045\n",
      "Gradient Descent(90/999): loss=0.10002629309783072\n",
      "Gradient Descent(91/999): loss=0.09998468293757955\n",
      "Gradient Descent(92/999): loss=0.09994329180996897\n",
      "Gradient Descent(93/999): loss=0.09990211835506405\n",
      "Gradient Descent(94/999): loss=0.09986116122204114\n",
      "Gradient Descent(95/999): loss=0.09982041906911722\n",
      "Gradient Descent(96/999): loss=0.09977989056347947\n",
      "Gradient Descent(97/999): loss=0.09973957438121624\n",
      "Gradient Descent(98/999): loss=0.09969946920724834\n",
      "Gradient Descent(99/999): loss=0.09965957373526126\n",
      "Gradient Descent(100/999): loss=0.09961988666763803\n",
      "Gradient Descent(101/999): loss=0.09958040671539295\n",
      "Gradient Descent(102/999): loss=0.09954113259810583\n",
      "Gradient Descent(103/999): loss=0.09950206304385721\n",
      "Gradient Descent(104/999): loss=0.0994631967891638\n",
      "Gradient Descent(105/999): loss=0.09942453257891513\n",
      "Gradient Descent(106/999): loss=0.09938606916631047\n",
      "Gradient Descent(107/999): loss=0.09934780531279662\n",
      "Gradient Descent(108/999): loss=0.09930973978800622\n",
      "Gradient Descent(109/999): loss=0.09927187136969676\n",
      "Gradient Descent(110/999): loss=0.09923419884369006\n",
      "Gradient Descent(111/999): loss=0.0991967210038127\n",
      "Gradient Descent(112/999): loss=0.09915943665183664\n",
      "Gradient Descent(113/999): loss=0.09912234459742067\n",
      "Gradient Descent(114/999): loss=0.09908544365805236\n",
      "Gradient Descent(115/999): loss=0.09904873265899079\n",
      "Gradient Descent(116/999): loss=0.0990122104332095\n",
      "Gradient Descent(117/999): loss=0.09897587582134004\n",
      "Gradient Descent(118/999): loss=0.09893972767161646\n",
      "Gradient Descent(119/999): loss=0.09890376483981989\n",
      "Gradient Descent(120/999): loss=0.09886798618922386\n",
      "Gradient Descent(121/999): loss=0.09883239059054015\n",
      "Gradient Descent(122/999): loss=0.09879697692186491\n",
      "Gradient Descent(123/999): loss=0.09876174406862584\n",
      "Gradient Descent(124/999): loss=0.09872669092352909\n",
      "Gradient Descent(125/999): loss=0.09869181638650734\n",
      "Gradient Descent(126/999): loss=0.09865711936466802\n",
      "Gradient Descent(127/999): loss=0.09862259877224214\n",
      "Gradient Descent(128/999): loss=0.0985882535305333\n",
      "Gradient Descent(129/999): loss=0.09855408256786768\n",
      "Gradient Descent(130/999): loss=0.09852008481954404\n",
      "Gradient Descent(131/999): loss=0.0984862592277843\n",
      "Gradient Descent(132/999): loss=0.09845260474168469\n",
      "Gradient Descent(133/999): loss=0.09841912031716711\n",
      "Gradient Descent(134/999): loss=0.09838580491693125\n",
      "Gradient Descent(135/999): loss=0.09835265751040664\n",
      "Gradient Descent(136/999): loss=0.09831967707370574\n",
      "Gradient Descent(137/999): loss=0.09828686258957678\n",
      "Gradient Descent(138/999): loss=0.09825421304735762\n",
      "Gradient Descent(139/999): loss=0.09822172744292959\n",
      "Gradient Descent(140/999): loss=0.09818940477867191\n",
      "Gradient Descent(141/999): loss=0.09815724406341649\n",
      "Gradient Descent(142/999): loss=0.09812524431240294\n",
      "Gradient Descent(143/999): loss=0.09809340454723445\n",
      "Gradient Descent(144/999): loss=0.09806172379583335\n",
      "Gradient Descent(145/999): loss=0.09803020109239753\n",
      "Gradient Descent(146/999): loss=0.09799883547735723\n",
      "Gradient Descent(147/999): loss=0.09796762599733187\n",
      "Gradient Descent(148/999): loss=0.09793657170508754\n",
      "Gradient Descent(149/999): loss=0.09790567165949474\n",
      "Gradient Descent(150/999): loss=0.09787492492548629\n",
      "Gradient Descent(151/999): loss=0.0978443305740161\n",
      "Gradient Descent(152/999): loss=0.09781388768201761\n",
      "Gradient Descent(153/999): loss=0.097783595332363\n",
      "Gradient Descent(154/999): loss=0.09775345261382277\n",
      "Gradient Descent(155/999): loss=0.09772345862102526\n",
      "Gradient Descent(156/999): loss=0.09769361245441696\n",
      "Gradient Descent(157/999): loss=0.09766391322022273\n",
      "Gradient Descent(158/999): loss=0.09763436003040674\n",
      "Gradient Descent(159/999): loss=0.09760495200263328\n",
      "Gradient Descent(160/999): loss=0.0975756882602283\n",
      "Gradient Descent(161/999): loss=0.09754656793214102\n",
      "Gradient Descent(162/999): loss=0.09751759015290583\n",
      "Gradient Descent(163/999): loss=0.09748875406260452\n",
      "Gradient Descent(164/999): loss=0.097460058806829\n",
      "Gradient Descent(165/999): loss=0.09743150353664397\n",
      "Gradient Descent(166/999): loss=0.09740308740855023\n",
      "Gradient Descent(167/999): loss=0.0973748095844479\n",
      "Gradient Descent(168/999): loss=0.09734666923160021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(169/999): loss=0.09731866552259769\n",
      "Gradient Descent(170/999): loss=0.097290797635322\n",
      "Gradient Descent(171/999): loss=0.0972630647529109\n",
      "Gradient Descent(172/999): loss=0.09723546606372274\n",
      "Gradient Descent(173/999): loss=0.09720800076130172\n",
      "Gradient Descent(174/999): loss=0.09718066804434317\n",
      "Gradient Descent(175/999): loss=0.09715346711665905\n",
      "Gradient Descent(176/999): loss=0.09712639718714408\n",
      "Gradient Descent(177/999): loss=0.09709945746974154\n",
      "Gradient Descent(178/999): loss=0.09707264718340983\n",
      "Gradient Descent(179/999): loss=0.09704596555208904\n",
      "Gradient Descent(180/999): loss=0.09701941180466786\n",
      "Gradient Descent(181/999): loss=0.09699298517495063\n",
      "Gradient Descent(182/999): loss=0.09696668490162474\n",
      "Gradient Descent(183/999): loss=0.09694051022822828\n",
      "Gradient Descent(184/999): loss=0.09691446040311776\n",
      "Gradient Descent(185/999): loss=0.09688853467943631\n",
      "Gradient Descent(186/999): loss=0.09686273231508193\n",
      "Gradient Descent(187/999): loss=0.09683705257267607\n",
      "Gradient Descent(188/999): loss=0.09681149471953243\n",
      "Gradient Descent(189/999): loss=0.0967860580276258\n",
      "Gradient Descent(190/999): loss=0.09676074177356153\n",
      "Gradient Descent(191/999): loss=0.09673554523854479\n",
      "Gradient Descent(192/999): loss=0.09671046770835037\n",
      "Gradient Descent(193/999): loss=0.09668550847329248\n",
      "Gradient Descent(194/999): loss=0.09666066682819492\n",
      "Gradient Descent(195/999): loss=0.09663594207236138\n",
      "Gradient Descent(196/999): loss=0.096611333509546\n",
      "Gradient Descent(197/999): loss=0.09658684044792405\n",
      "Gradient Descent(198/999): loss=0.09656246220006295\n",
      "Gradient Descent(199/999): loss=0.09653819808289349\n",
      "Gradient Descent(200/999): loss=0.09651404741768102\n",
      "Gradient Descent(201/999): loss=0.09649000952999724\n",
      "Gradient Descent(202/999): loss=0.09646608374969182\n",
      "Gradient Descent(203/999): loss=0.0964422694108644\n",
      "Gradient Descent(204/999): loss=0.09641856585183681\n",
      "Gradient Descent(205/999): loss=0.0963949724151255\n",
      "Gradient Descent(206/999): loss=0.09637148844741389\n",
      "Gradient Descent(207/999): loss=0.09634811329952526\n",
      "Gradient Descent(208/999): loss=0.09632484632639593\n",
      "Gradient Descent(209/999): loss=0.0963016868870478\n",
      "Gradient Descent(210/999): loss=0.0962786343445623\n",
      "Gradient Descent(211/999): loss=0.09625568806605354\n",
      "Gradient Descent(212/999): loss=0.09623284742264214\n",
      "Gradient Descent(213/999): loss=0.09621011178942906\n",
      "Gradient Descent(214/999): loss=0.09618748054546962\n",
      "Gradient Descent(215/999): loss=0.09616495307374791\n",
      "Gradient Descent(216/999): loss=0.09614252876115097\n",
      "Gradient Descent(217/999): loss=0.09612020699844363\n",
      "Gradient Descent(218/999): loss=0.09609798718024301\n",
      "Gradient Descent(219/999): loss=0.09607586870499374\n",
      "Gradient Descent(220/999): loss=0.0960538509749429\n",
      "Gradient Descent(221/999): loss=0.09603193339611533\n",
      "Gradient Descent(222/999): loss=0.09601011537828917\n",
      "Gradient Descent(223/999): loss=0.09598839633497147\n",
      "Gradient Descent(224/999): loss=0.09596677568337383\n",
      "Gradient Descent(225/999): loss=0.09594525284438855\n",
      "Gradient Descent(226/999): loss=0.09592382724256465\n",
      "Gradient Descent(227/999): loss=0.09590249830608424\n",
      "Gradient Descent(228/999): loss=0.09588126546673886\n",
      "Gradient Descent(229/999): loss=0.09586012815990617\n",
      "Gradient Descent(230/999): loss=0.09583908582452666\n",
      "Gradient Descent(231/999): loss=0.09581813790308065\n",
      "Gradient Descent(232/999): loss=0.09579728384156526\n",
      "Gradient Descent(233/999): loss=0.09577652308947184\n",
      "Gradient Descent(234/999): loss=0.09575585509976306\n",
      "Gradient Descent(235/999): loss=0.09573527932885073\n",
      "Gradient Descent(236/999): loss=0.09571479523657339\n",
      "Gradient Descent(237/999): loss=0.09569440228617411\n",
      "Gradient Descent(238/999): loss=0.09567409994427865\n",
      "Gradient Descent(239/999): loss=0.09565388768087334\n",
      "Gradient Descent(240/999): loss=0.09563376496928364\n",
      "Gradient Descent(241/999): loss=0.09561373128615233\n",
      "Gradient Descent(242/999): loss=0.09559378611141832\n",
      "Gradient Descent(243/999): loss=0.09557392892829525\n",
      "Gradient Descent(244/999): loss=0.09555415922325036\n",
      "Gradient Descent(245/999): loss=0.09553447648598351\n",
      "Gradient Descent(246/999): loss=0.09551488020940635\n",
      "Gradient Descent(247/999): loss=0.09549536988962164\n",
      "Gradient Descent(248/999): loss=0.09547594502590252\n",
      "Gradient Descent(249/999): loss=0.09545660512067224\n",
      "Gradient Descent(250/999): loss=0.09543734967948389\n",
      "Gradient Descent(251/999): loss=0.09541817821100008\n",
      "Gradient Descent(252/999): loss=0.09539909022697292\n",
      "Gradient Descent(253/999): loss=0.09538008524222434\n",
      "Gradient Descent(254/999): loss=0.09536116277462607\n",
      "Gradient Descent(255/999): loss=0.09534232234508008\n",
      "Gradient Descent(256/999): loss=0.09532356347749923\n",
      "Gradient Descent(257/999): loss=0.09530488569878764\n",
      "Gradient Descent(258/999): loss=0.09528628853882169\n",
      "Gradient Descent(259/999): loss=0.09526777153043071\n",
      "Gradient Descent(260/999): loss=0.09524933420937816\n",
      "Gradient Descent(261/999): loss=0.09523097611434254\n",
      "Gradient Descent(262/999): loss=0.09521269678689898\n",
      "Gradient Descent(263/999): loss=0.09519449577150021\n",
      "Gradient Descent(264/999): loss=0.09517637261545846\n",
      "Gradient Descent(265/999): loss=0.09515832686892685\n",
      "Gradient Descent(266/999): loss=0.09514035808488114\n",
      "Gradient Descent(267/999): loss=0.09512246581910179\n",
      "Gradient Descent(268/999): loss=0.09510464963015576\n",
      "Gradient Descent(269/999): loss=0.09508690907937863\n",
      "Gradient Descent(270/999): loss=0.09506924373085693\n",
      "Gradient Descent(271/999): loss=0.09505165315141047\n",
      "Gradient Descent(272/999): loss=0.09503413691057472\n",
      "Gradient Descent(273/999): loss=0.09501669458058332\n",
      "Gradient Descent(274/999): loss=0.09499932573635103\n",
      "Gradient Descent(275/999): loss=0.09498202995545629\n",
      "Gradient Descent(276/999): loss=0.09496480681812422\n",
      "Gradient Descent(277/999): loss=0.09494765590720963\n",
      "Gradient Descent(278/999): loss=0.09493057680818025\n",
      "Gradient Descent(279/999): loss=0.09491356910909977\n",
      "Gradient Descent(280/999): loss=0.09489663240061137\n",
      "Gradient Descent(281/999): loss=0.09487976627592126\n",
      "Gradient Descent(282/999): loss=0.09486297033078199\n",
      "Gradient Descent(283/999): loss=0.0948462441634763\n",
      "Gradient Descent(284/999): loss=0.09482958737480093\n",
      "Gradient Descent(285/999): loss=0.09481299956805055\n",
      "Gradient Descent(286/999): loss=0.09479648034900147\n",
      "Gradient Descent(287/999): loss=0.09478002932589621\n",
      "Gradient Descent(288/999): loss=0.09476364610942727\n",
      "Gradient Descent(289/999): loss=0.0947473303127217\n",
      "Gradient Descent(290/999): loss=0.09473108155132544\n",
      "Gradient Descent(291/999): loss=0.09471489944318787\n",
      "Gradient Descent(292/999): loss=0.09469878360864631\n",
      "Gradient Descent(293/999): loss=0.0946827336704109\n",
      "Gradient Descent(294/999): loss=0.09466674925354933\n",
      "Gradient Descent(295/999): loss=0.0946508299854717\n",
      "Gradient Descent(296/999): loss=0.09463497549591562\n",
      "Gradient Descent(297/999): loss=0.0946191854169314\n",
      "Gradient Descent(298/999): loss=0.09460345938286696\n",
      "Gradient Descent(299/999): loss=0.09458779703035342\n",
      "Gradient Descent(300/999): loss=0.09457219799829043\n",
      "Gradient Descent(301/999): loss=0.09455666192783163\n",
      "Gradient Descent(302/999): loss=0.0945411884623702\n",
      "Gradient Descent(303/999): loss=0.09452577724752464\n",
      "Gradient Descent(304/999): loss=0.09451042793112445\n",
      "Gradient Descent(305/999): loss=0.09449514016319605\n",
      "Gradient Descent(306/999): loss=0.09447991359594869\n",
      "Gradient Descent(307/999): loss=0.09446474788376062\n",
      "Gradient Descent(308/999): loss=0.09444964268316512\n",
      "Gradient Descent(309/999): loss=0.09443459765283665\n",
      "Gradient Descent(310/999): loss=0.0944196124535774\n",
      "Gradient Descent(311/999): loss=0.09440468674830352\n",
      "Gradient Descent(312/999): loss=0.09438982020203167\n",
      "Gradient Descent(313/999): loss=0.09437501248186564\n",
      "Gradient Descent(314/999): loss=0.0943602632569829\n",
      "Gradient Descent(315/999): loss=0.09434557219862144\n",
      "Gradient Descent(316/999): loss=0.09433093898006675\n",
      "Gradient Descent(317/999): loss=0.09431636327663846\n",
      "Gradient Descent(318/999): loss=0.09430184476567752\n",
      "Gradient Descent(319/999): loss=0.09428738312653326\n",
      "Gradient Descent(320/999): loss=0.09427297804055071\n",
      "Gradient Descent(321/999): loss=0.09425862919105757\n",
      "Gradient Descent(322/999): loss=0.09424433626335177\n",
      "Gradient Descent(323/999): loss=0.09423009894468891\n",
      "Gradient Descent(324/999): loss=0.09421591692426963\n",
      "Gradient Descent(325/999): loss=0.09420178989322725\n",
      "Gradient Descent(326/999): loss=0.09418771754461551\n",
      "Gradient Descent(327/999): loss=0.09417369957339629\n",
      "Gradient Descent(328/999): loss=0.09415973567642731\n",
      "Gradient Descent(329/999): loss=0.09414582555245024\n",
      "Gradient Descent(330/999): loss=0.09413196890207852\n",
      "Gradient Descent(331/999): loss=0.09411816542778548\n",
      "Gradient Descent(332/999): loss=0.09410441483389252\n",
      "Gradient Descent(333/999): loss=0.09409071682655726\n",
      "Gradient Descent(334/999): loss=0.09407707111376182\n",
      "Gradient Descent(335/999): loss=0.09406347740530129\n",
      "Gradient Descent(336/999): loss=0.09404993541277204\n",
      "Gradient Descent(337/999): loss=0.09403644484956031\n",
      "Gradient Descent(338/999): loss=0.09402300543083081\n",
      "Gradient Descent(339/999): loss=0.09400961687351535\n",
      "Gradient Descent(340/999): loss=0.09399627889630155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(341/999): loss=0.09398299121962174\n",
      "Gradient Descent(342/999): loss=0.09396975356564176\n",
      "Gradient Descent(343/999): loss=0.09395656565824988\n",
      "Gradient Descent(344/999): loss=0.09394342722304597\n",
      "Gradient Descent(345/999): loss=0.09393033798733047\n",
      "Gradient Descent(346/999): loss=0.09391729768009353\n",
      "Gradient Descent(347/999): loss=0.09390430603200442\n",
      "Gradient Descent(348/999): loss=0.0938913627754006\n",
      "Gradient Descent(349/999): loss=0.09387846764427726\n",
      "Gradient Descent(350/999): loss=0.09386562037427677\n",
      "Gradient Descent(351/999): loss=0.09385282070267797\n",
      "Gradient Descent(352/999): loss=0.09384006836838603\n",
      "Gradient Descent(353/999): loss=0.09382736311192193\n",
      "Gradient Descent(354/999): loss=0.09381470467541222\n",
      "Gradient Descent(355/999): loss=0.09380209280257877\n",
      "Gradient Descent(356/999): loss=0.09378952723872858\n",
      "Gradient Descent(357/999): loss=0.09377700773074378\n",
      "Gradient Descent(358/999): loss=0.09376453402707158\n",
      "Gradient Descent(359/999): loss=0.09375210587771418\n",
      "Gradient Descent(360/999): loss=0.09373972303421907\n",
      "Gradient Descent(361/999): loss=0.09372738524966903\n",
      "Gradient Descent(362/999): loss=0.09371509227867243\n",
      "Gradient Descent(363/999): loss=0.09370284387735353\n",
      "Gradient Descent(364/999): loss=0.09369063980334276\n",
      "Gradient Descent(365/999): loss=0.09367847981576728\n",
      "Gradient Descent(366/999): loss=0.09366636367524127\n",
      "Gradient Descent(367/999): loss=0.09365429114385655\n",
      "Gradient Descent(368/999): loss=0.0936422619851733\n",
      "Gradient Descent(369/999): loss=0.0936302759642105\n",
      "Gradient Descent(370/999): loss=0.09361833284743679\n",
      "Gradient Descent(371/999): loss=0.09360643240276119\n",
      "Gradient Descent(372/999): loss=0.09359457439952398\n",
      "Gradient Descent(373/999): loss=0.09358275860848757\n",
      "Gradient Descent(374/999): loss=0.09357098480182743\n",
      "Gradient Descent(375/999): loss=0.09355925275312316\n",
      "Gradient Descent(376/999): loss=0.0935475622373495\n",
      "Gradient Descent(377/999): loss=0.09353591303086758\n",
      "Gradient Descent(378/999): loss=0.09352430491141583\n",
      "Gradient Descent(379/999): loss=0.09351273765810161\n",
      "Gradient Descent(380/999): loss=0.09350121105139216\n",
      "Gradient Descent(381/999): loss=0.09348972487310611\n",
      "Gradient Descent(382/999): loss=0.09347827890640488\n",
      "Gradient Descent(383/999): loss=0.09346687293578422\n",
      "Gradient Descent(384/999): loss=0.09345550674706549\n",
      "Gradient Descent(385/999): loss=0.09344418012738749\n",
      "Gradient Descent(386/999): loss=0.09343289286519792\n",
      "Gradient Descent(387/999): loss=0.09342164475024521\n",
      "Gradient Descent(388/999): loss=0.09341043557357008\n",
      "Gradient Descent(389/999): loss=0.09339926512749744\n",
      "Gradient Descent(390/999): loss=0.09338813320562822\n",
      "Gradient Descent(391/999): loss=0.0933770396028312\n",
      "Gradient Descent(392/999): loss=0.09336598411523502\n",
      "Gradient Descent(393/999): loss=0.09335496654022017\n",
      "Gradient Descent(394/999): loss=0.09334398667641103\n",
      "Gradient Descent(395/999): loss=0.09333304432366787\n",
      "Gradient Descent(396/999): loss=0.0933221392830792\n",
      "Gradient Descent(397/999): loss=0.09331127135695379\n",
      "Gradient Descent(398/999): loss=0.09330044034881296\n",
      "Gradient Descent(399/999): loss=0.09328964606338296\n",
      "Gradient Descent(400/999): loss=0.09327888830658725\n",
      "Gradient Descent(401/999): loss=0.09326816688553886\n",
      "Gradient Descent(402/999): loss=0.09325748160853291\n",
      "Gradient Descent(403/999): loss=0.09324683228503909\n",
      "Gradient Descent(404/999): loss=0.09323621872569417\n",
      "Gradient Descent(405/999): loss=0.09322564074229459\n",
      "Gradient Descent(406/999): loss=0.09321509814778917\n",
      "Gradient Descent(407/999): loss=0.09320459075627162\n",
      "Gradient Descent(408/999): loss=0.09319411838297356\n",
      "Gradient Descent(409/999): loss=0.093183680844257\n",
      "Gradient Descent(410/999): loss=0.09317327795760731\n",
      "Gradient Descent(411/999): loss=0.09316290954162613\n",
      "Gradient Descent(412/999): loss=0.09315257541602416\n",
      "Gradient Descent(413/999): loss=0.09314227540161421\n",
      "Gradient Descent(414/999): loss=0.09313200932030422\n",
      "Gradient Descent(415/999): loss=0.09312177699509024\n",
      "Gradient Descent(416/999): loss=0.09311157825004954\n",
      "Gradient Descent(417/999): loss=0.09310141291033383\n",
      "Gradient Descent(418/999): loss=0.09309128080216235\n",
      "Gradient Descent(419/999): loss=0.09308118175281511\n",
      "Gradient Descent(420/999): loss=0.09307111559062628\n",
      "Gradient Descent(421/999): loss=0.09306108214497728\n",
      "Gradient Descent(422/999): loss=0.09305108124629032\n",
      "Gradient Descent(423/999): loss=0.09304111272602178\n",
      "Gradient Descent(424/999): loss=0.09303117641665552\n",
      "Gradient Descent(425/999): loss=0.09302127215169656\n",
      "Gradient Descent(426/999): loss=0.09301139976566447\n",
      "Gradient Descent(427/999): loss=0.09300155909408687\n",
      "Gradient Descent(428/999): loss=0.09299174997349334\n",
      "Gradient Descent(429/999): loss=0.09298197224140865\n",
      "Gradient Descent(430/999): loss=0.09297222573634681\n",
      "Gradient Descent(431/999): loss=0.09296251029780458\n",
      "Gradient Descent(432/999): loss=0.09295282576625538\n",
      "Gradient Descent(433/999): loss=0.09294317198314292\n",
      "Gradient Descent(434/999): loss=0.0929335487908753\n",
      "Gradient Descent(435/999): loss=0.0929239560328187\n",
      "Gradient Descent(436/999): loss=0.0929143935532913\n",
      "Gradient Descent(437/999): loss=0.09290486119755746\n",
      "Gradient Descent(438/999): loss=0.09289535881182144\n",
      "Gradient Descent(439/999): loss=0.09288588624322171\n",
      "Gradient Descent(440/999): loss=0.0928764433398248\n",
      "Gradient Descent(441/999): loss=0.09286702995061961\n",
      "Gradient Descent(442/999): loss=0.0928576459255114\n",
      "Gradient Descent(443/999): loss=0.09284829111531614\n",
      "Gradient Descent(444/999): loss=0.09283896537175466\n",
      "Gradient Descent(445/999): loss=0.0928296685474469\n",
      "Gradient Descent(446/999): loss=0.09282040049590623\n",
      "Gradient Descent(447/999): loss=0.09281116107153392\n",
      "Gradient Descent(448/999): loss=0.09280195012961316\n",
      "Gradient Descent(449/999): loss=0.09279276752630403\n",
      "Gradient Descent(450/999): loss=0.09278361311863739\n",
      "Gradient Descent(451/999): loss=0.09277448676450975\n",
      "Gradient Descent(452/999): loss=0.09276538832267758\n",
      "Gradient Descent(453/999): loss=0.09275631765275197\n",
      "Gradient Descent(454/999): loss=0.09274727461519319\n",
      "Gradient Descent(455/999): loss=0.09273825907130533\n",
      "Gradient Descent(456/999): loss=0.09272927088323084\n",
      "Gradient Descent(457/999): loss=0.0927203099139455\n",
      "Gradient Descent(458/999): loss=0.09271137602725284\n",
      "Gradient Descent(459/999): loss=0.09270246908777909\n",
      "Gradient Descent(460/999): loss=0.09269358896096787\n",
      "Gradient Descent(461/999): loss=0.09268473551307511\n",
      "Gradient Descent(462/999): loss=0.0926759086111639\n",
      "Gradient Descent(463/999): loss=0.09266710812309926\n",
      "Gradient Descent(464/999): loss=0.09265833391754325\n",
      "Gradient Descent(465/999): loss=0.09264958586394981\n",
      "Gradient Descent(466/999): loss=0.09264086383255972\n",
      "Gradient Descent(467/999): loss=0.09263216769439578\n",
      "Gradient Descent(468/999): loss=0.09262349732125766\n",
      "Gradient Descent(469/999): loss=0.09261485258571721\n",
      "Gradient Descent(470/999): loss=0.09260623336111337\n",
      "Gradient Descent(471/999): loss=0.0925976395215474\n",
      "Gradient Descent(472/999): loss=0.09258907094187822\n",
      "Gradient Descent(473/999): loss=0.0925805274977173\n",
      "Gradient Descent(474/999): loss=0.09257200906542414\n",
      "Gradient Descent(475/999): loss=0.09256351552210151\n",
      "Gradient Descent(476/999): loss=0.09255504674559065\n",
      "Gradient Descent(477/999): loss=0.09254660261446673\n",
      "Gradient Descent(478/999): loss=0.09253818300803412\n",
      "Gradient Descent(479/999): loss=0.09252978780632182\n",
      "Gradient Descent(480/999): loss=0.0925214168900789\n",
      "Gradient Descent(481/999): loss=0.0925130701407699\n",
      "Gradient Descent(482/999): loss=0.09250474744057034\n",
      "Gradient Descent(483/999): loss=0.09249644867236223\n",
      "Gradient Descent(484/999): loss=0.0924881737197296\n",
      "Gradient Descent(485/999): loss=0.0924799224669541\n",
      "Gradient Descent(486/999): loss=0.09247169479901048\n",
      "Gradient Descent(487/999): loss=0.09246349060156239\n",
      "Gradient Descent(488/999): loss=0.09245530976095788\n",
      "Gradient Descent(489/999): loss=0.09244715216422517\n",
      "Gradient Descent(490/999): loss=0.09243901769906823\n",
      "Gradient Descent(491/999): loss=0.09243090625386273\n",
      "Gradient Descent(492/999): loss=0.0924228177176516\n",
      "Gradient Descent(493/999): loss=0.09241475198014086\n",
      "Gradient Descent(494/999): loss=0.09240670893169554\n",
      "Gradient Descent(495/999): loss=0.09239868846333538\n",
      "Gradient Descent(496/999): loss=0.09239069046673079\n",
      "Gradient Descent(497/999): loss=0.09238271483419876\n",
      "Gradient Descent(498/999): loss=0.09237476145869863\n",
      "Gradient Descent(499/999): loss=0.09236683023382827\n",
      "Gradient Descent(500/999): loss=0.09235892105381982\n",
      "Gradient Descent(501/999): loss=0.09235103381353586\n",
      "Gradient Descent(502/999): loss=0.09234316840846533\n",
      "Gradient Descent(503/999): loss=0.09233532473471963\n",
      "Gradient Descent(504/999): loss=0.09232750268902871\n",
      "Gradient Descent(505/999): loss=0.0923197021687371\n",
      "Gradient Descent(506/999): loss=0.09231192307180013\n",
      "Gradient Descent(507/999): loss=0.09230416529677987\n",
      "Gradient Descent(508/999): loss=0.09229642874284169\n",
      "Gradient Descent(509/999): loss=0.09228871330975004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(510/999): loss=0.09228101889786496\n",
      "Gradient Descent(511/999): loss=0.09227334540813814\n",
      "Gradient Descent(512/999): loss=0.09226569274210937\n",
      "Gradient Descent(513/999): loss=0.09225806080190264\n",
      "Gradient Descent(514/999): loss=0.0922504494902227\n",
      "Gradient Descent(515/999): loss=0.09224285871035105\n",
      "Gradient Descent(516/999): loss=0.09223528836614271\n",
      "Gradient Descent(517/999): loss=0.0922277383620223\n",
      "Gradient Descent(518/999): loss=0.09222020860298061\n",
      "Gradient Descent(519/999): loss=0.09221269899457094\n",
      "Gradient Descent(520/999): loss=0.0922052094429057\n",
      "Gradient Descent(521/999): loss=0.09219773985465264\n",
      "Gradient Descent(522/999): loss=0.09219029013703159\n",
      "Gradient Descent(523/999): loss=0.09218286019781101\n",
      "Gradient Descent(524/999): loss=0.09217544994530419\n",
      "Gradient Descent(525/999): loss=0.09216805928836627\n",
      "Gradient Descent(526/999): loss=0.09216068813639046\n",
      "Gradient Descent(527/999): loss=0.09215333639930487\n",
      "Gradient Descent(528/999): loss=0.09214600398756914\n",
      "Gradient Descent(529/999): loss=0.09213869081217092\n",
      "Gradient Descent(530/999): loss=0.09213139678462273\n",
      "Gradient Descent(531/999): loss=0.09212412181695856\n",
      "Gradient Descent(532/999): loss=0.09211686582173068\n",
      "Gradient Descent(533/999): loss=0.09210962871200622\n",
      "Gradient Descent(534/999): loss=0.09210241040136412\n",
      "Gradient Descent(535/999): loss=0.09209521080389177\n",
      "Gradient Descent(536/999): loss=0.09208802983418188\n",
      "Gradient Descent(537/999): loss=0.09208086740732932\n",
      "Gradient Descent(538/999): loss=0.09207372343892781\n",
      "Gradient Descent(539/999): loss=0.09206659784506709\n",
      "Gradient Descent(540/999): loss=0.09205949054232938\n",
      "Gradient Descent(541/999): loss=0.09205240144778676\n",
      "Gradient Descent(542/999): loss=0.09204533047899764\n",
      "Gradient Descent(543/999): loss=0.09203827755400411\n",
      "Gradient Descent(544/999): loss=0.0920312425913285\n",
      "Gradient Descent(545/999): loss=0.09202422550997066\n",
      "Gradient Descent(546/999): loss=0.09201722622940492\n",
      "Gradient Descent(547/999): loss=0.09201024466957694\n",
      "Gradient Descent(548/999): loss=0.09200328075090097\n",
      "Gradient Descent(549/999): loss=0.09199633439425677\n",
      "Gradient Descent(550/999): loss=0.09198940552098658\n",
      "Gradient Descent(551/999): loss=0.09198249405289256\n",
      "Gradient Descent(552/999): loss=0.09197559991223357\n",
      "Gradient Descent(553/999): loss=0.09196872302172249\n",
      "Gradient Descent(554/999): loss=0.09196186330452319\n",
      "Gradient Descent(555/999): loss=0.09195502068424795\n",
      "Gradient Descent(556/999): loss=0.09194819508495446\n",
      "Gradient Descent(557/999): loss=0.09194138643114305\n",
      "Gradient Descent(558/999): loss=0.09193459464775394\n",
      "Gradient Descent(559/999): loss=0.09192781966016451\n",
      "Gradient Descent(560/999): loss=0.09192106139418652\n",
      "Gradient Descent(561/999): loss=0.09191431977606336\n",
      "Gradient Descent(562/999): loss=0.09190759473246736\n",
      "Gradient Descent(563/999): loss=0.09190088619049717\n",
      "Gradient Descent(564/999): loss=0.09189419407767502\n",
      "Gradient Descent(565/999): loss=0.09188751832194396\n",
      "Gradient Descent(566/999): loss=0.09188085885166548\n",
      "Gradient Descent(567/999): loss=0.0918742155956167\n",
      "Gradient Descent(568/999): loss=0.09186758848298775\n",
      "Gradient Descent(569/999): loss=0.09186097744337929\n",
      "Gradient Descent(570/999): loss=0.09185438240679981\n",
      "Gradient Descent(571/999): loss=0.09184780330366325\n",
      "Gradient Descent(572/999): loss=0.09184124006478632\n",
      "Gradient Descent(573/999): loss=0.09183469262138594\n",
      "Gradient Descent(574/999): loss=0.09182816090507687\n",
      "Gradient Descent(575/999): loss=0.0918216448478692\n",
      "Gradient Descent(576/999): loss=0.09181514438216573\n",
      "Gradient Descent(577/999): loss=0.09180865944075968\n",
      "Gradient Descent(578/999): loss=0.09180218995683218\n",
      "Gradient Descent(579/999): loss=0.09179573586394972\n",
      "Gradient Descent(580/999): loss=0.09178929709606203\n",
      "Gradient Descent(581/999): loss=0.09178287358749937\n",
      "Gradient Descent(582/999): loss=0.09177646527297034\n",
      "Gradient Descent(583/999): loss=0.09177007208755944\n",
      "Gradient Descent(584/999): loss=0.09176369396672476\n",
      "Gradient Descent(585/999): loss=0.09175733084629553\n",
      "Gradient Descent(586/999): loss=0.09175098266247005\n",
      "Gradient Descent(587/999): loss=0.09174464935181302\n",
      "Gradient Descent(588/999): loss=0.09173833085125362\n",
      "Gradient Descent(589/999): loss=0.09173202709808292\n",
      "Gradient Descent(590/999): loss=0.09172573802995182\n",
      "Gradient Descent(591/999): loss=0.09171946358486871\n",
      "Gradient Descent(592/999): loss=0.09171320370119732\n",
      "Gradient Descent(593/999): loss=0.09170695831765435\n",
      "Gradient Descent(594/999): loss=0.09170072737330735\n",
      "Gradient Descent(595/999): loss=0.0916945108075727\n",
      "Gradient Descent(596/999): loss=0.09168830856021304\n",
      "Gradient Descent(597/999): loss=0.09168212057133539\n",
      "Gradient Descent(598/999): loss=0.09167594678138899\n",
      "Gradient Descent(599/999): loss=0.09166978713116311\n",
      "Gradient Descent(600/999): loss=0.09166364156178489\n",
      "Gradient Descent(601/999): loss=0.0916575100147173\n",
      "Gradient Descent(602/999): loss=0.09165139243175689\n",
      "Gradient Descent(603/999): loss=0.09164528875503201\n",
      "Gradient Descent(604/999): loss=0.0916391989270004\n",
      "Gradient Descent(605/999): loss=0.09163312289044741\n",
      "Gradient Descent(606/999): loss=0.09162706058848377\n",
      "Gradient Descent(607/999): loss=0.09162101196454366\n",
      "Gradient Descent(608/999): loss=0.09161497696238255\n",
      "Gradient Descent(609/999): loss=0.0916089555260755\n",
      "Gradient Descent(610/999): loss=0.09160294760001478\n",
      "Gradient Descent(611/999): loss=0.09159695312890821\n",
      "Gradient Descent(612/999): loss=0.09159097205777704\n",
      "Gradient Descent(613/999): loss=0.09158500433195396\n",
      "Gradient Descent(614/999): loss=0.09157904989708132\n",
      "Gradient Descent(615/999): loss=0.091573108699109\n",
      "Gradient Descent(616/999): loss=0.09156718068429266\n",
      "Gradient Descent(617/999): loss=0.09156126579919178\n",
      "Gradient Descent(618/999): loss=0.09155536399066772\n",
      "Gradient Descent(619/999): loss=0.09154947520588183\n",
      "Gradient Descent(620/999): loss=0.09154359939229377\n",
      "Gradient Descent(621/999): loss=0.0915377364976594\n",
      "Gradient Descent(622/999): loss=0.09153188647002906\n",
      "Gradient Descent(623/999): loss=0.09152604925774575\n",
      "Gradient Descent(624/999): loss=0.09152022480944327\n",
      "Gradient Descent(625/999): loss=0.09151441307404447\n",
      "Gradient Descent(626/999): loss=0.0915086140007593\n",
      "Gradient Descent(627/999): loss=0.09150282753908318\n",
      "Gradient Descent(628/999): loss=0.09149705363879523\n",
      "Gradient Descent(629/999): loss=0.09149129224995636\n",
      "Gradient Descent(630/999): loss=0.09148554332290765\n",
      "Gradient Descent(631/999): loss=0.09147980680826857\n",
      "Gradient Descent(632/999): loss=0.09147408265693524\n",
      "Gradient Descent(633/999): loss=0.09146837082007869\n",
      "Gradient Descent(634/999): loss=0.09146267124914316\n",
      "Gradient Descent(635/999): loss=0.09145698389584449\n",
      "Gradient Descent(636/999): loss=0.09145130871216828\n",
      "Gradient Descent(637/999): loss=0.09144564565036832\n",
      "Gradient Descent(638/999): loss=0.09143999466296494\n",
      "Gradient Descent(639/999): loss=0.09143435570274325\n",
      "Gradient Descent(640/999): loss=0.0914287287227516\n",
      "Gradient Descent(641/999): loss=0.09142311367629989\n",
      "Gradient Descent(642/999): loss=0.09141751051695796\n",
      "Gradient Descent(643/999): loss=0.09141191919855397\n",
      "Gradient Descent(644/999): loss=0.0914063396751728\n",
      "Gradient Descent(645/999): loss=0.09140077190115455\n",
      "Gradient Descent(646/999): loss=0.09139521583109268\n",
      "Gradient Descent(647/999): loss=0.0913896714198328\n",
      "Gradient Descent(648/999): loss=0.09138413862247091\n",
      "Gradient Descent(649/999): loss=0.09137861739435177\n",
      "Gradient Descent(650/999): loss=0.09137310769106759\n",
      "Gradient Descent(651/999): loss=0.09136760946845625\n",
      "Gradient Descent(652/999): loss=0.09136212268259998\n",
      "Gradient Descent(653/999): loss=0.09135664728982366\n",
      "Gradient Descent(654/999): loss=0.09135118324669356\n",
      "Gradient Descent(655/999): loss=0.09134573051001557\n",
      "Gradient Descent(656/999): loss=0.09134028903683392\n",
      "Gradient Descent(657/999): loss=0.09133485878442958\n",
      "Gradient Descent(658/999): loss=0.09132943971031887\n",
      "Gradient Descent(659/999): loss=0.09132403177225203\n",
      "Gradient Descent(660/999): loss=0.09131863492821159\n",
      "Gradient Descent(661/999): loss=0.0913132491364112\n",
      "Gradient Descent(662/999): loss=0.09130787435529399\n",
      "Gradient Descent(663/999): loss=0.09130251054353121\n",
      "Gradient Descent(664/999): loss=0.09129715766002094\n",
      "Gradient Descent(665/999): loss=0.09129181566388642\n",
      "Gradient Descent(666/999): loss=0.09128648451447495\n",
      "Gradient Descent(667/999): loss=0.09128116417135632\n",
      "Gradient Descent(668/999): loss=0.09127585459432147\n",
      "Gradient Descent(669/999): loss=0.09127055574338121\n",
      "Gradient Descent(670/999): loss=0.0912652675787647\n",
      "Gradient Descent(671/999): loss=0.09125999006091828\n",
      "Gradient Descent(672/999): loss=0.09125472315050397\n",
      "Gradient Descent(673/999): loss=0.09124946680839834\n",
      "Gradient Descent(674/999): loss=0.09124422099569084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(675/999): loss=0.09123898567368292\n",
      "Gradient Descent(676/999): loss=0.09123376080388639\n",
      "Gradient Descent(677/999): loss=0.09122854634802224\n",
      "Gradient Descent(678/999): loss=0.09122334226801941\n",
      "Gradient Descent(679/999): loss=0.09121814852601338\n",
      "Gradient Descent(680/999): loss=0.09121296508434504\n",
      "Gradient Descent(681/999): loss=0.09120779190555922\n",
      "Gradient Descent(682/999): loss=0.09120262895240365\n",
      "Gradient Descent(683/999): loss=0.09119747618782761\n",
      "Gradient Descent(684/999): loss=0.09119233357498069\n",
      "Gradient Descent(685/999): loss=0.0911872010772115\n",
      "Gradient Descent(686/999): loss=0.0911820786580666\n",
      "Gradient Descent(687/999): loss=0.09117696628128912\n",
      "Gradient Descent(688/999): loss=0.09117186391081765\n",
      "Gradient Descent(689/999): loss=0.09116677151078498\n",
      "Gradient Descent(690/999): loss=0.09116168904551693\n",
      "Gradient Descent(691/999): loss=0.09115661647953119\n",
      "Gradient Descent(692/999): loss=0.09115155377753606\n",
      "Gradient Descent(693/999): loss=0.09114650090442937\n",
      "Gradient Descent(694/999): loss=0.09114145782529728\n",
      "Gradient Descent(695/999): loss=0.09113642450541301\n",
      "Gradient Descent(696/999): loss=0.09113140091023594\n",
      "Gradient Descent(697/999): loss=0.09112638700541019\n",
      "Gradient Descent(698/999): loss=0.09112138275676379\n",
      "Gradient Descent(699/999): loss=0.0911163881303071\n",
      "Gradient Descent(700/999): loss=0.09111140309223222\n",
      "Gradient Descent(701/999): loss=0.0911064276089115\n",
      "Gradient Descent(702/999): loss=0.09110146164689657\n",
      "Gradient Descent(703/999): loss=0.09109650517291723\n",
      "Gradient Descent(704/999): loss=0.09109155815388034\n",
      "Gradient Descent(705/999): loss=0.09108662055686874\n",
      "Gradient Descent(706/999): loss=0.09108169234914018\n",
      "Gradient Descent(707/999): loss=0.09107677349812618\n",
      "Gradient Descent(708/999): loss=0.09107186397143119\n",
      "Gradient Descent(709/999): loss=0.09106696373683118\n",
      "Gradient Descent(710/999): loss=0.09106207276227286\n",
      "Gradient Descent(711/999): loss=0.09105719101587252\n",
      "Gradient Descent(712/999): loss=0.09105231846591504\n",
      "Gradient Descent(713/999): loss=0.09104745508085281\n",
      "Gradient Descent(714/999): loss=0.09104260082930477\n",
      "Gradient Descent(715/999): loss=0.09103775568005529\n",
      "Gradient Descent(716/999): loss=0.09103291960205327\n",
      "Gradient Descent(717/999): loss=0.09102809256441105\n",
      "Gradient Descent(718/999): loss=0.09102327453640345\n",
      "Gradient Descent(719/999): loss=0.09101846548746675\n",
      "Gradient Descent(720/999): loss=0.09101366538719777\n",
      "Gradient Descent(721/999): loss=0.09100887420535278\n",
      "Gradient Descent(722/999): loss=0.09100409191184658\n",
      "Gradient Descent(723/999): loss=0.09099931847675158\n",
      "Gradient Descent(724/999): loss=0.09099455387029678\n",
      "Gradient Descent(725/999): loss=0.0909897980628668\n",
      "Gradient Descent(726/999): loss=0.09098505102500098\n",
      "Gradient Descent(727/999): loss=0.09098031272739236\n",
      "Gradient Descent(728/999): loss=0.09097558314088687\n",
      "Gradient Descent(729/999): loss=0.09097086223648226\n",
      "Gradient Descent(730/999): loss=0.09096614998532725\n",
      "Gradient Descent(731/999): loss=0.09096144635872055\n",
      "Gradient Descent(732/999): loss=0.0909567513281101\n",
      "Gradient Descent(733/999): loss=0.09095206486509191\n",
      "Gradient Descent(734/999): loss=0.09094738694140941\n",
      "Gradient Descent(735/999): loss=0.09094271752895236\n",
      "Gradient Descent(736/999): loss=0.09093805659975604\n",
      "Gradient Descent(737/999): loss=0.0909334041260004\n",
      "Gradient Descent(738/999): loss=0.09092876008000912\n",
      "Gradient Descent(739/999): loss=0.09092412443424869\n",
      "Gradient Descent(740/999): loss=0.0909194971613277\n",
      "Gradient Descent(741/999): loss=0.09091487823399588\n",
      "Gradient Descent(742/999): loss=0.09091026762514315\n",
      "Gradient Descent(743/999): loss=0.0909056653077989\n",
      "Gradient Descent(744/999): loss=0.09090107125513111\n",
      "Gradient Descent(745/999): loss=0.09089648544044554\n",
      "Gradient Descent(746/999): loss=0.09089190783718472\n",
      "Gradient Descent(747/999): loss=0.09088733841892739\n",
      "Gradient Descent(748/999): loss=0.09088277715938747\n",
      "Gradient Descent(749/999): loss=0.09087822403241329\n",
      "Gradient Descent(750/999): loss=0.0908736790119868\n",
      "Gradient Descent(751/999): loss=0.09086914207222271\n",
      "Gradient Descent(752/999): loss=0.09086461318736785\n",
      "Gradient Descent(753/999): loss=0.09086009233180006\n",
      "Gradient Descent(754/999): loss=0.09085557948002765\n",
      "Gradient Descent(755/999): loss=0.09085107460668859\n",
      "Gradient Descent(756/999): loss=0.09084657768654957\n",
      "Gradient Descent(757/999): loss=0.09084208869450533\n",
      "Gradient Descent(758/999): loss=0.09083760760557798\n",
      "Gradient Descent(759/999): loss=0.09083313439491596\n",
      "Gradient Descent(760/999): loss=0.09082866903779356\n",
      "Gradient Descent(761/999): loss=0.09082421150960993\n",
      "Gradient Descent(762/999): loss=0.09081976178588855\n",
      "Gradient Descent(763/999): loss=0.09081531984227623\n",
      "Gradient Descent(764/999): loss=0.09081088565454257\n",
      "Gradient Descent(765/999): loss=0.0908064591985791\n",
      "Gradient Descent(766/999): loss=0.09080204045039864\n",
      "Gradient Descent(767/999): loss=0.09079762938613438\n",
      "Gradient Descent(768/999): loss=0.09079322598203943\n",
      "Gradient Descent(769/999): loss=0.09078883021448583\n",
      "Gradient Descent(770/999): loss=0.09078444205996403\n",
      "Gradient Descent(771/999): loss=0.09078006149508205\n",
      "Gradient Descent(772/999): loss=0.09077568849656477\n",
      "Gradient Descent(773/999): loss=0.0907713230412534\n",
      "Gradient Descent(774/999): loss=0.09076696510610449\n",
      "Gradient Descent(775/999): loss=0.09076261466818956\n",
      "Gradient Descent(776/999): loss=0.09075827170469407\n",
      "Gradient Descent(777/999): loss=0.09075393619291702\n",
      "Gradient Descent(778/999): loss=0.0907496081102701\n",
      "Gradient Descent(779/999): loss=0.09074528743427712\n",
      "Gradient Descent(780/999): loss=0.09074097414257316\n",
      "Gradient Descent(781/999): loss=0.09073666821290412\n",
      "Gradient Descent(782/999): loss=0.0907323696231259\n",
      "Gradient Descent(783/999): loss=0.09072807835120382\n",
      "Gradient Descent(784/999): loss=0.09072379437521194\n",
      "Gradient Descent(785/999): loss=0.09071951767333228\n",
      "Gradient Descent(786/999): loss=0.09071524822385447\n",
      "Gradient Descent(787/999): loss=0.09071098600517481\n",
      "Gradient Descent(788/999): loss=0.09070673099579575\n",
      "Gradient Descent(789/999): loss=0.09070248317432522\n",
      "Gradient Descent(790/999): loss=0.09069824251947613\n",
      "Gradient Descent(791/999): loss=0.09069400901006552\n",
      "Gradient Descent(792/999): loss=0.0906897826250141\n",
      "Gradient Descent(793/999): loss=0.09068556334334554\n",
      "Gradient Descent(794/999): loss=0.0906813511441859\n",
      "Gradient Descent(795/999): loss=0.09067714600676309\n",
      "Gradient Descent(796/999): loss=0.09067294791040603\n",
      "Gradient Descent(797/999): loss=0.0906687568345443\n",
      "Gradient Descent(798/999): loss=0.09066457275870736\n",
      "Gradient Descent(799/999): loss=0.09066039566252408\n",
      "Gradient Descent(800/999): loss=0.09065622552572203\n",
      "Gradient Descent(801/999): loss=0.09065206232812696\n",
      "Gradient Descent(802/999): loss=0.09064790604966225\n",
      "Gradient Descent(803/999): loss=0.09064375667034821\n",
      "Gradient Descent(804/999): loss=0.09063961417030161\n",
      "Gradient Descent(805/999): loss=0.090635478529735\n",
      "Gradient Descent(806/999): loss=0.09063134972895633\n",
      "Gradient Descent(807/999): loss=0.09062722774836805\n",
      "Gradient Descent(808/999): loss=0.09062311256846692\n",
      "Gradient Descent(809/999): loss=0.09061900416984321\n",
      "Gradient Descent(810/999): loss=0.0906149025331802\n",
      "Gradient Descent(811/999): loss=0.09061080763925362\n",
      "Gradient Descent(812/999): loss=0.09060671946893116\n",
      "Gradient Descent(813/999): loss=0.09060263800317185\n",
      "Gradient Descent(814/999): loss=0.0905985632230255\n",
      "Gradient Descent(815/999): loss=0.09059449510963222\n",
      "Gradient Descent(816/999): loss=0.09059043364422188\n",
      "Gradient Descent(817/999): loss=0.09058637880811359\n",
      "Gradient Descent(818/999): loss=0.0905823305827151\n",
      "Gradient Descent(819/999): loss=0.09057828894952227\n",
      "Gradient Descent(820/999): loss=0.09057425389011863\n",
      "Gradient Descent(821/999): loss=0.0905702253861749\n",
      "Gradient Descent(822/999): loss=0.09056620341944824\n",
      "Gradient Descent(823/999): loss=0.09056218797178192\n",
      "Gradient Descent(824/999): loss=0.0905581790251049\n",
      "Gradient Descent(825/999): loss=0.09055417656143108\n",
      "Gradient Descent(826/999): loss=0.09055018056285893\n",
      "Gradient Descent(827/999): loss=0.09054619101157099\n",
      "Gradient Descent(828/999): loss=0.09054220788983337\n",
      "Gradient Descent(829/999): loss=0.09053823117999522\n",
      "Gradient Descent(830/999): loss=0.09053426086448829\n",
      "Gradient Descent(831/999): loss=0.09053029692582636\n",
      "Gradient Descent(832/999): loss=0.09052633934660483\n",
      "Gradient Descent(833/999): loss=0.09052238810950024\n",
      "Gradient Descent(834/999): loss=0.09051844319726972\n",
      "Gradient Descent(835/999): loss=0.09051450459275062\n",
      "Gradient Descent(836/999): loss=0.09051057227885992\n",
      "Gradient Descent(837/999): loss=0.09050664623859381\n",
      "Gradient Descent(838/999): loss=0.09050272645502726\n",
      "Gradient Descent(839/999): loss=0.09049881291131352\n",
      "Gradient Descent(840/999): loss=0.09049490559068361\n",
      "Gradient Descent(841/999): loss=0.09049100447644602\n",
      "Gradient Descent(842/999): loss=0.09048710955198608\n",
      "Gradient Descent(843/999): loss=0.09048322080076551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(844/999): loss=0.09047933820632215\n",
      "Gradient Descent(845/999): loss=0.09047546175226936\n",
      "Gradient Descent(846/999): loss=0.09047159142229556\n",
      "Gradient Descent(847/999): loss=0.09046772720016391\n",
      "Gradient Descent(848/999): loss=0.09046386906971177\n",
      "Gradient Descent(849/999): loss=0.0904600170148503\n",
      "Gradient Descent(850/999): loss=0.09045617101956399\n",
      "Gradient Descent(851/999): loss=0.09045233106791037\n",
      "Gradient Descent(852/999): loss=0.09044849714401937\n",
      "Gradient Descent(853/999): loss=0.090444669232093\n",
      "Gradient Descent(854/999): loss=0.090440847316405\n",
      "Gradient Descent(855/999): loss=0.09043703138130028\n",
      "Gradient Descent(856/999): loss=0.09043322141119461\n",
      "Gradient Descent(857/999): loss=0.09042941739057415\n",
      "Gradient Descent(858/999): loss=0.09042561930399508\n",
      "Gradient Descent(859/999): loss=0.09042182713608309\n",
      "Gradient Descent(860/999): loss=0.09041804087153309\n",
      "Gradient Descent(861/999): loss=0.09041426049510885\n",
      "Gradient Descent(862/999): loss=0.0904104859916424\n",
      "Gradient Descent(863/999): loss=0.09040671734603373\n",
      "Gradient Descent(864/999): loss=0.09040295454325052\n",
      "Gradient Descent(865/999): loss=0.09039919756832751\n",
      "Gradient Descent(866/999): loss=0.09039544640636625\n",
      "Gradient Descent(867/999): loss=0.09039170104253484\n",
      "Gradient Descent(868/999): loss=0.09038796146206715\n",
      "Gradient Descent(869/999): loss=0.0903842276502629\n",
      "Gradient Descent(870/999): loss=0.09038049959248691\n",
      "Gradient Descent(871/999): loss=0.09037677727416892\n",
      "Gradient Descent(872/999): loss=0.09037306068080317\n",
      "Gradient Descent(873/999): loss=0.09036934979794801\n",
      "Gradient Descent(874/999): loss=0.09036564461122554\n",
      "Gradient Descent(875/999): loss=0.09036194510632117\n",
      "Gradient Descent(876/999): loss=0.09035825126898339\n",
      "Gradient Descent(877/999): loss=0.09035456308502338\n",
      "Gradient Descent(878/999): loss=0.09035088054031437\n",
      "Gradient Descent(879/999): loss=0.0903472036207918\n",
      "Gradient Descent(880/999): loss=0.09034353231245246\n",
      "Gradient Descent(881/999): loss=0.09033986660135439\n",
      "Gradient Descent(882/999): loss=0.09033620647361652\n",
      "Gradient Descent(883/999): loss=0.09033255191541822\n",
      "Gradient Descent(884/999): loss=0.09032890291299904\n",
      "Gradient Descent(885/999): loss=0.09032525945265832\n",
      "Gradient Descent(886/999): loss=0.09032162152075483\n",
      "Gradient Descent(887/999): loss=0.09031798910370646\n",
      "Gradient Descent(888/999): loss=0.09031436218798988\n",
      "Gradient Descent(889/999): loss=0.09031074076014013\n",
      "Gradient Descent(890/999): loss=0.09030712480675045\n",
      "Gradient Descent(891/999): loss=0.09030351431447174\n",
      "Gradient Descent(892/999): loss=0.09029990927001234\n",
      "Gradient Descent(893/999): loss=0.09029630966013771\n",
      "Gradient Descent(894/999): loss=0.09029271547167009\n",
      "Gradient Descent(895/999): loss=0.0902891266914881\n",
      "Gradient Descent(896/999): loss=0.0902855433065265\n",
      "Gradient Descent(897/999): loss=0.09028196530377582\n",
      "Gradient Descent(898/999): loss=0.0902783926702821\n",
      "Gradient Descent(899/999): loss=0.09027482539314655\n",
      "Gradient Descent(900/999): loss=0.09027126345952514\n",
      "Gradient Descent(901/999): loss=0.09026770685662838\n",
      "Gradient Descent(902/999): loss=0.09026415557172111\n",
      "Gradient Descent(903/999): loss=0.09026060959212183\n",
      "Gradient Descent(904/999): loss=0.09025706890520284\n",
      "Gradient Descent(905/999): loss=0.09025353349838967\n",
      "Gradient Descent(906/999): loss=0.09025000335916078\n",
      "Gradient Descent(907/999): loss=0.09024647847504728\n",
      "Gradient Descent(908/999): loss=0.09024295883363281\n",
      "Gradient Descent(909/999): loss=0.09023944442255294\n",
      "Gradient Descent(910/999): loss=0.09023593522949502\n",
      "Gradient Descent(911/999): loss=0.09023243124219796\n",
      "Gradient Descent(912/999): loss=0.09022893244845184\n",
      "Gradient Descent(913/999): loss=0.09022543883609759\n",
      "Gradient Descent(914/999): loss=0.09022195039302679\n",
      "Gradient Descent(915/999): loss=0.09021846710718132\n",
      "Gradient Descent(916/999): loss=0.09021498896655314\n",
      "Gradient Descent(917/999): loss=0.09021151595918386\n",
      "Gradient Descent(918/999): loss=0.09020804807316464\n",
      "Gradient Descent(919/999): loss=0.09020458529663582\n",
      "Gradient Descent(920/999): loss=0.09020112761778666\n",
      "Gradient Descent(921/999): loss=0.09019767502485494\n",
      "Gradient Descent(922/999): loss=0.0901942275061269\n",
      "Gradient Descent(923/999): loss=0.09019078504993686\n",
      "Gradient Descent(924/999): loss=0.09018734764466693\n",
      "Gradient Descent(925/999): loss=0.09018391527874668\n",
      "Gradient Descent(926/999): loss=0.09018048794065309\n",
      "Gradient Descent(927/999): loss=0.09017706561891001\n",
      "Gradient Descent(928/999): loss=0.09017364830208804\n",
      "Gradient Descent(929/999): loss=0.09017023597880439\n",
      "Gradient Descent(930/999): loss=0.09016682863772231\n",
      "Gradient Descent(931/999): loss=0.09016342626755108\n",
      "Gradient Descent(932/999): loss=0.09016002885704567\n",
      "Gradient Descent(933/999): loss=0.09015663639500646\n",
      "Gradient Descent(934/999): loss=0.09015324887027897\n",
      "Gradient Descent(935/999): loss=0.09014986627175377\n",
      "Gradient Descent(936/999): loss=0.09014648858836594\n",
      "Gradient Descent(937/999): loss=0.09014311580909509\n",
      "Gradient Descent(938/999): loss=0.09013974792296492\n",
      "Gradient Descent(939/999): loss=0.09013638491904317\n",
      "Gradient Descent(940/999): loss=0.09013302678644106\n",
      "Gradient Descent(941/999): loss=0.09012967351431346\n",
      "Gradient Descent(942/999): loss=0.09012632509185828\n",
      "Gradient Descent(943/999): loss=0.0901229815083164\n",
      "Gradient Descent(944/999): loss=0.09011964275297142\n",
      "Gradient Descent(945/999): loss=0.09011630881514945\n",
      "Gradient Descent(946/999): loss=0.09011297968421872\n",
      "Gradient Descent(947/999): loss=0.09010965534958958\n",
      "Gradient Descent(948/999): loss=0.09010633580071402\n",
      "Gradient Descent(949/999): loss=0.09010302102708562\n",
      "Gradient Descent(950/999): loss=0.09009971101823923\n",
      "Gradient Descent(951/999): loss=0.0900964057637508\n",
      "Gradient Descent(952/999): loss=0.09009310525323706\n",
      "Gradient Descent(953/999): loss=0.0900898094763554\n",
      "Gradient Descent(954/999): loss=0.09008651842280352\n",
      "Gradient Descent(955/999): loss=0.09008323208231939\n",
      "Gradient Descent(956/999): loss=0.0900799504446808\n",
      "Gradient Descent(957/999): loss=0.09007667349970529\n",
      "Gradient Descent(958/999): loss=0.09007340123724991\n",
      "Gradient Descent(959/999): loss=0.09007013364721099\n",
      "Gradient Descent(960/999): loss=0.09006687071952392\n",
      "Gradient Descent(961/999): loss=0.09006361244416287\n",
      "Gradient Descent(962/999): loss=0.09006035881114065\n",
      "Gradient Descent(963/999): loss=0.09005710981050856\n",
      "Gradient Descent(964/999): loss=0.09005386543235601\n",
      "Gradient Descent(965/999): loss=0.09005062566681042\n",
      "Gradient Descent(966/999): loss=0.09004739050403705\n",
      "Gradient Descent(967/999): loss=0.09004415993423863\n",
      "Gradient Descent(968/999): loss=0.0900409339476553\n",
      "Gradient Descent(969/999): loss=0.09003771253456441\n",
      "Gradient Descent(970/999): loss=0.09003449568528016\n",
      "Gradient Descent(971/999): loss=0.0900312833901536\n",
      "Gradient Descent(972/999): loss=0.09002807563957224\n",
      "Gradient Descent(973/999): loss=0.09002487242395997\n",
      "Gradient Descent(974/999): loss=0.09002167373377684\n",
      "Gradient Descent(975/999): loss=0.09001847955951889\n",
      "Gradient Descent(976/999): loss=0.0900152898917178\n",
      "Gradient Descent(977/999): loss=0.09001210472094093\n",
      "Gradient Descent(978/999): loss=0.09000892403779091\n",
      "Gradient Descent(979/999): loss=0.09000574783290557\n",
      "Gradient Descent(980/999): loss=0.09000257609695775\n",
      "Gradient Descent(981/999): loss=0.08999940882065502\n",
      "Gradient Descent(982/999): loss=0.08999624599473957\n",
      "Gradient Descent(983/999): loss=0.08999308760998803\n",
      "Gradient Descent(984/999): loss=0.0899899336572112\n",
      "Gradient Descent(985/999): loss=0.08998678412725393\n",
      "Gradient Descent(986/999): loss=0.0899836390109949\n",
      "Gradient Descent(987/999): loss=0.08998049829934653\n",
      "Gradient Descent(988/999): loss=0.08997736198325461\n",
      "Gradient Descent(989/999): loss=0.08997423005369831\n",
      "Gradient Descent(990/999): loss=0.0899711025016899\n",
      "Gradient Descent(991/999): loss=0.08996797931827456\n",
      "Gradient Descent(992/999): loss=0.0899648604945303\n",
      "Gradient Descent(993/999): loss=0.0899617460215676\n",
      "Gradient Descent(994/999): loss=0.0899586358905295\n",
      "Gradient Descent(995/999): loss=0.08995553009259116\n",
      "Gradient Descent(996/999): loss=0.08995242861895981\n",
      "Gradient Descent(997/999): loss=0.0899493314608746\n",
      "Gradient Descent(998/999): loss=0.08994623860960638\n",
      "Gradient Descent(999/999): loss=0.0899431500564575\n",
      "Gradient Descent(0/999): loss=0.1484316010937374\n",
      "Gradient Descent(1/999): loss=0.1355325532612387\n",
      "Gradient Descent(2/999): loss=0.1264492523190962\n",
      "Gradient Descent(3/999): loss=0.1200470762385438\n",
      "Gradient Descent(4/999): loss=0.11552881952888355\n",
      "Gradient Descent(5/999): loss=0.11233435130173375\n",
      "Gradient Descent(6/999): loss=0.1100701133208119\n",
      "Gradient Descent(7/999): loss=0.10845958413292153\n",
      "Gradient Descent(8/999): loss=0.10730847432407727\n",
      "Gradient Descent(9/999): loss=0.10648027211462503\n",
      "Gradient Descent(10/999): loss=0.10587906127852643\n",
      "Gradient Descent(11/999): loss=0.10543744872125252\n",
      "Gradient Descent(12/999): loss=0.1051080821906956\n",
      "Gradient Descent(13/999): loss=0.10485769047658378\n",
      "Gradient Descent(14/999): loss=0.10466289595320705\n",
      "Gradient Descent(15/999): loss=0.1045072724006902\n",
      "Gradient Descent(16/999): loss=0.10437927778014669\n",
      "Gradient Descent(17/999): loss=0.1042708017663064\n",
      "Gradient Descent(18/999): loss=0.10417614521919837\n",
      "Gradient Descent(19/999): loss=0.10409130314355779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(20/999): loss=0.10401346088386959\n",
      "Gradient Descent(21/999): loss=0.10394064014239571\n",
      "Gradient Descent(22/999): loss=0.10387145026537949\n",
      "Gradient Descent(23/999): loss=0.1038049134924613\n",
      "Gradient Descent(24/999): loss=0.10374034217390479\n",
      "Gradient Descent(25/999): loss=0.10367725250129078\n",
      "Gradient Descent(26/999): loss=0.10361530389319515\n",
      "Gradient Descent(27/999): loss=0.10355425640649538\n",
      "Gradient Descent(28/999): loss=0.1034939408127902\n",
      "Gradient Descent(29/999): loss=0.1034342375735444\n",
      "Gradient Descent(30/999): loss=0.10337506206762895\n",
      "Gradient Descent(31/999): loss=0.10331635421190145\n",
      "Gradient Descent(32/999): loss=0.10325807116841179\n",
      "Gradient Descent(33/999): loss=0.1032001822203233\n",
      "Gradient Descent(34/999): loss=0.1031426651716113\n",
      "Gradient Descent(35/999): loss=0.10308550381739355\n",
      "Gradient Descent(36/999): loss=0.10302868616650558\n",
      "Gradient Descent(37/999): loss=0.10297220319261648\n",
      "Gradient Descent(38/999): loss=0.10291604795670667\n",
      "Gradient Descent(39/999): loss=0.10286021499047114\n",
      "Gradient Descent(40/999): loss=0.10280469986305403\n",
      "Gradient Descent(41/999): loss=0.10274949887659532\n",
      "Gradient Descent(42/999): loss=0.10269460885228325\n",
      "Gradient Descent(43/999): loss=0.10264002697999824\n",
      "Gradient Descent(44/999): loss=0.10258575071263797\n",
      "Gradient Descent(45/999): loss=0.10253177769183539\n",
      "Gradient Descent(46/999): loss=0.10247810569573619\n",
      "Gradient Descent(47/999): loss=0.1024247326022742\n",
      "Gradient Descent(48/999): loss=0.10237165636333727\n",
      "Gradient Descent(49/999): loss=0.10231887498658519\n",
      "Gradient Descent(50/999): loss=0.10226638652264422\n",
      "Gradient Descent(51/999): loss=0.10221418905607965\n",
      "Gradient Descent(52/999): loss=0.10216228069902362\n",
      "Gradient Descent(53/999): loss=0.10211065958666825\n",
      "Gradient Descent(54/999): loss=0.10205932387407031\n",
      "Gradient Descent(55/999): loss=0.10200827173387728\n",
      "Gradient Descent(56/999): loss=0.10195750135470147\n",
      "Gradient Descent(57/999): loss=0.1019070109399494\n",
      "Gradient Descent(58/999): loss=0.10185679870697165\n",
      "Gradient Descent(59/999): loss=0.10180686288643816\n",
      "Gradient Descent(60/999): loss=0.10175720172187214\n",
      "Gradient Descent(61/999): loss=0.10170781346929542\n",
      "Gradient Descent(62/999): loss=0.10165869639695334\n",
      "Gradient Descent(63/999): loss=0.10160984878509438\n",
      "Gradient Descent(64/999): loss=0.10156126892579001\n",
      "Gradient Descent(65/999): loss=0.10151295512278179\n",
      "Gradient Descent(66/999): loss=0.10146490569134897\n",
      "Gradient Descent(67/999): loss=0.10141711895818975\n",
      "Gradient Descent(68/999): loss=0.10136959326131328\n",
      "Gradient Descent(69/999): loss=0.1013223269499388\n",
      "Gradient Descent(70/999): loss=0.10127531838440047\n",
      "Gradient Descent(71/999): loss=0.10122856593605634\n",
      "Gradient Descent(72/999): loss=0.10118206798720003\n",
      "Gradient Descent(73/999): loss=0.10113582293097553\n",
      "Gradient Descent(74/999): loss=0.10108982917129329\n",
      "Gradient Descent(75/999): loss=0.10104408512274836\n",
      "Gradient Descent(76/999): loss=0.10099858921053961\n",
      "Gradient Descent(77/999): loss=0.10095333987039026\n",
      "Gradient Descent(78/999): loss=0.10090833554846933\n",
      "Gradient Descent(79/999): loss=0.10086357470131428\n",
      "Gradient Descent(80/999): loss=0.10081905579575422\n",
      "Gradient Descent(81/999): loss=0.10077477730883416\n",
      "Gradient Descent(82/999): loss=0.1007307377277399\n",
      "Gradient Descent(83/999): loss=0.10068693554972388\n",
      "Gradient Descent(84/999): loss=0.10064336928203167\n",
      "Gradient Descent(85/999): loss=0.100600037441829\n",
      "Gradient Descent(86/999): loss=0.10055693855612967\n",
      "Gradient Descent(87/999): loss=0.10051407116172427\n",
      "Gradient Descent(88/999): loss=0.10047143380510928\n",
      "Gradient Descent(89/999): loss=0.10042902504241684\n",
      "Gradient Descent(90/999): loss=0.10038684343934562\n",
      "Gradient Descent(91/999): loss=0.10034488757109164\n",
      "Gradient Descent(92/999): loss=0.10030315602228022\n",
      "Gradient Descent(93/999): loss=0.10026164738689852\n",
      "Gradient Descent(94/999): loss=0.10022036026822818\n",
      "Gradient Descent(95/999): loss=0.10017929327877939\n",
      "Gradient Descent(96/999): loss=0.10013844504022479\n",
      "Gradient Descent(97/999): loss=0.10009781418333435\n",
      "Gradient Descent(98/999): loss=0.1000573993479108\n",
      "Gradient Descent(99/999): loss=0.10001719918272547\n",
      "Gradient Descent(100/999): loss=0.09997721234545484\n",
      "Gradient Descent(101/999): loss=0.09993743750261753\n",
      "Gradient Descent(102/999): loss=0.09989787332951197\n",
      "Gradient Descent(103/999): loss=0.09985851851015444\n",
      "Gradient Descent(104/999): loss=0.0998193717372178\n",
      "Gradient Descent(105/999): loss=0.09978043171197061\n",
      "Gradient Descent(106/999): loss=0.09974169714421688\n",
      "Gradient Descent(107/999): loss=0.0997031667522363\n",
      "Gradient Descent(108/999): loss=0.09966483926272482\n",
      "Gradient Descent(109/999): loss=0.099626713410736\n",
      "Gradient Descent(110/999): loss=0.0995887879396226\n",
      "Gradient Descent(111/999): loss=0.09955106160097897\n",
      "Gradient Descent(112/999): loss=0.09951353315458336\n",
      "Gradient Descent(113/999): loss=0.09947620136834151\n",
      "Gradient Descent(114/999): loss=0.09943906501822981\n",
      "Gradient Descent(115/999): loss=0.09940212288823969\n",
      "Gradient Descent(116/999): loss=0.09936537377032204\n",
      "Gradient Descent(117/999): loss=0.09932881646433207\n",
      "Gradient Descent(118/999): loss=0.09929244977797505\n",
      "Gradient Descent(119/999): loss=0.09925627252675173\n",
      "Gradient Descent(120/999): loss=0.09922028353390507\n",
      "Gradient Descent(121/999): loss=0.09918448163036668\n",
      "Gradient Descent(122/999): loss=0.09914886565470417\n",
      "Gradient Descent(123/999): loss=0.0991134344530685\n",
      "Gradient Descent(124/999): loss=0.09907818687914222\n",
      "Gradient Descent(125/999): loss=0.09904312179408768\n",
      "Gradient Descent(126/999): loss=0.09900823806649596\n",
      "Gradient Descent(127/999): loss=0.09897353457233608\n",
      "Gradient Descent(128/999): loss=0.09893901019490449\n",
      "Gradient Descent(129/999): loss=0.09890466382477522\n",
      "Gradient Descent(130/999): loss=0.09887049435975022\n",
      "Gradient Descent(131/999): loss=0.09883650070481005\n",
      "Gradient Descent(132/999): loss=0.09880268177206517\n",
      "Gradient Descent(133/999): loss=0.09876903648070744\n",
      "Gradient Descent(134/999): loss=0.09873556375696196\n",
      "Gradient Descent(135/999): loss=0.09870226253403942\n",
      "Gradient Descent(136/999): loss=0.09866913175208873\n",
      "Gradient Descent(137/999): loss=0.09863617035814998\n",
      "Gradient Descent(138/999): loss=0.09860337730610781\n",
      "Gradient Descent(139/999): loss=0.0985707515566452\n",
      "Gradient Descent(140/999): loss=0.09853829207719746\n",
      "Gradient Descent(141/999): loss=0.09850599784190656\n",
      "Gradient Descent(142/999): loss=0.09847386783157613\n",
      "Gradient Descent(143/999): loss=0.09844190103362628\n",
      "Gradient Descent(144/999): loss=0.09841009644204912\n",
      "Gradient Descent(145/999): loss=0.09837845305736466\n",
      "Gradient Descent(146/999): loss=0.09834696988657673\n",
      "Gradient Descent(147/999): loss=0.09831564594312937\n",
      "Gradient Descent(148/999): loss=0.09828448024686373\n",
      "Gradient Descent(149/999): loss=0.09825347182397508\n",
      "Gradient Descent(150/999): loss=0.09822261970697002\n",
      "Gradient Descent(151/999): loss=0.09819192293462445\n",
      "Gradient Descent(152/999): loss=0.09816138055194125\n",
      "Gradient Descent(153/999): loss=0.09813099161010894\n",
      "Gradient Descent(154/999): loss=0.09810075516645993\n",
      "Gradient Descent(155/999): loss=0.09807067028442974\n",
      "Gradient Descent(156/999): loss=0.09804073603351598\n",
      "Gradient Descent(157/999): loss=0.09801095148923805\n",
      "Gradient Descent(158/999): loss=0.09798131573309685\n",
      "Gradient Descent(159/999): loss=0.09795182785253477\n",
      "Gradient Descent(160/999): loss=0.0979224869408963\n",
      "Gradient Descent(161/999): loss=0.09789329209738852\n",
      "Gradient Descent(162/999): loss=0.0978642424270421\n",
      "Gradient Descent(163/999): loss=0.09783533704067258\n",
      "Gradient Descent(164/999): loss=0.09780657505484176\n",
      "Gradient Descent(165/999): loss=0.09777795559181966\n",
      "Gradient Descent(166/999): loss=0.09774947777954636\n",
      "Gradient Descent(167/999): loss=0.09772114075159458\n",
      "Gradient Descent(168/999): loss=0.09769294364713199\n",
      "Gradient Descent(169/999): loss=0.09766488561088432\n",
      "Gradient Descent(170/999): loss=0.09763696579309834\n",
      "Gradient Descent(171/999): loss=0.09760918334950532\n",
      "Gradient Descent(172/999): loss=0.0975815374412846\n",
      "Gradient Descent(173/999): loss=0.09755402723502755\n",
      "Gradient Descent(174/999): loss=0.09752665190270175\n",
      "Gradient Descent(175/999): loss=0.09749941062161534\n",
      "Gradient Descent(176/999): loss=0.09747230257438161\n",
      "Gradient Descent(177/999): loss=0.09744532694888414\n",
      "Gradient Descent(178/999): loss=0.09741848293824168\n",
      "Gradient Descent(179/999): loss=0.09739176974077376\n",
      "Gradient Descent(180/999): loss=0.09736518655996618\n",
      "Gradient Descent(181/999): loss=0.09733873260443697\n",
      "Gradient Descent(182/999): loss=0.09731240708790245\n",
      "Gradient Descent(183/999): loss=0.09728620922914363\n",
      "Gradient Descent(184/999): loss=0.09726013825197286\n",
      "Gradient Descent(185/999): loss=0.09723419338520038\n",
      "Gradient Descent(186/999): loss=0.09720837386260175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(187/999): loss=0.09718267892288476\n",
      "Gradient Descent(188/999): loss=0.0971571078096572\n",
      "Gradient Descent(189/999): loss=0.0971316597713945\n",
      "Gradient Descent(190/999): loss=0.09710633406140753\n",
      "Gradient Descent(191/999): loss=0.09708112993781108\n",
      "Gradient Descent(192/999): loss=0.09705604666349193\n",
      "Gradient Descent(193/999): loss=0.09703108350607766\n",
      "Gradient Descent(194/999): loss=0.09700623973790538\n",
      "Gradient Descent(195/999): loss=0.09698151463599089\n",
      "Gradient Descent(196/999): loss=0.0969569074819977\n",
      "Gradient Descent(197/999): loss=0.0969324175622067\n",
      "Gradient Descent(198/999): loss=0.09690804416748573\n",
      "Gradient Descent(199/999): loss=0.0968837865932595\n",
      "Gradient Descent(200/999): loss=0.09685964413947959\n",
      "Gradient Descent(201/999): loss=0.0968356161105949\n",
      "Gradient Descent(202/999): loss=0.09681170181552193\n",
      "Gradient Descent(203/999): loss=0.0967879005676156\n",
      "Gradient Descent(204/999): loss=0.09676421168464013\n",
      "Gradient Descent(205/999): loss=0.09674063448874007\n",
      "Gradient Descent(206/999): loss=0.09671716830641167\n",
      "Gradient Descent(207/999): loss=0.09669381246847421\n",
      "Gradient Descent(208/999): loss=0.09667056631004182\n",
      "Gradient Descent(209/999): loss=0.09664742917049532\n",
      "Gradient Descent(210/999): loss=0.0966244003934542\n",
      "Gradient Descent(211/999): loss=0.09660147932674891\n",
      "Gradient Descent(212/999): loss=0.09657866532239325\n",
      "Gradient Descent(213/999): loss=0.09655595773655704\n",
      "Gradient Descent(214/999): loss=0.09653335592953899\n",
      "Gradient Descent(215/999): loss=0.09651085926573937\n",
      "Gradient Descent(216/999): loss=0.09648846711363362\n",
      "Gradient Descent(217/999): loss=0.0964661788457454\n",
      "Gradient Descent(218/999): loss=0.09644399383862012\n",
      "Gradient Descent(219/999): loss=0.09642191147279888\n",
      "Gradient Descent(220/999): loss=0.096399931132792\n",
      "Gradient Descent(221/999): loss=0.09637805220705337\n",
      "Gradient Descent(222/999): loss=0.09635627408795458\n",
      "Gradient Descent(223/999): loss=0.09633459617175928\n",
      "Gradient Descent(224/999): loss=0.0963130178585979\n",
      "Gradient Descent(225/999): loss=0.09629153855244205\n",
      "Gradient Descent(226/999): loss=0.09627015766107994\n",
      "Gradient Descent(227/999): loss=0.09624887459609106\n",
      "Gradient Descent(228/999): loss=0.09622768877282163\n",
      "Gradient Descent(229/999): loss=0.09620659961035986\n",
      "Gradient Descent(230/999): loss=0.09618560653151169\n",
      "Gradient Descent(231/999): loss=0.09616470896277649\n",
      "Gradient Descent(232/999): loss=0.09614390633432285\n",
      "Gradient Descent(233/999): loss=0.0961231980799648\n",
      "Gradient Descent(234/999): loss=0.096102583637138\n",
      "Gradient Descent(235/999): loss=0.09608206244687598\n",
      "Gradient Descent(236/999): loss=0.09606163395378683\n",
      "Gradient Descent(237/999): loss=0.09604129760602993\n",
      "Gradient Descent(238/999): loss=0.09602105285529267\n",
      "Gradient Descent(239/999): loss=0.09600089915676745\n",
      "Gradient Descent(240/999): loss=0.09598083596912899\n",
      "Gradient Descent(241/999): loss=0.09596086275451159\n",
      "Gradient Descent(242/999): loss=0.0959409789784864\n",
      "Gradient Descent(243/999): loss=0.09592118411003928\n",
      "Gradient Descent(244/999): loss=0.0959014776215485\n",
      "Gradient Descent(245/999): loss=0.09588185898876253\n",
      "Gradient Descent(246/999): loss=0.0958623276907782\n",
      "Gradient Descent(247/999): loss=0.09584288321001885\n",
      "Gradient Descent(248/999): loss=0.09582352503221273\n",
      "Gradient Descent(249/999): loss=0.09580425264637141\n",
      "Gradient Descent(250/999): loss=0.09578506554476848\n",
      "Gradient Descent(251/999): loss=0.09576596322291826\n",
      "Gradient Descent(252/999): loss=0.09574694517955482\n",
      "Gradient Descent(253/999): loss=0.09572801091661079\n",
      "Gradient Descent(254/999): loss=0.09570915993919685\n",
      "Gradient Descent(255/999): loss=0.09569039175558086\n",
      "Gradient Descent(256/999): loss=0.09567170587716739\n",
      "Gradient Descent(257/999): loss=0.0956531018184773\n",
      "Gradient Descent(258/999): loss=0.09563457909712747\n",
      "Gradient Descent(259/999): loss=0.09561613723381072\n",
      "Gradient Descent(260/999): loss=0.09559777575227571\n",
      "Gradient Descent(261/999): loss=0.09557949417930715\n",
      "Gradient Descent(262/999): loss=0.0955612920447061\n",
      "Gradient Descent(263/999): loss=0.09554316888127022\n",
      "Gradient Descent(264/999): loss=0.09552512422477438\n",
      "Gradient Descent(265/999): loss=0.09550715761395143\n",
      "Gradient Descent(266/999): loss=0.0954892685904727\n",
      "Gradient Descent(267/999): loss=0.09547145669892917\n",
      "Gradient Descent(268/999): loss=0.09545372148681232\n",
      "Gradient Descent(269/999): loss=0.09543606250449548\n",
      "Gradient Descent(270/999): loss=0.09541847930521481\n",
      "Gradient Descent(271/999): loss=0.09540097144505102\n",
      "Gradient Descent(272/999): loss=0.09538353848291073\n",
      "Gradient Descent(273/999): loss=0.09536617998050809\n",
      "Gradient Descent(274/999): loss=0.09534889550234668\n",
      "Gradient Descent(275/999): loss=0.09533168461570118\n",
      "Gradient Descent(276/999): loss=0.09531454689059972\n",
      "Gradient Descent(277/999): loss=0.0952974818998056\n",
      "Gradient Descent(278/999): loss=0.09528048921879989\n",
      "Gradient Descent(279/999): loss=0.09526356842576345\n",
      "Gradient Descent(280/999): loss=0.09524671910155974\n",
      "Gradient Descent(281/999): loss=0.09522994082971717\n",
      "Gradient Descent(282/999): loss=0.09521323319641192\n",
      "Gradient Descent(283/999): loss=0.09519659579045071\n",
      "Gradient Descent(284/999): loss=0.09518002820325377\n",
      "Gradient Descent(285/999): loss=0.09516353002883773\n",
      "Gradient Descent(286/999): loss=0.0951471008637991\n",
      "Gradient Descent(287/999): loss=0.09513074030729711\n",
      "Gradient Descent(288/999): loss=0.09511444796103752\n",
      "Gradient Descent(289/999): loss=0.09509822342925568\n",
      "Gradient Descent(290/999): loss=0.09508206631870045\n",
      "Gradient Descent(291/999): loss=0.0950659762386178\n",
      "Gradient Descent(292/999): loss=0.09504995280073449\n",
      "Gradient Descent(293/999): loss=0.09503399561924208\n",
      "Gradient Descent(294/999): loss=0.09501810431078112\n",
      "Gradient Descent(295/999): loss=0.09500227849442479\n",
      "Gradient Descent(296/999): loss=0.09498651779166369\n",
      "Gradient Descent(297/999): loss=0.09497082182638987\n",
      "Gradient Descent(298/999): loss=0.0949551902248812\n",
      "Gradient Descent(299/999): loss=0.09493962261578608\n",
      "Gradient Descent(300/999): loss=0.09492411863010794\n",
      "Gradient Descent(301/999): loss=0.0949086779011901\n",
      "Gradient Descent(302/999): loss=0.09489330006470051\n",
      "Gradient Descent(303/999): loss=0.09487798475861667\n",
      "Gradient Descent(304/999): loss=0.09486273162321071\n",
      "Gradient Descent(305/999): loss=0.09484754030103462\n",
      "Gradient Descent(306/999): loss=0.09483241043690516\n",
      "Gradient Descent(307/999): loss=0.0948173416778896\n",
      "Gradient Descent(308/999): loss=0.09480233367329079\n",
      "Gradient Descent(309/999): loss=0.09478738607463291\n",
      "Gradient Descent(310/999): loss=0.0947724985356469\n",
      "Gradient Descent(311/999): loss=0.0947576707122563\n",
      "Gradient Descent(312/999): loss=0.09474290226256302\n",
      "Gradient Descent(313/999): loss=0.09472819284683315\n",
      "Gradient Descent(314/999): loss=0.09471354212748301\n",
      "Gradient Descent(315/999): loss=0.09469894976906522\n",
      "Gradient Descent(316/999): loss=0.09468441543825491\n",
      "Gradient Descent(317/999): loss=0.09466993880383583\n",
      "Gradient Descent(318/999): loss=0.09465551953668681\n",
      "Gradient Descent(319/999): loss=0.09464115730976821\n",
      "Gradient Descent(320/999): loss=0.09462685179810837\n",
      "Gradient Descent(321/999): loss=0.0946126026787903\n",
      "Gradient Descent(322/999): loss=0.0945984096309382\n",
      "Gradient Descent(323/999): loss=0.09458427233570453\n",
      "Gradient Descent(324/999): loss=0.09457019047625664\n",
      "Gradient Descent(325/999): loss=0.09455616373776381\n",
      "Gradient Descent(326/999): loss=0.09454219180738435\n",
      "Gradient Descent(327/999): loss=0.09452827437425264\n",
      "Gradient Descent(328/999): loss=0.09451441112946629\n",
      "Gradient Descent(329/999): loss=0.09450060176607364\n",
      "Gradient Descent(330/999): loss=0.09448684597906096\n",
      "Gradient Descent(331/999): loss=0.09447314346533989\n",
      "Gradient Descent(332/999): loss=0.09445949392373514\n",
      "Gradient Descent(333/999): loss=0.09444589705497206\n",
      "Gradient Descent(334/999): loss=0.09443235256166409\n",
      "Gradient Descent(335/999): loss=0.09441886014830098\n",
      "Gradient Descent(336/999): loss=0.09440541952123636\n",
      "Gradient Descent(337/999): loss=0.0943920303886757\n",
      "Gradient Descent(338/999): loss=0.0943786924606644\n",
      "Gradient Descent(339/999): loss=0.09436540544907591\n",
      "Gradient Descent(340/999): loss=0.09435216906759979\n",
      "Gradient Descent(341/999): loss=0.09433898303173015\n",
      "Gradient Descent(342/999): loss=0.0943258470587537\n",
      "Gradient Descent(343/999): loss=0.09431276086773849\n",
      "Gradient Descent(344/999): loss=0.09429972417952207\n",
      "Gradient Descent(345/999): loss=0.0942867367167003\n",
      "Gradient Descent(346/999): loss=0.09427379820361588\n",
      "Gradient Descent(347/999): loss=0.09426090836634698\n",
      "Gradient Descent(348/999): loss=0.09424806693269615\n",
      "Gradient Descent(349/999): loss=0.09423527363217908\n",
      "Gradient Descent(350/999): loss=0.09422252819601355\n",
      "Gradient Descent(351/999): loss=0.0942098303571085\n",
      "Gradient Descent(352/999): loss=0.09419717985005292\n",
      "Gradient Descent(353/999): loss=0.09418457641110518\n",
      "Gradient Descent(354/999): loss=0.09417201977818217\n",
      "Gradient Descent(355/999): loss=0.09415950969084844\n",
      "Gradient Descent(356/999): loss=0.09414704589030584\n",
      "Gradient Descent(357/999): loss=0.0941346281193827\n",
      "Gradient Descent(358/999): loss=0.09412225612252337\n",
      "Gradient Descent(359/999): loss=0.09410992964577784\n",
      "Gradient Descent(360/999): loss=0.09409764843679135\n",
      "Gradient Descent(361/999): loss=0.09408541224479394\n",
      "Gradient Descent(362/999): loss=0.09407322082059053\n",
      "Gradient Descent(363/999): loss=0.09406107391655032\n",
      "Gradient Descent(364/999): loss=0.09404897128659703\n",
      "Gradient Descent(365/999): loss=0.09403691268619874\n",
      "Gradient Descent(366/999): loss=0.09402489787235781\n",
      "Gradient Descent(367/999): loss=0.09401292660360114\n",
      "Gradient Descent(368/999): loss=0.09400099863997027\n",
      "Gradient Descent(369/999): loss=0.09398911374301148\n",
      "Gradient Descent(370/999): loss=0.09397727167576624\n",
      "Gradient Descent(371/999): loss=0.09396547220276143\n",
      "Gradient Descent(372/999): loss=0.09395371508999979\n",
      "Gradient Descent(373/999): loss=0.09394200010495043\n",
      "Gradient Descent(374/999): loss=0.09393032701653933\n",
      "Gradient Descent(375/999): loss=0.0939186955951398\n",
      "Gradient Descent(376/999): loss=0.09390710561256341\n",
      "Gradient Descent(377/999): loss=0.09389555684205045\n",
      "Gradient Descent(378/999): loss=0.09388404905826092\n",
      "Gradient Descent(379/999): loss=0.0938725820372651\n",
      "Gradient Descent(380/999): loss=0.09386115555653472\n",
      "Gradient Descent(381/999): loss=0.09384976939493371\n",
      "Gradient Descent(382/999): loss=0.09383842333270931\n",
      "Gradient Descent(383/999): loss=0.09382711715148313\n",
      "Gradient Descent(384/999): loss=0.09381585063424235\n",
      "Gradient Descent(385/999): loss=0.09380462356533065\n",
      "Gradient Descent(386/999): loss=0.09379343573043976\n",
      "Gradient Descent(387/999): loss=0.09378228691660066\n",
      "Gradient Descent(388/999): loss=0.09377117691217479\n",
      "Gradient Descent(389/999): loss=0.09376010550684569\n",
      "Gradient Descent(390/999): loss=0.0937490724916103\n",
      "Gradient Descent(391/999): loss=0.0937380776587706\n",
      "Gradient Descent(392/999): loss=0.09372712080192505\n",
      "Gradient Descent(393/999): loss=0.09371620171596044\n",
      "Gradient Descent(394/999): loss=0.09370532019704335\n",
      "Gradient Descent(395/999): loss=0.09369447604261202\n",
      "Gradient Descent(396/999): loss=0.09368366905136817\n",
      "Gradient Descent(397/999): loss=0.09367289902326878\n",
      "Gradient Descent(398/999): loss=0.09366216575951797\n",
      "Gradient Descent(399/999): loss=0.09365146906255901\n",
      "Gradient Descent(400/999): loss=0.0936408087360664\n",
      "Gradient Descent(401/999): loss=0.09363018458493778\n",
      "Gradient Descent(402/999): loss=0.09361959641528612\n",
      "Gradient Descent(403/999): loss=0.09360904403443192\n",
      "Gradient Descent(404/999): loss=0.09359852725089528\n",
      "Gradient Descent(405/999): loss=0.0935880458743884\n",
      "Gradient Descent(406/999): loss=0.09357759971580765\n",
      "Gradient Descent(407/999): loss=0.09356718858722607\n",
      "Gradient Descent(408/999): loss=0.09355681230188581\n",
      "Gradient Descent(409/999): loss=0.09354647067419043\n",
      "Gradient Descent(410/999): loss=0.0935361635196977\n",
      "Gradient Descent(411/999): loss=0.09352589065511181\n",
      "Gradient Descent(412/999): loss=0.09351565189827622\n",
      "Gradient Descent(413/999): loss=0.0935054470681663\n",
      "Gradient Descent(414/999): loss=0.09349527598488194\n",
      "Gradient Descent(415/999): loss=0.09348513846964032\n",
      "Gradient Descent(416/999): loss=0.09347503434476886\n",
      "Gradient Descent(417/999): loss=0.0934649634336978\n",
      "Gradient Descent(418/999): loss=0.09345492556095333\n",
      "Gradient Descent(419/999): loss=0.09344492055215044\n",
      "Gradient Descent(420/999): loss=0.09343494823398585\n",
      "Gradient Descent(421/999): loss=0.09342500843423118\n",
      "Gradient Descent(422/999): loss=0.09341510098172585\n",
      "Gradient Descent(423/999): loss=0.09340522570637039\n",
      "Gradient Descent(424/999): loss=0.09339538243911942\n",
      "Gradient Descent(425/999): loss=0.09338557101197505\n",
      "Gradient Descent(426/999): loss=0.09337579125797987\n",
      "Gradient Descent(427/999): loss=0.09336604301121064\n",
      "Gradient Descent(428/999): loss=0.0933563261067712\n",
      "Gradient Descent(429/999): loss=0.09334664038078613\n",
      "Gradient Descent(430/999): loss=0.09333698567039418\n",
      "Gradient Descent(431/999): loss=0.09332736181374156\n",
      "Gradient Descent(432/999): loss=0.0933177686499756\n",
      "Gradient Descent(433/999): loss=0.09330820601923827\n",
      "Gradient Descent(434/999): loss=0.09329867376265981\n",
      "Gradient Descent(435/999): loss=0.09328917172235225\n",
      "Gradient Descent(436/999): loss=0.0932796997414032\n",
      "Gradient Descent(437/999): loss=0.0932702576638696\n",
      "Gradient Descent(438/999): loss=0.09326084533477132\n",
      "Gradient Descent(439/999): loss=0.09325146260008518\n",
      "Gradient Descent(440/999): loss=0.09324210930673855\n",
      "Gradient Descent(441/999): loss=0.09323278530260348\n",
      "Gradient Descent(442/999): loss=0.09322349043649045\n",
      "Gradient Descent(443/999): loss=0.09321422455814234\n",
      "Gradient Descent(444/999): loss=0.0932049875182286\n",
      "Gradient Descent(445/999): loss=0.09319577916833899\n",
      "Gradient Descent(446/999): loss=0.09318659936097798\n",
      "Gradient Descent(447/999): loss=0.09317744794955864\n",
      "Gradient Descent(448/999): loss=0.09316832478839697\n",
      "Gradient Descent(449/999): loss=0.09315922973270584\n",
      "Gradient Descent(450/999): loss=0.09315016263858952\n",
      "Gradient Descent(451/999): loss=0.09314112336303777\n",
      "Gradient Descent(452/999): loss=0.09313211176392013\n",
      "Gradient Descent(453/999): loss=0.09312312769998038\n",
      "Gradient Descent(454/999): loss=0.0931141710308308\n",
      "Gradient Descent(455/999): loss=0.09310524161694674\n",
      "Gradient Descent(456/999): loss=0.09309633931966084\n",
      "Gradient Descent(457/999): loss=0.09308746400115772\n",
      "Gradient Descent(458/999): loss=0.09307861552446846\n",
      "Gradient Descent(459/999): loss=0.09306979375346507\n",
      "Gradient Descent(460/999): loss=0.0930609985528552\n",
      "Gradient Descent(461/999): loss=0.09305222978817668\n",
      "Gradient Descent(462/999): loss=0.09304348732579223\n",
      "Gradient Descent(463/999): loss=0.0930347710328842\n",
      "Gradient Descent(464/999): loss=0.09302608077744919\n",
      "Gradient Descent(465/999): loss=0.09301741642829289\n",
      "Gradient Descent(466/999): loss=0.09300877785502491\n",
      "Gradient Descent(467/999): loss=0.09300016492805356\n",
      "Gradient Descent(468/999): loss=0.09299157751858074\n",
      "Gradient Descent(469/999): loss=0.09298301549859687\n",
      "Gradient Descent(470/999): loss=0.09297447874087574\n",
      "Gradient Descent(471/999): loss=0.09296596711896958\n",
      "Gradient Descent(472/999): loss=0.09295748050720395\n",
      "Gradient Descent(473/999): loss=0.09294901878067298\n",
      "Gradient Descent(474/999): loss=0.09294058181523414\n",
      "Gradient Descent(475/999): loss=0.09293216948750357\n",
      "Gradient Descent(476/999): loss=0.09292378167485116\n",
      "Gradient Descent(477/999): loss=0.09291541825539561\n",
      "Gradient Descent(478/999): loss=0.09290707910799974\n",
      "Gradient Descent(479/999): loss=0.09289876411226565\n",
      "Gradient Descent(480/999): loss=0.09289047314853004\n",
      "Gradient Descent(481/999): loss=0.09288220609785935\n",
      "Gradient Descent(482/999): loss=0.09287396284204529\n",
      "Gradient Descent(483/999): loss=0.09286574326360005\n",
      "Gradient Descent(484/999): loss=0.0928575472457516\n",
      "Gradient Descent(485/999): loss=0.09284937467243932\n",
      "Gradient Descent(486/999): loss=0.09284122542830923\n",
      "Gradient Descent(487/999): loss=0.0928330993987096\n",
      "Gradient Descent(488/999): loss=0.0928249964696863\n",
      "Gradient Descent(489/999): loss=0.09281691652797852\n",
      "Gradient Descent(490/999): loss=0.09280885946101412\n",
      "Gradient Descent(491/999): loss=0.09280082515690534\n",
      "Gradient Descent(492/999): loss=0.09279281350444435\n",
      "Gradient Descent(493/999): loss=0.09278482439309899\n",
      "Gradient Descent(494/999): loss=0.09277685771300827\n",
      "Gradient Descent(495/999): loss=0.09276891335497821\n",
      "Gradient Descent(496/999): loss=0.09276099121047753\n",
      "Gradient Descent(497/999): loss=0.09275309117163331\n",
      "Gradient Descent(498/999): loss=0.09274521313122695\n",
      "Gradient Descent(499/999): loss=0.09273735698268984\n",
      "Gradient Descent(500/999): loss=0.09272952262009912\n",
      "Gradient Descent(501/999): loss=0.0927217099381738\n",
      "Gradient Descent(502/999): loss=0.09271391883227036\n",
      "Gradient Descent(503/999): loss=0.09270614919837894\n",
      "Gradient Descent(504/999): loss=0.092698400933119\n",
      "Gradient Descent(505/999): loss=0.09269067393373556\n",
      "Gradient Descent(506/999): loss=0.09268296809809494\n",
      "Gradient Descent(507/999): loss=0.09267528332468097\n",
      "Gradient Descent(508/999): loss=0.09266761951259093\n",
      "Gradient Descent(509/999): loss=0.0926599765615317\n",
      "Gradient Descent(510/999): loss=0.09265235437181578\n",
      "Gradient Descent(511/999): loss=0.09264475284435744\n",
      "Gradient Descent(512/999): loss=0.0926371718806689\n",
      "Gradient Descent(513/999): loss=0.09262961138285644\n",
      "Gradient Descent(514/999): loss=0.09262207125361664\n",
      "Gradient Descent(515/999): loss=0.09261455139623265\n",
      "Gradient Descent(516/999): loss=0.09260705171457027\n",
      "Gradient Descent(517/999): loss=0.0925995721130744\n",
      "Gradient Descent(518/999): loss=0.09259211249676524\n",
      "Gradient Descent(519/999): loss=0.09258467277123465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(520/999): loss=0.09257725284264244\n",
      "Gradient Descent(521/999): loss=0.09256985261771278\n",
      "Gradient Descent(522/999): loss=0.0925624720037306\n",
      "Gradient Descent(523/999): loss=0.09255511090853795\n",
      "Gradient Descent(524/999): loss=0.0925477692405305\n",
      "Gradient Descent(525/999): loss=0.09254044690865389\n",
      "Gradient Descent(526/999): loss=0.09253314382240041\n",
      "Gradient Descent(527/999): loss=0.0925258598918053\n",
      "Gradient Descent(528/999): loss=0.09251859502744339\n",
      "Gradient Descent(529/999): loss=0.09251134914042562\n",
      "Gradient Descent(530/999): loss=0.09250412214239559\n",
      "Gradient Descent(531/999): loss=0.09249691394552617\n",
      "Gradient Descent(532/999): loss=0.09248972446251623\n",
      "Gradient Descent(533/999): loss=0.09248255360658703\n",
      "Gradient Descent(534/999): loss=0.09247540129147912\n",
      "Gradient Descent(535/999): loss=0.09246826743144894\n",
      "Gradient Descent(536/999): loss=0.09246115194126543\n",
      "Gradient Descent(537/999): loss=0.09245405473620692\n",
      "Gradient Descent(538/999): loss=0.09244697573205769\n",
      "Gradient Descent(539/999): loss=0.09243991484510497\n",
      "Gradient Descent(540/999): loss=0.09243287199213554\n",
      "Gradient Descent(541/999): loss=0.09242584709043258\n",
      "Gradient Descent(542/999): loss=0.09241884005777255\n",
      "Gradient Descent(543/999): loss=0.09241185081242195\n",
      "Gradient Descent(544/999): loss=0.09240487927313437\n",
      "Gradient Descent(545/999): loss=0.0923979253591471\n",
      "Gradient Descent(546/999): loss=0.09239098899017839\n",
      "Gradient Descent(547/999): loss=0.092384070086424\n",
      "Gradient Descent(548/999): loss=0.0923771685685545\n",
      "Gradient Descent(549/999): loss=0.092370284357712\n",
      "Gradient Descent(550/999): loss=0.0923634173755073\n",
      "Gradient Descent(551/999): loss=0.0923565675440167\n",
      "Gradient Descent(552/999): loss=0.09234973478577929\n",
      "Gradient Descent(553/999): loss=0.09234291902379377\n",
      "Gradient Descent(554/999): loss=0.09233612018151571\n",
      "Gradient Descent(555/999): loss=0.09232933818285445\n",
      "Gradient Descent(556/999): loss=0.09232257295217033\n",
      "Gradient Descent(557/999): loss=0.09231582441427183\n",
      "Gradient Descent(558/999): loss=0.09230909249441255\n",
      "Gradient Descent(559/999): loss=0.09230237711828858\n",
      "Gradient Descent(560/999): loss=0.09229567821203558\n",
      "Gradient Descent(561/999): loss=0.0922889957022259\n",
      "Gradient Descent(562/999): loss=0.09228232951586596\n",
      "Gradient Descent(563/999): loss=0.09227567958039339\n",
      "Gradient Descent(564/999): loss=0.09226904582367433\n",
      "Gradient Descent(565/999): loss=0.09226242817400057\n",
      "Gradient Descent(566/999): loss=0.092255826560087\n",
      "Gradient Descent(567/999): loss=0.09224924091106888\n",
      "Gradient Descent(568/999): loss=0.09224267115649909\n",
      "Gradient Descent(569/999): loss=0.09223611722634545\n",
      "Gradient Descent(570/999): loss=0.09222957905098826\n",
      "Gradient Descent(571/999): loss=0.09222305656121749\n",
      "Gradient Descent(572/999): loss=0.09221654968823026\n",
      "Gradient Descent(573/999): loss=0.09221005836362822\n",
      "Gradient Descent(574/999): loss=0.092203582519415\n",
      "Gradient Descent(575/999): loss=0.0921971220879936\n",
      "Gradient Descent(576/999): loss=0.09219067700216385\n",
      "Gradient Descent(577/999): loss=0.09218424719512\n",
      "Gradient Descent(578/999): loss=0.09217783260044808\n",
      "Gradient Descent(579/999): loss=0.09217143315212342\n",
      "Gradient Descent(580/999): loss=0.09216504878450833\n",
      "Gradient Descent(581/999): loss=0.09215867943234934\n",
      "Gradient Descent(582/999): loss=0.09215232503077515\n",
      "Gradient Descent(583/999): loss=0.09214598551529382\n",
      "Gradient Descent(584/999): loss=0.09213966082179051\n",
      "Gradient Descent(585/999): loss=0.09213335088652519\n",
      "Gradient Descent(586/999): loss=0.0921270556461302\n",
      "Gradient Descent(587/999): loss=0.0921207750376077\n",
      "Gradient Descent(588/999): loss=0.09211450899832757\n",
      "Gradient Descent(589/999): loss=0.09210825746602494\n",
      "Gradient Descent(590/999): loss=0.09210202037879796\n",
      "Gradient Descent(591/999): loss=0.09209579767510534\n",
      "Gradient Descent(592/999): loss=0.09208958929376423\n",
      "Gradient Descent(593/999): loss=0.09208339517394779\n",
      "Gradient Descent(594/999): loss=0.09207721525518307\n",
      "Gradient Descent(595/999): loss=0.09207104947734876\n",
      "Gradient Descent(596/999): loss=0.09206489778067271\n",
      "Gradient Descent(597/999): loss=0.09205876010573011\n",
      "Gradient Descent(598/999): loss=0.09205263639344095\n",
      "Gradient Descent(599/999): loss=0.09204652658506803\n",
      "Gradient Descent(600/999): loss=0.09204043062221469\n",
      "Gradient Descent(601/999): loss=0.09203434844682262\n",
      "Gradient Descent(602/999): loss=0.09202828000116986\n",
      "Gradient Descent(603/999): loss=0.09202222522786853\n",
      "Gradient Descent(604/999): loss=0.09201618406986274\n",
      "Gradient Descent(605/999): loss=0.0920101564704266\n",
      "Gradient Descent(606/999): loss=0.09200414237316187\n",
      "Gradient Descent(607/999): loss=0.09199814172199616\n",
      "Gradient Descent(608/999): loss=0.09199215446118071\n",
      "Gradient Descent(609/999): loss=0.09198618053528841\n",
      "Gradient Descent(610/999): loss=0.09198021988921166\n",
      "Gradient Descent(611/999): loss=0.0919742724681605\n",
      "Gradient Descent(612/999): loss=0.09196833821766043\n",
      "Gradient Descent(613/999): loss=0.09196241708355053\n",
      "Gradient Descent(614/999): loss=0.0919565090119815\n",
      "Gradient Descent(615/999): loss=0.09195061394941348\n",
      "Gradient Descent(616/999): loss=0.09194473184261436\n",
      "Gradient Descent(617/999): loss=0.0919388626386577\n",
      "Gradient Descent(618/999): loss=0.0919330062849207\n",
      "Gradient Descent(619/999): loss=0.0919271627290825\n",
      "Gradient Descent(620/999): loss=0.09192133191912204\n",
      "Gradient Descent(621/999): loss=0.09191551380331642\n",
      "Gradient Descent(622/999): loss=0.09190970833023875\n",
      "Gradient Descent(623/999): loss=0.09190391544875638\n",
      "Gradient Descent(624/999): loss=0.09189813510802915\n",
      "Gradient Descent(625/999): loss=0.09189236725750739\n",
      "Gradient Descent(626/999): loss=0.0918866118469301\n",
      "Gradient Descent(627/999): loss=0.09188086882632325\n",
      "Gradient Descent(628/999): loss=0.09187513814599782\n",
      "Gradient Descent(629/999): loss=0.0918694197565481\n",
      "Gradient Descent(630/999): loss=0.09186371360884979\n",
      "Gradient Descent(631/999): loss=0.09185801965405835\n",
      "Gradient Descent(632/999): loss=0.09185233784360713\n",
      "Gradient Descent(633/999): loss=0.09184666812920574\n",
      "Gradient Descent(634/999): loss=0.09184101046283812\n",
      "Gradient Descent(635/999): loss=0.09183536479676095\n",
      "Gradient Descent(636/999): loss=0.09182973108350194\n",
      "Gradient Descent(637/999): loss=0.09182410927585798\n",
      "Gradient Descent(638/999): loss=0.09181849932689361\n",
      "Gradient Descent(639/999): loss=0.09181290118993918\n",
      "Gradient Descent(640/999): loss=0.09180731481858934\n",
      "Gradient Descent(641/999): loss=0.09180174016670115\n",
      "Gradient Descent(642/999): loss=0.09179617718839266\n",
      "Gradient Descent(643/999): loss=0.09179062583804107\n",
      "Gradient Descent(644/999): loss=0.09178508607028127\n",
      "Gradient Descent(645/999): loss=0.09177955784000406\n",
      "Gradient Descent(646/999): loss=0.09177404110235467\n",
      "Gradient Descent(647/999): loss=0.09176853581273096\n",
      "Gradient Descent(648/999): loss=0.09176304192678215\n",
      "Gradient Descent(649/999): loss=0.09175755940040686\n",
      "Gradient Descent(650/999): loss=0.09175208818975186\n",
      "Gradient Descent(651/999): loss=0.09174662825121029\n",
      "Gradient Descent(652/999): loss=0.09174117954142025\n",
      "Gradient Descent(653/999): loss=0.0917357420172631\n",
      "Gradient Descent(654/999): loss=0.09173031563586216\n",
      "Gradient Descent(655/999): loss=0.09172490035458096\n",
      "Gradient Descent(656/999): loss=0.09171949613102184\n",
      "Gradient Descent(657/999): loss=0.09171410292302448\n",
      "Gradient Descent(658/999): loss=0.0917087206886643\n",
      "Gradient Descent(659/999): loss=0.09170334938625099\n",
      "Gradient Descent(660/999): loss=0.09169798897432722\n",
      "Gradient Descent(661/999): loss=0.09169263941166689\n",
      "Gradient Descent(662/999): loss=0.09168730065727396\n",
      "Gradient Descent(663/999): loss=0.09168197267038074\n",
      "Gradient Descent(664/999): loss=0.09167665541044662\n",
      "Gradient Descent(665/999): loss=0.09167134883715668\n",
      "Gradient Descent(666/999): loss=0.09166605291042011\n",
      "Gradient Descent(667/999): loss=0.0916607675903689\n",
      "Gradient Descent(668/999): loss=0.09165549283735644\n",
      "Gradient Descent(669/999): loss=0.09165022861195621\n",
      "Gradient Descent(670/999): loss=0.09164497487496011\n",
      "Gradient Descent(671/999): loss=0.09163973158737748\n",
      "Gradient Descent(672/999): loss=0.0916344987104334\n",
      "Gradient Descent(673/999): loss=0.09162927620556761\n",
      "Gradient Descent(674/999): loss=0.09162406403443296\n",
      "Gradient Descent(675/999): loss=0.09161886215889409\n",
      "Gradient Descent(676/999): loss=0.09161367054102632\n",
      "Gradient Descent(677/999): loss=0.09160848914311398\n",
      "Gradient Descent(678/999): loss=0.09160331792764939\n",
      "Gradient Descent(679/999): loss=0.0915981568573315\n",
      "Gradient Descent(680/999): loss=0.09159300589506444\n",
      "Gradient Descent(681/999): loss=0.09158786500395641\n",
      "Gradient Descent(682/999): loss=0.0915827341473183\n",
      "Gradient Descent(683/999): loss=0.09157761328866251\n",
      "Gradient Descent(684/999): loss=0.09157250239170159\n",
      "Gradient Descent(685/999): loss=0.09156740142034699\n",
      "Gradient Descent(686/999): loss=0.091562310338708\n",
      "Gradient Descent(687/999): loss=0.0915572291110902\n",
      "Gradient Descent(688/999): loss=0.09155215770199453\n",
      "Gradient Descent(689/999): loss=0.09154709607611586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(690/999): loss=0.09154204419834189\n",
      "Gradient Descent(691/999): loss=0.0915370020337519\n",
      "Gradient Descent(692/999): loss=0.09153196954761558\n",
      "Gradient Descent(693/999): loss=0.09152694670539176\n",
      "Gradient Descent(694/999): loss=0.0915219334727274\n",
      "Gradient Descent(695/999): loss=0.09151692981545625\n",
      "Gradient Descent(696/999): loss=0.09151193569959767\n",
      "Gradient Descent(697/999): loss=0.09150695109135565\n",
      "Gradient Descent(698/999): loss=0.0915019759571175\n",
      "Gradient Descent(699/999): loss=0.09149701026345276\n",
      "Gradient Descent(700/999): loss=0.09149205397711205\n",
      "Gradient Descent(701/999): loss=0.09148710706502587\n",
      "Gradient Descent(702/999): loss=0.09148216949430373\n",
      "Gradient Descent(703/999): loss=0.0914772412322327\n",
      "Gradient Descent(704/999): loss=0.09147232224627656\n",
      "Gradient Descent(705/999): loss=0.09146741250407445\n",
      "Gradient Descent(706/999): loss=0.09146251197344016\n",
      "Gradient Descent(707/999): loss=0.09145762062236061\n",
      "Gradient Descent(708/999): loss=0.09145273841899496\n",
      "Gradient Descent(709/999): loss=0.09144786533167375\n",
      "Gradient Descent(710/999): loss=0.0914430013288974\n",
      "Gradient Descent(711/999): loss=0.0914381463793355\n",
      "Gradient Descent(712/999): loss=0.09143330045182561\n",
      "Gradient Descent(713/999): loss=0.09142846351537227\n",
      "Gradient Descent(714/999): loss=0.09142363553914583\n",
      "Gradient Descent(715/999): loss=0.0914188164924817\n",
      "Gradient Descent(716/999): loss=0.09141400634487891\n",
      "Gradient Descent(717/999): loss=0.09140920506599955\n",
      "Gradient Descent(718/999): loss=0.09140441262566742\n",
      "Gradient Descent(719/999): loss=0.09139962899386714\n",
      "Gradient Descent(720/999): loss=0.09139485414074319\n",
      "Gradient Descent(721/999): loss=0.09139008803659884\n",
      "Gradient Descent(722/999): loss=0.09138533065189523\n",
      "Gradient Descent(723/999): loss=0.09138058195725028\n",
      "Gradient Descent(724/999): loss=0.09137584192343792\n",
      "Gradient Descent(725/999): loss=0.09137111052138691\n",
      "Gradient Descent(726/999): loss=0.09136638772218\n",
      "Gradient Descent(727/999): loss=0.09136167349705293\n",
      "Gradient Descent(728/999): loss=0.09135696781739353\n",
      "Gradient Descent(729/999): loss=0.09135227065474062\n",
      "Gradient Descent(730/999): loss=0.09134758198078338\n",
      "Gradient Descent(731/999): loss=0.09134290176736007\n",
      "Gradient Descent(732/999): loss=0.09133822998645738\n",
      "Gradient Descent(733/999): loss=0.09133356661020928\n",
      "Gradient Descent(734/999): loss=0.09132891161089635\n",
      "Gradient Descent(735/999): loss=0.09132426496094469\n",
      "Gradient Descent(736/999): loss=0.09131962663292506\n",
      "Gradient Descent(737/999): loss=0.09131499659955201\n",
      "Gradient Descent(738/999): loss=0.09131037483368311\n",
      "Gradient Descent(739/999): loss=0.09130576130831769\n",
      "Gradient Descent(740/999): loss=0.09130115599659641\n",
      "Gradient Descent(741/999): loss=0.09129655887180017\n",
      "Gradient Descent(742/999): loss=0.09129196990734913\n",
      "Gradient Descent(743/999): loss=0.09128738907680213\n",
      "Gradient Descent(744/999): loss=0.09128281635385566\n",
      "Gradient Descent(745/999): loss=0.09127825171234288\n",
      "Gradient Descent(746/999): loss=0.09127369512623318\n",
      "Gradient Descent(747/999): loss=0.09126914656963091\n",
      "Gradient Descent(748/999): loss=0.0912646060167747\n",
      "Gradient Descent(749/999): loss=0.09126007344203688\n",
      "Gradient Descent(750/999): loss=0.0912555488199222\n",
      "Gradient Descent(751/999): loss=0.09125103212506731\n",
      "Gradient Descent(752/999): loss=0.09124652333224\n",
      "Gradient Descent(753/999): loss=0.0912420224163381\n",
      "Gradient Descent(754/999): loss=0.09123752935238894\n",
      "Gradient Descent(755/999): loss=0.09123304411554854\n",
      "Gradient Descent(756/999): loss=0.09122856668110059\n",
      "Gradient Descent(757/999): loss=0.09122409702445593\n",
      "Gradient Descent(758/999): loss=0.09121963512115165\n",
      "Gradient Descent(759/999): loss=0.09121518094685026\n",
      "Gradient Descent(760/999): loss=0.09121073447733902\n",
      "Gradient Descent(761/999): loss=0.09120629568852912\n",
      "Gradient Descent(762/999): loss=0.09120186455645493\n",
      "Gradient Descent(763/999): loss=0.09119744105727326\n",
      "Gradient Descent(764/999): loss=0.09119302516726253\n",
      "Gradient Descent(765/999): loss=0.09118861686282216\n",
      "Gradient Descent(766/999): loss=0.09118421612047169\n",
      "Gradient Descent(767/999): loss=0.09117982291685015\n",
      "Gradient Descent(768/999): loss=0.09117543722871521\n",
      "Gradient Descent(769/999): loss=0.09117105903294258\n",
      "Gradient Descent(770/999): loss=0.09116668830652523\n",
      "Gradient Descent(771/999): loss=0.09116232502657262\n",
      "Gradient Descent(772/999): loss=0.09115796917031006\n",
      "Gradient Descent(773/999): loss=0.091153620715078\n",
      "Gradient Descent(774/999): loss=0.09114927963833125\n",
      "Gradient Descent(775/999): loss=0.09114494591763835\n",
      "Gradient Descent(776/999): loss=0.09114061953068084\n",
      "Gradient Descent(777/999): loss=0.09113630045525264\n",
      "Gradient Descent(778/999): loss=0.09113198866925917\n",
      "Gradient Descent(779/999): loss=0.09112768415071695\n",
      "Gradient Descent(780/999): loss=0.09112338687775266\n",
      "Gradient Descent(781/999): loss=0.09111909682860267\n",
      "Gradient Descent(782/999): loss=0.09111481398161218\n",
      "Gradient Descent(783/999): loss=0.0911105383152347\n",
      "Gradient Descent(784/999): loss=0.09110626980803135\n",
      "Gradient Descent(785/999): loss=0.09110200843867017\n",
      "Gradient Descent(786/999): loss=0.09109775418592553\n",
      "Gradient Descent(787/999): loss=0.0910935070286774\n",
      "Gradient Descent(788/999): loss=0.09108926694591074\n",
      "Gradient Descent(789/999): loss=0.09108503391671492\n",
      "Gradient Descent(790/999): loss=0.09108080792028295\n",
      "Gradient Descent(791/999): loss=0.09107658893591102\n",
      "Gradient Descent(792/999): loss=0.09107237694299775\n",
      "Gradient Descent(793/999): loss=0.09106817192104355\n",
      "Gradient Descent(794/999): loss=0.09106397384965008\n",
      "Gradient Descent(795/999): loss=0.09105978270851964\n",
      "Gradient Descent(796/999): loss=0.09105559847745447\n",
      "Gradient Descent(797/999): loss=0.09105142113635616\n",
      "Gradient Descent(798/999): loss=0.09104725066522515\n",
      "Gradient Descent(799/999): loss=0.09104308704416003\n",
      "Gradient Descent(800/999): loss=0.09103893025335696\n",
      "Gradient Descent(801/999): loss=0.09103478027310907\n",
      "Gradient Descent(802/999): loss=0.09103063708380593\n",
      "Gradient Descent(803/999): loss=0.09102650066593292\n",
      "Gradient Descent(804/999): loss=0.09102237100007055\n",
      "Gradient Descent(805/999): loss=0.09101824806689411\n",
      "Gradient Descent(806/999): loss=0.09101413184717298\n",
      "Gradient Descent(807/999): loss=0.09101002232176993\n",
      "Gradient Descent(808/999): loss=0.09100591947164072\n",
      "Gradient Descent(809/999): loss=0.09100182327783354\n",
      "Gradient Descent(810/999): loss=0.09099773372148838\n",
      "Gradient Descent(811/999): loss=0.09099365078383644\n",
      "Gradient Descent(812/999): loss=0.09098957444619968\n",
      "Gradient Descent(813/999): loss=0.09098550468999023\n",
      "Gradient Descent(814/999): loss=0.09098144149670981\n",
      "Gradient Descent(815/999): loss=0.09097738484794919\n",
      "Gradient Descent(816/999): loss=0.09097333472538778\n",
      "Gradient Descent(817/999): loss=0.09096929111079292\n",
      "Gradient Descent(818/999): loss=0.09096525398601946\n",
      "Gradient Descent(819/999): loss=0.09096122333300907\n",
      "Gradient Descent(820/999): loss=0.09095719913379001\n",
      "Gradient Descent(821/999): loss=0.09095318137047637\n",
      "Gradient Descent(822/999): loss=0.09094917002526755\n",
      "Gradient Descent(823/999): loss=0.09094516508044795\n",
      "Gradient Descent(824/999): loss=0.0909411665183862\n",
      "Gradient Descent(825/999): loss=0.09093717432153486\n",
      "Gradient Descent(826/999): loss=0.0909331884724298\n",
      "Gradient Descent(827/999): loss=0.09092920895368974\n",
      "Gradient Descent(828/999): loss=0.09092523574801573\n",
      "Gradient Descent(829/999): loss=0.09092126883819064\n",
      "Gradient Descent(830/999): loss=0.09091730820707876\n",
      "Gradient Descent(831/999): loss=0.09091335383762521\n",
      "Gradient Descent(832/999): loss=0.09090940571285547\n",
      "Gradient Descent(833/999): loss=0.09090546381587497\n",
      "Gradient Descent(834/999): loss=0.09090152812986846\n",
      "Gradient Descent(835/999): loss=0.09089759863809978\n",
      "Gradient Descent(836/999): loss=0.09089367532391109\n",
      "Gradient Descent(837/999): loss=0.09088975817072266\n",
      "Gradient Descent(838/999): loss=0.09088584716203225\n",
      "Gradient Descent(839/999): loss=0.09088194228141465\n",
      "Gradient Descent(840/999): loss=0.09087804351252127\n",
      "Gradient Descent(841/999): loss=0.0908741508390797\n",
      "Gradient Descent(842/999): loss=0.09087026424489321\n",
      "Gradient Descent(843/999): loss=0.0908663837138403\n",
      "Gradient Descent(844/999): loss=0.09086250922987424\n",
      "Gradient Descent(845/999): loss=0.09085864077702266\n",
      "Gradient Descent(846/999): loss=0.09085477833938709\n",
      "Gradient Descent(847/999): loss=0.09085092190114251\n",
      "Gradient Descent(848/999): loss=0.09084707144653695\n",
      "Gradient Descent(849/999): loss=0.09084322695989096\n",
      "Gradient Descent(850/999): loss=0.09083938842559722\n",
      "Gradient Descent(851/999): loss=0.09083555582812028\n",
      "Gradient Descent(852/999): loss=0.09083172915199583\n",
      "Gradient Descent(853/999): loss=0.0908279083818305\n",
      "Gradient Descent(854/999): loss=0.09082409350230129\n",
      "Gradient Descent(855/999): loss=0.09082028449815535\n",
      "Gradient Descent(856/999): loss=0.09081648135420933\n",
      "Gradient Descent(857/999): loss=0.09081268405534913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(858/999): loss=0.09080889258652942\n",
      "Gradient Descent(859/999): loss=0.09080510693277326\n",
      "Gradient Descent(860/999): loss=0.09080132707917166\n",
      "Gradient Descent(861/999): loss=0.09079755301088319\n",
      "Gradient Descent(862/999): loss=0.09079378471313364\n",
      "Gradient Descent(863/999): loss=0.09079002217121553\n",
      "Gradient Descent(864/999): loss=0.09078626537048767\n",
      "Gradient Descent(865/999): loss=0.09078251429637506\n",
      "Gradient Descent(866/999): loss=0.09077876893436806\n",
      "Gradient Descent(867/999): loss=0.0907750292700224\n",
      "Gradient Descent(868/999): loss=0.09077129528895854\n",
      "Gradient Descent(869/999): loss=0.09076756697686135\n",
      "Gradient Descent(870/999): loss=0.09076384431947981\n",
      "Gradient Descent(871/999): loss=0.09076012730262659\n",
      "Gradient Descent(872/999): loss=0.09075641591217755\n",
      "Gradient Descent(873/999): loss=0.09075271013407159\n",
      "Gradient Descent(874/999): loss=0.09074900995431007\n",
      "Gradient Descent(875/999): loss=0.09074531535895662\n",
      "Gradient Descent(876/999): loss=0.09074162633413664\n",
      "Gradient Descent(877/999): loss=0.09073794286603691\n",
      "Gradient Descent(878/999): loss=0.09073426494090547\n",
      "Gradient Descent(879/999): loss=0.0907305925450509\n",
      "Gradient Descent(880/999): loss=0.09072692566484233\n",
      "Gradient Descent(881/999): loss=0.09072326428670877\n",
      "Gradient Descent(882/999): loss=0.09071960839713897\n",
      "Gradient Descent(883/999): loss=0.09071595798268098\n",
      "Gradient Descent(884/999): loss=0.09071231302994179\n",
      "Gradient Descent(885/999): loss=0.09070867352558709\n",
      "Gradient Descent(886/999): loss=0.09070503945634073\n",
      "Gradient Descent(887/999): loss=0.09070141080898457\n",
      "Gradient Descent(888/999): loss=0.09069778757035808\n",
      "Gradient Descent(889/999): loss=0.09069416972735793\n",
      "Gradient Descent(890/999): loss=0.09069055726693775\n",
      "Gradient Descent(891/999): loss=0.09068695017610778\n",
      "Gradient Descent(892/999): loss=0.09068334844193447\n",
      "Gradient Descent(893/999): loss=0.09067975205154027\n",
      "Gradient Descent(894/999): loss=0.09067616099210314\n",
      "Gradient Descent(895/999): loss=0.09067257525085637\n",
      "Gradient Descent(896/999): loss=0.09066899481508824\n",
      "Gradient Descent(897/999): loss=0.09066541967214159\n",
      "Gradient Descent(898/999): loss=0.09066184980941369\n",
      "Gradient Descent(899/999): loss=0.09065828521435569\n",
      "Gradient Descent(900/999): loss=0.09065472587447247\n",
      "Gradient Descent(901/999): loss=0.09065117177732232\n",
      "Gradient Descent(902/999): loss=0.09064762291051659\n",
      "Gradient Descent(903/999): loss=0.09064407926171927\n",
      "Gradient Descent(904/999): loss=0.09064054081864703\n",
      "Gradient Descent(905/999): loss=0.09063700756906844\n",
      "Gradient Descent(906/999): loss=0.09063347950080405\n",
      "Gradient Descent(907/999): loss=0.09062995660172593\n",
      "Gradient Descent(908/999): loss=0.09062643885975734\n",
      "Gradient Descent(909/999): loss=0.09062292626287259\n",
      "Gradient Descent(910/999): loss=0.09061941879909652\n",
      "Gradient Descent(911/999): loss=0.09061591645650441\n",
      "Gradient Descent(912/999): loss=0.09061241922322155\n",
      "Gradient Descent(913/999): loss=0.09060892708742307\n",
      "Gradient Descent(914/999): loss=0.09060544003733355\n",
      "Gradient Descent(915/999): loss=0.09060195806122674\n",
      "Gradient Descent(916/999): loss=0.09059848114742536\n",
      "Gradient Descent(917/999): loss=0.09059500928430075\n",
      "Gradient Descent(918/999): loss=0.09059154246027257\n",
      "Gradient Descent(919/999): loss=0.09058808066380868\n",
      "Gradient Descent(920/999): loss=0.09058462388342454\n",
      "Gradient Descent(921/999): loss=0.09058117210768331\n",
      "Gradient Descent(922/999): loss=0.09057772532519537\n",
      "Gradient Descent(923/999): loss=0.09057428352461792\n",
      "Gradient Descent(924/999): loss=0.09057084669465512\n",
      "Gradient Descent(925/999): loss=0.09056741482405743\n",
      "Gradient Descent(926/999): loss=0.09056398790162141\n",
      "Gradient Descent(927/999): loss=0.09056056591618972\n",
      "Gradient Descent(928/999): loss=0.09055714885665056\n",
      "Gradient Descent(929/999): loss=0.09055373671193753\n",
      "Gradient Descent(930/999): loss=0.09055032947102928\n",
      "Gradient Descent(931/999): loss=0.09054692712294947\n",
      "Gradient Descent(932/999): loss=0.09054352965676625\n",
      "Gradient Descent(933/999): loss=0.0905401370615922\n",
      "Gradient Descent(934/999): loss=0.090536749326584\n",
      "Gradient Descent(935/999): loss=0.09053336644094209\n",
      "Gradient Descent(936/999): loss=0.09052998839391065\n",
      "Gradient Descent(937/999): loss=0.09052661517477707\n",
      "Gradient Descent(938/999): loss=0.09052324677287199\n",
      "Gradient Descent(939/999): loss=0.09051988317756882\n",
      "Gradient Descent(940/999): loss=0.09051652437828356\n",
      "Gradient Descent(941/999): loss=0.09051317036447462\n",
      "Gradient Descent(942/999): loss=0.09050982112564258\n",
      "Gradient Descent(943/999): loss=0.09050647665132985\n",
      "Gradient Descent(944/999): loss=0.09050313693112055\n",
      "Gradient Descent(945/999): loss=0.09049980195464018\n",
      "Gradient Descent(946/999): loss=0.09049647171155542\n",
      "Gradient Descent(947/999): loss=0.09049314619157395\n",
      "Gradient Descent(948/999): loss=0.09048982538444408\n",
      "Gradient Descent(949/999): loss=0.0904865092799547\n",
      "Gradient Descent(950/999): loss=0.0904831978679349\n",
      "Gradient Descent(951/999): loss=0.09047989113825386\n",
      "Gradient Descent(952/999): loss=0.09047658908082053\n",
      "Gradient Descent(953/999): loss=0.09047329168558339\n",
      "Gradient Descent(954/999): loss=0.09046999894253037\n",
      "Gradient Descent(955/999): loss=0.09046671084168852\n",
      "Gradient Descent(956/999): loss=0.0904634273731237\n",
      "Gradient Descent(957/999): loss=0.09046014852694059\n",
      "Gradient Descent(958/999): loss=0.0904568742932823\n",
      "Gradient Descent(959/999): loss=0.09045360466233023\n",
      "Gradient Descent(960/999): loss=0.0904503396243038\n",
      "Gradient Descent(961/999): loss=0.09044707916946021\n",
      "Gradient Descent(962/999): loss=0.09044382328809444\n",
      "Gradient Descent(963/999): loss=0.09044057197053873\n",
      "Gradient Descent(964/999): loss=0.09043732520716256\n",
      "Gradient Descent(965/999): loss=0.09043408298837247\n",
      "Gradient Descent(966/999): loss=0.09043084530461165\n",
      "Gradient Descent(967/999): loss=0.09042761214636004\n",
      "Gradient Descent(968/999): loss=0.09042438350413382\n",
      "Gradient Descent(969/999): loss=0.09042115936848538\n",
      "Gradient Descent(970/999): loss=0.09041793973000319\n",
      "Gradient Descent(971/999): loss=0.09041472457931125\n",
      "Gradient Descent(972/999): loss=0.09041151390706939\n",
      "Gradient Descent(973/999): loss=0.09040830770397264\n",
      "Gradient Descent(974/999): loss=0.09040510596075128\n",
      "Gradient Descent(975/999): loss=0.09040190866817058\n",
      "Gradient Descent(976/999): loss=0.09039871581703056\n",
      "Gradient Descent(977/999): loss=0.09039552739816585\n",
      "Gradient Descent(978/999): loss=0.09039234340244548\n",
      "Gradient Descent(979/999): loss=0.09038916382077267\n",
      "Gradient Descent(980/999): loss=0.09038598864408469\n",
      "Gradient Descent(981/999): loss=0.09038281786335269\n",
      "Gradient Descent(982/999): loss=0.09037965146958134\n",
      "Gradient Descent(983/999): loss=0.0903764894538089\n",
      "Gradient Descent(984/999): loss=0.09037333180710679\n",
      "Gradient Descent(985/999): loss=0.0903701785205796\n",
      "Gradient Descent(986/999): loss=0.09036702958536484\n",
      "Gradient Descent(987/999): loss=0.0903638849926327\n",
      "Gradient Descent(988/999): loss=0.09036074473358592\n",
      "Gradient Descent(989/999): loss=0.09035760879945966\n",
      "Gradient Descent(990/999): loss=0.0903544771815212\n",
      "Gradient Descent(991/999): loss=0.09035134987106991\n",
      "Gradient Descent(992/999): loss=0.0903482268594369\n",
      "Gradient Descent(993/999): loss=0.09034510813798506\n",
      "Gradient Descent(994/999): loss=0.09034199369810868\n",
      "Gradient Descent(995/999): loss=0.09033888353123339\n",
      "Gradient Descent(996/999): loss=0.09033577762881605\n",
      "Gradient Descent(997/999): loss=0.09033267598234433\n",
      "Gradient Descent(998/999): loss=0.09032957858333684\n",
      "Gradient Descent(999/999): loss=0.09032648542334283\n",
      "58.78297758102417\n"
     ]
    }
   ],
   "source": [
    "meanacc_dtest_gd = []\n",
    "meanacc_dtrain_gd = []\n",
    "\n",
    "from time import time\n",
    "\n",
    "start = time()\n",
    "# Execute for data originally with no NaNs\n",
    "dtmp_tr,dtmp_te=cross_validation(Y_no_nans,X_no_nans,k_fold=4,seed=1, function_name='gd', gamma=0.000002, max_iters=1000)\n",
    "meanacc_dtest_gd.append(dtmp_te)\n",
    "meanacc_dtrain_gd.append(dtmp_tr)\n",
    "\n",
    "# Execute for data originally with NaNs\n",
    "dtmp_tr,dtmp_te=cross_validation(Y_cleaned,X_cleaned,k_fold=4,seed=1, function_name='gd', gamma=0.000002, max_iters=1000)\n",
    "meanacc_dtest_gd.append(dtmp_te)\n",
    "meanacc_dtrain_gd.append(dtmp_tr)\n",
    "end = time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No NaNs error:\n",
      "Test: 0.664963589382194 Data: 0.6653648892020985\n",
      "\n",
      "Cleaned NaNs error:\n",
      "Test: 0.7165281168217105 Data: 0.7165702682295676\n"
     ]
    }
   ],
   "source": [
    "print(\"No NaNs accuracy:\")\n",
    "print(\"Test:\",meanacc_dtest_gd[0],\"Data:\", meanacc_dtrain_gd[0])\n",
    "print(\"\\nCleaned NaNs accuracy:\", )\n",
    "print(\"Test:\",meanacc_dtest_gd[1],\"Data:\", meanacc_dtrain_gd[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Comment:</b> Apparently, GD is not converging fast enough. We get higher error than with Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descsent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "meanacc_dtest_sgd = []\n",
    "meanacc_dtrain_sgd = []\n",
    "\n",
    "now = time()\n",
    "# Execute for data originally with no NaNs\n",
    "dtmp_tr,dtmp_te=cross_validation(Y_no_nans,X_no_nans,k_fold=4,seed=1, function_name='sgd', gamma=0.0000001, max_iters=1000)\n",
    "meanacc_dtest_sgd.append(dtmp_te)\n",
    "meanacc_dtrain_sgd.append(dtmp_tr)\n",
    "\n",
    "# Execute for data originally with NaNs\n",
    "dtmp_tr,dtmp_te=cross_validation(Y_cleaned,X_cleaned,k_fold=4,seed=1, function_name='sgd', gamma=0.0000001, max_iters=1000)\n",
    "meanacc_dtest_sgd.append(dtmp_te)\n",
    "meanacc_dtrain_sgd.append(dtmp_tr)\n",
    "final = time()\n",
    "print(final - now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"No NaNs accuracy:\")\n",
    "print(\"Test:\",meanacc_dtest_sgd[0],\"Data:\", meanacc_dtrain_sgd[0])\n",
    "print(\"\\nCleaned NaNs accuracy:\", )\n",
    "print(\"Test:\",meanacc_dtest_sgd[1],\"Data:\", meanacc_dtrain_sgd[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Comment: </b>same result we had with GD. Apparently is not even faster. Might be worth measuring with time()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanacc_dtest_ls = []\n",
    "meanacc_dtrain_ls = []\n",
    "\n",
    "# Execute for data originally with no NaNs\n",
    "dtmp_tr,dtmp_te=cross_validation(Y_no_nans,X_no_nans,k_fold=4,seed=1, function_name='least_squares')\n",
    "meanacc_dtest_ls.append(dtmp_te)\n",
    "meanacc_dtrain_ls.append(dtmp_tr)\n",
    "\n",
    "# Execute for data originally with NaNs\n",
    "dtmp_tr,dtmp_te=cross_validation(Y_cleaned,X_cleaned,k_fold=4,seed=1, function_name='least_squares')\n",
    "meanacc_dtest_ls.append(dtmp_te)\n",
    "meanacc_dtrain_ls.append(dtmp_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"No NaNs accuracy:\")\n",
    "print(\"Test:\",meanacc_dtest_ls[0],\"Data:\", meanacc_dtrain_ls[0])\n",
    "print(\"\\nCleaned NaNs accuracy:\", )\n",
    "print(\"Test:\",meanacc_dtest_ls[1],\"Data:\", meanacc_dtrain_ls[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Comment: </b>same result we got in the other notebook. Fine accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanacc_dtest_rr = []\n",
    "meanacc_dtrain_rr = []\n",
    "\n",
    "for lambda_ in np.logspace(-5,0,15):\n",
    "    # Execute for data originally with no NaNs\n",
    "    dtmp_tr,dtmp_te=cross_validation(Y_no_nans,X_no_nans,k_fold=4,seed=1, function_name='ridge_regression', lambda_=lambda_)\n",
    "    meanacc_dtest_rr.append(dtmp_te)\n",
    "    meanacc_dtrain_rr.append(dtmp_tr)\n",
    "    \n",
    "    # Execute for data originally with NaNs\n",
    "    dtmp_tr,dtmp_te=cross_validation(Y_cleaned,X_cleaned,k_fold=4,seed=1, function_name='ridge_regression', lambda_=lambda_)\n",
    "    meanacc_dtest_rr.append(dtmp_te)\n",
    "    meanacc_dtrain_rr.append(dtmp_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.725452196382429, 0.7254668780831572, 0.725452196382429, 0.7254668780831572, 0.725452196382429, 0.7254668780831571, 0.7254815597838853, 0.7254962414846136, 0.7254668780831571, 0.7254375146817007, 0.7254228329809724, 0.7254228329809724, 0.725452196382429, 0.725452196382429, 0.725452196382429]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.logspace(-5,0,15), meanacc_dtrain_rr[1::2])\n",
    "\n",
    "plt.plot(np.logspace(-5,0,15), meanacc_dtrain_rr[::2])\n",
    "plt.show()\n",
    "\n",
    "print(meanacc_dtest_rr[::2])\n",
    "\"\"\"b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No NaNs error:\n",
      "Test: [0.725452196382429, 0.7254668780831572, 0.725452196382429, 0.7254668780831572, 0.725452196382429, 0.7254668780831571, 0.7254815597838853, 0.7254962414846136, 0.7254668780831571, 0.7254375146817007, 0.7254228329809724, 0.7254228329809724, 0.725452196382429, 0.725452196382429, 0.725452196382429] Data: [0.7259317986062173, 0.7259220108057317, 0.7259220108057317, 0.7259269047059744, 0.725917116905489, 0.7259122230052463, 0.7258828596037898, 0.725843708401848, 0.7258437084018479, 0.725843708401848, 0.7258534962023334, 0.7258632840028189, 0.725843708401848, 0.7258388145016053, 0.7258388145016051]\n",
      "\n",
      "Cleaned NaNs error:\n",
      "Test: [0.7648281322161378, 0.7648281322161378, 0.7648226342064173, 0.7648226342064173, 0.7648226342064173, 0.7648171361966967, 0.7648171361966968, 0.7648171361966968, 0.7648061401772559, 0.7648061401772559, 0.7647896461480943, 0.7647786501286534, 0.7647951441578149, 0.7647896461480944, 0.7647896461480944] Data: [0.7650223952262614, 0.7650242278961682, 0.7650242278961682, 0.7650278932359819, 0.7650297259058887, 0.7650297259058887, 0.7650242278961682, 0.7650242278961682, 0.7649967378475657, 0.7649930725077523, 0.764983909158218, 0.7649765784785908, 0.764983909158218, 0.7649802438184045, 0.7649784111484976]\n"
     ]
    }
   ],
   "source": [
    "print(\"No NaNs accuracy:\")\n",
    "print(\"Test:\",meanacc_dtest_rr[::2],\"Data:\", meanacc_dtrain_rr[::2])\n",
    "print(\"\\nCleaned NaNs accuracy:\", )\n",
    "print(\"Test:\",meanacc_dtest_rr[1::2],\"Data:\", meanacc_dtrain_rr[1::2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=426371.94111903745\n",
      "Current iteration=1, loss=304412.28049685917\n",
      "Current iteration=2, loss=182517.70687172943\n",
      "Current iteration=3, loss=75007.6071168799\n",
      "Current iteration=4, loss=43018.768026563506\n",
      "Current iteration=5, loss=40429.84931408414\n",
      "Current iteration=6, loss=38828.43388929998\n",
      "Current iteration=7, loss=37389.65898451601\n",
      "Current iteration=8, loss=36117.05518427581\n",
      "Current iteration=9, loss=35011.50945624462\n",
      "Current iteration=10, loss=34069.1108166949\n",
      "Current iteration=11, loss=33280.67184909558\n",
      "Current iteration=12, loss=32631.947149979005\n",
      "Current iteration=13, loss=32104.66628223557\n",
      "Current iteration=14, loss=31678.268293028483\n",
      "Current iteration=15, loss=31331.964252862\n",
      "Current iteration=16, loss=31046.628370914168\n",
      "Current iteration=17, loss=30806.095699402777\n",
      "Current iteration=18, loss=30597.698511703184\n",
      "Current iteration=19, loss=30412.141789082416\n",
      "Current iteration=20, loss=30242.972376525762\n",
      "Current iteration=21, loss=30085.903759645746\n",
      "Current iteration=22, loss=29938.178848206677\n",
      "Current iteration=23, loss=29798.05713520986\n",
      "Current iteration=24, loss=29664.44274650247\n",
      "Current iteration=25, loss=29536.63377923877\n",
      "Current iteration=26, loss=29414.162278981305\n",
      "Current iteration=27, loss=29296.696154590918\n",
      "Current iteration=28, loss=29183.980922858613\n",
      "Current iteration=29, loss=29075.805979840337\n",
      "Current iteration=30, loss=28971.98550146545\n",
      "Current iteration=31, loss=28872.347878727393\n",
      "Current iteration=32, loss=28776.730030587285\n",
      "Current iteration=33, loss=28684.97448444912\n",
      "Current iteration=34, loss=28596.92797902975\n",
      "Current iteration=35, loss=28512.440922193586\n",
      "Current iteration=36, loss=28431.367271774114\n",
      "Current iteration=37, loss=28353.564661187855\n",
      "Current iteration=38, loss=28278.89458617603\n",
      "Current iteration=39, loss=28207.222649700016\n",
      "Current iteration=40, loss=28138.418743070015\n",
      "Current iteration=41, loss=28072.357241050984\n",
      "Current iteration=42, loss=28008.91708694071\n",
      "Current iteration=43, loss=27947.981900588624\n",
      "Current iteration=44, loss=27889.439956295613\n",
      "Current iteration=45, loss=27833.184214524877\n",
      "Current iteration=46, loss=27779.11220920377\n",
      "Current iteration=47, loss=27727.126028923398\n",
      "Current iteration=48, loss=27677.132136060198\n",
      "Current iteration=49, loss=27629.0413239819\n",
      "Current iteration=50, loss=27582.768487075282\n",
      "Current iteration=51, loss=27538.232575911934\n",
      "Current iteration=52, loss=27495.356331243547\n",
      "Current iteration=53, loss=27454.06625373439\n",
      "Current iteration=54, loss=27414.292309271306\n",
      "Current iteration=55, loss=27375.967925537145\n",
      "Current iteration=56, loss=27339.02967154818\n",
      "Current iteration=57, loss=27303.41729036955\n",
      "Current iteration=58, loss=27269.073352644118\n",
      "Current iteration=59, loss=27235.943332904826\n",
      "Current iteration=60, loss=27203.975234697427\n",
      "Current iteration=61, loss=27173.119716877958\n",
      "Current iteration=62, loss=27143.329686448695\n",
      "Current iteration=63, loss=27114.560480653352\n",
      "Current iteration=64, loss=27086.769422610614\n",
      "Current iteration=65, loss=27059.9160647429\n",
      "Current iteration=66, loss=27033.96170154529\n",
      "Current iteration=67, loss=27008.869679799933\n",
      "Current iteration=68, loss=26984.604862240136\n",
      "Current iteration=69, loss=26961.134010079622\n",
      "Current iteration=70, loss=26938.42519080346\n",
      "Current iteration=71, loss=26916.448238820336\n",
      "Current iteration=72, loss=26895.174099978194\n",
      "Current iteration=73, loss=26874.575376761324\n",
      "Current iteration=74, loss=26854.62560114872\n",
      "Current iteration=75, loss=26835.299871989606\n",
      "Current iteration=76, loss=26816.57404611238\n",
      "Current iteration=77, loss=26798.42547777888\n",
      "Current iteration=78, loss=26780.832114901525\n",
      "Current iteration=79, loss=26763.77335453928\n",
      "Current iteration=80, loss=26747.22902574666\n",
      "Current iteration=81, loss=26731.180382080114\n",
      "Current iteration=82, loss=26715.608943568404\n",
      "Current iteration=83, loss=26700.49765885262\n",
      "Current iteration=84, loss=26685.829563978088\n",
      "Current iteration=85, loss=26671.58916553397\n",
      "Current iteration=86, loss=26657.760850546194\n",
      "Current iteration=87, loss=26644.330571168975\n",
      "Current iteration=88, loss=26631.283904032327\n",
      "Current iteration=89, loss=26618.60816109896\n",
      "Current iteration=90, loss=26606.28994338498\n",
      "Current iteration=91, loss=26594.31786712782\n",
      "Current iteration=92, loss=26582.679379683632\n",
      "Current iteration=93, loss=26571.36438188226\n",
      "Current iteration=94, loss=26560.360965893185\n",
      "Current iteration=95, loss=26549.66034120076\n",
      "Current iteration=96, loss=26539.251007409497\n",
      "Current iteration=97, loss=26529.125560699635\n",
      "Current iteration=98, loss=26519.272620939057\n",
      "Current iteration=99, loss=26509.68631543666\n",
      "Current iteration=100, loss=26500.355032266343\n",
      "Current iteration=101, loss=26491.274654783396\n",
      "Current iteration=102, loss=26482.432906839167\n",
      "Current iteration=103, loss=26473.827748115542\n",
      "Current iteration=104, loss=26465.445710693606\n",
      "Current iteration=105, loss=26457.28726055089\n",
      "Current iteration=106, loss=26449.337102822155\n",
      "Current iteration=107, loss=26441.598761462075\n",
      "Current iteration=108, loss=26434.054363391846\n",
      "Current iteration=109, loss=26426.711171580686\n",
      "Current iteration=110, loss=26419.547864946777\n",
      "Current iteration=111, loss=26412.5762568968\n",
      "Current iteration=112, loss=26405.770595602145\n",
      "Current iteration=113, loss=26399.148178979936\n",
      "Current iteration=114, loss=26392.677744022178\n",
      "Current iteration=115, loss=26386.383111603685\n",
      "Current iteration=116, loss=26380.226355529336\n",
      "Current iteration=117, loss=26374.238932546654\n",
      "Current iteration=118, loss=26368.375066985885\n",
      "Current iteration=119, loss=26362.67499718963\n",
      "Current iteration=120, loss=26357.083925221115\n",
      "Current iteration=121, loss=26351.651997185494\n",
      "Current iteration=122, loss=26346.31428997693\n",
      "Current iteration=123, loss=26341.13190332489\n",
      "Current iteration=124, loss=26336.028817952436\n",
      "Current iteration=125, loss=26331.07798713909\n",
      "Current iteration=126, loss=26326.191519965138\n",
      "Current iteration=127, loss=26321.454911216573\n",
      "Current iteration=128, loss=26316.767878967177\n",
      "Current iteration=129, loss=26312.228874124023\n",
      "Current iteration=130, loss=26307.725013096217\n",
      "Current iteration=131, loss=26303.36779263566\n",
      "Current iteration=132, loss=26299.031865470013\n",
      "Current iteration=133, loss=26294.84150202171\n",
      "Current iteration=134, loss=26290.659401305256\n",
      "Current iteration=135, loss=26286.62195462861\n",
      "Current iteration=136, loss=26282.580793265897\n",
      "Current iteration=137, loss=26278.683397951772\n",
      "Current iteration=138, loss=26274.771577682342\n",
      "Current iteration=139, loss=26271.002515748958\n",
      "Current iteration=140, loss=26267.20976724283\n",
      "Current iteration=141, loss=26263.558519223494\n",
      "Current iteration=142, loss=26259.875909638944\n",
      "Current iteration=143, loss=26256.333179571087\n",
      "Current iteration=144, loss=26252.75308606845\n",
      "Current iteration=145, loss=26249.310797822713\n",
      "Current iteration=146, loss=26245.826848053814\n",
      "Current iteration=147, loss=26242.47811251199\n",
      "Current iteration=148, loss=26239.085095335155\n",
      "Current iteration=149, loss=26235.824149870685\n",
      "Current iteration=150, loss=26232.51790131533\n",
      "Current iteration=151, loss=26229.340024708355\n",
      "Current iteration=152, loss=26226.117295432472\n",
      "Current iteration=153, loss=26223.01870266051\n",
      "Current iteration=154, loss=26219.877013775622\n",
      "Current iteration=155, loss=26216.854736000725\n",
      "Current iteration=156, loss=26213.79223020782\n",
      "Current iteration=157, loss=26210.84398571558\n",
      "Current iteration=158, loss=26207.85928027603\n",
      "Current iteration=159, loss=26204.983342133663\n",
      "Current iteration=160, loss=26202.07538939552\n",
      "Current iteration=161, loss=26199.27045523988\n",
      "Current iteration=162, loss=26196.43841537334\n",
      "Current iteration=163, loss=26193.703484089416\n",
      "Current iteration=164, loss=26190.946613477172\n",
      "Current iteration=165, loss=26188.28087267068\n",
      "Current iteration=166, loss=26185.598430159564\n",
      "Current iteration=167, loss=26183.001157350704\n",
      "Current iteration=168, loss=26180.39232939265\n",
      "Current iteration=169, loss=26177.862808845002\n",
      "Current iteration=170, loss=26175.326653506774\n",
      "Current iteration=171, loss=26172.864109626058\n",
      "Current iteration=172, loss=26170.399518573086\n",
      "Current iteration=173, loss=26168.003065921643\n",
      "Current iteration=174, loss=26165.60874280511\n",
      "Current iteration=175, loss=26163.277352019413\n",
      "Current iteration=176, loss=26160.951805220855\n",
      "Current iteration=177, loss=26158.68428351536\n",
      "Current iteration=178, loss=26156.425830919212\n",
      "Current iteration=179, loss=26154.220815420005\n",
      "Current iteration=180, loss=26152.02759877038\n",
      "Current iteration=181, loss=26149.883560643804\n",
      "Current iteration=182, loss=26147.753567069973\n",
      "Current iteration=183, loss=26145.66882428206\n",
      "Current iteration=184, loss=26143.599912717553\n",
      "Current iteration=185, loss=26141.572649260666\n",
      "Current iteration=186, loss=26139.562579702702\n",
      "Current iteration=187, loss=26137.59086923257\n",
      "Current iteration=188, loss=26135.637333063765\n",
      "Current iteration=189, loss=26133.71916507737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=190, loss=26131.819814974806\n",
      "Current iteration=191, loss=26129.95312190246\n",
      "Current iteration=192, loss=26128.105600168932\n",
      "Current iteration=193, loss=26126.288284028367\n",
      "Current iteration=194, loss=26124.49024848031\n",
      "Current iteration=195, loss=26122.720206026122\n",
      "Current iteration=196, loss=26120.96935284955\n",
      "Current iteration=197, loss=26119.244498430595\n",
      "Current iteration=198, loss=26117.53858166188\n",
      "Current iteration=199, loss=26115.85686725894\n",
      "Current iteration=200, loss=26114.193714756315\n",
      "Current iteration=201, loss=26112.553146903152\n",
      "Current iteration=202, loss=26110.930672845338\n",
      "Current iteration=203, loss=26109.329326332394\n",
      "Current iteration=204, loss=26107.745540413103\n",
      "Current iteration=205, loss=26106.181568831846\n",
      "Current iteration=206, loss=26104.634582415336\n",
      "Current iteration=207, loss=26103.10622572301\n",
      "Current iteration=208, loss=26101.59425529006\n",
      "Current iteration=209, loss=26100.099844661054\n",
      "Current iteration=210, loss=26098.621212910413\n",
      "Current iteration=211, loss=26097.159173195654\n",
      "Current iteration=212, loss=26095.712308178816\n",
      "Current iteration=213, loss=26094.281158322778\n",
      "Current iteration=214, loss=26092.864590983667\n",
      "Current iteration=215, loss=26091.462942754453\n",
      "Current iteration=216, loss=26090.07530322576\n",
      "Current iteration=217, loss=26088.701858602806\n",
      "Current iteration=218, loss=26087.3418715806\n",
      "Current iteration=219, loss=26085.99541912073\n",
      "Current iteration=220, loss=26084.661898603008\n",
      "Current iteration=221, loss=26083.34130907369\n",
      "Current iteration=222, loss=26082.033152709275\n",
      "Current iteration=223, loss=26080.737374241096\n",
      "Current iteration=224, loss=26079.453557495406\n",
      "Current iteration=225, loss=26078.18161046727\n",
      "Current iteration=226, loss=26076.92118077295\n",
      "Current iteration=227, loss=26075.672152605253\n",
      "Current iteration=228, loss=26074.43422362957\n",
      "Current iteration=229, loss=26073.207263624587\n",
      "Current iteration=230, loss=26071.99100975316\n",
      "Current iteration=231, loss=26070.785324088964\n",
      "Current iteration=232, loss=26069.589975196894\n",
      "Current iteration=233, loss=26068.404822152217\n",
      "Current iteration=234, loss=26067.229658709355\n",
      "Current iteration=235, loss=26066.06434417188\n",
      "Current iteration=236, loss=26064.90869270922\n",
      "Current iteration=237, loss=26063.762565999223\n",
      "Current iteration=238, loss=26062.625794946816\n",
      "Current iteration=239, loss=26061.49824497142\n",
      "Current iteration=240, loss=26060.379760866068\n",
      "Current iteration=241, loss=26059.270212606283\n",
      "Current iteration=242, loss=26058.16945665763\n",
      "Current iteration=243, loss=26057.07736798067\n",
      "Current iteration=244, loss=26055.99381297765\n",
      "Current iteration=245, loss=26054.918671759886\n",
      "Current iteration=246, loss=26053.85181929516\n",
      "Current iteration=247, loss=26052.793140836424\n",
      "Current iteration=248, loss=26051.7425188238\n",
      "Current iteration=249, loss=26050.699843530892\n",
      "Current iteration=250, loss=26049.665003989565\n",
      "Current iteration=251, loss=26048.63789530546\n",
      "Current iteration=252, loss=26047.618412384974\n",
      "Current iteration=253, loss=26046.60645494001\n",
      "Current iteration=254, loss=26045.601923160604\n",
      "Current iteration=255, loss=26044.604721122494\n",
      "Current iteration=256, loss=26043.614753806785\n",
      "Current iteration=257, loss=26042.631929407493\n",
      "Current iteration=258, loss=26041.656157281326\n",
      "Current iteration=259, loss=26040.68734950033\n",
      "Current iteration=260, loss=26039.725419442377\n",
      "Current iteration=261, loss=26038.770282827598\n",
      "Current iteration=262, loss=26037.821856749422\n",
      "Current iteration=263, loss=26036.88006035907\n",
      "Current iteration=264, loss=26035.944814199185\n",
      "Current iteration=265, loss=26035.01604064929\n",
      "Current iteration=266, loss=26034.093663466898\n",
      "Current iteration=267, loss=26033.177608071353\n",
      "Current iteration=268, loss=26032.26780122704\n",
      "Current iteration=269, loss=26031.364171218447\n",
      "Current iteration=270, loss=26030.466647630747\n",
      "Current iteration=271, loss=26029.57516145196\n",
      "Current iteration=272, loss=26028.689644920305\n",
      "Current iteration=273, loss=26027.8100315779\n",
      "Current iteration=274, loss=26026.936256163775\n",
      "Current iteration=275, loss=26026.068254635793\n",
      "Current iteration=276, loss=26025.20596409491\n",
      "Current iteration=277, loss=26024.34932278649\n",
      "Current iteration=278, loss=26023.498270046013\n",
      "Current iteration=279, loss=26022.652746287305\n",
      "Current iteration=280, loss=26021.81269296295\n",
      "Current iteration=281, loss=26020.978052544513\n",
      "Current iteration=282, loss=26020.148768493033\n",
      "Current iteration=283, loss=26019.32478523464\n",
      "Current iteration=284, loss=26018.50604813795\n",
      "Current iteration=285, loss=26017.692503487357\n",
      "Current iteration=286, loss=26016.88409846509\n",
      "Current iteration=287, loss=26016.080781123703\n",
      "Current iteration=288, loss=26015.28250037129\n",
      "Current iteration=289, loss=26014.48920594416\n",
      "Current iteration=290, loss=26013.700848394164\n",
      "Current iteration=291, loss=26012.91737906212\n",
      "Current iteration=292, loss=26012.13875006663\n",
      "Current iteration=293, loss=26011.36491427858\n",
      "Current iteration=294, loss=26010.59582531094\n",
      "Current iteration=295, loss=26009.831437494606\n",
      "Current iteration=296, loss=26009.071705868864\n",
      "Current iteration=297, loss=26008.316586158584\n",
      "Current iteration=298, loss=26007.56603476523\n",
      "Current iteration=299, loss=26006.820008745417\n",
      "Current iteration=300, loss=26006.07846580228\n",
      "Current iteration=301, loss=26005.341364265405\n",
      "Current iteration=302, loss=26004.60866308249\n",
      "Current iteration=303, loss=26003.8803218006\n",
      "Current iteration=304, loss=26003.156300558003\n",
      "Current iteration=305, loss=26002.436560066762\n",
      "Current iteration=306, loss=26001.7210616047\n",
      "Current iteration=307, loss=26001.009766999225\n",
      "Current iteration=308, loss=26000.30263861941\n",
      "Current iteration=309, loss=25999.599639360982\n",
      "Current iteration=310, loss=25998.900732638584\n",
      "Current iteration=311, loss=25998.205882371745\n",
      "Current iteration=312, loss=25997.515052977342\n",
      "Current iteration=313, loss=25996.828209356532\n",
      "Current iteration=314, loss=25996.14531688736\n",
      "Current iteration=315, loss=25995.466341412623\n",
      "Current iteration=316, loss=25994.791249232625\n",
      "Current iteration=317, loss=25994.120007093858\n",
      "Current iteration=318, loss=25993.452582181944\n",
      "Current iteration=319, loss=25992.78894211108\n",
      "Current iteration=320, loss=25992.129054917168\n",
      "Current iteration=321, loss=25991.472889047953\n",
      "Current iteration=322, loss=25990.820413356334\n",
      "Current iteration=323, loss=25990.17159709115\n",
      "Current iteration=324, loss=25989.5264098907\n",
      "Current iteration=325, loss=25988.884821774114\n",
      "Current iteration=326, loss=25988.24680313505\n",
      "Current iteration=327, loss=25987.61232473364\n",
      "Current iteration=328, loss=25986.981357690376\n",
      "Current iteration=329, loss=25986.353873478543\n",
      "Current iteration=330, loss=25985.72984391833\n",
      "Current iteration=331, loss=25985.109241169695\n",
      "Current iteration=332, loss=25984.492037726723\n",
      "Current iteration=333, loss=25983.878206410867\n",
      "Current iteration=334, loss=25983.267720365544\n",
      "Current iteration=335, loss=25982.66055304973\n",
      "Current iteration=336, loss=25982.056678232784\n",
      "Current iteration=337, loss=25981.456069988417\n",
      "Current iteration=338, loss=25980.85870268967\n",
      "Current iteration=339, loss=25980.264551003253\n",
      "Current iteration=340, loss=25979.67358988469\n",
      "Current iteration=341, loss=25979.08579457298\n",
      "Current iteration=342, loss=25978.501140585948\n",
      "Current iteration=343, loss=25977.919603715152\n",
      "Current iteration=344, loss=25977.341160021453\n",
      "Current iteration=345, loss=25976.765785830175\n",
      "Current iteration=346, loss=25976.19345772684\n",
      "Current iteration=347, loss=25975.62415255261\n",
      "Current iteration=348, loss=25975.05784740017\n",
      "Current iteration=349, loss=25974.49451960939\n",
      "Current iteration=350, loss=25973.934146763397\n",
      "Current iteration=351, loss=25973.376706684445\n",
      "Current iteration=352, loss=25972.822177430156\n",
      "Current iteration=353, loss=25972.270537289573\n",
      "Current iteration=354, loss=25971.721764779562\n",
      "Current iteration=355, loss=25971.175838641084\n",
      "Current iteration=356, loss=25970.632737835684\n",
      "Current iteration=357, loss=25970.092441541983\n",
      "Current iteration=358, loss=25969.554929152324\n",
      "Current iteration=359, loss=25969.020180269385\n",
      "Current iteration=360, loss=25968.48817470298\n",
      "Current iteration=361, loss=25967.958892466835\n",
      "Current iteration=362, loss=25967.432313775542\n",
      "Current iteration=363, loss=25966.90841904142\n",
      "Current iteration=364, loss=25966.387188871635\n",
      "Current iteration=365, loss=25965.868604065206\n",
      "Current iteration=366, loss=25965.35264561022\n",
      "Current iteration=367, loss=25964.839294680998\n",
      "Current iteration=368, loss=25964.32853263538\n",
      "Current iteration=369, loss=25963.82034101204\n",
      "Current iteration=370, loss=25963.314701527903\n",
      "Current iteration=371, loss=25962.811596075535\n",
      "Current iteration=372, loss=25962.311006720684\n",
      "Current iteration=373, loss=25961.812915699775\n",
      "Current iteration=374, loss=25961.317305417535\n",
      "Current iteration=375, loss=25960.82415844463\n",
      "Current iteration=376, loss=25960.33345751536\n",
      "Current iteration=377, loss=25959.84518552538\n",
      "Current iteration=378, loss=25959.359325529505\n",
      "Current iteration=379, loss=25958.875860739536\n",
      "Current iteration=380, loss=25958.394774522105\n",
      "Current iteration=381, loss=25957.91605039663\n",
      "Current iteration=382, loss=25957.43967203325\n",
      "Current iteration=383, loss=25956.96562325081\n",
      "Current iteration=384, loss=25956.493888014917\n",
      "Current iteration=385, loss=25956.024450436005\n",
      "Current iteration=386, loss=25955.55729476744\n",
      "Current iteration=387, loss=25955.092405403684\n",
      "Current iteration=388, loss=25954.629766878443\n",
      "Current iteration=389, loss=25954.169363862926\n",
      "Current iteration=390, loss=25953.71118116407\n",
      "Current iteration=391, loss=25953.255203722838\n",
      "Current iteration=392, loss=25952.801416612518\n",
      "Current iteration=393, loss=25952.349805037094\n",
      "Current iteration=394, loss=25951.90035432959\n",
      "Current iteration=395, loss=25951.45304995051\n",
      "Current iteration=396, loss=25951.00787748627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=397, loss=25950.564822647637\n",
      "Current iteration=398, loss=25950.12387126826\n",
      "Current iteration=399, loss=25949.685009303168\n",
      "Current iteration=400, loss=25949.248222827304\n",
      "Current iteration=401, loss=25948.813498034135\n",
      "Current iteration=402, loss=25948.380821234223\n",
      "Current iteration=403, loss=25947.950178853855\n",
      "Current iteration=404, loss=25947.52155743366\n",
      "Current iteration=405, loss=25947.094943627362\n",
      "Current iteration=406, loss=25946.67032420038\n",
      "Current iteration=407, loss=25946.24768602856\n",
      "Current iteration=408, loss=25945.827016096995\n",
      "Current iteration=409, loss=25945.408301498668\n",
      "Current iteration=410, loss=25944.991529433308\n",
      "Current iteration=411, loss=25944.57668720617\n",
      "Current iteration=412, loss=25944.16376222683\n",
      "Current iteration=413, loss=25943.75274200808\n",
      "Current iteration=414, loss=25943.343614164725\n",
      "Current iteration=415, loss=25942.936366412498\n",
      "Current iteration=416, loss=25942.530986566955\n",
      "Current iteration=417, loss=25942.12746254235\n",
      "Current iteration=418, loss=25941.72578235063\n",
      "Current iteration=419, loss=25941.32593410032\n",
      "Current iteration=420, loss=25940.92790599553\n",
      "Current iteration=421, loss=25940.531686334907\n",
      "Current iteration=422, loss=25940.13726351066\n",
      "Current iteration=423, loss=25939.744626007534\n",
      "Current iteration=424, loss=25939.35376240188\n",
      "Current iteration=425, loss=25938.964661360664\n",
      "Current iteration=426, loss=25938.577311640543\n",
      "Current iteration=427, loss=25938.191702086926\n",
      "Current iteration=428, loss=25937.80782163307\n",
      "Current iteration=429, loss=25937.425659299155\n",
      "Current iteration=430, loss=25937.045204191436\n",
      "Current iteration=431, loss=25936.666445501334\n",
      "Current iteration=432, loss=25936.289372504576\n",
      "Current iteration=433, loss=25935.91397456038\n",
      "Current iteration=434, loss=25935.54024111057\n",
      "Current iteration=435, loss=25935.168161678783\n",
      "Current iteration=436, loss=25934.797725869656\n",
      "Current iteration=437, loss=25934.428923368007\n",
      "Current iteration=438, loss=25934.061743938055\n",
      "Current iteration=439, loss=25933.69617742265\n",
      "Current iteration=440, loss=25933.33221374248\n",
      "Current iteration=441, loss=25932.969842895345\n",
      "Current iteration=442, loss=25932.60905495539\n",
      "Current iteration=443, loss=25932.249840072356\n",
      "Current iteration=444, loss=25931.89218847091\n",
      "Current iteration=445, loss=25931.536090449838\n",
      "Current iteration=446, loss=25931.181536381435\n",
      "Current iteration=447, loss=25930.82851671073\n",
      "Current iteration=448, loss=25930.477021954837\n",
      "Current iteration=449, loss=25930.127042702276\n",
      "Current iteration=450, loss=25929.77856961228\n",
      "Current iteration=451, loss=25929.43159341417\n",
      "Current iteration=452, loss=25929.086104906648\n",
      "Current iteration=453, loss=25928.74209495722\n",
      "Current iteration=454, loss=25928.399554501488\n",
      "Current iteration=455, loss=25928.058474542595\n",
      "Current iteration=456, loss=25927.718846150543\n",
      "Current iteration=457, loss=25927.380660461615\n",
      "Current iteration=458, loss=25927.043908677773\n",
      "Current iteration=459, loss=25926.708582066018\n",
      "Current iteration=460, loss=25926.374671957874\n",
      "Current iteration=461, loss=25926.042169748733\n",
      "Current iteration=462, loss=25925.71106689733\n",
      "Current iteration=463, loss=25925.38135492514\n",
      "Current iteration=464, loss=25925.05302541586\n",
      "Current iteration=465, loss=25924.726070014785\n",
      "Current iteration=466, loss=25924.400480428354\n",
      "Current iteration=467, loss=25924.076248423513\n",
      "Current iteration=468, loss=25923.753365827255\n",
      "Current iteration=469, loss=25923.43182452605\n",
      "Current iteration=470, loss=25923.11161646534\n",
      "Current iteration=471, loss=25922.79273364901\n",
      "Current iteration=472, loss=25922.4751681389\n",
      "Current iteration=473, loss=25922.158912054267\n",
      "Current iteration=474, loss=25921.843957571324\n",
      "Current iteration=475, loss=25921.530296922716\n",
      "Current iteration=476, loss=25921.217922397063\n",
      "Current iteration=477, loss=25920.90682633844\n",
      "Current iteration=478, loss=25920.597001145947\n",
      "Current iteration=479, loss=25920.28843927319\n",
      "Current iteration=480, loss=25919.98113322787\n",
      "Current iteration=481, loss=25919.67507557126\n",
      "Current iteration=482, loss=25919.370258917803\n",
      "Current iteration=483, loss=25919.066675934635\n",
      "Current iteration=484, loss=25918.76431934115\n",
      "Current iteration=485, loss=25918.46318190856\n",
      "Current iteration=486, loss=25918.163256459433\n",
      "Current iteration=487, loss=25917.8645358673\n",
      "Current iteration=488, loss=25917.56701305622\n",
      "Current iteration=489, loss=25917.270681000347\n",
      "Current iteration=490, loss=25916.975532723503\n",
      "Current iteration=491, loss=25916.681561298796\n",
      "Current iteration=492, loss=25916.38875984818\n",
      "Current iteration=493, loss=25916.097121542094\n",
      "Current iteration=494, loss=25915.806639598984\n",
      "Current iteration=495, loss=25915.51730728501\n",
      "Current iteration=496, loss=25915.229117913565\n",
      "Current iteration=497, loss=25914.942064844945\n",
      "Current iteration=498, loss=25914.656141485935\n",
      "Current iteration=499, loss=25914.371341289432\n",
      "Current iteration=500, loss=25914.087657754088\n",
      "Current iteration=501, loss=25913.805084423926\n",
      "Current iteration=502, loss=25913.52361488796\n",
      "Current iteration=503, loss=25913.24324277985\n",
      "Current iteration=504, loss=25912.963961777532\n",
      "Current iteration=505, loss=25912.685765602866\n",
      "Current iteration=506, loss=25912.40864802127\n",
      "Current iteration=507, loss=25912.132602841386\n",
      "Current iteration=508, loss=25911.857623914722\n",
      "Current iteration=509, loss=25911.583705135305\n",
      "Current iteration=510, loss=25911.31084043937\n",
      "Current iteration=511, loss=25911.03902380498\n",
      "Current iteration=512, loss=25910.768249251723\n",
      "Current iteration=513, loss=25910.49851084037\n",
      "Current iteration=514, loss=25910.22980267256\n",
      "Current iteration=515, loss=25909.96211889046\n",
      "Current iteration=516, loss=25909.695453676457\n",
      "Current iteration=517, loss=25909.42980125283\n",
      "Current iteration=518, loss=25909.16515588145\n",
      "Current iteration=519, loss=25908.90151186344\n",
      "Current iteration=520, loss=25908.63886353892\n",
      "Current iteration=521, loss=25908.377205286637\n",
      "Current iteration=522, loss=25908.11653152371\n",
      "Current iteration=523, loss=25907.856836705294\n",
      "Current iteration=524, loss=25907.598115324323\n",
      "Current iteration=525, loss=25907.34036191119\n",
      "Current iteration=526, loss=25907.083571033447\n",
      "Current iteration=527, loss=25906.827737295538\n",
      "Current iteration=528, loss=25906.572855338512\n",
      "Current iteration=529, loss=25906.31891983971\n",
      "Current iteration=530, loss=25906.065925512514\n",
      "Current iteration=531, loss=25905.813867106073\n",
      "Current iteration=532, loss=25905.562739404995\n",
      "Current iteration=533, loss=25905.312537229096\n",
      "Current iteration=534, loss=25905.063255433142\n",
      "Current iteration=535, loss=25904.81488890654\n",
      "Current iteration=536, loss=25904.567432573112\n",
      "Current iteration=537, loss=25904.320881390813\n",
      "Current iteration=538, loss=25904.07523035145\n",
      "Current iteration=539, loss=25903.83047448049\n",
      "Current iteration=540, loss=25903.58660883671\n",
      "Current iteration=541, loss=25903.343628512033\n",
      "Current iteration=542, loss=25903.1015286312\n",
      "Current iteration=543, loss=25902.860304351583\n",
      "Current iteration=544, loss=25902.619950862878\n",
      "Current iteration=545, loss=25902.380463386922\n",
      "Current iteration=546, loss=25902.1418371774\n",
      "Current iteration=547, loss=25901.904067519627\n",
      "Current iteration=548, loss=25901.667149730303\n",
      "Current iteration=549, loss=25901.431079157264\n",
      "Current iteration=550, loss=25901.195851179287\n",
      "Current iteration=551, loss=25900.961461205814\n",
      "Current iteration=552, loss=25900.727904676733\n",
      "Current iteration=553, loss=25900.49517706217\n",
      "Current iteration=554, loss=25900.263273862227\n",
      "Current iteration=555, loss=25900.03219060679\n",
      "Current iteration=556, loss=25899.801922855295\n",
      "Current iteration=557, loss=25899.572466196485\n",
      "Current iteration=558, loss=25899.343816248227\n",
      "Current iteration=559, loss=25899.115968657272\n",
      "Current iteration=560, loss=25898.88891909905\n",
      "Current iteration=561, loss=25898.662663277446\n",
      "Current iteration=562, loss=25898.43719692459\n",
      "Current iteration=563, loss=25898.21251580067\n",
      "Current iteration=564, loss=25897.988615693695\n",
      "Current iteration=565, loss=25897.765492419283\n",
      "Current iteration=566, loss=25897.54314182049\n",
      "Current iteration=567, loss=25897.321559767588\n",
      "Current iteration=568, loss=25897.100742157858\n",
      "Current iteration=569, loss=25896.880684915403\n",
      "Current iteration=570, loss=25896.661383990944\n",
      "Current iteration=571, loss=25896.442835361617\n",
      "Current iteration=572, loss=25896.22503503079\n",
      "Current iteration=573, loss=25896.007979027876\n",
      "Current iteration=574, loss=25895.791663408112\n",
      "Current iteration=575, loss=25895.576084252403\n",
      "Current iteration=576, loss=25895.361237667115\n",
      "Current iteration=577, loss=25895.147119783884\n",
      "Current iteration=578, loss=25894.933726759456\n",
      "Current iteration=579, loss=25894.721054775477\n",
      "Current iteration=580, loss=25894.50910003832\n",
      "Current iteration=581, loss=25894.29785877891\n",
      "Current iteration=582, loss=25894.087327252535\n",
      "Current iteration=583, loss=25893.87750173868\n",
      "Current iteration=584, loss=25893.66837854084\n",
      "Current iteration=585, loss=25893.459953986345\n",
      "Current iteration=586, loss=25893.252224426204\n",
      "Current iteration=587, loss=25893.045186234907\n",
      "Current iteration=588, loss=25892.83883581028\n",
      "Current iteration=589, loss=25892.633169573288\n",
      "Current iteration=590, loss=25892.428183967902\n",
      "Current iteration=591, loss=25892.223875460913\n",
      "Current iteration=592, loss=25892.020240541744\n",
      "Current iteration=593, loss=25891.817275722322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=594, loss=25891.61497753692\n",
      "Current iteration=595, loss=25891.413342541968\n",
      "Current iteration=596, loss=25891.21236731589\n",
      "Current iteration=597, loss=25891.012048458982\n",
      "Current iteration=598, loss=25890.81238259322\n",
      "Current iteration=599, loss=25890.61336636213\n",
      "Current iteration=600, loss=25890.414996430598\n",
      "Current iteration=601, loss=25890.21726948477\n",
      "Current iteration=602, loss=25890.020182231834\n",
      "Current iteration=603, loss=25889.82373139992\n",
      "Current iteration=604, loss=25889.627913737946\n",
      "Current iteration=605, loss=25889.432726015442\n",
      "Current iteration=606, loss=25889.23816502241\n",
      "Current iteration=607, loss=25889.044227569208\n",
      "Current iteration=608, loss=25888.850910486362\n",
      "Current iteration=609, loss=25888.658210624446\n",
      "Current iteration=610, loss=25888.46612485395\n",
      "Current iteration=611, loss=25888.274650065105\n",
      "Current iteration=612, loss=25888.08378316776\n",
      "Current iteration=613, loss=25887.89352109126\n",
      "Current iteration=614, loss=25887.703860784277\n",
      "Current iteration=615, loss=25887.5147992147\n",
      "Current iteration=616, loss=25887.326333369463\n",
      "Current iteration=617, loss=25887.138460254453\n",
      "Current iteration=618, loss=25886.951176894352\n",
      "Current iteration=619, loss=25886.764480332487\n",
      "Current iteration=620, loss=25886.578367630747\n",
      "Current iteration=621, loss=25886.392835869403\n",
      "Current iteration=622, loss=25886.207882147002\n",
      "Current iteration=623, loss=25886.02350358024\n",
      "Current iteration=624, loss=25885.83969730382\n",
      "Current iteration=625, loss=25885.65646047033\n",
      "Current iteration=626, loss=25885.473790250144\n",
      "Current iteration=627, loss=25885.291683831238\n",
      "Current iteration=628, loss=25885.110138419124\n",
      "Current iteration=629, loss=25884.9291512367\n",
      "Current iteration=630, loss=25884.748719524126\n",
      "Current iteration=631, loss=25884.568840538715\n",
      "Current iteration=632, loss=25884.389511554797\n",
      "Current iteration=633, loss=25884.210729863626\n",
      "Current iteration=634, loss=25884.032492773218\n",
      "Current iteration=635, loss=25883.854797608292\n",
      "Current iteration=636, loss=25883.677641710092\n",
      "Current iteration=637, loss=25883.50102243632\n",
      "Current iteration=638, loss=25883.324937160993\n",
      "Current iteration=639, loss=25883.149383274347\n",
      "Current iteration=640, loss=25882.97435818271\n",
      "Current iteration=641, loss=25882.799859308376\n",
      "Current iteration=642, loss=25882.62588408955\n",
      "Current iteration=643, loss=25882.452429980174\n",
      "Current iteration=644, loss=25882.279494449846\n",
      "Current iteration=645, loss=25882.107074983716\n",
      "Current iteration=646, loss=25881.935169082364\n",
      "Current iteration=647, loss=25881.7637742617\n",
      "Current iteration=648, loss=25881.592888052874\n",
      "Current iteration=649, loss=25881.42250800212\n",
      "Current iteration=650, loss=25881.252631670715\n",
      "Current iteration=651, loss=25881.083256634833\n",
      "Current iteration=652, loss=25880.914380485454\n",
      "Current iteration=653, loss=25880.74600082826\n",
      "Current iteration=654, loss=25880.578115283544\n",
      "Current iteration=655, loss=25880.41072148609\n",
      "Current iteration=656, loss=25880.24381708509\n",
      "Current iteration=657, loss=25880.077399744026\n",
      "Current iteration=658, loss=25879.911467140602\n",
      "Current iteration=659, loss=25879.746016966616\n",
      "Current iteration=660, loss=25879.581046927877\n",
      "Current iteration=661, loss=25879.4165547441\n",
      "Current iteration=662, loss=25879.252538148823\n",
      "Current iteration=663, loss=25879.088994889313\n",
      "Current iteration=664, loss=25878.925922726452\n",
      "Current iteration=665, loss=25878.763319434653\n",
      "Current iteration=666, loss=25878.601182801787\n",
      "Current iteration=667, loss=25878.43951062906\n",
      "Current iteration=668, loss=25878.278300730944\n",
      "Current iteration=669, loss=25878.117550935072\n",
      "Current iteration=670, loss=25877.957259082163\n",
      "Current iteration=671, loss=25877.79742302591\n",
      "Current iteration=672, loss=25877.63804063292\n",
      "Current iteration=673, loss=25877.4791097826\n",
      "Current iteration=674, loss=25877.320628367088\n",
      "Current iteration=675, loss=25877.16259429115\n",
      "Current iteration=676, loss=25877.005005472125\n",
      "Current iteration=677, loss=25876.84785983978\n",
      "Current iteration=678, loss=25876.69115533631\n",
      "Current iteration=679, loss=25876.53488991616\n",
      "Current iteration=680, loss=25876.379061546028\n",
      "Current iteration=681, loss=25876.22366820471\n",
      "Current iteration=682, loss=25876.068707883074\n",
      "Current iteration=683, loss=25875.914178583942\n",
      "Current iteration=684, loss=25875.76007832203\n",
      "Current iteration=685, loss=25875.60640512384\n",
      "Current iteration=686, loss=25875.45315702762\n",
      "Current iteration=687, loss=25875.30033208325\n",
      "Current iteration=688, loss=25875.1479283522\n",
      "Current iteration=689, loss=25874.995943907394\n",
      "Current iteration=690, loss=25874.844376833193\n",
      "Current iteration=691, loss=25874.69322522528\n",
      "Current iteration=692, loss=25874.542487190607\n",
      "Current iteration=693, loss=25874.392160847292\n",
      "Current iteration=694, loss=25874.24224432458\n",
      "Current iteration=695, loss=25874.092735762737\n",
      "Current iteration=696, loss=25873.943633312978\n",
      "Current iteration=697, loss=25873.79493513742\n",
      "Current iteration=698, loss=25873.646639408977\n",
      "Current iteration=699, loss=25873.49874431132\n",
      "Current iteration=700, loss=25873.351248038758\n",
      "Current iteration=701, loss=25873.20414879623\n",
      "Current iteration=702, loss=25873.057444799168\n",
      "Current iteration=703, loss=25872.911134273476\n",
      "Current iteration=704, loss=25872.76521545543\n",
      "Current iteration=705, loss=25872.619686591643\n",
      "Current iteration=706, loss=25872.474545938938\n",
      "Current iteration=707, loss=25872.329791764347\n",
      "Current iteration=708, loss=25872.185422345\n",
      "Current iteration=709, loss=25872.041435968054\n",
      "Current iteration=710, loss=25871.897830930673\n",
      "Current iteration=711, loss=25871.754605539903\n",
      "Current iteration=712, loss=25871.611758112635\n",
      "Current iteration=713, loss=25871.469286975545\n",
      "Current iteration=714, loss=25871.327190465017\n",
      "Current iteration=715, loss=25871.185466927094\n",
      "Current iteration=716, loss=25871.044114717384\n",
      "Current iteration=717, loss=25870.90313220102\n",
      "Current iteration=718, loss=25870.7625177526\n",
      "Current iteration=719, loss=25870.622269756106\n",
      "Current iteration=720, loss=25870.48238660486\n",
      "Current iteration=721, loss=25870.342866701445\n",
      "Current iteration=722, loss=25870.203708457666\n",
      "Current iteration=723, loss=25870.064910294463\n",
      "Current iteration=724, loss=25869.926470641865\n",
      "Current iteration=725, loss=25869.788387938937\n",
      "Current iteration=726, loss=25869.6506606337\n",
      "Current iteration=727, loss=25869.51328718309\n",
      "Current iteration=728, loss=25869.376266052896\n",
      "Current iteration=729, loss=25869.239595717692\n",
      "Current iteration=730, loss=25869.10327466079\n",
      "Current iteration=731, loss=25868.96730137418\n",
      "Current iteration=732, loss=25868.831674358462\n",
      "Current iteration=733, loss=25868.69639212281\n",
      "Current iteration=734, loss=25868.5614531849\n",
      "Current iteration=735, loss=25868.42685607085\n",
      "Current iteration=736, loss=25868.292599315184\n",
      "Current iteration=737, loss=25868.158681460773\n",
      "Current iteration=738, loss=25868.025101058753\n",
      "Current iteration=739, loss=25867.891856668495\n",
      "Current iteration=740, loss=25867.75894685757\n",
      "Current iteration=741, loss=25867.626370201644\n",
      "Current iteration=742, loss=25867.49412528448\n",
      "Current iteration=743, loss=25867.36221069783\n",
      "Current iteration=744, loss=25867.230625041437\n",
      "Current iteration=745, loss=25867.09936692295\n",
      "Current iteration=746, loss=25866.968434957875\n",
      "Current iteration=747, loss=25866.837827769526\n",
      "Current iteration=748, loss=25866.707543989\n",
      "Current iteration=749, loss=25866.577582255075\n",
      "Current iteration=750, loss=25866.44794121421\n",
      "Current iteration=751, loss=25866.318619520454\n",
      "Current iteration=752, loss=25866.18961583544\n",
      "Current iteration=753, loss=25866.060928828283\n",
      "Current iteration=754, loss=25865.932557175594\n",
      "Current iteration=755, loss=25865.804499561378\n",
      "Current iteration=756, loss=25865.676754677002\n",
      "Current iteration=757, loss=25865.54932122117\n",
      "Current iteration=758, loss=25865.422197899836\n",
      "Current iteration=759, loss=25865.295383426193\n",
      "Current iteration=760, loss=25865.168876520613\n",
      "Current iteration=761, loss=25865.04267591059\n",
      "Current iteration=762, loss=25864.9167803307\n",
      "Current iteration=763, loss=25864.79118852257\n",
      "Current iteration=764, loss=25864.665899234817\n",
      "Current iteration=765, loss=25864.540911223\n",
      "Current iteration=766, loss=25864.4162232496\n",
      "Current iteration=767, loss=25864.291834083928\n",
      "Current iteration=768, loss=25864.16774250214\n",
      "Current iteration=769, loss=25864.04394728716\n",
      "Current iteration=770, loss=25863.920447228615\n",
      "Current iteration=771, loss=25863.797241122844\n",
      "Current iteration=772, loss=25863.67432777283\n",
      "Current iteration=773, loss=25863.55170598812\n",
      "Current iteration=774, loss=25863.42937458487\n",
      "Current iteration=775, loss=25863.30733238571\n",
      "Current iteration=776, loss=25863.185578219764\n",
      "Current iteration=777, loss=25863.06411092258\n",
      "Current iteration=778, loss=25862.94292933611\n",
      "Current iteration=779, loss=25862.822032308635\n",
      "Current iteration=780, loss=25862.701418694764\n",
      "Current iteration=781, loss=25862.58108735538\n",
      "Current iteration=782, loss=25862.461037157565\n",
      "Current iteration=783, loss=25862.341266974636\n",
      "Current iteration=784, loss=25862.221775686015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=785, loss=25862.10256217727\n",
      "Current iteration=786, loss=25861.983625340024\n",
      "Current iteration=787, loss=25861.864964071945\n",
      "Current iteration=788, loss=25861.746577276685\n",
      "Current iteration=789, loss=25861.628463863864\n",
      "Current iteration=790, loss=25861.510622749018\n",
      "Current iteration=791, loss=25861.393052853553\n",
      "Current iteration=792, loss=25861.27575310475\n",
      "Current iteration=793, loss=25861.15872243567\n",
      "Current iteration=794, loss=25861.041959785154\n",
      "Current iteration=795, loss=25860.92546409779\n",
      "Current iteration=796, loss=25860.809234323842\n",
      "Current iteration=797, loss=25860.69326941926\n",
      "Current iteration=798, loss=25860.5775683456\n",
      "Current iteration=799, loss=25860.46213007003\n",
      "Current iteration=800, loss=25860.346953565255\n",
      "Current iteration=801, loss=25860.232037809514\n",
      "Current iteration=802, loss=25860.117381786527\n",
      "Current iteration=803, loss=25860.002984485473\n",
      "Current iteration=804, loss=25859.888844900932\n",
      "Current iteration=805, loss=25859.77496203289\n",
      "Current iteration=806, loss=25859.661334886663\n",
      "Current iteration=807, loss=25859.547962472894\n",
      "Current iteration=808, loss=25859.434843807518\n",
      "Current iteration=809, loss=25859.32197791169\n",
      "Current iteration=810, loss=25859.20936381181\n",
      "Current iteration=811, loss=25859.097000539452\n",
      "Current iteration=812, loss=25858.984887131348\n",
      "Current iteration=813, loss=25858.87302262934\n",
      "Current iteration=814, loss=25858.761406080357\n",
      "Current iteration=815, loss=25858.650036536397\n",
      "Current iteration=816, loss=25858.538913054465\n",
      "Current iteration=817, loss=25858.428034696575\n",
      "Current iteration=818, loss=25858.3174005297\n",
      "Current iteration=819, loss=25858.207009625723\n",
      "Current iteration=820, loss=25858.09686106147\n",
      "Current iteration=821, loss=25857.986953918593\n",
      "Current iteration=822, loss=25857.8772872836\n",
      "Current iteration=823, loss=25857.767860247826\n",
      "Current iteration=824, loss=25857.658671907353\n",
      "Current iteration=825, loss=25857.54972136304\n",
      "Current iteration=826, loss=25857.44100772046\n",
      "Current iteration=827, loss=25857.332530089858\n",
      "Current iteration=828, loss=25857.22428758617\n",
      "Current iteration=829, loss=25857.11627932895\n",
      "Current iteration=830, loss=25857.008504442358\n",
      "Current iteration=831, loss=25856.90096205513\n",
      "Current iteration=832, loss=25856.793651300555\n",
      "Current iteration=833, loss=25856.686571316437\n",
      "Current iteration=834, loss=25856.579721245063\n",
      "Current iteration=835, loss=25856.473100233212\n",
      "Current iteration=836, loss=25856.366707432073\n",
      "Current iteration=837, loss=25856.260541997253\n",
      "Current iteration=838, loss=25856.15460308875\n",
      "Current iteration=839, loss=25856.0488898709\n",
      "Current iteration=840, loss=25855.943401512384\n",
      "Current iteration=841, loss=25855.838137186176\n",
      "Current iteration=842, loss=25855.73309606953\n",
      "Current iteration=843, loss=25855.628277343953\n",
      "Current iteration=844, loss=25855.523680195158\n",
      "Current iteration=845, loss=25855.419303813076\n",
      "Current iteration=846, loss=25855.3151473918\n",
      "Current iteration=847, loss=25855.211210129568\n",
      "Current iteration=848, loss=25855.107491228748\n",
      "Current iteration=849, loss=25855.003989895795\n",
      "Current iteration=850, loss=25854.900705341235\n",
      "Current iteration=851, loss=25854.797636779655\n",
      "Current iteration=852, loss=25854.69478342963\n",
      "Current iteration=853, loss=25854.59214451378\n",
      "Current iteration=854, loss=25854.48971925864\n",
      "Current iteration=855, loss=25854.38750689475\n",
      "Current iteration=856, loss=25854.28550665653\n",
      "Current iteration=857, loss=25854.183717782325\n",
      "Current iteration=858, loss=25854.08213951435\n",
      "Current iteration=859, loss=25853.98077109868\n",
      "Current iteration=860, loss=25853.879611785193\n",
      "Current iteration=861, loss=25853.778660827615\n",
      "Current iteration=862, loss=25853.67791748343\n",
      "Current iteration=863, loss=25853.57738101388\n",
      "Current iteration=864, loss=25853.477050683963\n",
      "Current iteration=865, loss=25853.37692576236\n",
      "Current iteration=866, loss=25853.277005521486\n",
      "Current iteration=867, loss=25853.177289237407\n",
      "Current iteration=868, loss=25853.077776189817\n",
      "Current iteration=869, loss=25852.978465662083\n",
      "Current iteration=870, loss=25852.87935694113\n",
      "Current iteration=871, loss=25852.780449317488\n",
      "Current iteration=872, loss=25852.681742085257\n",
      "Current iteration=873, loss=25852.583234542057\n",
      "Current iteration=874, loss=25852.48492598904\n",
      "Current iteration=875, loss=25852.386815730846\n",
      "Current iteration=876, loss=25852.288903075594\n",
      "Current iteration=877, loss=25852.191187334876\n",
      "Current iteration=878, loss=25852.09366782369\n",
      "Current iteration=879, loss=25851.99634386047\n",
      "Current iteration=880, loss=25851.899214767025\n",
      "Current iteration=881, loss=25851.802279868563\n",
      "Current iteration=882, loss=25851.705538493617\n",
      "Current iteration=883, loss=25851.608989974084\n",
      "Current iteration=884, loss=25851.51263364514\n",
      "Current iteration=885, loss=25851.416468845287\n",
      "Current iteration=886, loss=25851.320494916275\n",
      "Current iteration=887, loss=25851.224711203133\n",
      "Current iteration=888, loss=25851.1291170541\n",
      "Current iteration=889, loss=25851.03371182065\n",
      "Current iteration=890, loss=25850.938494857448\n",
      "Current iteration=891, loss=25850.843465522325\n",
      "Current iteration=892, loss=25850.748623176296\n",
      "Current iteration=893, loss=25850.65396718349\n",
      "Current iteration=894, loss=25850.55949691118\n",
      "Current iteration=895, loss=25850.465211729726\n",
      "Current iteration=896, loss=25850.37111101258\n",
      "Current iteration=897, loss=25850.27719413625\n",
      "Current iteration=898, loss=25850.183460480315\n",
      "Current iteration=899, loss=25850.089909427355\n",
      "Current iteration=900, loss=25849.996540362998\n",
      "Current iteration=901, loss=25849.90335267582\n",
      "Current iteration=902, loss=25849.810345757418\n",
      "Current iteration=903, loss=25849.717519002326\n",
      "Current iteration=904, loss=25849.624871808013\n",
      "Current iteration=905, loss=25849.5324035749\n",
      "Current iteration=906, loss=25849.440113706292\n",
      "Current iteration=907, loss=25849.348001608392\n",
      "Current iteration=908, loss=25849.25606669027\n",
      "Current iteration=909, loss=25849.16430836387\n",
      "Current iteration=910, loss=25849.072726043945\n",
      "Current iteration=911, loss=25848.98131914811\n",
      "Current iteration=912, loss=25848.890087096748\n",
      "Current iteration=913, loss=25848.799029313064\n",
      "Current iteration=914, loss=25848.70814522302\n",
      "Current iteration=915, loss=25848.617434255335\n",
      "Current iteration=916, loss=25848.526895841474\n",
      "Current iteration=917, loss=25848.43652941562\n",
      "Current iteration=918, loss=25848.34633441468\n",
      "Current iteration=919, loss=25848.256310278248\n",
      "Current iteration=920, loss=25848.166456448587\n",
      "Current iteration=921, loss=25848.076772370638\n",
      "Current iteration=922, loss=25847.987257491972\n",
      "Current iteration=923, loss=25847.897911262804\n",
      "Current iteration=924, loss=25847.80873313596\n",
      "Current iteration=925, loss=25847.719722566864\n",
      "Current iteration=926, loss=25847.63087901354\n",
      "Current iteration=927, loss=25847.542201936565\n",
      "Current iteration=928, loss=25847.45369079909\n",
      "Current iteration=929, loss=25847.365345066784\n",
      "Current iteration=930, loss=25847.277164207866\n",
      "Current iteration=931, loss=25847.189147693047\n",
      "Current iteration=932, loss=25847.101294995555\n",
      "Current iteration=933, loss=25847.01360559109\n",
      "Current iteration=934, loss=25846.92607895781\n",
      "Current iteration=935, loss=25846.838714576355\n",
      "Current iteration=936, loss=25846.751511929782\n",
      "Current iteration=937, loss=25846.664470503576\n",
      "Current iteration=938, loss=25846.577589785655\n",
      "Current iteration=939, loss=25846.490869266298\n",
      "Current iteration=940, loss=25846.404308438206\n",
      "Current iteration=941, loss=25846.317906796437\n",
      "Current iteration=942, loss=25846.2316638384\n",
      "Current iteration=943, loss=25846.145579063847\n",
      "Current iteration=944, loss=25846.05965197487\n",
      "Current iteration=945, loss=25845.973882075872\n",
      "Current iteration=946, loss=25845.88826887357\n",
      "Current iteration=947, loss=25845.80281187695\n",
      "Current iteration=948, loss=25845.717510597304\n",
      "Current iteration=949, loss=25845.63236454816\n",
      "Current iteration=950, loss=25845.54737324531\n",
      "Current iteration=951, loss=25845.462536206796\n",
      "Current iteration=952, loss=25845.377852952864\n",
      "Current iteration=953, loss=25845.293323005986\n",
      "Current iteration=954, loss=25845.208945890838\n",
      "Current iteration=955, loss=25845.12472113427\n",
      "Current iteration=956, loss=25845.04064826532\n",
      "Current iteration=957, loss=25844.956726815195\n",
      "Current iteration=958, loss=25844.87295631723\n",
      "Current iteration=959, loss=25844.789336306923\n",
      "Current iteration=960, loss=25844.705866321878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=961, loss=25844.62254590183\n",
      "Current iteration=962, loss=25844.539374588618\n",
      "Current iteration=963, loss=25844.45635192615\n",
      "Current iteration=964, loss=25844.373477460438\n",
      "Current iteration=965, loss=25844.29075073955\n",
      "Current iteration=966, loss=25844.2081713136\n",
      "Current iteration=967, loss=25844.125738734772\n",
      "Current iteration=968, loss=25844.04345255725\n",
      "Current iteration=969, loss=25843.961312337273\n",
      "Current iteration=970, loss=25843.879317633062\n",
      "Current iteration=971, loss=25843.79746800485\n",
      "Current iteration=972, loss=25843.71576301485\n",
      "Current iteration=973, loss=25843.634202227266\n",
      "Current iteration=974, loss=25843.552785208245\n",
      "Current iteration=975, loss=25843.471511525902\n",
      "Current iteration=976, loss=25843.39038075029\n",
      "Current iteration=977, loss=25843.309392453404\n",
      "Current iteration=978, loss=25843.228546209142\n",
      "Current iteration=979, loss=25843.147841593323\n",
      "Current iteration=980, loss=25843.06727818368\n",
      "Current iteration=981, loss=25842.986855559804\n",
      "Current iteration=982, loss=25842.906573303186\n",
      "Current iteration=983, loss=25842.82643099719\n",
      "Current iteration=984, loss=25842.746428227012\n",
      "Current iteration=985, loss=25842.666564579726\n",
      "Current iteration=986, loss=25842.586839644224\n",
      "Current iteration=987, loss=25842.507253011223\n",
      "Current iteration=988, loss=25842.427804273277\n",
      "Current iteration=989, loss=25842.348493024725\n",
      "Current iteration=990, loss=25842.269318861716\n",
      "Current iteration=991, loss=25842.190281382173\n",
      "Current iteration=992, loss=25842.111380185812\n",
      "Current iteration=993, loss=25842.032614874104\n",
      "Current iteration=994, loss=25841.953985050284\n",
      "Current iteration=995, loss=25841.875490319337\n",
      "Current iteration=996, loss=25841.79713028797\n",
      "Current iteration=997, loss=25841.718904564634\n",
      "Current iteration=998, loss=25841.6408127595\n",
      "Current iteration=999, loss=25841.56285448443\n",
      "Current iteration=1000, loss=25841.485029353003\n",
      "Current iteration=1001, loss=25841.40733698049\n",
      "Current iteration=1002, loss=25841.329776983832\n",
      "Current iteration=1003, loss=25841.252348981645\n",
      "Current iteration=1004, loss=25841.17505259422\n",
      "Current iteration=1005, loss=25841.097887443477\n",
      "Current iteration=1006, loss=25841.020853153008\n",
      "Current iteration=1007, loss=25840.943949348017\n",
      "Current iteration=1008, loss=25840.867175655352\n",
      "Current iteration=1009, loss=25840.790531703475\n",
      "Current iteration=1010, loss=25840.714017122453\n",
      "Current iteration=1011, loss=25840.637631543956\n",
      "Current iteration=1012, loss=25840.561374601235\n",
      "Current iteration=1013, loss=25840.485245929143\n",
      "Current iteration=1014, loss=25840.409245164094\n",
      "Current iteration=1015, loss=25840.33337194407\n",
      "Current iteration=1016, loss=25840.257625908613\n",
      "Current iteration=1017, loss=25840.182006698808\n",
      "Current iteration=1018, loss=25840.106513957286\n",
      "Current iteration=1019, loss=25840.031147328205\n",
      "Current iteration=1020, loss=25839.95590645726\n",
      "Current iteration=1021, loss=25839.88079099163\n",
      "Current iteration=1022, loss=25839.805800580034\n",
      "Current iteration=1023, loss=25839.73093487268\n",
      "Current iteration=1024, loss=25839.65619352126\n",
      "Current iteration=1025, loss=25839.581576178956\n",
      "Current iteration=1026, loss=25839.507082500408\n",
      "Current iteration=1027, loss=25839.432712141755\n",
      "Current iteration=1028, loss=25839.35846476056\n",
      "Current iteration=1029, loss=25839.28434001587\n",
      "Current iteration=1030, loss=25839.210337568144\n",
      "Current iteration=1031, loss=25839.1364570793\n",
      "Current iteration=1032, loss=25839.062698212663\n",
      "Current iteration=1033, loss=25838.98906063301\n",
      "Current iteration=1034, loss=25838.91554400649\n",
      "Current iteration=1035, loss=25838.842148000684\n",
      "Current iteration=1036, loss=25838.76887228456\n",
      "Current iteration=1037, loss=25838.695716528484\n",
      "Current iteration=1038, loss=25838.622680404205\n",
      "Current iteration=1039, loss=25838.549763584826\n",
      "Current iteration=1040, loss=25838.47696574485\n",
      "Current iteration=1041, loss=25838.40428656011\n",
      "Current iteration=1042, loss=25838.331725707812\n",
      "Current iteration=1043, loss=25838.259282866504\n",
      "Current iteration=1044, loss=25838.18695771606\n",
      "Current iteration=1045, loss=25838.114749937715\n",
      "Current iteration=1046, loss=25838.04265921399\n",
      "Current iteration=1047, loss=25837.970685228753\n",
      "Current iteration=1048, loss=25837.898827667166\n",
      "Current iteration=1049, loss=25837.827086215715\n",
      "Current iteration=1050, loss=25837.75546056215\n",
      "Current iteration=1051, loss=25837.68395039554\n",
      "Current iteration=1052, loss=25837.612555406224\n",
      "Current iteration=1053, loss=25837.541275285825\n",
      "Current iteration=1054, loss=25837.470109727226\n",
      "Current iteration=1055, loss=25837.399058424577\n",
      "Current iteration=1056, loss=25837.32812107328\n",
      "Current iteration=1057, loss=25837.257297369993\n",
      "Current iteration=1058, loss=25837.186587012617\n",
      "Current iteration=1059, loss=25837.11598970028\n",
      "Current iteration=1060, loss=25837.04550513335\n",
      "Current iteration=1061, loss=25836.975133013417\n",
      "Current iteration=1062, loss=25836.904873043273\n",
      "Current iteration=1063, loss=25836.834724926954\n",
      "Current iteration=1064, loss=25836.764688369654\n",
      "Current iteration=1065, loss=25836.69476307782\n",
      "Current iteration=1066, loss=25836.624948759032\n",
      "Current iteration=1067, loss=25836.555245122097\n",
      "Current iteration=1068, loss=25836.485651876992\n",
      "Current iteration=1069, loss=25836.416168734846\n",
      "Current iteration=1070, loss=25836.34679540799\n",
      "Current iteration=1071, loss=25836.277531609892\n",
      "Current iteration=1072, loss=25836.20837705518\n",
      "Current iteration=1073, loss=25836.139331459628\n",
      "Current iteration=1074, loss=25836.07039454016\n",
      "Current iteration=1075, loss=25836.001566014824\n",
      "Current iteration=1076, loss=25835.93284560282\n",
      "Current iteration=1077, loss=25835.864233024455\n",
      "Current iteration=1078, loss=25835.795728001158\n",
      "Current iteration=1079, loss=25835.72733025548\n",
      "Current iteration=1080, loss=25835.65903951107\n",
      "Current iteration=1081, loss=25835.59085549268\n",
      "Current iteration=1082, loss=25835.522777926166\n",
      "Current iteration=1083, loss=25835.454806538466\n",
      "Current iteration=1084, loss=25835.386941057615\n",
      "Current iteration=1085, loss=25835.319181212704\n",
      "Current iteration=1086, loss=25835.25152673393\n",
      "Current iteration=1087, loss=25835.183977352528\n",
      "Current iteration=1088, loss=25835.116532800806\n",
      "Current iteration=1089, loss=25835.049192812145\n",
      "Current iteration=1090, loss=25834.981957120952\n",
      "Current iteration=1091, loss=25834.914825462693\n",
      "Current iteration=1092, loss=25834.847797573893\n",
      "Current iteration=1093, loss=25834.780873192078\n",
      "Current iteration=1094, loss=25834.714052055824\n",
      "Current iteration=1095, loss=25834.647333904726\n",
      "Current iteration=1096, loss=25834.580718479414\n",
      "Current iteration=1097, loss=25834.51420552151\n",
      "Current iteration=1098, loss=25834.447794773656\n",
      "Current iteration=1099, loss=25834.381485979513\n",
      "Current iteration=1100, loss=25834.31527888371\n",
      "Current iteration=1101, loss=25834.249173231896\n",
      "Current iteration=1102, loss=25834.183168770698\n",
      "Current iteration=1103, loss=25834.117265247733\n",
      "Current iteration=1104, loss=25834.05146241159\n",
      "Current iteration=1105, loss=25833.985760011834\n",
      "Current iteration=1106, loss=25833.920157799013\n",
      "Current iteration=1107, loss=25833.85465552461\n",
      "Current iteration=1108, loss=25833.789252941096\n",
      "Current iteration=1109, loss=25833.723949801883\n",
      "Current iteration=1110, loss=25833.658745861332\n",
      "Current iteration=1111, loss=25833.593640874755\n",
      "Current iteration=1112, loss=25833.528634598402\n",
      "Current iteration=1113, loss=25833.463726789447\n",
      "Current iteration=1114, loss=25833.39891720602\n",
      "Current iteration=1115, loss=25833.334205607152\n",
      "Current iteration=1116, loss=25833.269591752804\n",
      "Current iteration=1117, loss=25833.205075403865\n",
      "Current iteration=1118, loss=25833.140656322114\n",
      "Current iteration=1119, loss=25833.076334270263\n",
      "Current iteration=1120, loss=25833.0121090119\n",
      "Current iteration=1121, loss=25832.947980311532\n",
      "Current iteration=1122, loss=25832.883947934555\n",
      "Current iteration=1123, loss=25832.820011647247\n",
      "Current iteration=1124, loss=25832.75617121678\n",
      "Current iteration=1125, loss=25832.692426411202\n",
      "Current iteration=1126, loss=25832.628776999445\n",
      "Current iteration=1127, loss=25832.5652227513\n",
      "Current iteration=1128, loss=25832.501763437427\n",
      "Current iteration=1129, loss=25832.43839882936\n",
      "Current iteration=1130, loss=25832.37512869949\n",
      "Current iteration=1131, loss=25832.311952821048\n",
      "Current iteration=1132, loss=25832.24887096814\n",
      "Current iteration=1133, loss=25832.185882915688\n",
      "Current iteration=1134, loss=25832.122988439485\n",
      "Current iteration=1135, loss=25832.060187316147\n",
      "Current iteration=1136, loss=25831.997479323123\n",
      "Current iteration=1137, loss=25831.934864238687\n",
      "Current iteration=1138, loss=25831.872341841958\n",
      "Current iteration=1139, loss=25831.809911912856\n",
      "Current iteration=1140, loss=25831.747574232137\n",
      "Current iteration=1141, loss=25831.68532858135\n",
      "Current iteration=1142, loss=25831.62317474286\n",
      "Current iteration=1143, loss=25831.561112499854\n",
      "Current iteration=1144, loss=25831.499141636294\n",
      "Current iteration=1145, loss=25831.43726193696\n",
      "Current iteration=1146, loss=25831.375473187418\n",
      "Current iteration=1147, loss=25831.313775174014\n",
      "Current iteration=1148, loss=25831.25216768389\n",
      "Current iteration=1149, loss=25831.190650504985\n",
      "Current iteration=1150, loss=25831.12922342599\n",
      "Current iteration=1151, loss=25831.067886236367\n",
      "Current iteration=1152, loss=25831.00663872638\n",
      "Current iteration=1153, loss=25830.94548068704\n",
      "Current iteration=1154, loss=25830.884411910116\n",
      "Current iteration=1155, loss=25830.823432188146\n",
      "Current iteration=1156, loss=25830.762541314416\n",
      "Current iteration=1157, loss=25830.70173908297\n",
      "Current iteration=1158, loss=25830.641025288613\n",
      "Current iteration=1159, loss=25830.58039972686\n",
      "Current iteration=1160, loss=25830.519862193996\n",
      "Current iteration=1161, loss=25830.45941248704\n",
      "Current iteration=1162, loss=25830.39905040373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=1163, loss=25830.33877574255\n",
      "Current iteration=1164, loss=25830.2785883027\n",
      "Current iteration=1165, loss=25830.218487884118\n",
      "Current iteration=1166, loss=25830.158474287444\n",
      "Current iteration=1167, loss=25830.098547314043\n",
      "Current iteration=1168, loss=25830.038706765987\n",
      "Current iteration=1169, loss=25829.978952446076\n",
      "Current iteration=1170, loss=25829.91928415779\n",
      "Current iteration=1171, loss=25829.859701705336\n",
      "Current iteration=1172, loss=25829.800204893596\n",
      "Current iteration=1173, loss=25829.740793528166\n",
      "Current iteration=1174, loss=25829.681467415336\n",
      "Current iteration=1175, loss=25829.622226362066\n",
      "Current iteration=1176, loss=25829.563070176024\n",
      "Current iteration=1177, loss=25829.50399866554\n",
      "Current iteration=1178, loss=25829.44501163965\n",
      "Current iteration=1179, loss=25829.386108908035\n",
      "Current iteration=1180, loss=25829.327290281075\n",
      "Current iteration=1181, loss=25829.268555569804\n",
      "Current iteration=1182, loss=25829.209904585936\n",
      "Current iteration=1183, loss=25829.15133714183\n",
      "Current iteration=1184, loss=25829.092853050523\n",
      "Current iteration=1185, loss=25829.034452125692\n",
      "Current iteration=1186, loss=25828.97613418169\n",
      "Current iteration=1187, loss=25828.9178990335\n",
      "Current iteration=1188, loss=25828.859746496768\n",
      "Current iteration=1189, loss=25828.80167638777\n",
      "Current iteration=1190, loss=25828.74368852344\n",
      "Current iteration=1191, loss=25828.685782721335\n",
      "Current iteration=1192, loss=25828.62795879967\n",
      "Current iteration=1193, loss=25828.57021657726\n",
      "Current iteration=1194, loss=25828.51255587357\n",
      "Current iteration=1195, loss=25828.454976508696\n",
      "Current iteration=1196, loss=25828.39747830336\n",
      "Current iteration=1197, loss=25828.34006107888\n",
      "Current iteration=1198, loss=25828.28272465721\n",
      "Current iteration=1199, loss=25828.225468860925\n",
      "Current iteration=1200, loss=25828.168293513198\n",
      "Current iteration=1201, loss=25828.11119843782\n",
      "Current iteration=1202, loss=25828.05418345919\n",
      "Current iteration=1203, loss=25827.997248402295\n",
      "Current iteration=1204, loss=25827.940393092747\n",
      "Current iteration=1205, loss=25827.883617356736\n",
      "Current iteration=1206, loss=25827.826921021056\n",
      "Current iteration=1207, loss=25827.770303913083\n",
      "Current iteration=1208, loss=25827.713765860815\n",
      "Current iteration=1209, loss=25827.657306692792\n",
      "Current iteration=1210, loss=25827.60092623817\n",
      "Current iteration=1211, loss=25827.544624326678\n",
      "Current iteration=1212, loss=25827.488400788614\n",
      "Current iteration=1213, loss=25827.432255454863\n",
      "Current iteration=1214, loss=25827.376188156886\n",
      "Current iteration=1215, loss=25827.320198726706\n",
      "Current iteration=1216, loss=25827.264286996917\n",
      "Current iteration=1217, loss=25827.208452800674\n",
      "Current iteration=1218, loss=25827.152695971712\n",
      "Current iteration=1219, loss=25827.097016344313\n",
      "Current iteration=1220, loss=25827.0414137533\n",
      "Current iteration=1221, loss=25826.985888034098\n",
      "Current iteration=1222, loss=25826.930439022635\n",
      "Current iteration=1223, loss=25826.875066555418\n",
      "Current iteration=1224, loss=25826.81977046949\n",
      "Current iteration=1225, loss=25826.764550602453\n",
      "Current iteration=1226, loss=25826.70940679243\n",
      "Current iteration=1227, loss=25826.654338878107\n",
      "Current iteration=1228, loss=25826.599346698677\n",
      "Current iteration=1229, loss=25826.544430093916\n",
      "Current iteration=1230, loss=25826.48958890408\n",
      "Current iteration=1231, loss=25826.434822969997\n",
      "Current iteration=1232, loss=25826.38013213299\n",
      "Current iteration=1233, loss=25826.325516234927\n",
      "Current iteration=1234, loss=25826.270975118205\n",
      "Current iteration=1235, loss=25826.21650862572\n",
      "Current iteration=1236, loss=25826.162116600903\n",
      "Current iteration=1237, loss=25826.107798887704\n",
      "Current iteration=1238, loss=25826.053555330567\n",
      "Current iteration=1239, loss=25825.99938577446\n",
      "Current iteration=1240, loss=25825.945290064865\n",
      "Current iteration=1241, loss=25825.891268047766\n",
      "Current iteration=1242, loss=25825.837319569644\n",
      "Current iteration=1243, loss=25825.783444477493\n",
      "Current iteration=1244, loss=25825.729642618804\n",
      "Current iteration=1245, loss=25825.675913841562\n",
      "Current iteration=1246, loss=25825.622257994248\n",
      "Current iteration=1247, loss=25825.568674925842\n",
      "Current iteration=1248, loss=25825.515164485812\n",
      "Current iteration=1249, loss=25825.461726524103\n",
      "Current iteration=1250, loss=25825.408360891168\n",
      "Current iteration=1251, loss=25825.355067437937\n",
      "Current iteration=1252, loss=25825.301846015802\n",
      "Current iteration=1253, loss=25825.248696476665\n",
      "Current iteration=1254, loss=25825.195618672897\n",
      "Current iteration=1255, loss=25825.142612457337\n",
      "Current iteration=1256, loss=25825.08967768329\n",
      "Current iteration=1257, loss=25825.036814204555\n",
      "Current iteration=1258, loss=25824.984021875393\n",
      "Current iteration=1259, loss=25824.93130055052\n",
      "Current iteration=1260, loss=25824.878650085127\n",
      "Current iteration=1261, loss=25824.826070334875\n",
      "Current iteration=1262, loss=25824.77356115588\n",
      "Current iteration=1263, loss=25824.721122404702\n",
      "Current iteration=1264, loss=25824.66875393839\n",
      "Current iteration=1265, loss=25824.616455614414\n",
      "Current iteration=1266, loss=25824.564227290728\n",
      "Current iteration=1267, loss=25824.51206882571\n",
      "Current iteration=1268, loss=25824.459980078205\n",
      "Current iteration=1269, loss=25824.407960907505\n",
      "Current iteration=1270, loss=25824.35601117333\n",
      "Current iteration=1271, loss=25824.30413073587\n",
      "Current iteration=1272, loss=25824.25231945573\n",
      "Current iteration=1273, loss=25824.200577193966\n",
      "Current iteration=1274, loss=25824.148903812078\n",
      "Current iteration=1275, loss=25824.09729917199\n",
      "Current iteration=1276, loss=25824.045763136062\n",
      "Current iteration=1277, loss=25823.994295567092\n",
      "Current iteration=1278, loss=25823.94289632831\n",
      "Current iteration=1279, loss=25823.89156528336\n",
      "Current iteration=1280, loss=25823.84030229631\n",
      "Current iteration=1281, loss=25823.78910723169\n",
      "Current iteration=1282, loss=25823.737979954392\n",
      "Current iteration=1283, loss=25823.68692032978\n",
      "Current iteration=1284, loss=25823.635928223615\n",
      "Current iteration=1285, loss=25823.585003502078\n",
      "Current iteration=1286, loss=25823.534146031754\n",
      "Current iteration=1287, loss=25823.483355679662\n",
      "Current iteration=1288, loss=25823.432632313223\n",
      "Current iteration=1289, loss=25823.38197580027\n",
      "Current iteration=1290, loss=25823.33138600902\n",
      "Current iteration=1291, loss=25823.28086280814\n",
      "Current iteration=1292, loss=25823.230406066672\n",
      "Current iteration=1293, loss=25823.180015654067\n",
      "Current iteration=1294, loss=25823.129691440172\n",
      "Current iteration=1295, loss=25823.079433295243\n",
      "Current iteration=1296, loss=25823.02924108993\n",
      "Current iteration=1297, loss=25822.979114695274\n",
      "Current iteration=1298, loss=25822.929053982713\n",
      "Current iteration=1299, loss=25822.879058824084\n",
      "Current iteration=1300, loss=25822.829129091606\n",
      "Current iteration=1301, loss=25822.779264657886\n",
      "Current iteration=1302, loss=25822.729465395925\n",
      "Current iteration=1303, loss=25822.67973117911\n",
      "Current iteration=1304, loss=25822.630061881202\n",
      "Current iteration=1305, loss=25822.580457376367\n",
      "Current iteration=1306, loss=25822.53091753912\n",
      "Current iteration=1307, loss=25822.481442244378\n",
      "Current iteration=1308, loss=25822.43203136743\n",
      "Current iteration=1309, loss=25822.382684783934\n",
      "Current iteration=1310, loss=25822.333402369943\n",
      "Current iteration=1311, loss=25822.284184001855\n",
      "Current iteration=1312, loss=25822.23502955646\n",
      "Current iteration=1313, loss=25822.18593891091\n",
      "Current iteration=1314, loss=25822.136911942718\n",
      "Current iteration=1315, loss=25822.087948529777\n",
      "Current iteration=1316, loss=25822.03904855035\n",
      "Current iteration=1317, loss=25821.990211883036\n",
      "Current iteration=1318, loss=25821.941438406815\n",
      "Current iteration=1319, loss=25821.892728001025\n",
      "Current iteration=1320, loss=25821.84408054537\n",
      "Current iteration=1321, loss=25821.795495919905\n",
      "Current iteration=1322, loss=25821.746974005033\n",
      "Current iteration=1323, loss=25821.69851468152\n",
      "Current iteration=1324, loss=25821.650117830486\n",
      "Current iteration=1325, loss=25821.6017833334\n",
      "Current iteration=1326, loss=25821.55351107208\n",
      "Current iteration=1327, loss=25821.505300928693\n",
      "Current iteration=1328, loss=25821.457152785755\n",
      "Current iteration=1329, loss=25821.409066526125\n",
      "Current iteration=1330, loss=25821.361042033008\n",
      "Current iteration=1331, loss=25821.313079189957\n",
      "Current iteration=1332, loss=25821.265177880847\n",
      "Current iteration=1333, loss=25821.21733798992\n",
      "Current iteration=1334, loss=25821.169559401733\n",
      "Current iteration=1335, loss=25821.121842001194\n",
      "Current iteration=1336, loss=25821.074185673548\n",
      "Current iteration=1337, loss=25821.02659030436\n",
      "Current iteration=1338, loss=25820.979055779542\n",
      "Current iteration=1339, loss=25820.93158198533\n",
      "Current iteration=1340, loss=25820.88416880829\n",
      "Current iteration=1341, loss=25820.836816135336\n",
      "Current iteration=1342, loss=25820.78952385367\n",
      "Current iteration=1343, loss=25820.74229185086\n",
      "Current iteration=1344, loss=25820.69512001477\n",
      "Current iteration=1345, loss=25820.648008233613\n",
      "Current iteration=1346, loss=25820.6009563959\n",
      "Current iteration=1347, loss=25820.55396439048\n",
      "Current iteration=1348, loss=25820.507032106507\n",
      "Current iteration=1349, loss=25820.460159433467\n",
      "Current iteration=1350, loss=25820.41334626116\n",
      "Current iteration=1351, loss=25820.366592479688\n",
      "Current iteration=1352, loss=25820.31989797949\n",
      "Current iteration=1353, loss=25820.273262651295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=1354, loss=25820.226686386166\n",
      "Current iteration=1355, loss=25820.18016907546\n",
      "Current iteration=1356, loss=25820.133710610848\n",
      "Current iteration=1357, loss=25820.087310884304\n",
      "Current iteration=1358, loss=25820.040969788126\n",
      "Current iteration=1359, loss=25819.99468721489\n",
      "Current iteration=1360, loss=25819.94846305751\n",
      "Current iteration=1361, loss=25819.902297209173\n",
      "Current iteration=1362, loss=25819.85618956339\n",
      "Current iteration=1363, loss=25819.81014001395\n",
      "Current iteration=1364, loss=25819.76414845496\n",
      "Current iteration=1365, loss=25819.71821478082\n",
      "Current iteration=1366, loss=25819.672338886226\n",
      "Current iteration=1367, loss=25819.62652066617\n",
      "Current iteration=1368, loss=25819.58076001593\n",
      "Current iteration=1369, loss=25819.535056831097\n",
      "Current iteration=1370, loss=25819.489411007537\n",
      "Current iteration=1371, loss=25819.443822441415\n",
      "Current iteration=1372, loss=25819.39829102919\n",
      "Current iteration=1373, loss=25819.352816667582\n",
      "Current iteration=1374, loss=25819.30739925365\n",
      "Current iteration=1375, loss=25819.262038684687\n",
      "Current iteration=1376, loss=25819.216734858303\n",
      "Current iteration=1377, loss=25819.171487672378\n",
      "Current iteration=1378, loss=25819.12629702509\n",
      "Current iteration=1379, loss=25819.081162814884\n",
      "Current iteration=1380, loss=25819.03608494049\n",
      "Current iteration=1381, loss=25818.991063300913\n",
      "Current iteration=1382, loss=25818.94609779545\n",
      "Current iteration=1383, loss=25818.901188323667\n",
      "Current iteration=1384, loss=25818.856334785403\n",
      "Current iteration=1385, loss=25818.811537080783\n",
      "Current iteration=1386, loss=25818.766795110198\n",
      "Current iteration=1387, loss=25818.722108774302\n",
      "Current iteration=1388, loss=25818.677477974048\n",
      "Current iteration=1389, loss=25818.632902610636\n",
      "Current iteration=1390, loss=25818.588382585545\n",
      "Current iteration=1391, loss=25818.54391780053\n",
      "Current iteration=1392, loss=25818.49950815759\n",
      "Current iteration=1393, loss=25818.45515355902\n",
      "Current iteration=1394, loss=25818.410853907368\n",
      "Current iteration=1395, loss=25818.36660910544\n",
      "Current iteration=1396, loss=25818.32241905631\n",
      "Current iteration=1397, loss=25818.278283663327\n",
      "Current iteration=1398, loss=25818.234202830085\n",
      "Current iteration=1399, loss=25818.19017646044\n",
      "Current iteration=1400, loss=25818.14620445851\n",
      "Current iteration=1401, loss=25818.102286728692\n",
      "Current iteration=1402, loss=25818.058423175604\n",
      "Current iteration=1403, loss=25818.014613704137\n",
      "Current iteration=1404, loss=25817.97085821946\n",
      "Current iteration=1405, loss=25817.927156626956\n",
      "Current iteration=1406, loss=25817.88350883229\n",
      "Current iteration=1407, loss=25817.839914741362\n",
      "Current iteration=1408, loss=25817.796374260342\n",
      "Current iteration=1409, loss=25817.75288729564\n",
      "Current iteration=1410, loss=25817.709453753916\n",
      "Current iteration=1411, loss=25817.666073542066\n",
      "Current iteration=1412, loss=25817.622746567267\n",
      "Current iteration=1413, loss=25817.579472736914\n",
      "Current iteration=1414, loss=25817.536251958652\n",
      "Current iteration=1415, loss=25817.493084140384\n",
      "Current iteration=1416, loss=25817.449969190246\n",
      "Current iteration=1417, loss=25817.40690701662\n",
      "Current iteration=1418, loss=25817.36389752812\n",
      "Current iteration=1419, loss=25817.320940633632\n",
      "Current iteration=1420, loss=25817.278036242238\n",
      "Current iteration=1421, loss=25817.2351842633\n",
      "Current iteration=1422, loss=25817.192384606387\n",
      "Current iteration=1423, loss=25817.149637181326\n",
      "Current iteration=1424, loss=25817.10694189818\n",
      "Current iteration=1425, loss=25817.06429866723\n",
      "Current iteration=1426, loss=25817.021707399013\n",
      "Current iteration=1427, loss=25816.979168004284\n",
      "Current iteration=1428, loss=25816.936680394036\n",
      "Current iteration=1429, loss=25816.8942444795\n",
      "Current iteration=1430, loss=25816.851860172133\n",
      "Current iteration=1431, loss=25816.80952738363\n",
      "Current iteration=1432, loss=25816.767246025895\n",
      "Current iteration=1433, loss=25816.72501601108\n",
      "Current iteration=1434, loss=25816.682837251563\n",
      "Current iteration=1435, loss=25816.640709659943\n",
      "Current iteration=1436, loss=25816.59863314905\n",
      "Current iteration=1437, loss=25816.55660763193\n",
      "Current iteration=1438, loss=25816.514633021863\n",
      "Current iteration=1439, loss=25816.472709232352\n",
      "Current iteration=1440, loss=25816.43083617712\n",
      "Current iteration=1441, loss=25816.38901377011\n",
      "Current iteration=1442, loss=25816.347241925483\n",
      "Current iteration=1443, loss=25816.30552055764\n",
      "Current iteration=1444, loss=25816.26384958117\n",
      "Current iteration=1445, loss=25816.222228910905\n",
      "Current iteration=1446, loss=25816.180658461897\n",
      "Current iteration=1447, loss=25816.139138149385\n",
      "Current iteration=1448, loss=25816.097667888855\n",
      "Current iteration=1449, loss=25816.056247596003\n",
      "Current iteration=1450, loss=25816.014877186728\n",
      "Current iteration=1451, loss=25815.973556577148\n",
      "Current iteration=1452, loss=25815.932285683593\n",
      "Current iteration=1453, loss=25815.89106442262\n",
      "Current iteration=1454, loss=25815.849892710972\n",
      "Current iteration=1455, loss=25815.808770465614\n",
      "Current iteration=1456, loss=25815.76769760373\n",
      "Current iteration=1457, loss=25815.726674042708\n",
      "Current iteration=1458, loss=25815.68569970013\n",
      "Current iteration=1459, loss=25815.644774493805\n",
      "Current iteration=1460, loss=25815.60389834174\n",
      "Current iteration=1461, loss=25815.563071162145\n",
      "Current iteration=1462, loss=25815.522292873444\n",
      "Current iteration=1463, loss=25815.48156339426\n",
      "Current iteration=1464, loss=25815.440882643412\n",
      "Current iteration=1465, loss=25815.40025053994\n",
      "Current iteration=1466, loss=25815.359667003075\n",
      "Current iteration=1467, loss=25815.31913195225\n",
      "Current iteration=1468, loss=25815.278645307102\n",
      "Current iteration=1469, loss=25815.23820698746\n",
      "Current iteration=1470, loss=25815.197816913365\n",
      "Current iteration=1471, loss=25815.157475005042\n",
      "Current iteration=1472, loss=25815.11718118294\n",
      "Current iteration=1473, loss=25815.076935367666\n",
      "Current iteration=1474, loss=25815.03673748006\n",
      "Current iteration=1475, loss=25814.99658744113\n",
      "Current iteration=1476, loss=25814.956485172104\n",
      "Current iteration=1477, loss=25814.916430594385\n",
      "Current iteration=1478, loss=25814.87642362958\n",
      "Current iteration=1479, loss=25814.836464199478\n",
      "Current iteration=1480, loss=25814.79655222608\n",
      "Current iteration=1481, loss=25814.75668763155\n",
      "Current iteration=1482, loss=25814.716870338274\n",
      "Current iteration=1483, loss=25814.677100268807\n",
      "Current iteration=1484, loss=25814.6373773459\n",
      "Current iteration=1485, loss=25814.597701492497\n",
      "Current iteration=1486, loss=25814.558072631724\n",
      "Current iteration=1487, loss=25814.518490686893\n",
      "Current iteration=1488, loss=25814.4789555815\n",
      "Current iteration=1489, loss=25814.439467239252\n",
      "Current iteration=1490, loss=25814.40002558401\n",
      "Current iteration=1491, loss=25814.36063053984\n",
      "Current iteration=1492, loss=25814.321282030982\n",
      "Current iteration=1493, loss=25814.28197998185\n",
      "Current iteration=1494, loss=25814.242724317068\n",
      "Current iteration=1495, loss=25814.203514961428\n",
      "Current iteration=1496, loss=25814.1643518399\n",
      "Current iteration=1497, loss=25814.125234877632\n",
      "Current iteration=1498, loss=25814.086163999964\n",
      "Current iteration=1499, loss=25814.047139132406\n",
      "Current iteration=1500, loss=25814.00816020066\n",
      "Current iteration=1501, loss=25813.96922713059\n",
      "Current iteration=1502, loss=25813.930339848244\n",
      "Current iteration=1503, loss=25813.891498279852\n",
      "Current iteration=1504, loss=25813.852702351818\n",
      "Current iteration=1505, loss=25813.813951990716\n",
      "Current iteration=1506, loss=25813.7752471233\n",
      "Current iteration=1507, loss=25813.736587676503\n",
      "Current iteration=1508, loss=25813.697973577422\n",
      "Current iteration=1509, loss=25813.659404753336\n",
      "Current iteration=1510, loss=25813.620881131697\n",
      "Current iteration=1511, loss=25813.582402640124\n",
      "Current iteration=1512, loss=25813.543969206414\n",
      "Current iteration=1513, loss=25813.505580758516\n",
      "Current iteration=1514, loss=25813.467237224577\n",
      "Current iteration=1515, loss=25813.4289385329\n",
      "Current iteration=1516, loss=25813.39068461195\n",
      "Current iteration=1517, loss=25813.352475390384\n",
      "Current iteration=1518, loss=25813.314310797\n",
      "Current iteration=1519, loss=25813.276190760775\n",
      "Current iteration=1520, loss=25813.238115210857\n",
      "Current iteration=1521, loss=25813.20008407656\n",
      "Current iteration=1522, loss=25813.16209728736\n",
      "Current iteration=1523, loss=25813.124154772893\n",
      "Current iteration=1524, loss=25813.086256462968\n",
      "Current iteration=1525, loss=25813.048402287557\n",
      "Current iteration=1526, loss=25813.010592176794\n",
      "Current iteration=1527, loss=25812.972826060977\n",
      "Current iteration=1528, loss=25812.93510387056\n",
      "Current iteration=1529, loss=25812.897425536165\n",
      "Current iteration=1530, loss=25812.859790988576\n",
      "Current iteration=1531, loss=25812.822200158742\n",
      "Current iteration=1532, loss=25812.784652977756\n",
      "Current iteration=1533, loss=25812.747149376886\n",
      "Current iteration=1534, loss=25812.709689287556\n",
      "Current iteration=1535, loss=25812.67227264134\n",
      "Current iteration=1536, loss=25812.63489936998\n",
      "Current iteration=1537, loss=25812.597569405374\n",
      "Current iteration=1538, loss=25812.560282679577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=1539, loss=25812.523039124786\n",
      "Current iteration=1540, loss=25812.485838673376\n",
      "Current iteration=1541, loss=25812.44868125787\n",
      "Current iteration=1542, loss=25812.41156681093\n",
      "Current iteration=1543, loss=25812.374495265394\n",
      "Current iteration=1544, loss=25812.337466554243\n",
      "Current iteration=1545, loss=25812.300480610622\n",
      "Current iteration=1546, loss=25812.26353736781\n",
      "Current iteration=1547, loss=25812.226636759253\n",
      "Current iteration=1548, loss=25812.189778718537\n",
      "Current iteration=1549, loss=25812.152963179415\n",
      "Current iteration=1550, loss=25812.11619007578\n",
      "Current iteration=1551, loss=25812.079459341672\n",
      "Current iteration=1552, loss=25812.042770911285\n",
      "Current iteration=1553, loss=25812.00612471897\n",
      "Current iteration=1554, loss=25811.96952069922\n",
      "Current iteration=1555, loss=25811.932958786667\n",
      "Current iteration=1556, loss=25811.89643891611\n",
      "Current iteration=1557, loss=25811.859961022466\n",
      "Current iteration=1558, loss=25811.82352504084\n",
      "Current iteration=1559, loss=25811.787130906447\n",
      "Current iteration=1560, loss=25811.750778554662\n",
      "Current iteration=1561, loss=25811.714467921007\n",
      "Current iteration=1562, loss=25811.67819894114\n",
      "Current iteration=1563, loss=25811.641971550886\n",
      "Current iteration=1564, loss=25811.605785686177\n",
      "Current iteration=1565, loss=25811.569641283124\n",
      "Current iteration=1566, loss=25811.53353827795\n",
      "Current iteration=1567, loss=25811.49747660705\n",
      "Current iteration=1568, loss=25811.46145620694\n",
      "Current iteration=1569, loss=25811.42547701428\n",
      "Current iteration=1570, loss=25811.38953896589\n",
      "Current iteration=1571, loss=25811.353641998696\n",
      "Current iteration=1572, loss=25811.317786049796\n",
      "Current iteration=1573, loss=25811.281971056407\n",
      "Current iteration=1574, loss=25811.2461969559\n",
      "Current iteration=1575, loss=25811.210463685784\n",
      "Current iteration=1576, loss=25811.17477118368\n",
      "Current iteration=1577, loss=25811.139119387386\n",
      "Current iteration=1578, loss=25811.103508234806\n",
      "Current iteration=1579, loss=25811.067937664\n",
      "Current iteration=1580, loss=25811.03240761316\n",
      "Current iteration=1581, loss=25810.996918020595\n",
      "Current iteration=1582, loss=25810.961468824793\n",
      "Current iteration=1583, loss=25810.926059964324\n",
      "Current iteration=1584, loss=25810.89069137793\n",
      "Current iteration=1585, loss=25810.855363004477\n",
      "Current iteration=1586, loss=25810.820074782965\n",
      "Current iteration=1587, loss=25810.784826652525\n",
      "Current iteration=1588, loss=25810.749618552418\n",
      "Current iteration=1589, loss=25810.714450422045\n",
      "Current iteration=1590, loss=25810.679322200926\n",
      "Current iteration=1591, loss=25810.64423382873\n",
      "Current iteration=1592, loss=25810.60918524526\n",
      "Current iteration=1593, loss=25810.574176390423\n",
      "Current iteration=1594, loss=25810.53920720428\n",
      "Current iteration=1595, loss=25810.504277627013\n",
      "Current iteration=1596, loss=25810.469387598936\n",
      "Current iteration=1597, loss=25810.434537060486\n",
      "Current iteration=1598, loss=25810.399725952244\n",
      "Current iteration=1599, loss=25810.364954214892\n",
      "Current iteration=1600, loss=25810.33022178928\n",
      "Current iteration=1601, loss=25810.29552861634\n",
      "Current iteration=1602, loss=25810.260874637177\n",
      "Current iteration=1603, loss=25810.22625979298\n",
      "Current iteration=1604, loss=25810.19168402509\n",
      "Current iteration=1605, loss=25810.157147274964\n",
      "Current iteration=1606, loss=25810.122649484194\n",
      "Current iteration=1607, loss=25810.08819059449\n",
      "Current iteration=1608, loss=25810.053770547685\n",
      "Current iteration=1609, loss=25810.01938928574\n",
      "Current iteration=1610, loss=25809.985046750735\n",
      "Current iteration=1611, loss=25809.950742884877\n",
      "Current iteration=1612, loss=25809.916477630508\n",
      "Current iteration=1613, loss=25809.882250930066\n",
      "Current iteration=1614, loss=25809.848062726138\n",
      "Current iteration=1615, loss=25809.813912961414\n",
      "Current iteration=1616, loss=25809.779801578705\n",
      "Current iteration=1617, loss=25809.745728520975\n",
      "Current iteration=1618, loss=25809.711693731264\n",
      "Current iteration=1619, loss=25809.67769715276\n",
      "Current iteration=1620, loss=25809.64373872877\n",
      "Current iteration=1621, loss=25809.6098184027\n",
      "Current iteration=1622, loss=25809.575936118104\n",
      "Current iteration=1623, loss=25809.54209181864\n",
      "Current iteration=1624, loss=25809.50828544807\n",
      "Current iteration=1625, loss=25809.47451695031\n",
      "Current iteration=1626, loss=25809.440786269366\n",
      "Current iteration=1627, loss=25809.407093349368\n",
      "Current iteration=1628, loss=25809.373438134564\n",
      "Current iteration=1629, loss=25809.33982056932\n",
      "Current iteration=1630, loss=25809.30624059811\n",
      "Current iteration=1631, loss=25809.272698165543\n",
      "Current iteration=1632, loss=25809.23919321633\n",
      "Current iteration=1633, loss=25809.20572569529\n",
      "Current iteration=1634, loss=25809.172295547374\n",
      "Current iteration=1635, loss=25809.138902717634\n",
      "Current iteration=1636, loss=25809.105547151245\n",
      "Current iteration=1637, loss=25809.072228793495\n",
      "Current iteration=1638, loss=25809.038947589783\n",
      "Current iteration=1639, loss=25809.005703485615\n",
      "Current iteration=1640, loss=25808.972496426617\n",
      "Current iteration=1641, loss=25808.93932635853\n",
      "Current iteration=1642, loss=25808.90619322721\n",
      "Current iteration=1643, loss=25808.873096978612\n",
      "Current iteration=1644, loss=25808.840037558803\n",
      "Current iteration=1645, loss=25808.80701491397\n",
      "Current iteration=1646, loss=25808.77402899042\n",
      "Current iteration=1647, loss=25808.741079734544\n",
      "Current iteration=1648, loss=25808.708167092856\n",
      "Current iteration=1649, loss=25808.675291011994\n",
      "Current iteration=1650, loss=25808.64245143868\n",
      "Current iteration=1651, loss=25808.60964831977\n",
      "Current iteration=1652, loss=25808.576881602206\n",
      "Current iteration=1653, loss=25808.54415123305\n",
      "Current iteration=1654, loss=25808.51145715948\n",
      "Current iteration=1655, loss=25808.478799328754\n",
      "Current iteration=1656, loss=25808.44617768827\n",
      "Current iteration=1657, loss=25808.41359218552\n",
      "Current iteration=1658, loss=25808.381042768095\n",
      "Current iteration=1659, loss=25808.348529383704\n",
      "Current iteration=1660, loss=25808.316051980153\n",
      "Current iteration=1661, loss=25808.28361050536\n",
      "Current iteration=1662, loss=25808.251204907356\n",
      "Current iteration=1663, loss=25808.218835134263\n",
      "Current iteration=1664, loss=25808.186501134296\n",
      "Current iteration=1665, loss=25808.154202855814\n",
      "Current iteration=1666, loss=25808.12194024725\n",
      "Current iteration=1667, loss=25808.089713257148\n",
      "Current iteration=1668, loss=25808.05752183415\n",
      "Current iteration=1669, loss=25808.025365927024\n",
      "Current iteration=1670, loss=25807.993245484606\n",
      "Current iteration=1671, loss=25807.961160455867\n",
      "Current iteration=1672, loss=25807.92911078986\n",
      "Current iteration=1673, loss=25807.897096435754\n",
      "Current iteration=1674, loss=25807.86511734281\n",
      "Current iteration=1675, loss=25807.833173460378\n",
      "Current iteration=1676, loss=25807.801264737947\n",
      "Current iteration=1677, loss=25807.76939112508\n",
      "Current iteration=1678, loss=25807.73755257143\n",
      "Current iteration=1679, loss=25807.705749026776\n",
      "Current iteration=1680, loss=25807.673980440988\n",
      "Current iteration=1681, loss=25807.64224676403\n",
      "Current iteration=1682, loss=25807.61054794597\n",
      "Current iteration=1683, loss=25807.57888393697\n",
      "Current iteration=1684, loss=25807.547254687313\n",
      "Current iteration=1685, loss=25807.51566014734\n",
      "Current iteration=1686, loss=25807.484100267524\n",
      "Current iteration=1687, loss=25807.45257499842\n",
      "Current iteration=1688, loss=25807.421084290698\n",
      "Current iteration=1689, loss=25807.3896280951\n",
      "Current iteration=1690, loss=25807.358206362485\n",
      "Current iteration=1691, loss=25807.3268190438\n",
      "Current iteration=1692, loss=25807.295466090098\n",
      "Current iteration=1693, loss=25807.264147452504\n",
      "Current iteration=1694, loss=25807.23286308227\n",
      "Current iteration=1695, loss=25807.20161293073\n",
      "Current iteration=1696, loss=25807.170396949296\n",
      "Current iteration=1697, loss=25807.13921508951\n",
      "Current iteration=1698, loss=25807.108067302994\n",
      "Current iteration=1699, loss=25807.07695354145\n",
      "Current iteration=1700, loss=25807.04587375668\n",
      "Current iteration=1701, loss=25807.014827900603\n",
      "Current iteration=1702, loss=25806.983815925203\n",
      "Current iteration=1703, loss=25806.952837782577\n",
      "Current iteration=1704, loss=25806.9218934249\n",
      "Current iteration=1705, loss=25806.89098280446\n",
      "Current iteration=1706, loss=25806.86010587361\n",
      "Current iteration=1707, loss=25806.829262584815\n",
      "Current iteration=1708, loss=25806.798452890635\n",
      "Current iteration=1709, loss=25806.767676743704\n",
      "Current iteration=1710, loss=25806.736934096774\n",
      "Current iteration=1711, loss=25806.706224902657\n",
      "Current iteration=1712, loss=25806.67554911428\n",
      "Current iteration=1713, loss=25806.644906684647\n",
      "Current iteration=1714, loss=25806.614297566863\n",
      "Current iteration=1715, loss=25806.583721714123\n",
      "Current iteration=1716, loss=25806.553179079696\n",
      "Current iteration=1717, loss=25806.52266961697\n",
      "Current iteration=1718, loss=25806.492193279388\n",
      "Current iteration=1719, loss=25806.461750020506\n",
      "Current iteration=1720, loss=25806.431339793962\n",
      "Current iteration=1721, loss=25806.40096255349\n",
      "Current iteration=1722, loss=25806.370618252895\n",
      "Current iteration=1723, loss=25806.34030684609\n",
      "Current iteration=1724, loss=25806.310028287066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=1725, loss=25806.279782529895\n",
      "Current iteration=1726, loss=25806.249569528758\n",
      "Current iteration=1727, loss=25806.2193892379\n",
      "Current iteration=1728, loss=25806.18924161167\n",
      "Current iteration=1729, loss=25806.159126604492\n",
      "Current iteration=1730, loss=25806.129044170884\n",
      "Current iteration=1731, loss=25806.098994265445\n",
      "Current iteration=1732, loss=25806.06897684287\n",
      "Current iteration=1733, loss=25806.038991857928\n",
      "Current iteration=1734, loss=25806.009039265482\n",
      "Current iteration=1735, loss=25805.979119020467\n",
      "Current iteration=1736, loss=25805.949231077928\n",
      "Current iteration=1737, loss=25805.91937539297\n",
      "Current iteration=1738, loss=25805.889551920787\n",
      "Current iteration=1739, loss=25805.859760616684\n",
      "Current iteration=1740, loss=25805.83000143601\n",
      "Current iteration=1741, loss=25805.80027433423\n",
      "Current iteration=1742, loss=25805.770579266864\n",
      "Current iteration=1743, loss=25805.740916189552\n",
      "Current iteration=1744, loss=25805.71128505798\n",
      "Current iteration=1745, loss=25805.68168582795\n",
      "Current iteration=1746, loss=25805.652118455313\n",
      "Current iteration=1747, loss=25805.62258289603\n",
      "Current iteration=1748, loss=25805.59307910614\n",
      "Current iteration=1749, loss=25805.563607041746\n",
      "Current iteration=1750, loss=25805.53416665905\n",
      "Current iteration=1751, loss=25805.50475791434\n",
      "Current iteration=1752, loss=25805.47538076397\n",
      "Current iteration=1753, loss=25805.44603516438\n",
      "Current iteration=1754, loss=25805.416721072095\n",
      "Current iteration=1755, loss=25805.387438443722\n",
      "Current iteration=1756, loss=25805.35818723594\n",
      "Current iteration=1757, loss=25805.328967405523\n",
      "Current iteration=1758, loss=25805.299778909302\n",
      "Current iteration=1759, loss=25805.270621704214\n",
      "Current iteration=1760, loss=25805.241495747254\n",
      "Current iteration=1761, loss=25805.212400995522\n",
      "Current iteration=1762, loss=25805.183337406164\n",
      "Current iteration=1763, loss=25805.15430493643\n",
      "Current iteration=1764, loss=25805.12530354364\n",
      "Current iteration=1765, loss=25805.09633318519\n",
      "Current iteration=1766, loss=25805.067393818565\n",
      "Current iteration=1767, loss=25805.03848540132\n",
      "Current iteration=1768, loss=25805.009607891086\n",
      "Current iteration=1769, loss=25804.980761245573\n",
      "Current iteration=1770, loss=25804.95194542258\n",
      "Current iteration=1771, loss=25804.92316037997\n",
      "Current iteration=1772, loss=25804.894406075688\n",
      "Current iteration=1773, loss=25804.865682467742\n",
      "Current iteration=1774, loss=25804.836989514253\n",
      "Current iteration=1775, loss=25804.808327173374\n",
      "Current iteration=1776, loss=25804.77969540337\n",
      "Current iteration=1777, loss=25804.75109416256\n",
      "Current iteration=1778, loss=25804.722523409346\n",
      "Current iteration=1779, loss=25804.69398310222\n",
      "Current iteration=1780, loss=25804.66547319971\n",
      "Current iteration=1781, loss=25804.636993660475\n",
      "Current iteration=1782, loss=25804.608544443196\n",
      "Current iteration=1783, loss=25804.58012550667\n",
      "Current iteration=1784, loss=25804.551736809735\n",
      "Current iteration=1785, loss=25804.523378311325\n",
      "Current iteration=1786, loss=25804.49504997045\n",
      "Current iteration=1787, loss=25804.46675174618\n",
      "Current iteration=1788, loss=25804.438483597663\n",
      "Current iteration=1789, loss=25804.410245484134\n",
      "Current iteration=1790, loss=25804.38203736488\n",
      "Current iteration=1791, loss=25804.353859199276\n",
      "Current iteration=1792, loss=25804.325710946767\n",
      "Current iteration=1793, loss=25804.29759256687\n",
      "Current iteration=1794, loss=25804.269504019176\n",
      "Current iteration=1795, loss=25804.241445263346\n",
      "Current iteration=1796, loss=25804.21341625911\n",
      "Current iteration=1797, loss=25804.1854169663\n",
      "Current iteration=1798, loss=25804.15744734476\n",
      "Current iteration=1799, loss=25804.12950735446\n",
      "Current iteration=1800, loss=25804.10159695542\n",
      "Current iteration=1801, loss=25804.07371610774\n",
      "Current iteration=1802, loss=25804.04586477158\n",
      "Current iteration=1803, loss=25804.018042907162\n",
      "Current iteration=1804, loss=25803.990250474817\n",
      "Current iteration=1805, loss=25803.962487434907\n",
      "Current iteration=1806, loss=25803.93475374789\n",
      "Current iteration=1807, loss=25803.907049374284\n",
      "Current iteration=1808, loss=25803.879374274664\n",
      "Current iteration=1809, loss=25803.851728409703\n",
      "Current iteration=1810, loss=25803.824111740134\n",
      "Current iteration=1811, loss=25803.79652422673\n",
      "Current iteration=1812, loss=25803.76896583039\n",
      "Current iteration=1813, loss=25803.741436512028\n",
      "Current iteration=1814, loss=25803.713936232656\n",
      "Current iteration=1815, loss=25803.686464953353\n",
      "Current iteration=1816, loss=25803.659022635256\n",
      "Current iteration=1817, loss=25803.63160923958\n",
      "Current iteration=1818, loss=25803.604224727613\n",
      "Current iteration=1819, loss=25803.576869060686\n",
      "Current iteration=1820, loss=25803.549542200224\n",
      "Current iteration=1821, loss=25803.522244107717\n",
      "Current iteration=1822, loss=25803.494974744706\n",
      "Current iteration=1823, loss=25803.46773407282\n",
      "Current iteration=1824, loss=25803.440522053734\n",
      "Current iteration=1825, loss=25803.41333864921\n",
      "Current iteration=1826, loss=25803.386183821072\n",
      "Current iteration=1827, loss=25803.3590575312\n",
      "Current iteration=1828, loss=25803.331959741547\n",
      "Current iteration=1829, loss=25803.30489041414\n",
      "Current iteration=1830, loss=25803.277849511058\n",
      "Current iteration=1831, loss=25803.25083699446\n",
      "Current iteration=1832, loss=25803.22385282656\n",
      "Current iteration=1833, loss=25803.19689696965\n",
      "Current iteration=1834, loss=25803.16996938607\n",
      "Current iteration=1835, loss=25803.14307003824\n",
      "Current iteration=1836, loss=25803.116198888638\n",
      "Current iteration=1837, loss=25803.089355899818\n",
      "Current iteration=1838, loss=25803.06254103438\n",
      "Current iteration=1839, loss=25803.035754255005\n",
      "Current iteration=1840, loss=25803.008995524433\n",
      "Current iteration=1841, loss=25802.982264805472\n",
      "Current iteration=1842, loss=25802.955562060983\n",
      "Current iteration=1843, loss=25802.928887253904\n",
      "Current iteration=1844, loss=25802.90224034723\n",
      "Current iteration=1845, loss=25802.87562130402\n",
      "Current iteration=1846, loss=25802.8490300874\n",
      "Current iteration=1847, loss=25802.82246666056\n",
      "Current iteration=1848, loss=25802.795930986755\n",
      "Current iteration=1849, loss=25802.769423029287\n",
      "Current iteration=1850, loss=25802.742942751545\n",
      "Current iteration=1851, loss=25802.71649011696\n",
      "Current iteration=1852, loss=25802.69006508904\n",
      "Current iteration=1853, loss=25802.66366763135\n",
      "Current iteration=1854, loss=25802.637297707515\n",
      "Current iteration=1855, loss=25802.610955281227\n",
      "Current iteration=1856, loss=25802.584640316243\n",
      "Current iteration=1857, loss=25802.55835277637\n",
      "Current iteration=1858, loss=25802.532092625483\n",
      "Current iteration=1859, loss=25802.50585982752\n",
      "Current iteration=1860, loss=25802.479654346487\n",
      "Current iteration=1861, loss=25802.453476146435\n",
      "Current iteration=1862, loss=25802.427325191493\n",
      "Current iteration=1863, loss=25802.401201445835\n",
      "Current iteration=1864, loss=25802.37510487371\n",
      "Current iteration=1865, loss=25802.349035439427\n",
      "Current iteration=1866, loss=25802.322993107333\n",
      "Current iteration=1867, loss=25802.29697784187\n",
      "Current iteration=1868, loss=25802.270989607518\n",
      "Current iteration=1869, loss=25802.245028368812\n",
      "Current iteration=1870, loss=25802.219094090375\n",
      "Current iteration=1871, loss=25802.19318673686\n",
      "Current iteration=1872, loss=25802.16730627299\n",
      "Current iteration=1873, loss=25802.141452663556\n",
      "Current iteration=1874, loss=25802.1156258734\n",
      "Current iteration=1875, loss=25802.089825867424\n",
      "Current iteration=1876, loss=25802.06405261058\n",
      "Current iteration=1877, loss=25802.038306067912\n",
      "Current iteration=1878, loss=25802.01258620448\n",
      "Current iteration=1879, loss=25801.98689298542\n",
      "Current iteration=1880, loss=25801.96122637595\n",
      "Current iteration=1881, loss=25801.935586341304\n",
      "Current iteration=1882, loss=25801.909972846806\n",
      "Current iteration=1883, loss=25801.884385857822\n",
      "Current iteration=1884, loss=25801.858825339787\n",
      "Current iteration=1885, loss=25801.833291258175\n",
      "Current iteration=1886, loss=25801.80778357855\n",
      "Current iteration=1887, loss=25801.782302266507\n",
      "Current iteration=1888, loss=25801.756847287703\n",
      "Current iteration=1889, loss=25801.73141860785\n",
      "Current iteration=1890, loss=25801.706016192737\n",
      "Current iteration=1891, loss=25801.68064000818\n",
      "Current iteration=1892, loss=25801.655290020077\n",
      "Current iteration=1893, loss=25801.62996619437\n",
      "Current iteration=1894, loss=25801.604668497064\n",
      "Current iteration=1895, loss=25801.5793968942\n",
      "Current iteration=1896, loss=25801.554151351913\n",
      "Current iteration=1897, loss=25801.528931836365\n",
      "Current iteration=1898, loss=25801.50373831378\n",
      "Current iteration=1899, loss=25801.478570750445\n",
      "Current iteration=1900, loss=25801.45342911269\n",
      "Current iteration=1901, loss=25801.428313366916\n",
      "Current iteration=1902, loss=25801.403223479567\n",
      "Current iteration=1903, loss=25801.37815941716\n",
      "Current iteration=1904, loss=25801.35312114624\n",
      "Current iteration=1905, loss=25801.32810863343\n",
      "Current iteration=1906, loss=25801.303121845387\n",
      "Current iteration=1907, loss=25801.27816074885\n",
      "Current iteration=1908, loss=25801.253225310593\n",
      "Current iteration=1909, loss=25801.228315497454\n",
      "Current iteration=1910, loss=25801.203431276317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=1911, loss=25801.178572614124\n",
      "Current iteration=1912, loss=25801.153739477875\n",
      "Current iteration=1913, loss=25801.12893183462\n",
      "Current iteration=1914, loss=25801.104149651463\n",
      "Current iteration=1915, loss=25801.079392895565\n",
      "Current iteration=1916, loss=25801.054661534134\n",
      "Current iteration=1917, loss=25801.02995553444\n",
      "Current iteration=1918, loss=25801.0052748638\n",
      "Current iteration=1919, loss=25800.980619489594\n",
      "Current iteration=1920, loss=25800.955989379243\n",
      "Current iteration=1921, loss=25800.93138450022\n",
      "Current iteration=1922, loss=25800.90680482007\n",
      "Current iteration=1923, loss=25800.882250306367\n",
      "Current iteration=1924, loss=25800.85772092675\n",
      "Current iteration=1925, loss=25800.83321664892\n",
      "Current iteration=1926, loss=25800.808737440602\n",
      "Current iteration=1927, loss=25800.784283269604\n",
      "Current iteration=1928, loss=25800.759854103773\n",
      "Current iteration=1929, loss=25800.735449910997\n",
      "Current iteration=1930, loss=25800.71107065925\n",
      "Current iteration=1931, loss=25800.686716316508\n",
      "Current iteration=1932, loss=25800.66238685084\n",
      "Current iteration=1933, loss=25800.638082230354\n",
      "Current iteration=1934, loss=25800.613802423202\n",
      "Current iteration=1935, loss=25800.589547397598\n",
      "Current iteration=1936, loss=25800.5653171218\n",
      "Current iteration=1937, loss=25800.54111156412\n",
      "Current iteration=1938, loss=25800.516930692924\n",
      "Current iteration=1939, loss=25800.49277447662\n",
      "Current iteration=1940, loss=25800.46864288368\n",
      "Current iteration=1941, loss=25800.44453588261\n",
      "Current iteration=1942, loss=25800.420453441984\n",
      "Current iteration=1943, loss=25800.396395530417\n",
      "Current iteration=1944, loss=25800.372362116574\n",
      "Current iteration=1945, loss=25800.348353169164\n",
      "Current iteration=1946, loss=25800.324368656962\n",
      "Current iteration=1947, loss=25800.30040854879\n",
      "Current iteration=1948, loss=25800.276472813504\n",
      "Current iteration=1949, loss=25800.252561420024\n",
      "Current iteration=1950, loss=25800.228674337322\n",
      "Current iteration=1951, loss=25800.204811534404\n",
      "Current iteration=1952, loss=25800.180972980343\n",
      "Current iteration=1953, loss=25800.157158644248\n",
      "Current iteration=1954, loss=25800.133368495284\n",
      "Current iteration=1955, loss=25800.10960250267\n",
      "Current iteration=1956, loss=25800.085860635656\n",
      "Current iteration=1957, loss=25800.06214286356\n",
      "Current iteration=1958, loss=25800.03844915574\n",
      "Current iteration=1959, loss=25800.014779481608\n",
      "Current iteration=1960, loss=25799.991133810603\n",
      "Current iteration=1961, loss=25799.967512112253\n",
      "Current iteration=1962, loss=25799.943914356096\n",
      "Current iteration=1963, loss=25799.920340511737\n",
      "Current iteration=1964, loss=25799.896790548835\n",
      "Current iteration=1965, loss=25799.873264437072\n",
      "Current iteration=1966, loss=25799.8497621462\n",
      "Current iteration=1967, loss=25799.82628364602\n",
      "Current iteration=1968, loss=25799.802828906355\n",
      "Current iteration=1969, loss=25799.779397897106\n",
      "Current iteration=1970, loss=25799.75599058821\n",
      "Current iteration=1971, loss=25799.73260694965\n",
      "Current iteration=1972, loss=25799.709246951443\n",
      "Current iteration=1973, loss=25799.685910563687\n",
      "Current iteration=1974, loss=25799.662597756487\n",
      "Current iteration=1975, loss=25799.639308500024\n",
      "Current iteration=1976, loss=25799.61604276452\n",
      "Current iteration=1977, loss=25799.592800520233\n",
      "Current iteration=1978, loss=25799.569581737476\n",
      "Current iteration=1979, loss=25799.546386386613\n",
      "Current iteration=1980, loss=25799.523214438042\n",
      "Current iteration=1981, loss=25799.500065862216\n",
      "Current iteration=1982, loss=25799.476940629625\n",
      "Current iteration=1983, loss=25799.45383871083\n",
      "Current iteration=1984, loss=25799.43076007641\n",
      "Current iteration=1985, loss=25799.407704696998\n",
      "Current iteration=1986, loss=25799.384672543278\n",
      "Current iteration=1987, loss=25799.361663585973\n",
      "Current iteration=1988, loss=25799.33867779585\n",
      "Current iteration=1989, loss=25799.315715143744\n",
      "Current iteration=1990, loss=25799.29277560051\n",
      "Current iteration=1991, loss=25799.269859137054\n",
      "Current iteration=1992, loss=25799.24696572432\n",
      "Current iteration=1993, loss=25799.22409533333\n",
      "Current iteration=1994, loss=25799.201247935114\n",
      "Current iteration=1995, loss=25799.178423500758\n",
      "Current iteration=1996, loss=25799.155622001403\n",
      "Current iteration=1997, loss=25799.132843408224\n",
      "Current iteration=1998, loss=25799.11008769244\n",
      "Current iteration=1999, loss=25799.08735482532\n",
      "Current iteration=2000, loss=25799.06464477818\n",
      "Current iteration=2001, loss=25799.04195752238\n",
      "Current iteration=2002, loss=25799.01929302931\n",
      "Current iteration=2003, loss=25798.996651270416\n",
      "Current iteration=2004, loss=25798.97403221719\n",
      "Current iteration=2005, loss=25798.95143584116\n",
      "Current iteration=2006, loss=25798.928862113917\n",
      "Current iteration=2007, loss=25798.906311007064\n",
      "Current iteration=2008, loss=25798.883782492267\n",
      "Current iteration=2009, loss=25798.861276541247\n",
      "Current iteration=2010, loss=25798.838793125735\n",
      "Current iteration=2011, loss=25798.81633221754\n",
      "Current iteration=2012, loss=25798.79389378849\n",
      "Current iteration=2013, loss=25798.77147781048\n",
      "Current iteration=2014, loss=25798.749084255418\n",
      "Current iteration=2015, loss=25798.726713095275\n",
      "Current iteration=2016, loss=25798.70436430206\n",
      "Current iteration=2017, loss=25798.682037847837\n",
      "Current iteration=2018, loss=25798.659733704684\n",
      "Current iteration=2019, loss=25798.63745184475\n",
      "Current iteration=2020, loss=25798.615192240206\n",
      "Current iteration=2021, loss=25798.59295486329\n",
      "Current iteration=2022, loss=25798.570739686253\n",
      "Current iteration=2023, loss=25798.54854668141\n",
      "Current iteration=2024, loss=25798.526375821108\n",
      "Current iteration=2025, loss=25798.504227077745\n",
      "Current iteration=2026, loss=25798.482100423742\n",
      "Current iteration=2027, loss=25798.459995831585\n",
      "Current iteration=2028, loss=25798.43791327379\n",
      "Current iteration=2029, loss=25798.415852722912\n",
      "Current iteration=2030, loss=25798.393814151554\n",
      "Current iteration=2031, loss=25798.371797532363\n",
      "Current iteration=2032, loss=25798.34980283802\n",
      "Current iteration=2033, loss=25798.327830041246\n",
      "Current iteration=2034, loss=25798.305879114814\n",
      "Current iteration=2035, loss=25798.28395003153\n",
      "Current iteration=2036, loss=25798.262042764243\n",
      "Current iteration=2037, loss=25798.240157285843\n",
      "Current iteration=2038, loss=25798.218293569258\n",
      "Current iteration=2039, loss=25798.196451587468\n",
      "Current iteration=2040, loss=25798.17463131348\n",
      "Current iteration=2041, loss=25798.15283272035\n",
      "Current iteration=2042, loss=25798.13105578117\n",
      "Current iteration=2043, loss=25798.109300469067\n",
      "Current iteration=2044, loss=25798.087566757236\n",
      "Current iteration=2045, loss=25798.06585461887\n",
      "Current iteration=2046, loss=25798.044164027237\n",
      "Current iteration=2047, loss=25798.022494955636\n",
      "Current iteration=2048, loss=25798.000847377396\n",
      "Current iteration=2049, loss=25797.979221265898\n",
      "Current iteration=2050, loss=25797.957616594555\n",
      "Current iteration=2051, loss=25797.936033336817\n",
      "Current iteration=2052, loss=25797.91447146619\n",
      "Current iteration=2053, loss=25797.892930956208\n",
      "Current iteration=2054, loss=25797.87141178044\n",
      "Current iteration=2055, loss=25797.849913912505\n",
      "Current iteration=2056, loss=25797.828437326058\n",
      "Current iteration=2057, loss=25797.806981994785\n",
      "Current iteration=2058, loss=25797.785547892425\n",
      "Current iteration=2059, loss=25797.76413499275\n",
      "Current iteration=2060, loss=25797.742743269577\n",
      "Current iteration=2061, loss=25797.721372696735\n",
      "Current iteration=2062, loss=25797.700023248133\n",
      "Current iteration=2063, loss=25797.678694897688\n",
      "Current iteration=2064, loss=25797.65738761938\n",
      "Current iteration=2065, loss=25797.636101387194\n",
      "Current iteration=2066, loss=25797.61483617519\n",
      "Current iteration=2067, loss=25797.593591957448\n",
      "Current iteration=2068, loss=25797.572368708086\n",
      "Current iteration=2069, loss=25797.55116640127\n",
      "Current iteration=2070, loss=25797.529985011184\n",
      "Current iteration=2071, loss=25797.50882451208\n",
      "Current iteration=2072, loss=25797.487684878215\n",
      "Current iteration=2073, loss=25797.466566083924\n",
      "Current iteration=2074, loss=25797.445468103535\n",
      "Current iteration=2075, loss=25797.424390911452\n",
      "Current iteration=2076, loss=25797.403334482093\n",
      "Current iteration=2077, loss=25797.382298789922\n",
      "Current iteration=2078, loss=25797.36128380945\n",
      "Current iteration=2079, loss=25797.340289515203\n",
      "Current iteration=2080, loss=25797.319315881767\n",
      "Current iteration=2081, loss=25797.298362883754\n",
      "Current iteration=2082, loss=25797.277430495815\n",
      "Current iteration=2083, loss=25797.256518692637\n",
      "Current iteration=2084, loss=25797.235627448958\n",
      "Current iteration=2085, loss=25797.214756739522\n",
      "Current iteration=2086, loss=25797.193906539145\n",
      "Current iteration=2087, loss=25797.173076822663\n",
      "Current iteration=2088, loss=25797.152267564947\n",
      "Current iteration=2089, loss=25797.131478740914\n",
      "Current iteration=2090, loss=25797.1107103255\n",
      "Current iteration=2091, loss=25797.0899622937\n",
      "Current iteration=2092, loss=25797.06923462054\n",
      "Current iteration=2093, loss=25797.04852728107\n",
      "Current iteration=2094, loss=25797.027840250386\n",
      "Current iteration=2095, loss=25797.007173503618\n",
      "Current iteration=2096, loss=25796.986527015943\n",
      "Current iteration=2097, loss=25796.965900762556\n",
      "Current iteration=2098, loss=25796.945294718695\n",
      "Current iteration=2099, loss=25796.924708859653\n",
      "Current iteration=2100, loss=25796.904143160726\n",
      "Current iteration=2101, loss=25796.88359759726\n",
      "Current iteration=2102, loss=25796.863072144657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=2103, loss=25796.842566778323\n",
      "Current iteration=2104, loss=25796.82208147372\n",
      "Current iteration=2105, loss=25796.80161620634\n",
      "Current iteration=2106, loss=25796.781170951705\n",
      "Current iteration=2107, loss=25796.760745685384\n",
      "Current iteration=2108, loss=25796.74034038297\n",
      "Current iteration=2109, loss=25796.719955020108\n",
      "Current iteration=2110, loss=25796.699589572454\n",
      "Current iteration=2111, loss=25796.679244015722\n",
      "Current iteration=2112, loss=25796.65891832565\n",
      "Current iteration=2113, loss=25796.63861247801\n",
      "Current iteration=2114, loss=25796.61832644861\n",
      "Current iteration=2115, loss=25796.598060213306\n",
      "Current iteration=2116, loss=25796.577813747972\n",
      "Current iteration=2117, loss=25796.557587028517\n",
      "Current iteration=2118, loss=25796.537380030903\n",
      "Current iteration=2119, loss=25796.51719273111\n",
      "Current iteration=2120, loss=25796.497025105153\n",
      "Current iteration=2121, loss=25796.47687712909\n",
      "Current iteration=2122, loss=25796.45674877901\n",
      "Current iteration=2123, loss=25796.436640031032\n",
      "Current iteration=2124, loss=25796.416550861315\n",
      "Current iteration=2125, loss=25796.396481246058\n",
      "Current iteration=2126, loss=25796.376431161476\n",
      "Current iteration=2127, loss=25796.35640058383\n",
      "Current iteration=2128, loss=25796.336389489432\n",
      "Current iteration=2129, loss=25796.31639785459\n",
      "Current iteration=2130, loss=25796.296425655673\n",
      "Current iteration=2131, loss=25796.276472869078\n",
      "Current iteration=2132, loss=25796.25653947124\n",
      "Current iteration=2133, loss=25796.236625438614\n",
      "Current iteration=2134, loss=25796.21673074771\n",
      "Current iteration=2135, loss=25796.196855375052\n",
      "Current iteration=2136, loss=25796.176999297204\n",
      "Current iteration=2137, loss=25796.157162490766\n",
      "Current iteration=2138, loss=25796.137344932373\n",
      "Current iteration=2139, loss=25796.11754659869\n",
      "Current iteration=2140, loss=25796.09776746641\n",
      "Current iteration=2141, loss=25796.078007512275\n",
      "Current iteration=2142, loss=25796.058266713047\n",
      "Current iteration=2143, loss=25796.03854504552\n",
      "Current iteration=2144, loss=25796.018842486537\n",
      "Current iteration=2145, loss=25795.999159012947\n",
      "Current iteration=2146, loss=25795.979494601666\n",
      "Current iteration=2147, loss=25795.95984922961\n",
      "Current iteration=2148, loss=25795.940222873756\n",
      "Current iteration=2149, loss=25795.92061551109\n",
      "Current iteration=2150, loss=25795.90102711864\n",
      "Current iteration=2151, loss=25795.88145767348\n",
      "Current iteration=2152, loss=25795.861907152695\n",
      "Current iteration=2153, loss=25795.84237553341\n",
      "Current iteration=2154, loss=25795.8228627928\n",
      "Current iteration=2155, loss=25795.803368908033\n",
      "Current iteration=2156, loss=25795.783893856355\n",
      "Current iteration=2157, loss=25795.76443761501\n",
      "Current iteration=2158, loss=25795.745000161292\n",
      "Current iteration=2159, loss=25795.72558147252\n",
      "Current iteration=2160, loss=25795.70618152605\n",
      "Current iteration=2161, loss=25795.686800299267\n",
      "Current iteration=2162, loss=25795.66743776958\n",
      "Current iteration=2163, loss=25795.648093914446\n",
      "Current iteration=2164, loss=25795.62876871135\n",
      "Current iteration=2165, loss=25795.6094621378\n",
      "Current iteration=2166, loss=25795.59017417134\n",
      "Current iteration=2167, loss=25795.570904789543\n",
      "Current iteration=2168, loss=25795.55165397002\n",
      "Current iteration=2169, loss=25795.532421690412\n",
      "Current iteration=2170, loss=25795.51320792839\n",
      "Current iteration=2171, loss=25795.494012661657\n",
      "Current iteration=2172, loss=25795.474835867943\n",
      "Current iteration=2173, loss=25795.455677525024\n",
      "Current iteration=2174, loss=25795.43653761068\n",
      "Current iteration=2175, loss=25795.417416102748\n",
      "Current iteration=2176, loss=25795.398312979083\n",
      "Current iteration=2177, loss=25795.379228217585\n",
      "Current iteration=2178, loss=25795.36016179616\n",
      "Current iteration=2179, loss=25795.341113692775\n",
      "Current iteration=2180, loss=25795.322083885403\n",
      "Current iteration=2181, loss=25795.30307235206\n",
      "Current iteration=2182, loss=25795.284079070785\n",
      "Current iteration=2183, loss=25795.26510401967\n",
      "Current iteration=2184, loss=25795.2461471768\n",
      "Current iteration=2185, loss=25795.227208520337\n",
      "Current iteration=2186, loss=25795.208288028418\n",
      "Current iteration=2187, loss=25795.18938567926\n",
      "Current iteration=2188, loss=25795.170501451095\n",
      "Current iteration=2189, loss=25795.15163532217\n",
      "Current iteration=2190, loss=25795.132787270777\n",
      "Current iteration=2191, loss=25795.113957275236\n",
      "Current iteration=2192, loss=25795.0951453139\n",
      "Current iteration=2193, loss=25795.076351365144\n",
      "Current iteration=2194, loss=25795.057575407387\n",
      "Current iteration=2195, loss=25795.038817419052\n",
      "Current iteration=2196, loss=25795.020077378627\n",
      "Current iteration=2197, loss=25795.001355264598\n",
      "Current iteration=2198, loss=25794.982651055503\n",
      "Current iteration=2199, loss=25794.9639647299\n",
      "Current iteration=2200, loss=25794.945296266385\n",
      "Current iteration=2201, loss=25794.92664564356\n",
      "Current iteration=2202, loss=25794.908012840096\n",
      "Current iteration=2203, loss=25794.889397834653\n",
      "Current iteration=2204, loss=25794.87080060595\n",
      "Current iteration=2205, loss=25794.85222113273\n",
      "Current iteration=2206, loss=25794.833659393735\n",
      "Current iteration=2207, loss=25794.815115367786\n",
      "Current iteration=2208, loss=25794.796589033707\n",
      "Current iteration=2209, loss=25794.778080370346\n",
      "Current iteration=2210, loss=25794.759589356585\n",
      "Current iteration=2211, loss=25794.74111597135\n",
      "Current iteration=2212, loss=25794.722660193573\n",
      "Current iteration=2213, loss=25794.70422200223\n",
      "Current iteration=2214, loss=25794.685801376323\n",
      "Current iteration=2215, loss=25794.667398294878\n",
      "Current iteration=2216, loss=25794.649012736958\n",
      "Current iteration=2217, loss=25794.63064468166\n",
      "Current iteration=2218, loss=25794.61229410808\n",
      "Current iteration=2219, loss=25794.593960995375\n",
      "Current iteration=2220, loss=25794.575645322726\n",
      "Current iteration=2221, loss=25794.557347069327\n",
      "Current iteration=2222, loss=25794.539066214413\n",
      "Current iteration=2223, loss=25794.520802737246\n",
      "Current iteration=2224, loss=25794.50255661711\n",
      "Current iteration=2225, loss=25794.48432783333\n",
      "Current iteration=2226, loss=25794.466116365234\n",
      "Current iteration=2227, loss=25794.44792219222\n",
      "Current iteration=2228, loss=25794.429745293673\n",
      "Current iteration=2229, loss=25794.41158564903\n",
      "Current iteration=2230, loss=25794.39344323775\n",
      "Current iteration=2231, loss=25794.375318039325\n",
      "Current iteration=2232, loss=25794.35721003326\n",
      "Current iteration=2233, loss=25794.339119199103\n",
      "Current iteration=2234, loss=25794.32104551643\n",
      "Current iteration=2235, loss=25794.30298896483\n",
      "Current iteration=2236, loss=25794.284949523946\n",
      "Current iteration=2237, loss=25794.266927173416\n",
      "Current iteration=2238, loss=25794.248921892933\n",
      "Current iteration=2239, loss=25794.230933662202\n",
      "Current iteration=2240, loss=25794.21296246097\n",
      "Current iteration=2241, loss=25794.195008268995\n",
      "Current iteration=2242, loss=25794.17707106607\n",
      "Current iteration=2243, loss=25794.159150832023\n",
      "Current iteration=2244, loss=25794.1412475467\n",
      "Current iteration=2245, loss=25794.123361189973\n",
      "Current iteration=2246, loss=25794.105491741753\n",
      "Current iteration=2247, loss=25794.087639181966\n",
      "Current iteration=2248, loss=25794.069803490576\n",
      "Current iteration=2249, loss=25794.05198464756\n",
      "Current iteration=2250, loss=25794.03418263294\n",
      "Current iteration=2251, loss=25794.016397426745\n",
      "Current iteration=2252, loss=25793.998629009056\n",
      "Current iteration=2253, loss=25793.980877359958\n",
      "Current iteration=2254, loss=25793.963142459575\n",
      "Current iteration=2255, loss=25793.94542428806\n",
      "Current iteration=2256, loss=25793.927722825578\n",
      "Current iteration=2257, loss=25793.91003805234\n",
      "Current iteration=2258, loss=25793.892369948575\n",
      "Current iteration=2259, loss=25793.87471849453\n",
      "Current iteration=2260, loss=25793.857083670504\n",
      "Current iteration=2261, loss=25793.83946545679\n",
      "Current iteration=2262, loss=25793.821863833728\n",
      "Current iteration=2263, loss=25793.80427878169\n",
      "Current iteration=2264, loss=25793.78671028105\n",
      "Current iteration=2265, loss=25793.76915831224\n",
      "Current iteration=2266, loss=25793.751622855692\n",
      "Current iteration=2267, loss=25793.73410389188\n",
      "Current iteration=2268, loss=25793.7166014013\n",
      "Current iteration=2269, loss=25793.699115364463\n",
      "Current iteration=2270, loss=25793.681645761928\n",
      "Current iteration=2271, loss=25793.66419257426\n",
      "Current iteration=2272, loss=25793.646755782065\n",
      "Current iteration=2273, loss=25793.62933536597\n",
      "Current iteration=2274, loss=25793.611931306627\n",
      "Current iteration=2275, loss=25793.59454358471\n",
      "Current iteration=2276, loss=25793.57717218093\n",
      "Current iteration=2277, loss=25793.559817076013\n",
      "Current iteration=2278, loss=25793.542478250718\n",
      "Current iteration=2279, loss=25793.525155685827\n",
      "Current iteration=2280, loss=25793.507849362144\n",
      "Current iteration=2281, loss=25793.490559260514\n",
      "Current iteration=2282, loss=25793.47328536179\n",
      "Current iteration=2283, loss=25793.456027646847\n",
      "Current iteration=2284, loss=25793.438786096616\n",
      "Current iteration=2285, loss=25793.42156069202\n",
      "Current iteration=2286, loss=25793.40435141403\n",
      "Current iteration=2287, loss=25793.38715824362\n",
      "Current iteration=2288, loss=25793.369981161813\n",
      "Current iteration=2289, loss=25793.35282014966\n",
      "Current iteration=2290, loss=25793.335675188195\n",
      "Current iteration=2291, loss=25793.31854625854\n",
      "Current iteration=2292, loss=25793.301433341792\n",
      "Current iteration=2293, loss=25793.28433641909\n",
      "Current iteration=2294, loss=25793.267255471605\n",
      "Current iteration=2295, loss=25793.250190480525\n",
      "Current iteration=2296, loss=25793.233141427067\n",
      "Current iteration=2297, loss=25793.216108292476\n",
      "Current iteration=2298, loss=25793.19909105801\n",
      "Current iteration=2299, loss=25793.182089704966\n",
      "Current iteration=2300, loss=25793.165104214655\n",
      "Current iteration=2301, loss=25793.148134568422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=2302, loss=25793.131180747627\n",
      "Current iteration=2303, loss=25793.11424273367\n",
      "Current iteration=2304, loss=25793.097320507964\n",
      "Current iteration=2305, loss=25793.080414051932\n",
      "Current iteration=2306, loss=25793.06352334706\n",
      "Current iteration=2307, loss=25793.046648374828\n",
      "Current iteration=2308, loss=25793.02978911676\n",
      "Current iteration=2309, loss=25793.01294555438\n",
      "Current iteration=2310, loss=25792.99611766926\n",
      "Current iteration=2311, loss=25792.979305442983\n",
      "Current iteration=2312, loss=25792.962508857163\n",
      "Current iteration=2313, loss=25792.94572789344\n",
      "Current iteration=2314, loss=25792.92896253347\n",
      "Current iteration=2315, loss=25792.912212758947\n",
      "Current iteration=2316, loss=25792.89547855157\n",
      "Current iteration=2317, loss=25792.87875989308\n",
      "Current iteration=2318, loss=25792.86205676523\n",
      "Current iteration=2319, loss=25792.845369149807\n",
      "Current iteration=2320, loss=25792.828697028614\n",
      "Current iteration=2321, loss=25792.812040383487\n",
      "Current iteration=2322, loss=25792.79539919627\n",
      "Current iteration=2323, loss=25792.778773448856\n",
      "Current iteration=2324, loss=25792.762163123134\n",
      "Current iteration=2325, loss=25792.745568201037\n",
      "Current iteration=2326, loss=25792.728988664512\n",
      "Current iteration=2327, loss=25792.71242449554\n",
      "Current iteration=2328, loss=25792.695875676116\n",
      "Current iteration=2329, loss=25792.679342188265\n",
      "Current iteration=2330, loss=25792.662824014023\n",
      "Current iteration=2331, loss=25792.646321135464\n",
      "Current iteration=2332, loss=25792.62983353468\n",
      "Current iteration=2333, loss=25792.6133611938\n",
      "Current iteration=2334, loss=25792.596904094942\n",
      "Current iteration=2335, loss=25792.580462220292\n",
      "Current iteration=2336, loss=25792.564035552015\n",
      "Current iteration=2337, loss=25792.54762407234\n",
      "Current iteration=2338, loss=25792.5312277635\n",
      "Current iteration=2339, loss=25792.514846607733\n",
      "Current iteration=2340, loss=25792.498480587346\n",
      "Current iteration=2341, loss=25792.482129684628\n",
      "Current iteration=2342, loss=25792.46579388191\n",
      "Current iteration=2343, loss=25792.44947316154\n",
      "Current iteration=2344, loss=25792.433167505893\n",
      "Current iteration=2345, loss=25792.41687689737\n",
      "Current iteration=2346, loss=25792.400601318383\n",
      "Current iteration=2347, loss=25792.384340751378\n",
      "Current iteration=2348, loss=25792.36809517883\n",
      "Current iteration=2349, loss=25792.35186458322\n",
      "Current iteration=2350, loss=25792.33564894706\n",
      "Current iteration=2351, loss=25792.319448252885\n",
      "Current iteration=2352, loss=25792.303262483256\n",
      "Current iteration=2353, loss=25792.28709162075\n",
      "Current iteration=2354, loss=25792.270935647975\n",
      "Current iteration=2355, loss=25792.254794547556\n",
      "Current iteration=2356, loss=25792.238668302143\n",
      "Current iteration=2357, loss=25792.2225568944\n",
      "Current iteration=2358, loss=25792.206460307032\n",
      "Current iteration=2359, loss=25792.190378522755\n",
      "Current iteration=2360, loss=25792.1743115243\n",
      "Current iteration=2361, loss=25792.158259294436\n",
      "Current iteration=2362, loss=25792.142221815942\n",
      "Current iteration=2363, loss=25792.126199071638\n",
      "Current iteration=2364, loss=25792.110191044343\n",
      "Current iteration=2365, loss=25792.09419771691\n",
      "Current iteration=2366, loss=25792.078219072217\n",
      "Current iteration=2367, loss=25792.06225509315\n",
      "Current iteration=2368, loss=25792.046305762648\n",
      "Current iteration=2369, loss=25792.030371063633\n",
      "Current iteration=2370, loss=25792.01445097908\n",
      "Current iteration=2371, loss=25791.998545491973\n",
      "Current iteration=2372, loss=25791.982654585314\n",
      "Current iteration=2373, loss=25791.966778242135\n",
      "Current iteration=2374, loss=25791.950916445494\n",
      "Current iteration=2375, loss=25791.93506917846\n",
      "Current iteration=2376, loss=25791.91923642413\n",
      "Current iteration=2377, loss=25791.90341816563\n",
      "Current iteration=2378, loss=25791.88761438609\n",
      "Current iteration=2379, loss=25791.871825068665\n",
      "Current iteration=2380, loss=25791.85605019656\n",
      "Current iteration=2381, loss=25791.840289752956\n",
      "Current iteration=2382, loss=25791.824543721097\n",
      "Current iteration=2383, loss=25791.808812084237\n",
      "Current iteration=2384, loss=25791.793094825625\n",
      "Current iteration=2385, loss=25791.777391928572\n",
      "Current iteration=2386, loss=25791.761703376393\n",
      "Current iteration=2387, loss=25791.74602915241\n",
      "Current iteration=2388, loss=25791.730369239987\n",
      "Current iteration=2389, loss=25791.714723622514\n",
      "Current iteration=2390, loss=25791.69909228337\n",
      "Current iteration=2391, loss=25791.68347520599\n",
      "Current iteration=2392, loss=25791.667872373822\n",
      "Current iteration=2393, loss=25791.65228377032\n",
      "Current iteration=2394, loss=25791.636709378967\n",
      "Current iteration=2395, loss=25791.62114918329\n",
      "Current iteration=2396, loss=25791.605603166805\n",
      "Current iteration=2397, loss=25791.590071313058\n",
      "Current iteration=2398, loss=25791.57455360563\n",
      "Current iteration=2399, loss=25791.5590500281\n",
      "Current iteration=2400, loss=25791.5435605641\n",
      "Current iteration=2401, loss=25791.52808519725\n",
      "Current iteration=2402, loss=25791.512623911214\n",
      "Current iteration=2403, loss=25791.497176689667\n",
      "Current iteration=2404, loss=25791.481743516306\n",
      "Current iteration=2405, loss=25791.466324374847\n",
      "Current iteration=2406, loss=25791.450919249037\n",
      "Current iteration=2407, loss=25791.43552812263\n",
      "Current iteration=2408, loss=25791.42015097941\n",
      "Current iteration=2409, loss=25791.404787803185\n",
      "Current iteration=2410, loss=25791.38943857777\n",
      "Current iteration=2411, loss=25791.374103287017\n",
      "Current iteration=2412, loss=25791.358781914794\n",
      "Current iteration=2413, loss=25791.34347444497\n",
      "Current iteration=2414, loss=25791.328180861467\n",
      "Current iteration=2415, loss=25791.312901148205\n",
      "Current iteration=2416, loss=25791.297635289142\n",
      "Current iteration=2417, loss=25791.282383268226\n",
      "Current iteration=2418, loss=25791.267145069472\n",
      "Current iteration=2419, loss=25791.25192067687\n",
      "Current iteration=2420, loss=25791.236710074452\n",
      "Current iteration=2421, loss=25791.22151324628\n",
      "Current iteration=2422, loss=25791.206330176417\n",
      "Current iteration=2423, loss=25791.191160848957\n",
      "Current iteration=2424, loss=25791.17600524801\n",
      "Current iteration=2425, loss=25791.16086335771\n",
      "Current iteration=2426, loss=25791.145735162205\n",
      "Current iteration=2427, loss=25791.130620645672\n",
      "Current iteration=2428, loss=25791.1155197923\n",
      "Current iteration=2429, loss=25791.10043258631\n",
      "Current iteration=2430, loss=25791.085359011926\n",
      "Current iteration=2431, loss=25791.07029905341\n",
      "Current iteration=2432, loss=25791.05525269503\n",
      "Current iteration=2433, loss=25791.04021992108\n",
      "Current iteration=2434, loss=25791.02520071588\n",
      "Current iteration=2435, loss=25791.01019506375\n",
      "Current iteration=2436, loss=25790.995202949067\n",
      "Current iteration=2437, loss=25790.980224356183\n",
      "Current iteration=2438, loss=25790.9652592695\n",
      "Current iteration=2439, loss=25790.950307673433\n",
      "Current iteration=2440, loss=25790.93536955241\n",
      "Current iteration=2441, loss=25790.920444890893\n",
      "Current iteration=2442, loss=25790.90553367335\n",
      "Current iteration=2443, loss=25790.890635884272\n",
      "Current iteration=2444, loss=25790.875751508174\n",
      "Current iteration=2445, loss=25790.860880529595\n",
      "Current iteration=2446, loss=25790.846022933078\n",
      "Current iteration=2447, loss=25790.8311787032\n",
      "Current iteration=2448, loss=25790.816347824548\n",
      "Current iteration=2449, loss=25790.801530281733\n",
      "Current iteration=2450, loss=25790.786726059392\n",
      "Current iteration=2451, loss=25790.771935142173\n",
      "Current iteration=2452, loss=25790.757157514738\n",
      "Current iteration=2453, loss=25790.74239316179\n",
      "Current iteration=2454, loss=25790.72764206802\n",
      "Current iteration=2455, loss=25790.71290421817\n",
      "Current iteration=2456, loss=25790.698179596988\n",
      "Current iteration=2457, loss=25790.68346818923\n",
      "Current iteration=2458, loss=25790.66876997969\n",
      "Current iteration=2459, loss=25790.654084953167\n",
      "Current iteration=2460, loss=25790.639413094497\n",
      "Current iteration=2461, loss=25790.62475438851\n",
      "Current iteration=2462, loss=25790.610108820085\n",
      "Current iteration=2463, loss=25790.595476374092\n",
      "Current iteration=2464, loss=25790.580857035427\n",
      "Current iteration=2465, loss=25790.56625078903\n",
      "Current iteration=2466, loss=25790.55165761982\n",
      "Current iteration=2467, loss=25790.537077512774\n",
      "Current iteration=2468, loss=25790.522510452854\n",
      "Current iteration=2469, loss=25790.507956425066\n",
      "Current iteration=2470, loss=25790.49341541443\n",
      "Current iteration=2471, loss=25790.47888740597\n",
      "Current iteration=2472, loss=25790.464372384744\n",
      "Current iteration=2473, loss=25790.449870335822\n",
      "Current iteration=2474, loss=25790.4353812443\n",
      "Current iteration=2475, loss=25790.420905095285\n",
      "Current iteration=2476, loss=25790.40644187391\n",
      "Current iteration=2477, loss=25790.391991565313\n",
      "Current iteration=2478, loss=25790.377554154664\n",
      "Current iteration=2479, loss=25790.36312962716\n",
      "Current iteration=2480, loss=25790.348717967987\n",
      "Current iteration=2481, loss=25790.334319162383\n",
      "Current iteration=2482, loss=25790.31993319558\n",
      "Current iteration=2483, loss=25790.305560052846\n",
      "Current iteration=2484, loss=25790.291199719442\n",
      "Current iteration=2485, loss=25790.27685218068\n",
      "Current iteration=2486, loss=25790.262517421874\n",
      "Current iteration=2487, loss=25790.248195428358\n",
      "Current iteration=2488, loss=25790.233886185477\n",
      "Current iteration=2489, loss=25790.219589678607\n",
      "Current iteration=2490, loss=25790.205305893134\n",
      "Current iteration=2491, loss=25790.191034814474\n",
      "Current iteration=2492, loss=25790.17677642804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=2493, loss=25790.162530719288\n",
      "Current iteration=2494, loss=25790.148297673677\n",
      "Current iteration=2495, loss=25790.134077276678\n",
      "Current iteration=2496, loss=25790.11986951381\n",
      "Current iteration=2497, loss=25790.10567437057\n",
      "Current iteration=2498, loss=25790.091491832507\n",
      "Current iteration=2499, loss=25790.07732188516\n",
      "Current iteration=2500, loss=25790.06316451412\n",
      "Current iteration=2501, loss=25790.049019704962\n",
      "Current iteration=2502, loss=25790.034887443297\n",
      "Current iteration=2503, loss=25790.020767714755\n",
      "Current iteration=2504, loss=25790.00666050498\n",
      "Current iteration=2505, loss=25789.99256579963\n",
      "Current iteration=2506, loss=25789.978483584386\n",
      "Current iteration=2507, loss=25789.964413844937\n",
      "Current iteration=2508, loss=25789.950356567017\n",
      "Current iteration=2509, loss=25789.936311736346\n",
      "Current iteration=2510, loss=25789.92227933868\n",
      "Current iteration=2511, loss=25789.908259359792\n",
      "Current iteration=2512, loss=25789.89425178546\n",
      "Current iteration=2513, loss=25789.88025660149\n",
      "Current iteration=2514, loss=25789.866273793712\n",
      "Current iteration=2515, loss=25789.852303347965\n",
      "Current iteration=2516, loss=25789.838345250104\n",
      "Current iteration=2517, loss=25789.824399486\n",
      "Current iteration=2518, loss=25789.810466041556\n",
      "Current iteration=2519, loss=25789.796544902674\n",
      "Current iteration=2520, loss=25789.782636055286\n",
      "Current iteration=2521, loss=25789.768739485346\n",
      "Current iteration=2522, loss=25789.754855178806\n",
      "Current iteration=2523, loss=25789.74098312165\n",
      "Current iteration=2524, loss=25789.727123299883\n",
      "Current iteration=2525, loss=25789.713275699512\n",
      "Current iteration=2526, loss=25789.699440306576\n",
      "Current iteration=2527, loss=25789.685617107127\n",
      "Current iteration=2528, loss=25789.671806087226\n",
      "Current iteration=2529, loss=25789.65800723297\n",
      "Current iteration=2530, loss=25789.644220530452\n",
      "Current iteration=2531, loss=25789.630445965795\n",
      "Current iteration=2532, loss=25789.616683525135\n",
      "Current iteration=2533, loss=25789.602933194637\n",
      "Current iteration=2534, loss=25789.58919496046\n",
      "Current iteration=2535, loss=25789.5754688088\n",
      "Current iteration=2536, loss=25789.561754725863\n",
      "Current iteration=2537, loss=25789.54805269787\n",
      "Current iteration=2538, loss=25789.53436271107\n",
      "Current iteration=2539, loss=25789.520684751704\n",
      "Current iteration=2540, loss=25789.507018806067\n",
      "Current iteration=2541, loss=25789.49336486043\n",
      "Current iteration=2542, loss=25789.479722901124\n",
      "Current iteration=2543, loss=25789.466092914456\n",
      "Current iteration=2544, loss=25789.45247488678\n",
      "Current iteration=2545, loss=25789.438868804453\n",
      "Current iteration=2546, loss=25789.425274653855\n",
      "Current iteration=2547, loss=25789.411692421374\n",
      "Current iteration=2548, loss=25789.39812209342\n",
      "Current iteration=2549, loss=25789.38456365643\n",
      "Current iteration=2550, loss=25789.371017096833\n",
      "Current iteration=2551, loss=25789.35748240111\n",
      "Current iteration=2552, loss=25789.343959555714\n",
      "Current iteration=2553, loss=25789.33044854716\n",
      "Current iteration=2554, loss=25789.316949361953\n",
      "Current iteration=2555, loss=25789.30346198662\n",
      "Current iteration=2556, loss=25789.289986407708\n",
      "Current iteration=2557, loss=25789.276522611777\n",
      "Current iteration=2558, loss=25789.2630705854\n",
      "Current iteration=2559, loss=25789.249630315182\n",
      "Current iteration=2560, loss=25789.236201787724\n",
      "Current iteration=2561, loss=25789.222784989663\n",
      "Current iteration=2562, loss=25789.209379907632\n",
      "Current iteration=2563, loss=25789.1959865283\n",
      "Current iteration=2564, loss=25789.18260483835\n",
      "Current iteration=2565, loss=25789.16923482446\n",
      "Current iteration=2566, loss=25789.155876473356\n",
      "Current iteration=2567, loss=25789.14252977175\n",
      "Current iteration=2568, loss=25789.129194706395\n",
      "Current iteration=2569, loss=25789.115871264046\n",
      "Current iteration=2570, loss=25789.102559431485\n",
      "Current iteration=2571, loss=25789.089259195494\n",
      "Current iteration=2572, loss=25789.075970542886\n",
      "Current iteration=2573, loss=25789.062693460488\n",
      "Current iteration=2574, loss=25789.04942793514\n",
      "Current iteration=2575, loss=25789.036173953693\n",
      "Current iteration=2576, loss=25789.02293150303\n",
      "Current iteration=2577, loss=25789.009700570037\n",
      "Current iteration=2578, loss=25788.996481141614\n",
      "Current iteration=2579, loss=25788.98327320469\n",
      "Current iteration=2580, loss=25788.97007674619\n",
      "Current iteration=2581, loss=25788.956891753078\n",
      "Current iteration=2582, loss=25788.943718212333\n",
      "Current iteration=2583, loss=25788.93055611093\n",
      "Current iteration=2584, loss=25788.917405435866\n",
      "Current iteration=2585, loss=25788.904266174166\n",
      "Current iteration=2586, loss=25788.891138312865\n",
      "Current iteration=2587, loss=25788.878021839013\n",
      "Current iteration=2588, loss=25788.86491673967\n",
      "Current iteration=2589, loss=25788.85182300192\n",
      "Current iteration=2590, loss=25788.838740612868\n",
      "Current iteration=2591, loss=25788.825669559614\n",
      "Current iteration=2592, loss=25788.8126098293\n",
      "Current iteration=2593, loss=25788.799561409065\n",
      "Current iteration=2594, loss=25788.786524286068\n",
      "Current iteration=2595, loss=25788.77349844749\n",
      "Current iteration=2596, loss=25788.760483880516\n",
      "Current iteration=2597, loss=25788.747480572365\n",
      "Current iteration=2598, loss=25788.73448851025\n",
      "Current iteration=2599, loss=25788.721507681425\n",
      "Current iteration=2600, loss=25788.70853807313\n",
      "Current iteration=2601, loss=25788.695579672636\n",
      "Current iteration=2602, loss=25788.68263246724\n",
      "Current iteration=2603, loss=25788.669696444238\n",
      "Current iteration=2604, loss=25788.656771590948\n",
      "Current iteration=2605, loss=25788.6438578947\n",
      "Current iteration=2606, loss=25788.630955342855\n",
      "Current iteration=2607, loss=25788.618063922757\n",
      "Current iteration=2608, loss=25788.605183621796\n",
      "Current iteration=2609, loss=25788.59231442737\n",
      "Current iteration=2610, loss=25788.579456326886\n",
      "Current iteration=2611, loss=25788.566609307763\n",
      "Current iteration=2612, loss=25788.553773357453\n",
      "Current iteration=2613, loss=25788.540948463415\n",
      "Current iteration=2614, loss=25788.5281346131\n",
      "Current iteration=2615, loss=25788.51533179402\n",
      "Current iteration=2616, loss=25788.50253999366\n",
      "Current iteration=2617, loss=25788.48975919955\n",
      "Current iteration=2618, loss=25788.476989399212\n",
      "Current iteration=2619, loss=25788.4642305802\n",
      "Current iteration=2620, loss=25788.451482730077\n",
      "Current iteration=2621, loss=25788.43874583642\n",
      "Current iteration=2622, loss=25788.42601988683\n",
      "Current iteration=2623, loss=25788.413304868896\n",
      "Current iteration=2624, loss=25788.400600770266\n",
      "Current iteration=2625, loss=25788.387907578563\n",
      "Current iteration=2626, loss=25788.375225281452\n",
      "Current iteration=2627, loss=25788.362553866595\n",
      "Current iteration=2628, loss=25788.349893321672\n",
      "Current iteration=2629, loss=25788.337243634393\n",
      "Current iteration=2630, loss=25788.32460479247\n",
      "Current iteration=2631, loss=25788.311976783625\n",
      "Current iteration=2632, loss=25788.299359595607\n",
      "Current iteration=2633, loss=25788.286753216176\n",
      "Current iteration=2634, loss=25788.27415763311\n",
      "Current iteration=2635, loss=25788.261572834184\n",
      "Current iteration=2636, loss=25788.248998807216\n",
      "Current iteration=2637, loss=25788.236435540013\n",
      "Current iteration=2638, loss=25788.223883020422\n",
      "Current iteration=2639, loss=25788.211341236278\n",
      "Current iteration=2640, loss=25788.198810175454\n",
      "Current iteration=2641, loss=25788.186289825826\n",
      "Current iteration=2642, loss=25788.173780175275\n",
      "Current iteration=2643, loss=25788.161281211724\n",
      "Current iteration=2644, loss=25788.148792923083\n",
      "Current iteration=2645, loss=25788.136315297295\n",
      "Current iteration=2646, loss=25788.123848322313\n",
      "Current iteration=2647, loss=25788.1113919861\n",
      "Current iteration=2648, loss=25788.09894627663\n",
      "Current iteration=2649, loss=25788.086511181908\n",
      "Current iteration=2650, loss=25788.074086689943\n",
      "Current iteration=2651, loss=25788.061672788754\n",
      "Current iteration=2652, loss=25788.04926946638\n",
      "Current iteration=2653, loss=25788.03687671088\n",
      "Current iteration=2654, loss=25788.024494510315\n",
      "Current iteration=2655, loss=25788.01212285277\n",
      "Current iteration=2656, loss=25787.999761726343\n",
      "Current iteration=2657, loss=25787.987411119146\n",
      "Current iteration=2658, loss=25787.975071019297\n",
      "Current iteration=2659, loss=25787.962741414947\n",
      "Current iteration=2660, loss=25787.950422294245\n",
      "Current iteration=2661, loss=25787.938113645352\n",
      "Current iteration=2662, loss=25787.925815456467\n",
      "Current iteration=2663, loss=25787.91352771578\n",
      "Current iteration=2664, loss=25787.901250411498\n",
      "Current iteration=2665, loss=25787.888983531848\n",
      "Current iteration=2666, loss=25787.876727065075\n",
      "Current iteration=2667, loss=25787.864480999433\n",
      "Current iteration=2668, loss=25787.85224532318\n",
      "Current iteration=2669, loss=25787.840020024618\n",
      "Current iteration=2670, loss=25787.827805092027\n",
      "Current iteration=2671, loss=25787.815600513728\n",
      "Current iteration=2672, loss=25787.803406278043\n",
      "Current iteration=2673, loss=25787.791222373304\n",
      "Current iteration=2674, loss=25787.779048787874\n",
      "Current iteration=2675, loss=25787.766885510115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=2676, loss=25787.754732528418\n",
      "Current iteration=2677, loss=25787.742589831167\n",
      "Current iteration=2678, loss=25787.730457406775\n",
      "Current iteration=2679, loss=25787.71833524367\n",
      "Current iteration=2680, loss=25787.70622333029\n",
      "Current iteration=2681, loss=25787.694121655077\n",
      "Current iteration=2682, loss=25787.682030206506\n",
      "Current iteration=2683, loss=25787.66994897305\n",
      "Current iteration=2684, loss=25787.65787794321\n",
      "Current iteration=2685, loss=25787.645817105487\n",
      "Current iteration=2686, loss=25787.633766448405\n",
      "Current iteration=2687, loss=25787.621725960496\n",
      "Current iteration=2688, loss=25787.609695630308\n",
      "Current iteration=2689, loss=25787.597675446406\n",
      "Current iteration=2690, loss=25787.58566539737\n",
      "Current iteration=2691, loss=25787.573665471784\n",
      "Current iteration=2692, loss=25787.561675658253\n",
      "Current iteration=2693, loss=25787.549695945396\n",
      "Current iteration=2694, loss=25787.537726321843\n",
      "Current iteration=2695, loss=25787.52576677624\n",
      "Current iteration=2696, loss=25787.513817297247\n",
      "Current iteration=2697, loss=25787.501877873536\n",
      "Current iteration=2698, loss=25787.489948493792\n",
      "Current iteration=2699, loss=25787.47802914671\n",
      "Current iteration=2700, loss=25787.46611982101\n",
      "Current iteration=2701, loss=25787.45422050541\n",
      "Current iteration=2702, loss=25787.442331188664\n",
      "Current iteration=2703, loss=25787.430451859516\n",
      "Current iteration=2704, loss=25787.418582506736\n",
      "Current iteration=2705, loss=25787.4067231191\n",
      "Current iteration=2706, loss=25787.39487368541\n",
      "Current iteration=2707, loss=25787.38303419447\n",
      "Current iteration=2708, loss=25787.3712046351\n",
      "Current iteration=2709, loss=25787.35938499614\n",
      "Current iteration=2710, loss=25787.34757526643\n",
      "Current iteration=2711, loss=25787.33577543484\n",
      "Current iteration=2712, loss=25787.323985490235\n",
      "Current iteration=2713, loss=25787.312205421513\n",
      "Current iteration=2714, loss=25787.30043521757\n",
      "Current iteration=2715, loss=25787.288674867326\n",
      "Current iteration=2716, loss=25787.276924359696\n",
      "Current iteration=2717, loss=25787.265183683638\n",
      "Current iteration=2718, loss=25787.2534528281\n",
      "Current iteration=2719, loss=25787.241731782044\n",
      "Current iteration=2720, loss=25787.23002053446\n",
      "Current iteration=2721, loss=25787.21831907434\n",
      "Current iteration=2722, loss=25787.20662739069\n",
      "Current iteration=2723, loss=25787.194945472533\n",
      "Current iteration=2724, loss=25787.183273308896\n",
      "Current iteration=2725, loss=25787.171610888843\n",
      "Current iteration=2726, loss=25787.15995820141\n",
      "Current iteration=2727, loss=25787.148315235685\n",
      "Current iteration=2728, loss=25787.13668198076\n",
      "Current iteration=2729, loss=25787.125058425718\n",
      "Current iteration=2730, loss=25787.11344455968\n",
      "Current iteration=2731, loss=25787.10184037178\n",
      "Current iteration=2732, loss=25787.09024585114\n",
      "Current iteration=2733, loss=25787.07866098692\n",
      "Current iteration=2734, loss=25787.06708576829\n",
      "Current iteration=2735, loss=25787.05552018442\n",
      "Current iteration=2736, loss=25787.043964224504\n",
      "Current iteration=2737, loss=25787.032417877737\n",
      "Current iteration=2738, loss=25787.020881133347\n",
      "Current iteration=2739, loss=25787.00935398056\n",
      "Current iteration=2740, loss=25786.99783640861\n",
      "Current iteration=2741, loss=25786.98632840676\n",
      "Current iteration=2742, loss=25786.974829964274\n",
      "Current iteration=2743, loss=25786.963341070437\n",
      "Current iteration=2744, loss=25786.951861714544\n",
      "Current iteration=2745, loss=25786.940391885888\n",
      "Current iteration=2746, loss=25786.9289315738\n",
      "Current iteration=2747, loss=25786.917480767603\n",
      "Current iteration=2748, loss=25786.906039456655\n",
      "Current iteration=2749, loss=25786.894607630304\n",
      "Current iteration=2750, loss=25786.88318527792\n",
      "Current iteration=2751, loss=25786.871772388884\n",
      "Current iteration=2752, loss=25786.86036895259\n",
      "Current iteration=2753, loss=25786.848974958455\n",
      "Current iteration=2754, loss=25786.83759039589\n",
      "Current iteration=2755, loss=25786.82621525433\n",
      "Current iteration=2756, loss=25786.814849523224\n",
      "Current iteration=2757, loss=25786.80349319203\n",
      "Current iteration=2758, loss=25786.792146250213\n",
      "Current iteration=2759, loss=25786.78080868727\n",
      "Current iteration=2760, loss=25786.76948049268\n",
      "Current iteration=2761, loss=25786.758161655962\n",
      "Current iteration=2762, loss=25786.746852166627\n",
      "Current iteration=2763, loss=25786.73555201422\n",
      "Current iteration=2764, loss=25786.724261188287\n",
      "Current iteration=2765, loss=25786.712979678374\n",
      "Current iteration=2766, loss=25786.701707474065\n",
      "Current iteration=2767, loss=25786.690444564934\n",
      "Current iteration=2768, loss=25786.67919094058\n",
      "Current iteration=2769, loss=25786.66794659061\n",
      "Current iteration=2770, loss=25786.656711504642\n",
      "Current iteration=2771, loss=25786.645485672318\n",
      "Current iteration=2772, loss=25786.63426908327\n",
      "Current iteration=2773, loss=25786.623061727172\n",
      "Current iteration=2774, loss=25786.611863593676\n",
      "Current iteration=2775, loss=25786.600674672474\n",
      "Current iteration=2776, loss=25786.589494953256\n",
      "Current iteration=2777, loss=25786.578324425725\n",
      "Current iteration=2778, loss=25786.56716307961\n",
      "Current iteration=2779, loss=25786.556010904635\n",
      "Current iteration=2780, loss=25786.544867890545\n",
      "Current iteration=2781, loss=25786.533734027093\n",
      "Current iteration=2782, loss=25786.522609304055\n",
      "Current iteration=2783, loss=25786.511493711194\n",
      "Current iteration=2784, loss=25786.500387238313\n",
      "Current iteration=2785, loss=25786.489289875215\n",
      "Current iteration=2786, loss=25786.478201611717\n",
      "Current iteration=2787, loss=25786.467122437643\n",
      "Current iteration=2788, loss=25786.456052342834\n",
      "Current iteration=2789, loss=25786.44499131714\n",
      "Current iteration=2790, loss=25786.433939350434\n",
      "Current iteration=2791, loss=25786.422896432585\n",
      "Current iteration=2792, loss=25786.411862553476\n",
      "Current iteration=2793, loss=25786.400837703015\n",
      "Current iteration=2794, loss=25786.38982187112\n",
      "Current iteration=2795, loss=25786.378815047698\n",
      "Current iteration=2796, loss=25786.3678172227\n",
      "Current iteration=2797, loss=25786.356828386073\n",
      "Current iteration=2798, loss=25786.345848527762\n",
      "Current iteration=2799, loss=25786.334877637757\n",
      "Current iteration=2800, loss=25786.323915706027\n",
      "Current iteration=2801, loss=25786.312962722575\n",
      "Current iteration=2802, loss=25786.30201867742\n",
      "Current iteration=2803, loss=25786.29108356055\n",
      "Current iteration=2804, loss=25786.280157362027\n",
      "Current iteration=2805, loss=25786.26924007188\n",
      "Current iteration=2806, loss=25786.25833168016\n",
      "Current iteration=2807, loss=25786.247432176948\n",
      "Current iteration=2808, loss=25786.23654155231\n",
      "Current iteration=2809, loss=25786.22565979633\n",
      "Current iteration=2810, loss=25786.214786899127\n",
      "Current iteration=2811, loss=25786.203922850797\n",
      "Current iteration=2812, loss=25786.193067641478\n",
      "Current iteration=2813, loss=25786.182221261304\n",
      "Current iteration=2814, loss=25786.171383700414\n",
      "Current iteration=2815, loss=25786.16055494898\n",
      "Current iteration=2816, loss=25786.14973499717\n",
      "Current iteration=2817, loss=25786.138923835166\n",
      "Current iteration=2818, loss=25786.128121453166\n",
      "Current iteration=2819, loss=25786.117327841363\n",
      "Current iteration=2820, loss=25786.10654298999\n",
      "Current iteration=2821, loss=25786.095766889273\n",
      "Current iteration=2822, loss=25786.08499952945\n",
      "Current iteration=2823, loss=25786.074240900776\n",
      "Current iteration=2824, loss=25786.06349099351\n",
      "Current iteration=2825, loss=25786.052749797935\n",
      "Current iteration=2826, loss=25786.04201730434\n",
      "Current iteration=2827, loss=25786.031293503005\n",
      "Current iteration=2828, loss=25786.020578384265\n",
      "Current iteration=2829, loss=25786.00987193842\n",
      "Current iteration=2830, loss=25785.999174155822\n",
      "Current iteration=2831, loss=25785.988485026803\n",
      "Current iteration=2832, loss=25785.977804541715\n",
      "Current iteration=2833, loss=25785.967132690937\n",
      "Current iteration=2834, loss=25785.956469464843\n",
      "Current iteration=2835, loss=25785.94581485382\n",
      "Current iteration=2836, loss=25785.935168848264\n",
      "Current iteration=2837, loss=25785.9245314386\n",
      "Current iteration=2838, loss=25785.91390261524\n",
      "Current iteration=2839, loss=25785.90328236863\n",
      "Current iteration=2840, loss=25785.8926706892\n",
      "Current iteration=2841, loss=25785.882067567425\n",
      "Current iteration=2842, loss=25785.871472993767\n",
      "Current iteration=2843, loss=25785.860886958704\n",
      "Current iteration=2844, loss=25785.85030945273\n",
      "Current iteration=2845, loss=25785.839740466334\n",
      "Current iteration=2846, loss=25785.829179990054\n",
      "Current iteration=2847, loss=25785.818628014396\n",
      "Current iteration=2848, loss=25785.808084529897\n",
      "Current iteration=2849, loss=25785.797549527113\n",
      "Current iteration=2850, loss=25785.787022996596\n",
      "Current iteration=2851, loss=25785.776504928923\n",
      "Current iteration=2852, loss=25785.765995314654\n",
      "Current iteration=2853, loss=25785.755494144403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=2854, loss=25785.745001408763\n",
      "Current iteration=2855, loss=25785.73451709835\n",
      "Current iteration=2856, loss=25785.724041203775\n",
      "Current iteration=2857, loss=25785.713573715697\n",
      "Current iteration=2858, loss=25785.70311462474\n",
      "Current iteration=2859, loss=25785.692663921578\n",
      "Current iteration=2860, loss=25785.68222159687\n",
      "Current iteration=2861, loss=25785.6717876413\n",
      "Current iteration=2862, loss=25785.661362045554\n",
      "Current iteration=2863, loss=25785.650944800338\n",
      "Current iteration=2864, loss=25785.640535896364\n",
      "Current iteration=2865, loss=25785.63013532435\n",
      "Current iteration=2866, loss=25785.619743075033\n",
      "Current iteration=2867, loss=25785.609359139162\n",
      "Current iteration=2868, loss=25785.598983507487\n",
      "Current iteration=2869, loss=25785.588616170782\n",
      "Current iteration=2870, loss=25785.57825711981\n",
      "Current iteration=2871, loss=25785.567906345375\n",
      "Current iteration=2872, loss=25785.55756383827\n",
      "Current iteration=2873, loss=25785.547229589298\n",
      "Current iteration=2874, loss=25785.536903589295\n",
      "Current iteration=2875, loss=25785.526585829077\n",
      "Current iteration=2876, loss=25785.516276299495\n",
      "Current iteration=2877, loss=25785.505974991396\n",
      "Current iteration=2878, loss=25785.495681895656\n",
      "Current iteration=2879, loss=25785.485397003136\n",
      "Current iteration=2880, loss=25785.47512030472\n",
      "Current iteration=2881, loss=25785.46485179132\n",
      "Current iteration=2882, loss=25785.454591453825\n",
      "Current iteration=2883, loss=25785.44433928316\n",
      "Current iteration=2884, loss=25785.43409527025\n",
      "Current iteration=2885, loss=25785.42385940604\n",
      "Current iteration=2886, loss=25785.41363168147\n",
      "Current iteration=2887, loss=25785.403412087504\n",
      "Current iteration=2888, loss=25785.39320061511\n",
      "Current iteration=2889, loss=25785.382997255274\n",
      "Current iteration=2890, loss=25785.372801998987\n",
      "Current iteration=2891, loss=25785.36261483724\n",
      "Current iteration=2892, loss=25785.35243576105\n",
      "Current iteration=2893, loss=25785.34226476145\n",
      "Current iteration=2894, loss=25785.332101829463\n",
      "Current iteration=2895, loss=25785.321946956134\n",
      "Current iteration=2896, loss=25785.31180013252\n",
      "Current iteration=2897, loss=25785.30166134969\n",
      "Current iteration=2898, loss=25785.29153059871\n",
      "Current iteration=2899, loss=25785.281407870665\n",
      "Current iteration=2900, loss=25785.271293156668\n",
      "Current iteration=2901, loss=25785.261186447806\n",
      "Current iteration=2902, loss=25785.2510877352\n",
      "Current iteration=2903, loss=25785.240997009983\n",
      "Current iteration=2904, loss=25785.230914263295\n",
      "Current iteration=2905, loss=25785.220839486276\n",
      "Current iteration=2906, loss=25785.21077267009\n",
      "Current iteration=2907, loss=25785.20071380589\n",
      "Current iteration=2908, loss=25785.190662884885\n",
      "Current iteration=2909, loss=25785.18061989824\n",
      "Current iteration=2910, loss=25785.17058483717\n",
      "Current iteration=2911, loss=25785.160557692867\n",
      "Current iteration=2912, loss=25785.150538456568\n",
      "Current iteration=2913, loss=25785.140527119496\n",
      "Current iteration=2914, loss=25785.130523672888\n",
      "Current iteration=2915, loss=25785.120528108\n",
      "Current iteration=2916, loss=25785.1105404161\n",
      "Current iteration=2917, loss=25785.100560588442\n",
      "Current iteration=2918, loss=25785.090588616324\n",
      "Current iteration=2919, loss=25785.080624491027\n",
      "Current iteration=2920, loss=25785.070668203854\n",
      "Current iteration=2921, loss=25785.06071974613\n",
      "Current iteration=2922, loss=25785.05077910916\n",
      "Current iteration=2923, loss=25785.040846284275\n",
      "Current iteration=2924, loss=25785.030921262827\n",
      "Current iteration=2925, loss=25785.021004036174\n",
      "Current iteration=2926, loss=25785.011094595673\n",
      "Current iteration=2927, loss=25785.001192932687\n",
      "Current iteration=2928, loss=25784.991299038607\n",
      "Current iteration=2929, loss=25784.981412904825\n",
      "Current iteration=2930, loss=25784.97153452275\n",
      "Current iteration=2931, loss=25784.961663883776\n",
      "Current iteration=2932, loss=25784.951800979343\n",
      "Current iteration=2933, loss=25784.94194580088\n",
      "Current iteration=2934, loss=25784.93209833983\n",
      "Current iteration=2935, loss=25784.922258587638\n",
      "Current iteration=2936, loss=25784.912426535775\n",
      "Current iteration=2937, loss=25784.902602175713\n",
      "Current iteration=2938, loss=25784.89278549893\n",
      "Current iteration=2939, loss=25784.88297649692\n",
      "Current iteration=2940, loss=25784.873175161185\n",
      "Current iteration=2941, loss=25784.863381483243\n",
      "Current iteration=2942, loss=25784.85359545461\n",
      "Current iteration=2943, loss=25784.843817066812\n",
      "Current iteration=2944, loss=25784.834046311407\n",
      "Current iteration=2945, loss=25784.824283179933\n",
      "Current iteration=2946, loss=25784.81452766395\n",
      "Current iteration=2947, loss=25784.804779755043\n",
      "Current iteration=2948, loss=25784.795039444783\n",
      "Current iteration=2949, loss=25784.785306724763\n",
      "Current iteration=2950, loss=25784.775581586586\n",
      "Current iteration=2951, loss=25784.765864021858\n",
      "Current iteration=2952, loss=25784.756154022198\n",
      "Current iteration=2953, loss=25784.746451579238\n",
      "Current iteration=2954, loss=25784.73675668462\n",
      "Current iteration=2955, loss=25784.727069329994\n",
      "Current iteration=2956, loss=25784.717389507015\n",
      "Current iteration=2957, loss=25784.70771720736\n",
      "Current iteration=2958, loss=25784.698052422693\n",
      "Current iteration=2959, loss=25784.688395144713\n",
      "Current iteration=2960, loss=25784.678745365112\n",
      "Current iteration=2961, loss=25784.669103075605\n",
      "Current iteration=2962, loss=25784.6594682679\n",
      "Current iteration=2963, loss=25784.64984093373\n",
      "Current iteration=2964, loss=25784.640221064823\n",
      "Current iteration=2965, loss=25784.63060865294\n",
      "Current iteration=2966, loss=25784.621003689826\n",
      "Current iteration=2967, loss=25784.61140616724\n",
      "Current iteration=2968, loss=25784.601816076964\n",
      "Current iteration=2969, loss=25784.592233410785\n",
      "Current iteration=2970, loss=25784.582658160496\n",
      "Current iteration=2971, loss=25784.57309031789\n",
      "Current iteration=2972, loss=25784.56352987479\n",
      "Current iteration=2973, loss=25784.553976823016\n",
      "Current iteration=2974, loss=25784.5444311544\n",
      "Current iteration=2975, loss=25784.534892860775\n",
      "Current iteration=2976, loss=25784.52536193401\n",
      "Current iteration=2977, loss=25784.515838365947\n",
      "Current iteration=2978, loss=25784.506322148456\n",
      "Current iteration=2979, loss=25784.49681327343\n",
      "Current iteration=2980, loss=25784.487311732744\n",
      "Current iteration=2981, loss=25784.4778175183\n",
      "Current iteration=2982, loss=25784.468330622003\n",
      "Current iteration=2983, loss=25784.45885103578\n",
      "Current iteration=2984, loss=25784.449378751542\n",
      "Current iteration=2985, loss=25784.439913761227\n",
      "Current iteration=2986, loss=25784.430456056787\n",
      "Current iteration=2987, loss=25784.42100563018\n",
      "Current iteration=2988, loss=25784.411562473357\n",
      "Current iteration=2989, loss=25784.402126578287\n",
      "Current iteration=2990, loss=25784.392697936968\n",
      "Current iteration=2991, loss=25784.38327654138\n",
      "Current iteration=2992, loss=25784.373862383523\n",
      "Current iteration=2993, loss=25784.36445545541\n",
      "Current iteration=2994, loss=25784.35505574906\n",
      "Current iteration=2995, loss=25784.3456632565\n",
      "Current iteration=2996, loss=25784.336277969767\n",
      "Current iteration=2997, loss=25784.326899880904\n",
      "Current iteration=2998, loss=25784.317528981977\n",
      "Current iteration=2999, loss=25784.308165265043\n",
      "Current iteration=3000, loss=25784.298808722175\n",
      "Current iteration=3001, loss=25784.289459345455\n",
      "Current iteration=3002, loss=25784.280117126982\n",
      "Current iteration=3003, loss=25784.27078205886\n",
      "Current iteration=3004, loss=25784.26145413319\n",
      "Current iteration=3005, loss=25784.252133342095\n",
      "Current iteration=3006, loss=25784.242819677704\n",
      "Current iteration=3007, loss=25784.23351313216\n",
      "Current iteration=3008, loss=25784.224213697602\n",
      "Current iteration=3009, loss=25784.21492136619\n",
      "Current iteration=3010, loss=25784.205636130086\n",
      "Current iteration=3011, loss=25784.196357981476\n",
      "Current iteration=3012, loss=25784.18708691253\n",
      "Current iteration=3013, loss=25784.177822915448\n",
      "Current iteration=3014, loss=25784.168565982418\n",
      "Current iteration=3015, loss=25784.15931610567\n",
      "Current iteration=3016, loss=25784.150073277415\n",
      "Current iteration=3017, loss=25784.140837489875\n",
      "Current iteration=3018, loss=25784.131608735297\n",
      "Current iteration=3019, loss=25784.122387005922\n",
      "Current iteration=3020, loss=25784.113172294008\n",
      "Current iteration=3021, loss=25784.10396459181\n",
      "Current iteration=3022, loss=25784.09476389162\n",
      "Current iteration=3023, loss=25784.085570185696\n",
      "Current iteration=3024, loss=25784.076383466352\n",
      "Current iteration=3025, loss=25784.067203725877\n",
      "Current iteration=3026, loss=25784.05803095657\n",
      "Current iteration=3027, loss=25784.04886515077\n",
      "Current iteration=3028, loss=25784.039706300784\n",
      "Current iteration=3029, loss=25784.030554398963\n",
      "Current iteration=3030, loss=25784.02140943764\n",
      "Current iteration=3031, loss=25784.01227140917\n",
      "Current iteration=3032, loss=25784.00314030592\n",
      "Current iteration=3033, loss=25783.994016120254\n",
      "Current iteration=3034, loss=25783.984898844556\n",
      "Current iteration=3035, loss=25783.97578847121\n",
      "Current iteration=3036, loss=25783.966684992614\n",
      "Current iteration=3037, loss=25783.957588401176\n",
      "Current iteration=3038, loss=25783.94849868931\n",
      "Current iteration=3039, loss=25783.939415849443\n",
      "Current iteration=3040, loss=25783.930339874\n",
      "Current iteration=3041, loss=25783.92127075542\n",
      "Current iteration=3042, loss=25783.91220848616\n",
      "Current iteration=3043, loss=25783.90315305868\n",
      "Current iteration=3044, loss=25783.89410446544\n",
      "Current iteration=3045, loss=25783.885062698908\n",
      "Current iteration=3046, loss=25783.876027751583\n",
      "Current iteration=3047, loss=25783.86699961595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=3048, loss=25783.85797828452\n",
      "Current iteration=3049, loss=25783.848963749788\n",
      "Current iteration=3050, loss=25783.839956004285\n",
      "Current iteration=3051, loss=25783.830955040532\n",
      "Current iteration=3052, loss=25783.821960851063\n",
      "Current iteration=3053, loss=25783.812973428423\n",
      "Current iteration=3054, loss=25783.803992765173\n",
      "Current iteration=3055, loss=25783.795018853863\n",
      "Current iteration=3056, loss=25783.78605168708\n",
      "Current iteration=3057, loss=25783.777091257387\n",
      "Current iteration=3058, loss=25783.76813755737\n",
      "Current iteration=3059, loss=25783.759190579643\n",
      "Current iteration=3060, loss=25783.75025031679\n",
      "Current iteration=3061, loss=25783.741316761436\n",
      "Current iteration=3062, loss=25783.732389906196\n",
      "Current iteration=3063, loss=25783.7234697437\n",
      "Current iteration=3064, loss=25783.714556266594\n",
      "Current iteration=3065, loss=25783.70564946752\n",
      "Current iteration=3066, loss=25783.69674933913\n",
      "Current iteration=3067, loss=25783.68785587409\n",
      "Current iteration=3068, loss=25783.678969065077\n",
      "Current iteration=3069, loss=25783.67008890476\n",
      "Current iteration=3070, loss=25783.661215385837\n",
      "Current iteration=3071, loss=25783.652348501004\n",
      "Current iteration=3072, loss=25783.643488242968\n",
      "Current iteration=3073, loss=25783.634634604437\n",
      "Current iteration=3074, loss=25783.625787578138\n",
      "Current iteration=3075, loss=25783.6169471568\n",
      "Current iteration=3076, loss=25783.60811333316\n",
      "Current iteration=3077, loss=25783.599286099976\n",
      "Current iteration=3078, loss=25783.590465449994\n",
      "Current iteration=3079, loss=25783.58165137598\n",
      "Current iteration=3080, loss=25783.57284387071\n",
      "Current iteration=3081, loss=25783.56404292695\n",
      "Current iteration=3082, loss=25783.555248537516\n",
      "Current iteration=3083, loss=25783.54646069518\n",
      "Current iteration=3084, loss=25783.53767939276\n",
      "Current iteration=3085, loss=25783.52890462307\n",
      "Current iteration=3086, loss=25783.520136378924\n",
      "Current iteration=3087, loss=25783.511374653164\n",
      "Current iteration=3088, loss=25783.502619438616\n",
      "Current iteration=3089, loss=25783.493870728144\n",
      "Current iteration=3090, loss=25783.485128514578\n",
      "Current iteration=3091, loss=25783.476392790802\n",
      "Current iteration=3092, loss=25783.467663549683\n",
      "Current iteration=3093, loss=25783.458940784094\n",
      "Current iteration=3094, loss=25783.45022448693\n",
      "Current iteration=3095, loss=25783.441514651084\n",
      "Current iteration=3096, loss=25783.432811269464\n",
      "Current iteration=3097, loss=25783.424114334965\n",
      "Current iteration=3098, loss=25783.415423840528\n",
      "Current iteration=3099, loss=25783.40673977908\n",
      "Current iteration=3100, loss=25783.398062143537\n",
      "Current iteration=3101, loss=25783.389390926866\n",
      "Current iteration=3102, loss=25783.38072612201\n",
      "Current iteration=3103, loss=25783.372067721928\n",
      "Current iteration=3104, loss=25783.3634157196\n",
      "Current iteration=3105, loss=25783.354770107984\n",
      "Current iteration=3106, loss=25783.346130880087\n",
      "Current iteration=3107, loss=25783.33749802888\n",
      "Current iteration=3108, loss=25783.32887154738\n",
      "Current iteration=3109, loss=25783.32025142859\n",
      "Current iteration=3110, loss=25783.311637665523\n",
      "Current iteration=3111, loss=25783.303030251212\n",
      "Current iteration=3112, loss=25783.294429178688\n",
      "Current iteration=3113, loss=25783.285834440987\n",
      "Current iteration=3114, loss=25783.277246031164\n",
      "Current iteration=3115, loss=25783.268663942268\n",
      "Current iteration=3116, loss=25783.260088167375\n",
      "Current iteration=3117, loss=25783.251518699548\n",
      "Current iteration=3118, loss=25783.242955531874\n",
      "Current iteration=3119, loss=25783.23439865744\n",
      "Current iteration=3120, loss=25783.225848069338\n",
      "Current iteration=3121, loss=25783.217303760677\n",
      "Current iteration=3122, loss=25783.20876572457\n",
      "Current iteration=3123, loss=25783.200233954136\n",
      "Current iteration=3124, loss=25783.191708442504\n",
      "Current iteration=3125, loss=25783.183189182808\n",
      "Current iteration=3126, loss=25783.174676168186\n",
      "Current iteration=3127, loss=25783.1661693918\n",
      "Current iteration=3128, loss=25783.157668846805\n",
      "Current iteration=3129, loss=25783.149174526374\n",
      "Current iteration=3130, loss=25783.14068642367\n",
      "Current iteration=3131, loss=25783.132204531885\n",
      "Current iteration=3132, loss=25783.12372884421\n",
      "Current iteration=3133, loss=25783.115259353835\n",
      "Current iteration=3134, loss=25783.106796053973\n",
      "Current iteration=3135, loss=25783.09833893784\n",
      "Current iteration=3136, loss=25783.089887998656\n",
      "Current iteration=3137, loss=25783.08144322965\n",
      "Current iteration=3138, loss=25783.073004624057\n",
      "Current iteration=3139, loss=25783.064572175124\n",
      "Current iteration=3140, loss=25783.0561458761\n",
      "Current iteration=3141, loss=25783.04772572025\n",
      "Current iteration=3142, loss=25783.03931170085\n",
      "Current iteration=3143, loss=25783.03090381115\n",
      "Current iteration=3144, loss=25783.022502044463\n",
      "Current iteration=3145, loss=25783.01410639407\n",
      "Current iteration=3146, loss=25783.005716853262\n",
      "Current iteration=3147, loss=25782.997333415344\n",
      "Current iteration=3148, loss=25782.98895607365\n",
      "Current iteration=3149, loss=25782.980584821475\n",
      "Current iteration=3150, loss=25782.97221965217\n",
      "Current iteration=3151, loss=25782.963860559063\n",
      "Current iteration=3152, loss=25782.9555075355\n",
      "Current iteration=3153, loss=25782.947160574826\n",
      "Current iteration=3154, loss=25782.938819670417\n",
      "Current iteration=3155, loss=25782.930484815624\n",
      "Current iteration=3156, loss=25782.922156003824\n",
      "Current iteration=3157, loss=25782.913833228416\n",
      "Current iteration=3158, loss=25782.905516482777\n",
      "Current iteration=3159, loss=25782.8972057603\n",
      "Current iteration=3160, loss=25782.888901054397\n",
      "Current iteration=3161, loss=25782.880602358477\n",
      "Current iteration=3162, loss=25782.87230966597\n",
      "Current iteration=3163, loss=25782.864022970294\n",
      "Current iteration=3164, loss=25782.85574226488\n",
      "Current iteration=3165, loss=25782.847467543183\n",
      "Current iteration=3166, loss=25782.839198798647\n",
      "Current iteration=3167, loss=25782.83093602473\n",
      "Current iteration=3168, loss=25782.822679214893\n",
      "Current iteration=3169, loss=25782.81442836262\n",
      "Current iteration=3170, loss=25782.806183461376\n",
      "Current iteration=3171, loss=25782.79794450465\n",
      "Current iteration=3172, loss=25782.789711485955\n",
      "Current iteration=3173, loss=25782.781484398776\n",
      "Current iteration=3174, loss=25782.773263236624\n",
      "Current iteration=3175, loss=25782.765047993023\n",
      "Current iteration=3176, loss=25782.75683866149\n",
      "Current iteration=3177, loss=25782.748635235566\n",
      "Current iteration=3178, loss=25782.740437708777\n",
      "Current iteration=3179, loss=25782.732246074684\n",
      "Current iteration=3180, loss=25782.724060326833\n",
      "Current iteration=3181, loss=25782.715880458785\n",
      "Current iteration=3182, loss=25782.707706464105\n",
      "Current iteration=3183, loss=25782.699538336376\n",
      "Current iteration=3184, loss=25782.69137606918\n",
      "Current iteration=3185, loss=25782.683219656104\n",
      "Current iteration=3186, loss=25782.675069090747\n",
      "Current iteration=3187, loss=25782.666924366717\n",
      "Current iteration=3188, loss=25782.65878547762\n",
      "Current iteration=3189, loss=25782.65065241709\n",
      "Current iteration=3190, loss=25782.642525178737\n",
      "Current iteration=3191, loss=25782.634403756198\n",
      "Current iteration=3192, loss=25782.62628814312\n",
      "Current iteration=3193, loss=25782.618178333152\n",
      "Current iteration=3194, loss=25782.610074319942\n",
      "Current iteration=3195, loss=25782.601976097165\n",
      "Current iteration=3196, loss=25782.593883658483\n",
      "Current iteration=3197, loss=25782.585796997577\n",
      "Current iteration=3198, loss=25782.577716108128\n",
      "Current iteration=3199, loss=25782.56964098383\n",
      "Current iteration=3200, loss=25782.561571618386\n",
      "Current iteration=3201, loss=25782.553508005487\n",
      "Current iteration=3202, loss=25782.545450138867\n",
      "Current iteration=3203, loss=25782.53739801223\n",
      "Current iteration=3204, loss=25782.529351619316\n",
      "Current iteration=3205, loss=25782.521310953853\n",
      "Current iteration=3206, loss=25782.51327600958\n",
      "Current iteration=3207, loss=25782.505246780256\n",
      "Current iteration=3208, loss=25782.497223259626\n",
      "Current iteration=3209, loss=25782.48920544146\n",
      "Current iteration=3210, loss=25782.481193319534\n",
      "Current iteration=3211, loss=25782.47318688761\n",
      "Current iteration=3212, loss=25782.465186139478\n",
      "Current iteration=3213, loss=25782.45719106894\n",
      "Current iteration=3214, loss=25782.449201669784\n",
      "Current iteration=3215, loss=25782.441217935815\n",
      "Current iteration=3216, loss=25782.43323986085\n",
      "Current iteration=3217, loss=25782.425267438703\n",
      "Current iteration=3218, loss=25782.41730066321\n",
      "Current iteration=3219, loss=25782.409339528207\n",
      "Current iteration=3220, loss=25782.40138402752\n",
      "Current iteration=3221, loss=25782.393434155\n",
      "Current iteration=3222, loss=25782.385489904518\n",
      "Current iteration=3223, loss=25782.377551269918\n",
      "Current iteration=3224, loss=25782.369618245066\n",
      "Current iteration=3225, loss=25782.36169082386\n",
      "Current iteration=3226, loss=25782.353769000158\n",
      "Current iteration=3227, loss=25782.345852767863\n",
      "Current iteration=3228, loss=25782.337942120874\n",
      "Current iteration=3229, loss=25782.330037053085\n",
      "Current iteration=3230, loss=25782.32213755841\n",
      "Current iteration=3231, loss=25782.31424363076\n",
      "Current iteration=3232, loss=25782.306355264078\n",
      "Current iteration=3233, loss=25782.298472452272\n",
      "Current iteration=3234, loss=25782.290595189297\n",
      "Current iteration=3235, loss=25782.28272346909\n",
      "Current iteration=3236, loss=25782.2748572856\n",
      "Current iteration=3237, loss=25782.266996632796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=3238, loss=25782.25914150463\n",
      "Current iteration=3239, loss=25782.251291895085\n",
      "Current iteration=3240, loss=25782.24344779813\n",
      "Current iteration=3241, loss=25782.23560920776\n",
      "Current iteration=3242, loss=25782.227776117972\n",
      "Current iteration=3243, loss=25782.21994852275\n",
      "Current iteration=3244, loss=25782.212126416107\n",
      "Current iteration=3245, loss=25782.20430979206\n",
      "Current iteration=3246, loss=25782.19649864463\n",
      "Current iteration=3247, loss=25782.188692967837\n",
      "Current iteration=3248, loss=25782.180892755714\n",
      "Current iteration=3249, loss=25782.173098002313\n",
      "Current iteration=3250, loss=25782.165308701664\n",
      "Current iteration=3251, loss=25782.15752484783\n",
      "Current iteration=3252, loss=25782.14974643488\n",
      "Current iteration=3253, loss=25782.141973456866\n",
      "Current iteration=3254, loss=25782.134205907867\n",
      "Current iteration=3255, loss=25782.12644378196\n",
      "Current iteration=3256, loss=25782.11868707324\n",
      "Current iteration=3257, loss=25782.110935775803\n",
      "Current iteration=3258, loss=25782.103189883743\n",
      "Current iteration=3259, loss=25782.095449391167\n",
      "Current iteration=3260, loss=25782.08771429219\n",
      "Current iteration=3261, loss=25782.079984580938\n",
      "Current iteration=3262, loss=25782.072260251527\n",
      "Current iteration=3263, loss=25782.064541298103\n",
      "Current iteration=3264, loss=25782.0568277148\n",
      "Current iteration=3265, loss=25782.049119495772\n",
      "Current iteration=3266, loss=25782.041416635162\n",
      "Current iteration=3267, loss=25782.03371912714\n",
      "Current iteration=3268, loss=25782.026026965872\n",
      "Current iteration=3269, loss=25782.01834014553\n",
      "Current iteration=3270, loss=25782.010658660285\n",
      "Current iteration=3271, loss=25782.002982504342\n",
      "Current iteration=3272, loss=25781.99531167189\n",
      "Current iteration=3273, loss=25781.98764615712\n",
      "Current iteration=3274, loss=25781.97998595424\n",
      "Current iteration=3275, loss=25781.972331057466\n",
      "Current iteration=3276, loss=25781.964681461024\n",
      "Current iteration=3277, loss=25781.957037159133\n",
      "Current iteration=3278, loss=25781.949398146033\n",
      "Current iteration=3279, loss=25781.94176441595\n",
      "Current iteration=3280, loss=25781.934135963143\n",
      "Current iteration=3281, loss=25781.926512781858\n",
      "Current iteration=3282, loss=25781.918894866358\n",
      "Current iteration=3283, loss=25781.911282210905\n",
      "Current iteration=3284, loss=25781.903674809768\n",
      "Current iteration=3285, loss=25781.896072657233\n",
      "Current iteration=3286, loss=25781.888475747583\n",
      "Current iteration=3287, loss=25781.8808840751\n",
      "Current iteration=3288, loss=25781.873297634098\n",
      "Current iteration=3289, loss=25781.86571641887\n",
      "Current iteration=3290, loss=25781.858140423727\n",
      "Current iteration=3291, loss=25781.850569642993\n",
      "Current iteration=3292, loss=25781.843004070983\n",
      "Current iteration=3293, loss=25781.835443702028\n",
      "Current iteration=3294, loss=25781.827888530475\n",
      "Current iteration=3295, loss=25781.82033855065\n",
      "Current iteration=3296, loss=25781.81279375691\n",
      "Current iteration=3297, loss=25781.80525414362\n",
      "Current iteration=3298, loss=25781.797719705126\n",
      "Current iteration=3299, loss=25781.79019043581\n",
      "Current iteration=3300, loss=25781.782666330044\n",
      "Current iteration=3301, loss=25781.7751473822\n",
      "Current iteration=3302, loss=25781.767633586664\n",
      "Current iteration=3303, loss=25781.760124937846\n",
      "Current iteration=3304, loss=25781.752621430132\n",
      "Current iteration=3305, loss=25781.745123057928\n",
      "Current iteration=3306, loss=25781.737629815663\n",
      "Current iteration=3307, loss=25781.730141697735\n",
      "Current iteration=3308, loss=25781.72265869858\n",
      "Current iteration=3309, loss=25781.71518081263\n",
      "Current iteration=3310, loss=25781.70770803433\n",
      "Current iteration=3311, loss=25781.700240358106\n",
      "Current iteration=3312, loss=25781.692777778415\n",
      "Current iteration=3313, loss=25781.685320289722\n",
      "Current iteration=3314, loss=25781.67786788648\n",
      "Current iteration=3315, loss=25781.670420563165\n",
      "Current iteration=3316, loss=25781.662978314245\n",
      "Current iteration=3317, loss=25781.655541134212\n",
      "Current iteration=3318, loss=25781.64810901755\n",
      "Current iteration=3319, loss=25781.640681958754\n",
      "Current iteration=3320, loss=25781.633259952312\n",
      "Current iteration=3321, loss=25781.625842992747\n",
      "Current iteration=3322, loss=25781.618431074567\n",
      "Current iteration=3323, loss=25781.611024192283\n",
      "Current iteration=3324, loss=25781.603622340434\n",
      "Current iteration=3325, loss=25781.59622551354\n",
      "Current iteration=3326, loss=25781.588833706137\n",
      "Current iteration=3327, loss=25781.58144691278\n",
      "Current iteration=3328, loss=25781.574065128007\n",
      "Current iteration=3329, loss=25781.56668834638\n",
      "Current iteration=3330, loss=25781.559316562467\n",
      "Current iteration=3331, loss=25781.551949770823\n",
      "Current iteration=3332, loss=25781.544587966033\n",
      "Current iteration=3333, loss=25781.53723114267\n",
      "Current iteration=3334, loss=25781.529879295333\n",
      "Current iteration=3335, loss=25781.522532418596\n",
      "Current iteration=3336, loss=25781.515190507067\n",
      "Current iteration=3337, loss=25781.50785355535\n",
      "Current iteration=3338, loss=25781.500521558068\n",
      "Current iteration=3339, loss=25781.493194509814\n",
      "Current iteration=3340, loss=25781.485872405236\n",
      "Current iteration=3341, loss=25781.478555238944\n",
      "Current iteration=3342, loss=25781.471243005588\n",
      "Current iteration=3343, loss=25781.463935699787\n",
      "Current iteration=3344, loss=25781.456633316215\n",
      "Current iteration=3345, loss=25781.449335849513\n",
      "Current iteration=3346, loss=25781.442043294337\n",
      "Current iteration=3347, loss=25781.43475564536\n",
      "Current iteration=3348, loss=25781.427472897245\n",
      "Current iteration=3349, loss=25781.420195044677\n",
      "Current iteration=3350, loss=25781.412922082338\n",
      "Current iteration=3351, loss=25781.405654004906\n",
      "Current iteration=3352, loss=25781.3983908071\n",
      "Current iteration=3353, loss=25781.391132483594\n",
      "Current iteration=3354, loss=25781.383879029112\n",
      "Current iteration=3355, loss=25781.376630438364\n",
      "Current iteration=3356, loss=25781.36938670607\n",
      "Current iteration=3357, loss=25781.362147826956\n",
      "Current iteration=3358, loss=25781.354913795745\n",
      "Current iteration=3359, loss=25781.347684607183\n",
      "Current iteration=3360, loss=25781.34046025601\n",
      "Current iteration=3361, loss=25781.333240736978\n",
      "Current iteration=3362, loss=25781.326026044833\n",
      "Current iteration=3363, loss=25781.318816174342\n",
      "Current iteration=3364, loss=25781.311611120273\n",
      "Current iteration=3365, loss=25781.304410877394\n",
      "Current iteration=3366, loss=25781.297215440492\n",
      "Current iteration=3367, loss=25781.290024804337\n",
      "Current iteration=3368, loss=25781.28283896373\n",
      "Current iteration=3369, loss=25781.275657913462\n",
      "Current iteration=3370, loss=25781.268481648338\n",
      "Current iteration=3371, loss=25781.261310163165\n",
      "Current iteration=3372, loss=25781.254143452752\n",
      "Current iteration=3373, loss=25781.246981511926\n",
      "Current iteration=3374, loss=25781.239824335506\n",
      "Current iteration=3375, loss=25781.232671918322\n",
      "Current iteration=3376, loss=25781.22552425522\n",
      "Current iteration=3377, loss=25781.21838134103\n",
      "Current iteration=3378, loss=25781.211243170615\n",
      "Current iteration=3379, loss=25781.204109738817\n",
      "Current iteration=3380, loss=25781.1969810405\n",
      "Current iteration=3381, loss=25781.18985707053\n",
      "Current iteration=3382, loss=25781.18273782378\n",
      "Current iteration=3383, loss=25781.175623295123\n",
      "Current iteration=3384, loss=25781.168513479442\n",
      "Current iteration=3385, loss=25781.161408371634\n",
      "Current iteration=3386, loss=25781.154307966586\n",
      "Current iteration=3387, loss=25781.147212259202\n",
      "Current iteration=3388, loss=25781.140121244385\n",
      "Current iteration=3389, loss=25781.13303491705\n",
      "Current iteration=3390, loss=25781.12595327211\n",
      "Current iteration=3391, loss=25781.11887630449\n",
      "Current iteration=3392, loss=25781.11180400912\n",
      "Current iteration=3393, loss=25781.10473638094\n",
      "Current iteration=3394, loss=25781.09767341488\n",
      "Current iteration=3395, loss=25781.09061510589\n",
      "Current iteration=3396, loss=25781.083561448926\n",
      "Current iteration=3397, loss=25781.076512438944\n",
      "Current iteration=3398, loss=25781.0694680709\n",
      "Current iteration=3399, loss=25781.062428339763\n",
      "Current iteration=3400, loss=25781.055393240516\n",
      "Current iteration=3401, loss=25781.048362768135\n",
      "Current iteration=3402, loss=25781.041336917602\n",
      "Current iteration=3403, loss=25781.03431568392\n",
      "Current iteration=3404, loss=25781.02729906207\n",
      "Current iteration=3405, loss=25781.020287047057\n",
      "Current iteration=3406, loss=25781.013279633906\n",
      "Current iteration=3407, loss=25781.006276817614\n",
      "Current iteration=3408, loss=25780.999278593197\n",
      "Current iteration=3409, loss=25780.992284955697\n",
      "Current iteration=3410, loss=25780.98529590013\n",
      "Current iteration=3411, loss=25780.978311421535\n",
      "Current iteration=3412, loss=25780.971331514964\n",
      "Current iteration=3413, loss=25780.964356175446\n",
      "Current iteration=3414, loss=25780.95738539805\n",
      "Current iteration=3415, loss=25780.95041917783\n",
      "Current iteration=3416, loss=25780.943457509835\n",
      "Current iteration=3417, loss=25780.93650038916\n",
      "Current iteration=3418, loss=25780.92954781087\n",
      "Current iteration=3419, loss=25780.922599770034\n",
      "Current iteration=3420, loss=25780.915656261746\n",
      "Current iteration=3421, loss=25780.908717281098\n",
      "Current iteration=3422, loss=25780.90178282319\n",
      "Current iteration=3423, loss=25780.894852883124\n",
      "Current iteration=3424, loss=25780.887927456\n",
      "Current iteration=3425, loss=25780.881006536933\n",
      "Current iteration=3426, loss=25780.87409012106\n",
      "Current iteration=3427, loss=25780.867178203476\n",
      "Current iteration=3428, loss=25780.860270779336\n",
      "Current iteration=3429, loss=25780.85336784376\n",
      "Current iteration=3430, loss=25780.846469391898\n",
      "Current iteration=3431, loss=25780.839575418893\n",
      "Current iteration=3432, loss=25780.83268591989\n",
      "Current iteration=3433, loss=25780.825800890063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=3434, loss=25780.81892032455\n",
      "Current iteration=3435, loss=25780.812044218546\n",
      "Current iteration=3436, loss=25780.805172567205\n",
      "Current iteration=3437, loss=25780.798305365715\n",
      "Current iteration=3438, loss=25780.791442609247\n",
      "Current iteration=3439, loss=25780.78458429301\n",
      "Current iteration=3440, loss=25780.777730412196\n",
      "Current iteration=3441, loss=25780.77088096199\n",
      "Current iteration=3442, loss=25780.76403593761\n",
      "Current iteration=3443, loss=25780.75719533427\n",
      "Current iteration=3444, loss=25780.750359147172\n",
      "Current iteration=3445, loss=25780.743527371556\n",
      "Current iteration=3446, loss=25780.73670000263\n",
      "Current iteration=3447, loss=25780.729877035646\n",
      "Current iteration=3448, loss=25780.723058465825\n",
      "Current iteration=3449, loss=25780.716244288422\n",
      "Current iteration=3450, loss=25780.70943449868\n",
      "Current iteration=3451, loss=25780.702629091855\n",
      "Current iteration=3452, loss=25780.69582806321\n",
      "Current iteration=3453, loss=25780.689031408\n",
      "Current iteration=3454, loss=25780.6822391215\n",
      "Current iteration=3455, loss=25780.675451198986\n",
      "Current iteration=3456, loss=25780.668667635735\n",
      "Current iteration=3457, loss=25780.661888427036\n",
      "Current iteration=3458, loss=25780.655113568188\n",
      "Current iteration=3459, loss=25780.648343054472\n",
      "Current iteration=3460, loss=25780.641576881197\n",
      "Current iteration=3461, loss=25780.63481504367\n",
      "Current iteration=3462, loss=25780.628057537204\n",
      "Current iteration=3463, loss=25780.62130435711\n",
      "Current iteration=3464, loss=25780.614555498716\n",
      "Current iteration=3465, loss=25780.607810957354\n",
      "Current iteration=3466, loss=25780.60107072835\n",
      "Current iteration=3467, loss=25780.594334807043\n",
      "Current iteration=3468, loss=25780.587603188775\n",
      "Current iteration=3469, loss=25780.580875868905\n",
      "Current iteration=3470, loss=25780.574152842775\n",
      "Current iteration=3471, loss=25780.567434105746\n",
      "Current iteration=3472, loss=25780.56071965319\n",
      "Current iteration=3473, loss=25780.554009480467\n",
      "Current iteration=3474, loss=25780.547303582953\n",
      "Current iteration=3475, loss=25780.540601956036\n",
      "Current iteration=3476, loss=25780.533904595093\n",
      "Current iteration=3477, loss=25780.527211495522\n",
      "Current iteration=3478, loss=25780.520522652703\n",
      "Current iteration=3479, loss=25780.513838062056\n",
      "Current iteration=3480, loss=25780.507157718974\n",
      "Current iteration=3481, loss=25780.50048161887\n",
      "Current iteration=3482, loss=25780.49380975716\n",
      "Current iteration=3483, loss=25780.48714212927\n",
      "Current iteration=3484, loss=25780.48047873062\n",
      "Current iteration=3485, loss=25780.473819556646\n",
      "Current iteration=3486, loss=25780.46716460278\n",
      "Current iteration=3487, loss=25780.460513864462\n",
      "Current iteration=3488, loss=25780.453867337146\n",
      "Current iteration=3489, loss=25780.44722501628\n",
      "Current iteration=3490, loss=25780.440586897323\n",
      "Current iteration=3491, loss=25780.433952975727\n",
      "Current iteration=3492, loss=25780.427323246968\n",
      "Current iteration=3493, loss=25780.420697706515\n",
      "Current iteration=3494, loss=25780.41407634985\n",
      "Current iteration=3495, loss=25780.407459172446\n",
      "Current iteration=3496, loss=25780.400846169803\n",
      "Current iteration=3497, loss=25780.394237337394\n",
      "Current iteration=3498, loss=25780.387632670736\n",
      "Current iteration=3499, loss=25780.38103216532\n",
      "Current iteration=3500, loss=25780.37443581666\n",
      "Current iteration=3501, loss=25780.367843620264\n",
      "Current iteration=3502, loss=25780.361255571646\n",
      "Current iteration=3503, loss=25780.354671666333\n",
      "Current iteration=3504, loss=25780.348091899858\n",
      "Current iteration=3505, loss=25780.341516267734\n",
      "Current iteration=3506, loss=25780.334944765524\n",
      "Current iteration=3507, loss=25780.328377388752\n",
      "Current iteration=3508, loss=25780.32181413297\n",
      "Current iteration=3509, loss=25780.315254993737\n",
      "Current iteration=3510, loss=25780.308699966597\n",
      "Current iteration=3511, loss=25780.302149047122\n",
      "Current iteration=3512, loss=25780.29560223088\n",
      "Current iteration=3513, loss=25780.28905951343\n",
      "Current iteration=3514, loss=25780.282520890367\n",
      "Current iteration=3515, loss=25780.275986357265\n",
      "Current iteration=3516, loss=25780.269455909714\n",
      "Current iteration=3517, loss=25780.262929543296\n",
      "Current iteration=3518, loss=25780.25640725362\n",
      "Current iteration=3519, loss=25780.249889036277\n",
      "Current iteration=3520, loss=25780.24337488688\n",
      "Current iteration=3521, loss=25780.236864801038\n",
      "Current iteration=3522, loss=25780.23035877437\n",
      "Current iteration=3523, loss=25780.2238568025\n",
      "Current iteration=3524, loss=25780.21735888104\n",
      "Current iteration=3525, loss=25780.210865005643\n",
      "Current iteration=3526, loss=25780.20437517193\n",
      "Current iteration=3527, loss=25780.19788937554\n",
      "Current iteration=3528, loss=25780.19140761213\n",
      "Current iteration=3529, loss=25780.18492987734\n",
      "Current iteration=3530, loss=25780.178456166832\n",
      "Current iteration=3531, loss=25780.17198647626\n",
      "Current iteration=3532, loss=25780.1655208013\n",
      "Current iteration=3533, loss=25780.15905913761\n",
      "Current iteration=3534, loss=25780.152601480873\n",
      "Current iteration=3535, loss=25780.14614782676\n",
      "Current iteration=3536, loss=25780.13969817097\n",
      "Current iteration=3537, loss=25780.133252509168\n",
      "Current iteration=3538, loss=25780.126810837082\n",
      "Current iteration=3539, loss=25780.120373150377\n",
      "Current iteration=3540, loss=25780.11393944478\n",
      "Current iteration=3541, loss=25780.10750971599\n",
      "Current iteration=3542, loss=25780.10108395972\n",
      "Current iteration=3543, loss=25780.094662171683\n",
      "Current iteration=3544, loss=25780.088244347615\n",
      "Current iteration=3545, loss=25780.081830483236\n",
      "Current iteration=3546, loss=25780.075420574278\n",
      "Current iteration=3547, loss=25780.069014616478\n",
      "Current iteration=3548, loss=25780.062612605572\n",
      "Current iteration=3549, loss=25780.05621453733\n",
      "Current iteration=3550, loss=25780.049820407472\n",
      "Current iteration=3551, loss=25780.043430211772\n",
      "Current iteration=3552, loss=25780.03704394599\n",
      "Current iteration=3553, loss=25780.030661605888\n",
      "Current iteration=3554, loss=25780.024283187242\n",
      "Current iteration=3555, loss=25780.01790868582\n",
      "Current iteration=3556, loss=25780.011538097402\n",
      "Current iteration=3557, loss=25780.00517141777\n",
      "Current iteration=3558, loss=25779.99880864272\n",
      "Current iteration=3559, loss=25779.992449768048\n",
      "Current iteration=3560, loss=25779.98609478954\n",
      "Current iteration=3561, loss=25779.97974370301\n",
      "Current iteration=3562, loss=25779.973396504265\n",
      "Current iteration=3563, loss=25779.967053189117\n",
      "Current iteration=3564, loss=25779.960713753368\n",
      "Current iteration=3565, loss=25779.954378192866\n",
      "Current iteration=3566, loss=25779.94804650342\n",
      "Current iteration=3567, loss=25779.941718680864\n",
      "Current iteration=3568, loss=25779.935394721033\n",
      "Current iteration=3569, loss=25779.929074619777\n",
      "Current iteration=3570, loss=25779.92275837293\n",
      "Current iteration=3571, loss=25779.91644597634\n",
      "Current iteration=3572, loss=25779.910137425875\n",
      "Current iteration=3573, loss=25779.90383271738\n",
      "Current iteration=3574, loss=25779.897531846727\n",
      "Current iteration=3575, loss=25779.891234809777\n",
      "Current iteration=3576, loss=25779.88494160241\n",
      "Current iteration=3577, loss=25779.878652220497\n",
      "Current iteration=3578, loss=25779.87236665993\n",
      "Current iteration=3579, loss=25779.866084916575\n",
      "Current iteration=3580, loss=25779.85980698635\n",
      "Current iteration=3581, loss=25779.85353286513\n",
      "Current iteration=3582, loss=25779.847262548832\n",
      "Current iteration=3583, loss=25779.84099603334\n",
      "Current iteration=3584, loss=25779.83473331458\n",
      "Current iteration=3585, loss=25779.82847438845\n",
      "Current iteration=3586, loss=25779.822219250895\n",
      "Current iteration=3587, loss=25779.81596789781\n",
      "Current iteration=3588, loss=25779.809720325142\n",
      "Current iteration=3589, loss=25779.803476528814\n",
      "Current iteration=3590, loss=25779.79723650476\n",
      "Current iteration=3591, loss=25779.791000248926\n",
      "Current iteration=3592, loss=25779.784767757257\n",
      "Current iteration=3593, loss=25779.778539025705\n",
      "Current iteration=3594, loss=25779.77231405022\n",
      "Current iteration=3595, loss=25779.76609282677\n",
      "Current iteration=3596, loss=25779.75987535131\n",
      "Current iteration=3597, loss=25779.7536616198\n",
      "Current iteration=3598, loss=25779.747451628235\n",
      "Current iteration=3599, loss=25779.741245372574\n",
      "Current iteration=3600, loss=25779.735042848803\n",
      "Current iteration=3601, loss=25779.728844052923\n",
      "Current iteration=3602, loss=25779.722648980896\n",
      "Current iteration=3603, loss=25779.71645762874\n",
      "Current iteration=3604, loss=25779.710269992444\n",
      "Current iteration=3605, loss=25779.704086068014\n",
      "Current iteration=3606, loss=25779.697905851463\n",
      "Current iteration=3607, loss=25779.691729338796\n",
      "Current iteration=3608, loss=25779.685556526034\n",
      "Current iteration=3609, loss=25779.679387409204\n",
      "Current iteration=3610, loss=25779.673221984314\n",
      "Current iteration=3611, loss=25779.66706024742\n",
      "Current iteration=3612, loss=25779.66090219454\n",
      "Current iteration=3613, loss=25779.65474782171\n",
      "Current iteration=3614, loss=25779.648597124993\n",
      "Current iteration=3615, loss=25779.64245010042\n",
      "Current iteration=3616, loss=25779.636306744047\n",
      "Current iteration=3617, loss=25779.63016705193\n",
      "Current iteration=3618, loss=25779.624031020136\n",
      "Current iteration=3619, loss=25779.61789864473\n",
      "Current iteration=3620, loss=25779.611769921776\n",
      "Current iteration=3621, loss=25779.605644847354\n",
      "Current iteration=3622, loss=25779.59952341754\n",
      "Current iteration=3623, loss=25779.593405628413\n",
      "Current iteration=3624, loss=25779.587291476066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=3625, loss=25779.58118095659\n",
      "Current iteration=3626, loss=25779.57507406608\n",
      "Current iteration=3627, loss=25779.56897080064\n",
      "Current iteration=3628, loss=25779.562871156373\n",
      "Current iteration=3629, loss=25779.55677512939\n",
      "Current iteration=3630, loss=25779.5506827158\n",
      "Current iteration=3631, loss=25779.544593911716\n",
      "Current iteration=3632, loss=25779.53850871327\n",
      "Current iteration=3633, loss=25779.53242711659\n",
      "Current iteration=3634, loss=25779.5263491178\n",
      "Current iteration=3635, loss=25779.52027471303\n",
      "Current iteration=3636, loss=25779.514203898438\n",
      "Current iteration=3637, loss=25779.508136670152\n",
      "Current iteration=3638, loss=25779.502073024316\n",
      "Current iteration=3639, loss=25779.496012957097\n",
      "Current iteration=3640, loss=25779.489956464644\n",
      "Current iteration=3641, loss=25779.483903543118\n",
      "Current iteration=3642, loss=25779.477854188677\n",
      "Current iteration=3643, loss=25779.471808397502\n",
      "Current iteration=3644, loss=25779.46576616577\n",
      "Current iteration=3645, loss=25779.459727489633\n",
      "Current iteration=3646, loss=25779.453692365303\n",
      "Current iteration=3647, loss=25779.447660788952\n",
      "Current iteration=3648, loss=25779.44163275677\n",
      "Current iteration=3649, loss=25779.435608264954\n",
      "Current iteration=3650, loss=25779.4295873097\n",
      "Current iteration=3651, loss=25779.42356988722\n",
      "Current iteration=3652, loss=25779.417555993707\n",
      "Current iteration=3653, loss=25779.411545625386\n",
      "Current iteration=3654, loss=25779.40553877847\n",
      "Current iteration=3655, loss=25779.39953544917\n",
      "Current iteration=3656, loss=25779.39353563372\n",
      "Current iteration=3657, loss=25779.38753932834\n",
      "Current iteration=3658, loss=25779.381546529272\n",
      "Current iteration=3659, loss=25779.375557232746\n",
      "Current iteration=3660, loss=25779.369571435\n",
      "Current iteration=3661, loss=25779.363589132292\n",
      "Current iteration=3662, loss=25779.357610320865\n",
      "Current iteration=3663, loss=25779.351634996965\n",
      "Current iteration=3664, loss=25779.345663156855\n",
      "Current iteration=3665, loss=25779.339694796796\n",
      "Current iteration=3666, loss=25779.333729913054\n",
      "Current iteration=3667, loss=25779.327768501902\n",
      "Current iteration=3668, loss=25779.321810559617\n",
      "Current iteration=3669, loss=25779.31585608246\n",
      "Current iteration=3670, loss=25779.309905066726\n",
      "Current iteration=3671, loss=25779.30395750871\n",
      "Current iteration=3672, loss=25779.29801340469\n",
      "Current iteration=3673, loss=25779.29207275096\n",
      "Current iteration=3674, loss=25779.28613554383\n",
      "Current iteration=3675, loss=25779.280201779588\n",
      "Current iteration=3676, loss=25779.274271454553\n",
      "Current iteration=3677, loss=25779.268344565033\n",
      "Current iteration=3678, loss=25779.262421107338\n",
      "Current iteration=3679, loss=25779.256501077794\n",
      "Current iteration=3680, loss=25779.250584472724\n",
      "Current iteration=3681, loss=25779.244671288456\n",
      "Current iteration=3682, loss=25779.23876152131\n",
      "Current iteration=3683, loss=25779.232855167636\n",
      "Current iteration=3684, loss=25779.22695222377\n",
      "Current iteration=3685, loss=25779.22105268605\n",
      "Current iteration=3686, loss=25779.21515655083\n",
      "Current iteration=3687, loss=25779.209263814457\n",
      "Current iteration=3688, loss=25779.203374473298\n",
      "Current iteration=3689, loss=25779.197488523696\n",
      "Current iteration=3690, loss=25779.191605962023\n",
      "Current iteration=3691, loss=25779.185726784654\n",
      "Current iteration=3692, loss=25779.17985098795\n",
      "Current iteration=3693, loss=25779.173978568288\n",
      "Current iteration=3694, loss=25779.16810952206\n",
      "Current iteration=3695, loss=25779.162243845636\n",
      "Current iteration=3696, loss=25779.156381535413\n",
      "Current iteration=3697, loss=25779.15052258777\n",
      "Current iteration=3698, loss=25779.14466699913\n",
      "Current iteration=3699, loss=25779.138814765858\n",
      "Current iteration=3700, loss=25779.13296588439\n",
      "Current iteration=3701, loss=25779.127120351113\n",
      "Current iteration=3702, loss=25779.121278162445\n",
      "Current iteration=3703, loss=25779.115439314806\n",
      "Current iteration=3704, loss=25779.109603804613\n",
      "Current iteration=3705, loss=25779.103771628288\n",
      "Current iteration=3706, loss=25779.097942782264\n",
      "Current iteration=3707, loss=25779.092117262964\n",
      "Current iteration=3708, loss=25779.086295066838\n",
      "Current iteration=3709, loss=25779.080476190305\n",
      "Current iteration=3710, loss=25779.07466062983\n",
      "Current iteration=3711, loss=25779.068848381852\n",
      "Current iteration=3712, loss=25779.06303944281\n",
      "Current iteration=3713, loss=25779.05723380918\n",
      "Current iteration=3714, loss=25779.05143147742\n",
      "Current iteration=3715, loss=25779.04563244398\n",
      "Current iteration=3716, loss=25779.039836705328\n",
      "Current iteration=3717, loss=25779.034044257947\n",
      "Current iteration=3718, loss=25779.028255098303\n",
      "Current iteration=3719, loss=25779.022469222877\n",
      "Current iteration=3720, loss=25779.016686628154\n",
      "Current iteration=3721, loss=25779.01090731062\n",
      "Current iteration=3722, loss=25779.005131266764\n",
      "Current iteration=3723, loss=25778.99935849308\n",
      "Current iteration=3724, loss=25778.993588986068\n",
      "Current iteration=3725, loss=25778.987822742238\n",
      "Current iteration=3726, loss=25778.982059758077\n",
      "Current iteration=3727, loss=25778.97630003011\n",
      "Current iteration=3728, loss=25778.970543554846\n",
      "Current iteration=3729, loss=25778.964790328806\n",
      "Current iteration=3730, loss=25778.95904034851\n",
      "Current iteration=3731, loss=25778.95329361048\n",
      "Current iteration=3732, loss=25778.94755011125\n",
      "Current iteration=3733, loss=25778.94180984735\n",
      "Current iteration=3734, loss=25778.936072815322\n",
      "Current iteration=3735, loss=25778.930339011706\n",
      "Current iteration=3736, loss=25778.924608433037\n",
      "Current iteration=3737, loss=25778.91888107587\n",
      "Current iteration=3738, loss=25778.91315693676\n",
      "Current iteration=3739, loss=25778.90743601226\n",
      "Current iteration=3740, loss=25778.901718298926\n",
      "Current iteration=3741, loss=25778.896003793336\n",
      "Current iteration=3742, loss=25778.890292492044\n",
      "Current iteration=3743, loss=25778.884584391624\n",
      "Current iteration=3744, loss=25778.87887948865\n",
      "Current iteration=3745, loss=25778.873177779707\n",
      "Current iteration=3746, loss=25778.867479261367\n",
      "Current iteration=3747, loss=25778.861783930235\n",
      "Current iteration=3748, loss=25778.856091782876\n",
      "Current iteration=3749, loss=25778.850402815908\n",
      "Current iteration=3750, loss=25778.84471702591\n",
      "Current iteration=3751, loss=25778.839034409495\n",
      "Current iteration=3752, loss=25778.833354963266\n",
      "Current iteration=3753, loss=25778.82767868383\n",
      "Current iteration=3754, loss=25778.8220055678\n",
      "Current iteration=3755, loss=25778.81633561179\n",
      "Current iteration=3756, loss=25778.81066881243\n",
      "Current iteration=3757, loss=25778.805005166334\n",
      "Current iteration=3758, loss=25778.79934467013\n",
      "Current iteration=3759, loss=25778.793687320453\n",
      "Current iteration=3760, loss=25778.78803311394\n",
      "Current iteration=3761, loss=25778.782382047226\n",
      "Current iteration=3762, loss=25778.77673411696\n",
      "Current iteration=3763, loss=25778.77108931978\n",
      "Current iteration=3764, loss=25778.765447652346\n",
      "Current iteration=3765, loss=25778.759809111296\n",
      "Current iteration=3766, loss=25778.754173693298\n",
      "Current iteration=3767, loss=25778.74854139502\n",
      "Current iteration=3768, loss=25778.742912213107\n",
      "Current iteration=3769, loss=25778.73728614425\n",
      "Current iteration=3770, loss=25778.7316631851\n",
      "Current iteration=3771, loss=25778.72604333235\n",
      "Current iteration=3772, loss=25778.72042658267\n",
      "Current iteration=3773, loss=25778.71481293275\n",
      "Current iteration=3774, loss=25778.70920237927\n",
      "Current iteration=3775, loss=25778.703594918923\n",
      "Current iteration=3776, loss=25778.697990548404\n",
      "Current iteration=3777, loss=25778.69238926441\n",
      "Current iteration=3778, loss=25778.686791063643\n",
      "Current iteration=3779, loss=25778.68119594281\n",
      "Current iteration=3780, loss=25778.675603898617\n",
      "Current iteration=3781, loss=25778.67001492777\n",
      "Current iteration=3782, loss=25778.664429027005\n",
      "Current iteration=3783, loss=25778.658846193022\n",
      "Current iteration=3784, loss=25778.65326642255\n",
      "Current iteration=3785, loss=25778.64768971232\n",
      "Current iteration=3786, loss=25778.642116059058\n",
      "Current iteration=3787, loss=25778.6365454595\n",
      "Current iteration=3788, loss=25778.63097791038\n",
      "Current iteration=3789, loss=25778.62541340845\n",
      "Current iteration=3790, loss=25778.619851950443\n",
      "Current iteration=3791, loss=25778.614293533108\n",
      "Current iteration=3792, loss=25778.608738153205\n",
      "Current iteration=3793, loss=25778.603185807486\n",
      "Current iteration=3794, loss=25778.597636492712\n",
      "Current iteration=3795, loss=25778.592090205642\n",
      "Current iteration=3796, loss=25778.58654694305\n",
      "Current iteration=3797, loss=25778.58100670169\n",
      "Current iteration=3798, loss=25778.57546947835\n",
      "Current iteration=3799, loss=25778.569935269807\n",
      "Current iteration=3800, loss=25778.56440407284\n",
      "Current iteration=3801, loss=25778.558875884224\n",
      "Current iteration=3802, loss=25778.553350700757\n",
      "Current iteration=3803, loss=25778.547828519222\n",
      "Current iteration=3804, loss=25778.542309336426\n",
      "Current iteration=3805, loss=25778.536793149164\n",
      "Current iteration=3806, loss=25778.531279954226\n",
      "Current iteration=3807, loss=25778.525769748427\n",
      "Current iteration=3808, loss=25778.52026252857\n",
      "Current iteration=3809, loss=25778.514758291483\n",
      "Current iteration=3810, loss=25778.50925703397\n",
      "Current iteration=3811, loss=25778.50375875285\n",
      "Current iteration=3812, loss=25778.498263444948\n",
      "Current iteration=3813, loss=25778.49277110709\n",
      "Current iteration=3814, loss=25778.487281736103\n",
      "Current iteration=3815, loss=25778.48179532883\n",
      "Current iteration=3816, loss=25778.476311882096\n",
      "Current iteration=3817, loss=25778.47083139275\n",
      "Current iteration=3818, loss=25778.46535385764\n",
      "Current iteration=3819, loss=25778.4598792736\n",
      "Current iteration=3820, loss=25778.45440763749\n",
      "Current iteration=3821, loss=25778.448938946167\n",
      "Current iteration=3822, loss=25778.44347319648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=3823, loss=25778.438010385296\n",
      "Current iteration=3824, loss=25778.43255050948\n",
      "Current iteration=3825, loss=25778.427093565893\n",
      "Current iteration=3826, loss=25778.421639551427\n",
      "Current iteration=3827, loss=25778.416188462925\n",
      "Current iteration=3828, loss=25778.4107402973\n",
      "Current iteration=3829, loss=25778.40529505141\n",
      "Current iteration=3830, loss=25778.39985272215\n",
      "Current iteration=3831, loss=25778.394413306403\n",
      "Current iteration=3832, loss=25778.38897680107\n",
      "Current iteration=3833, loss=25778.383543203043\n",
      "Current iteration=3834, loss=25778.378112509217\n",
      "Current iteration=3835, loss=25778.372684716505\n",
      "Current iteration=3836, loss=25778.367259821804\n",
      "Current iteration=3837, loss=25778.36183782202\n",
      "Current iteration=3838, loss=25778.356418714076\n",
      "Current iteration=3839, loss=25778.351002494885\n",
      "Current iteration=3840, loss=25778.345589161374\n",
      "Current iteration=3841, loss=25778.340178710445\n",
      "Current iteration=3842, loss=25778.334771139045\n",
      "Current iteration=3843, loss=25778.329366444097\n",
      "Current iteration=3844, loss=25778.323964622523\n",
      "Current iteration=3845, loss=25778.318565671278\n",
      "Current iteration=3846, loss=25778.313169587294\n",
      "Current iteration=3847, loss=25778.307776367514\n",
      "Current iteration=3848, loss=25778.302386008887\n",
      "Current iteration=3849, loss=25778.29699850836\n",
      "Current iteration=3850, loss=25778.29161386289\n",
      "Current iteration=3851, loss=25778.28623206943\n",
      "Current iteration=3852, loss=25778.28085312494\n",
      "Current iteration=3853, loss=25778.275477026385\n",
      "Current iteration=3854, loss=25778.270103770737\n",
      "Current iteration=3855, loss=25778.264733354958\n",
      "Current iteration=3856, loss=25778.259365776026\n",
      "Current iteration=3857, loss=25778.254001030917\n",
      "Current iteration=3858, loss=25778.24863911661\n",
      "Current iteration=3859, loss=25778.24328003009\n",
      "Current iteration=3860, loss=25778.23792376834\n",
      "Current iteration=3861, loss=25778.23257032836\n",
      "Current iteration=3862, loss=25778.227219707136\n",
      "Current iteration=3863, loss=25778.221871901664\n",
      "Current iteration=3864, loss=25778.21652690895\n",
      "Current iteration=3865, loss=25778.211184725984\n",
      "Current iteration=3866, loss=25778.205845349792\n",
      "Current iteration=3867, loss=25778.20050877737\n",
      "Current iteration=3868, loss=25778.195175005734\n",
      "Current iteration=3869, loss=25778.189844031906\n",
      "Current iteration=3870, loss=25778.184515852907\n",
      "Current iteration=3871, loss=25778.179190465744\n",
      "Current iteration=3872, loss=25778.17386786746\n",
      "Current iteration=3873, loss=25778.168548055073\n",
      "Current iteration=3874, loss=25778.163231025632\n",
      "Current iteration=3875, loss=25778.15791677616\n",
      "Current iteration=3876, loss=25778.152605303698\n",
      "Current iteration=3877, loss=25778.147296605293\n",
      "Current iteration=3878, loss=25778.14199067798\n",
      "Current iteration=3879, loss=25778.13668751883\n",
      "Current iteration=3880, loss=25778.131387124875\n",
      "Current iteration=3881, loss=25778.126089493177\n",
      "Current iteration=3882, loss=25778.120794620805\n",
      "Current iteration=3883, loss=25778.115502504806\n",
      "Current iteration=3884, loss=25778.110213142256\n",
      "Current iteration=3885, loss=25778.10492653021\n",
      "Current iteration=3886, loss=25778.09964266576\n",
      "Current iteration=3887, loss=25778.094361545962\n",
      "Current iteration=3888, loss=25778.08908316791\n",
      "Current iteration=3889, loss=25778.083807528674\n",
      "Current iteration=3890, loss=25778.078534625347\n",
      "Current iteration=3891, loss=25778.073264455015\n",
      "Current iteration=3892, loss=25778.06799701476\n",
      "Current iteration=3893, loss=25778.062732301696\n",
      "Current iteration=3894, loss=25778.0574703129\n",
      "Current iteration=3895, loss=25778.05221104548\n",
      "Current iteration=3896, loss=25778.04695449654\n",
      "Current iteration=3897, loss=25778.041700663198\n",
      "Current iteration=3898, loss=25778.03644954255\n",
      "Current iteration=3899, loss=25778.031201131715\n",
      "Current iteration=3900, loss=25778.02595542781\n",
      "Current iteration=3901, loss=25778.020712427948\n",
      "Current iteration=3902, loss=25778.015472129264\n",
      "Current iteration=3903, loss=25778.010234528876\n",
      "Current iteration=3904, loss=25778.004999623918\n",
      "Current iteration=3905, loss=25777.99976741152\n",
      "Current iteration=3906, loss=25777.99453788881\n",
      "Current iteration=3907, loss=25777.989311052937\n",
      "Current iteration=3908, loss=25777.984086901044\n",
      "Current iteration=3909, loss=25777.978865430276\n",
      "Current iteration=3910, loss=25777.973646637773\n",
      "Current iteration=3911, loss=25777.968430520697\n",
      "Current iteration=3912, loss=25777.96321707619\n",
      "Current iteration=3913, loss=25777.95800630142\n",
      "Current iteration=3914, loss=25777.95279819354\n",
      "Current iteration=3915, loss=25777.947592749715\n",
      "Current iteration=3916, loss=25777.942389967124\n",
      "Current iteration=3917, loss=25777.93718984292\n",
      "Current iteration=3918, loss=25777.93199237429\n",
      "Current iteration=3919, loss=25777.9267975584\n",
      "Current iteration=3920, loss=25777.92160539244\n",
      "Current iteration=3921, loss=25777.916415873588\n",
      "Current iteration=3922, loss=25777.91122899902\n",
      "Current iteration=3923, loss=25777.90604476594\n",
      "Current iteration=3924, loss=25777.900863171522\n",
      "Current iteration=3925, loss=25777.895684212985\n",
      "Current iteration=3926, loss=25777.89050788751\n",
      "Current iteration=3927, loss=25777.8853341923\n",
      "Current iteration=3928, loss=25777.880163124555\n",
      "Current iteration=3929, loss=25777.874994681497\n",
      "Current iteration=3930, loss=25777.869828860326\n",
      "Current iteration=3931, loss=25777.864665658253\n",
      "Current iteration=3932, loss=25777.859505072498\n",
      "Current iteration=3933, loss=25777.854347100285\n",
      "Current iteration=3934, loss=25777.849191738827\n",
      "Current iteration=3935, loss=25777.844038985357\n",
      "Current iteration=3936, loss=25777.8388888371\n",
      "Current iteration=3937, loss=25777.83374129129\n",
      "Current iteration=3938, loss=25777.828596345164\n",
      "Current iteration=3939, loss=25777.823453995952\n",
      "Current iteration=3940, loss=25777.8183142409\n",
      "Current iteration=3941, loss=25777.81317707725\n",
      "Current iteration=3942, loss=25777.80804250225\n",
      "Current iteration=3943, loss=25777.80291051315\n",
      "Current iteration=3944, loss=25777.7977811072\n",
      "Current iteration=3945, loss=25777.792654281664\n",
      "Current iteration=3946, loss=25777.787530033787\n",
      "Current iteration=3947, loss=25777.782408360847\n",
      "Current iteration=3948, loss=25777.77728926009\n",
      "Current iteration=3949, loss=25777.772172728815\n",
      "Current iteration=3950, loss=25777.767058764257\n",
      "Current iteration=3951, loss=25777.76194736371\n",
      "Current iteration=3952, loss=25777.75683852444\n",
      "Current iteration=3953, loss=25777.75173224374\n",
      "Current iteration=3954, loss=25777.74662851889\n",
      "Current iteration=3955, loss=25777.741527347163\n",
      "Current iteration=3956, loss=25777.736428725864\n",
      "Current iteration=3957, loss=25777.731332652278\n",
      "Current iteration=3958, loss=25777.726239123698\n",
      "Current iteration=3959, loss=25777.721148137425\n",
      "Current iteration=3960, loss=25777.716059690752\n",
      "Current iteration=3961, loss=25777.710973780995\n",
      "Current iteration=3962, loss=25777.705890405457\n",
      "Current iteration=3963, loss=25777.700809561436\n",
      "Current iteration=3964, loss=25777.695731246265\n",
      "Current iteration=3965, loss=25777.690655457238\n",
      "Current iteration=3966, loss=25777.68558219169\n",
      "Current iteration=3967, loss=25777.68051144693\n",
      "Current iteration=3968, loss=25777.675443220298\n",
      "Current iteration=3969, loss=25777.670377509105\n",
      "Current iteration=3970, loss=25777.66531431069\n",
      "Current iteration=3971, loss=25777.66025362238\n",
      "Current iteration=3972, loss=25777.655195441523\n",
      "Current iteration=3973, loss=25777.650139765443\n",
      "Current iteration=3974, loss=25777.645086591496\n",
      "Current iteration=3975, loss=25777.640035917007\n",
      "Current iteration=3976, loss=25777.634987739344\n",
      "Current iteration=3977, loss=25777.629942055857\n",
      "Current iteration=3978, loss=25777.624898863887\n",
      "Current iteration=3979, loss=25777.619858160797\n",
      "Current iteration=3980, loss=25777.61481994394\n",
      "Current iteration=3981, loss=25777.60978421069\n",
      "Current iteration=3982, loss=25777.604750958402\n",
      "Current iteration=3983, loss=25777.599720184448\n",
      "Current iteration=3984, loss=25777.594691886203\n",
      "Current iteration=3985, loss=25777.58966606103\n",
      "Current iteration=3986, loss=25777.58464270632\n",
      "Current iteration=3987, loss=25777.57962181944\n",
      "Current iteration=3988, loss=25777.57460339778\n",
      "Current iteration=3989, loss=25777.56958743872\n",
      "Current iteration=3990, loss=25777.564573939653\n",
      "Current iteration=3991, loss=25777.559562897968\n",
      "Current iteration=3992, loss=25777.554554311053\n",
      "Current iteration=3993, loss=25777.549548176314\n",
      "Current iteration=3994, loss=25777.544544491146\n",
      "Current iteration=3995, loss=25777.539543252962\n",
      "Current iteration=3996, loss=25777.534544459144\n",
      "Current iteration=3997, loss=25777.529548107123\n",
      "Current iteration=3998, loss=25777.524554194297\n",
      "Current iteration=3999, loss=25777.519562718084\n",
      "Current iteration=4000, loss=25777.514573675897\n",
      "Current iteration=4001, loss=25777.509587065164\n",
      "Current iteration=4002, loss=25777.5046028833\n",
      "Current iteration=4003, loss=25777.499621127732\n",
      "Current iteration=4004, loss=25777.49464179589\n",
      "Current iteration=4005, loss=25777.4896648852\n",
      "Current iteration=4006, loss=25777.4846903931\n",
      "Current iteration=4007, loss=25777.479718317023\n",
      "Current iteration=4008, loss=25777.47474865442\n",
      "Current iteration=4009, loss=25777.46978140271\n",
      "Current iteration=4010, loss=25777.46481655936\n",
      "Current iteration=4011, loss=25777.459854121804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=4012, loss=25777.454894087496\n",
      "Current iteration=4013, loss=25777.449936453897\n",
      "Current iteration=4014, loss=25777.444981218447\n",
      "Current iteration=4015, loss=25777.44002837862\n",
      "Current iteration=4016, loss=25777.43507793187\n",
      "Current iteration=4017, loss=25777.43012987567\n",
      "Current iteration=4018, loss=25777.425184207474\n",
      "Current iteration=4019, loss=25777.42024092476\n",
      "Current iteration=4020, loss=25777.415300024993\n",
      "Current iteration=4021, loss=25777.410361505663\n",
      "Current iteration=4022, loss=25777.40542536423\n",
      "Current iteration=4023, loss=25777.400491598193\n",
      "Current iteration=4024, loss=25777.395560205023\n",
      "Current iteration=4025, loss=25777.390631182214\n",
      "Current iteration=4026, loss=25777.38570452725\n",
      "Current iteration=4027, loss=25777.380780237625\n",
      "Current iteration=4028, loss=25777.37585831083\n",
      "Current iteration=4029, loss=25777.37093874437\n",
      "Current iteration=4030, loss=25777.36602153574\n",
      "Current iteration=4031, loss=25777.36110668245\n",
      "Current iteration=4032, loss=25777.356194181986\n",
      "Current iteration=4033, loss=25777.351284031887\n",
      "Current iteration=4034, loss=25777.346376229638\n",
      "Current iteration=4035, loss=25777.34147077276\n",
      "Current iteration=4036, loss=25777.336567658782\n",
      "Current iteration=4037, loss=25777.33166688521\n",
      "Current iteration=4038, loss=25777.326768449566\n",
      "Current iteration=4039, loss=25777.321872349385\n",
      "Current iteration=4040, loss=25777.316978582177\n",
      "Current iteration=4041, loss=25777.31208714549\n",
      "Current iteration=4042, loss=25777.307198036848\n",
      "Current iteration=4043, loss=25777.302311253792\n",
      "Current iteration=4044, loss=25777.29742679385\n",
      "Current iteration=4045, loss=25777.292544654578\n",
      "Current iteration=4046, loss=25777.287664833508\n",
      "Current iteration=4047, loss=25777.282787328186\n",
      "Current iteration=4048, loss=25777.277912136164\n",
      "Current iteration=4049, loss=25777.273039254997\n",
      "Current iteration=4050, loss=25777.268168682236\n",
      "Current iteration=4051, loss=25777.26330041544\n",
      "Current iteration=4052, loss=25777.258434452167\n",
      "Current iteration=4053, loss=25777.253570789977\n",
      "Current iteration=4054, loss=25777.248709426443\n",
      "Current iteration=4055, loss=25777.24385035913\n",
      "Current iteration=4056, loss=25777.2389935856\n",
      "Current iteration=4057, loss=25777.23413910343\n",
      "Current iteration=4058, loss=25777.22928691021\n",
      "Current iteration=4059, loss=25777.224437003493\n",
      "Current iteration=4060, loss=25777.21958938088\n",
      "Current iteration=4061, loss=25777.214744039946\n",
      "Current iteration=4062, loss=25777.209900978283\n",
      "Current iteration=4063, loss=25777.20506019347\n",
      "Current iteration=4064, loss=25777.20022168311\n",
      "Current iteration=4065, loss=25777.19538544479\n",
      "Current iteration=4066, loss=25777.190551476113\n",
      "Current iteration=4067, loss=25777.18571977467\n",
      "Current iteration=4068, loss=25777.180890338066\n",
      "Current iteration=4069, loss=25777.176063163908\n",
      "Current iteration=4070, loss=25777.171238249804\n",
      "Current iteration=4071, loss=25777.166415593365\n",
      "Current iteration=4072, loss=25777.161595192196\n",
      "Current iteration=4073, loss=25777.15677704392\n",
      "Current iteration=4074, loss=25777.15196114615\n",
      "Current iteration=4075, loss=25777.147147496522\n",
      "Current iteration=4076, loss=25777.142336092635\n",
      "Current iteration=4077, loss=25777.137526932118\n",
      "Current iteration=4078, loss=25777.132720012614\n",
      "Current iteration=4079, loss=25777.127915331745\n",
      "Current iteration=4080, loss=25777.12311288715\n",
      "Current iteration=4081, loss=25777.11831267645\n",
      "Current iteration=4082, loss=25777.113514697303\n",
      "Current iteration=4083, loss=25777.10871894734\n",
      "Current iteration=4084, loss=25777.103925424206\n",
      "Current iteration=4085, loss=25777.09913412555\n",
      "Current iteration=4086, loss=25777.094345049012\n",
      "Current iteration=4087, loss=25777.089558192245\n",
      "Current iteration=4088, loss=25777.08477355292\n",
      "Current iteration=4089, loss=25777.079991128674\n",
      "Current iteration=4090, loss=25777.075210917177\n",
      "Current iteration=4091, loss=25777.07043291608\n",
      "Current iteration=4092, loss=25777.065657123057\n",
      "Current iteration=4093, loss=25777.060883535778\n",
      "Current iteration=4094, loss=25777.0561121519\n",
      "Current iteration=4095, loss=25777.051342969105\n",
      "Current iteration=4096, loss=25777.046575985063\n",
      "Current iteration=4097, loss=25777.04181119745\n",
      "Current iteration=4098, loss=25777.037048603946\n",
      "Current iteration=4099, loss=25777.032288202237\n",
      "Current iteration=4100, loss=25777.027529990002\n",
      "Current iteration=4101, loss=25777.022773964934\n",
      "Current iteration=4102, loss=25777.018020124717\n",
      "Current iteration=4103, loss=25777.01326846705\n",
      "Current iteration=4104, loss=25777.008518989616\n",
      "Current iteration=4105, loss=25777.003771690124\n",
      "Current iteration=4106, loss=25776.999026566275\n",
      "Current iteration=4107, loss=25776.99428361575\n",
      "Current iteration=4108, loss=25776.989542836283\n",
      "Current iteration=4109, loss=25776.98480422557\n",
      "Current iteration=4110, loss=25776.98006778131\n",
      "Current iteration=4111, loss=25776.975333501225\n",
      "Current iteration=4112, loss=25776.97060138303\n",
      "Current iteration=4113, loss=25776.96587142444\n",
      "Current iteration=4114, loss=25776.96114362317\n",
      "Current iteration=4115, loss=25776.95641797696\n",
      "Current iteration=4116, loss=25776.951694483512\n",
      "Current iteration=4117, loss=25776.946973140566\n",
      "Current iteration=4118, loss=25776.942253945846\n",
      "Current iteration=4119, loss=25776.93753689709\n",
      "Current iteration=4120, loss=25776.93282199203\n",
      "Current iteration=4121, loss=25776.928109228404\n",
      "Current iteration=4122, loss=25776.923398603954\n",
      "Current iteration=4123, loss=25776.918690116414\n",
      "Current iteration=4124, loss=25776.913983763538\n",
      "Current iteration=4125, loss=25776.90927954306\n",
      "Current iteration=4126, loss=25776.904577452744\n",
      "Current iteration=4127, loss=25776.899877490334\n",
      "Current iteration=4128, loss=25776.89517965359\n",
      "Current iteration=4129, loss=25776.89048394026\n",
      "Current iteration=4130, loss=25776.88579034811\n",
      "Current iteration=4131, loss=25776.881098874903\n",
      "Current iteration=4132, loss=25776.87640951839\n",
      "Current iteration=4133, loss=25776.871722276355\n",
      "Current iteration=4134, loss=25776.86703714656\n",
      "Current iteration=4135, loss=25776.862354126773\n",
      "Current iteration=4136, loss=25776.857673214774\n",
      "Current iteration=4137, loss=25776.852994408335\n",
      "Current iteration=4138, loss=25776.848317705233\n",
      "Current iteration=4139, loss=25776.843643103253\n",
      "Current iteration=4140, loss=25776.838970600184\n",
      "Current iteration=4141, loss=25776.8343001938\n",
      "Current iteration=4142, loss=25776.829631881894\n",
      "Current iteration=4143, loss=25776.82496566226\n",
      "Current iteration=4144, loss=25776.820301532687\n",
      "Current iteration=4145, loss=25776.81563949098\n",
      "Current iteration=4146, loss=25776.810979534926\n",
      "Current iteration=4147, loss=25776.80632166233\n",
      "Current iteration=4148, loss=25776.80166587099\n",
      "Current iteration=4149, loss=25776.79701215872\n",
      "Current iteration=4150, loss=25776.792360523326\n",
      "Current iteration=4151, loss=25776.78771096261\n",
      "Current iteration=4152, loss=25776.783063474395\n",
      "Current iteration=4153, loss=25776.778418056485\n",
      "Current iteration=4154, loss=25776.773774706704\n",
      "Current iteration=4155, loss=25776.769133422877\n",
      "Current iteration=4156, loss=25776.764494202813\n",
      "Current iteration=4157, loss=25776.75985704434\n",
      "Current iteration=4158, loss=25776.755221945295\n",
      "Current iteration=4159, loss=25776.7505889035\n",
      "Current iteration=4160, loss=25776.74595791678\n",
      "Current iteration=4161, loss=25776.74132898298\n",
      "Current iteration=4162, loss=25776.736702099923\n",
      "Current iteration=4163, loss=25776.732077265464\n",
      "Current iteration=4164, loss=25776.727454477426\n",
      "Current iteration=4165, loss=25776.722833733675\n",
      "Current iteration=4166, loss=25776.718215032033\n",
      "Current iteration=4167, loss=25776.713598370363\n",
      "Current iteration=4168, loss=25776.70898374651\n",
      "Current iteration=4169, loss=25776.70437115833\n",
      "Current iteration=4170, loss=25776.699760603668\n",
      "Current iteration=4171, loss=25776.695152080396\n",
      "Current iteration=4172, loss=25776.69054558636\n",
      "Current iteration=4173, loss=25776.68594111943\n",
      "Current iteration=4174, loss=25776.68133867747\n",
      "Current iteration=4175, loss=25776.676738258346\n",
      "Current iteration=4176, loss=25776.672139859926\n",
      "Current iteration=4177, loss=25776.66754348008\n",
      "Current iteration=4178, loss=25776.66294911669\n",
      "Current iteration=4179, loss=25776.658356767613\n",
      "Current iteration=4180, loss=25776.653766430754\n",
      "Current iteration=4181, loss=25776.649178103962\n",
      "Current iteration=4182, loss=25776.64459178515\n",
      "Current iteration=4183, loss=25776.64000747219\n",
      "Current iteration=4184, loss=25776.635425162964\n",
      "Current iteration=4185, loss=25776.63084485537\n",
      "Current iteration=4186, loss=25776.62626654729\n",
      "Current iteration=4187, loss=25776.621690236636\n",
      "Current iteration=4188, loss=25776.61711592129\n",
      "Current iteration=4189, loss=25776.612543599153\n",
      "Current iteration=4190, loss=25776.60797326814\n",
      "Current iteration=4191, loss=25776.60340492613\n",
      "Current iteration=4192, loss=25776.59883857105\n",
      "Current iteration=4193, loss=25776.594274200797\n",
      "Current iteration=4194, loss=25776.589711813285\n",
      "Current iteration=4195, loss=25776.585151406427\n",
      "Current iteration=4196, loss=25776.580592978138\n",
      "Current iteration=4197, loss=25776.57603652633\n",
      "Current iteration=4198, loss=25776.571482048937\n",
      "Current iteration=4199, loss=25776.56692954386\n",
      "Current iteration=4200, loss=25776.56237900904\n",
      "Current iteration=4201, loss=25776.557830442394\n",
      "Current iteration=4202, loss=25776.55328384186\n",
      "Current iteration=4203, loss=25776.548739205355\n",
      "Current iteration=4204, loss=25776.54419653082\n",
      "Current iteration=4205, loss=25776.539655816196\n",
      "Current iteration=4206, loss=25776.53511705941\n",
      "Current iteration=4207, loss=25776.530580258408\n",
      "Current iteration=4208, loss=25776.526045411134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=4209, loss=25776.521512515523\n",
      "Current iteration=4210, loss=25776.516981569526\n",
      "Current iteration=4211, loss=25776.512452571096\n",
      "Current iteration=4212, loss=25776.507925518177\n",
      "Current iteration=4213, loss=25776.503400408736\n",
      "Current iteration=4214, loss=25776.4988772407\n",
      "Current iteration=4215, loss=25776.494356012066\n",
      "Current iteration=4216, loss=25776.48983672076\n",
      "Current iteration=4217, loss=25776.485319364765\n",
      "Current iteration=4218, loss=25776.48080394204\n",
      "Current iteration=4219, loss=25776.476290450537\n",
      "Current iteration=4220, loss=25776.47177888824\n",
      "Current iteration=4221, loss=25776.46726925312\n",
      "Current iteration=4222, loss=25776.462761543156\n",
      "Current iteration=4223, loss=25776.458255756304\n",
      "Current iteration=4224, loss=25776.45375189055\n",
      "Current iteration=4225, loss=25776.44924994388\n",
      "Current iteration=4226, loss=25776.444749914277\n",
      "Current iteration=4227, loss=25776.440251799708\n",
      "Current iteration=4228, loss=25776.435755598184\n",
      "Current iteration=4229, loss=25776.43126130767\n",
      "Current iteration=4230, loss=25776.42676892618\n",
      "Current iteration=4231, loss=25776.42227845169\n",
      "Current iteration=4232, loss=25776.417789882198\n",
      "Current iteration=4233, loss=25776.4133032157\n",
      "Current iteration=4234, loss=25776.408818450207\n",
      "Current iteration=4235, loss=25776.404335583702\n",
      "Current iteration=4236, loss=25776.39985461421\n",
      "Current iteration=4237, loss=25776.395375539723\n",
      "Current iteration=4238, loss=25776.390898358248\n",
      "Current iteration=4239, loss=25776.38642306781\n",
      "Current iteration=4240, loss=25776.3819496664\n",
      "Current iteration=4241, loss=25776.377478152055\n",
      "Current iteration=4242, loss=25776.373008522776\n",
      "Current iteration=4243, loss=25776.368540776588\n",
      "Current iteration=4244, loss=25776.36407491151\n",
      "Current iteration=4245, loss=25776.35961092557\n",
      "Current iteration=4246, loss=25776.355148816787\n",
      "Current iteration=4247, loss=25776.350688583196\n",
      "Current iteration=4248, loss=25776.346230222818\n",
      "Current iteration=4249, loss=25776.341773733686\n",
      "Current iteration=4250, loss=25776.337319113845\n",
      "Current iteration=4251, loss=25776.332866361325\n",
      "Current iteration=4252, loss=25776.328415474156\n",
      "Current iteration=4253, loss=25776.323966450385\n",
      "Current iteration=4254, loss=25776.319519288063\n",
      "Current iteration=4255, loss=25776.315073985217\n",
      "Current iteration=4256, loss=25776.310630539912\n",
      "Current iteration=4257, loss=25776.30618895019\n",
      "Current iteration=4258, loss=25776.301749214093\n",
      "Current iteration=4259, loss=25776.29731132969\n",
      "Current iteration=4260, loss=25776.292875295025\n",
      "Current iteration=4261, loss=25776.288441108154\n",
      "Current iteration=4262, loss=25776.284008767143\n",
      "Current iteration=4263, loss=25776.279578270056\n",
      "Current iteration=4264, loss=25776.27514961495\n",
      "Current iteration=4265, loss=25776.270722799894\n",
      "Current iteration=4266, loss=25776.266297822956\n",
      "Current iteration=4267, loss=25776.261874682205\n",
      "Current iteration=4268, loss=25776.25745337572\n",
      "Current iteration=4269, loss=25776.25303390156\n",
      "Current iteration=4270, loss=25776.248616257817\n",
      "Current iteration=4271, loss=25776.244200442557\n",
      "Current iteration=4272, loss=25776.239786453865\n",
      "Current iteration=4273, loss=25776.235374289834\n",
      "Current iteration=4274, loss=25776.230963948536\n",
      "Current iteration=4275, loss=25776.226555428064\n",
      "Current iteration=4276, loss=25776.222148726505\n",
      "Current iteration=4277, loss=25776.217743841946\n",
      "Current iteration=4278, loss=25776.213340772483\n",
      "Current iteration=4279, loss=25776.20893951621\n",
      "Current iteration=4280, loss=25776.204540071234\n",
      "Current iteration=4281, loss=25776.20014243564\n",
      "Current iteration=4282, loss=25776.195746607536\n",
      "Current iteration=4283, loss=25776.191352585025\n",
      "Current iteration=4284, loss=25776.186960366213\n",
      "Current iteration=4285, loss=25776.182569949207\n",
      "Current iteration=4286, loss=25776.178181332118\n",
      "Current iteration=4287, loss=25776.17379451305\n",
      "Current iteration=4288, loss=25776.16940949013\n",
      "Current iteration=4289, loss=25776.16502626146\n",
      "Current iteration=4290, loss=25776.160644825162\n",
      "Current iteration=4291, loss=25776.156265179372\n",
      "Current iteration=4292, loss=25776.151887322187\n",
      "Current iteration=4293, loss=25776.147511251736\n",
      "Current iteration=4294, loss=25776.143136966162\n",
      "Current iteration=4295, loss=25776.138764463576\n",
      "Current iteration=4296, loss=25776.13439374211\n",
      "Current iteration=4297, loss=25776.13002479991\n",
      "Current iteration=4298, loss=25776.125657635093\n",
      "Current iteration=4299, loss=25776.121292245803\n",
      "Current iteration=4300, loss=25776.116928630174\n",
      "Current iteration=4301, loss=25776.11256678635\n",
      "Current iteration=4302, loss=25776.108206712473\n",
      "Current iteration=4303, loss=25776.10384840669\n",
      "Current iteration=4304, loss=25776.09949186714\n",
      "Current iteration=4305, loss=25776.09513709197\n",
      "Current iteration=4306, loss=25776.090784079333\n",
      "Current iteration=4307, loss=25776.086432827393\n",
      "Current iteration=4308, loss=25776.082083334288\n",
      "Current iteration=4309, loss=25776.07773559818\n",
      "Current iteration=4310, loss=25776.073389617228\n",
      "Current iteration=4311, loss=25776.069045389595\n",
      "Current iteration=4312, loss=25776.06470291344\n",
      "Current iteration=4313, loss=25776.06036218692\n",
      "Current iteration=4314, loss=25776.056023208217\n",
      "Current iteration=4315, loss=25776.051685975493\n",
      "Current iteration=4316, loss=25776.047350486908\n",
      "Current iteration=4317, loss=25776.04301674064\n",
      "Current iteration=4318, loss=25776.03868473487\n",
      "Current iteration=4319, loss=25776.034354467774\n",
      "Current iteration=4320, loss=25776.030025937518\n",
      "Current iteration=4321, loss=25776.025699142294\n",
      "Current iteration=4322, loss=25776.021374080276\n",
      "Current iteration=4323, loss=25776.01705074965\n",
      "Current iteration=4324, loss=25776.012729148613\n",
      "Current iteration=4325, loss=25776.00840927534\n",
      "Current iteration=4326, loss=25776.004091128023\n",
      "Current iteration=4327, loss=25775.999774704855\n",
      "Current iteration=4328, loss=25775.99546000403\n",
      "Current iteration=4329, loss=25775.991147023742\n",
      "Current iteration=4330, loss=25775.986835762196\n",
      "Current iteration=4331, loss=25775.982526217584\n",
      "Current iteration=4332, loss=25775.978218388114\n",
      "Current iteration=4333, loss=25775.97391227198\n",
      "Current iteration=4334, loss=25775.9696078674\n",
      "Current iteration=4335, loss=25775.96530517257\n",
      "Current iteration=4336, loss=25775.961004185712\n",
      "Current iteration=4337, loss=25775.956704905027\n",
      "Current iteration=4338, loss=25775.952407328732\n",
      "Current iteration=4339, loss=25775.948111455044\n",
      "Current iteration=4340, loss=25775.943817282176\n",
      "Current iteration=4341, loss=25775.93952480835\n",
      "Current iteration=4342, loss=25775.93523403179\n",
      "Current iteration=4343, loss=25775.93094495071\n",
      "Current iteration=4344, loss=25775.92665756334\n",
      "Current iteration=4345, loss=25775.922371867913\n",
      "Current iteration=4346, loss=25775.918087862654\n",
      "Current iteration=4347, loss=25775.913805545784\n",
      "Current iteration=4348, loss=25775.90952491555\n",
      "Current iteration=4349, loss=25775.90524597018\n",
      "Current iteration=4350, loss=25775.90096870791\n",
      "Current iteration=4351, loss=25775.896693126982\n",
      "Current iteration=4352, loss=25775.89241922563\n",
      "Current iteration=4353, loss=25775.888147002108\n",
      "Current iteration=4354, loss=25775.88387645464\n",
      "Current iteration=4355, loss=25775.87960758149\n",
      "Current iteration=4356, loss=25775.875340380902\n",
      "Current iteration=4357, loss=25775.871074851122\n",
      "Current iteration=4358, loss=25775.866810990403\n",
      "Current iteration=4359, loss=25775.862548797006\n",
      "Current iteration=4360, loss=25775.858288269173\n",
      "Current iteration=4361, loss=25775.85402940517\n",
      "Current iteration=4362, loss=25775.84977220326\n",
      "Current iteration=4363, loss=25775.84551666169\n",
      "Current iteration=4364, loss=25775.841262778737\n",
      "Current iteration=4365, loss=25775.837010552663\n",
      "Current iteration=4366, loss=25775.832759981728\n",
      "Current iteration=4367, loss=25775.82851106421\n",
      "Current iteration=4368, loss=25775.82426379837\n",
      "Current iteration=4369, loss=25775.820018182487\n",
      "Current iteration=4370, loss=25775.81577421484\n",
      "Current iteration=4371, loss=25775.811531893694\n",
      "Current iteration=4372, loss=25775.807291217334\n",
      "Current iteration=4373, loss=25775.80305218404\n",
      "Current iteration=4374, loss=25775.798814792088\n",
      "Current iteration=4375, loss=25775.79457903977\n",
      "Current iteration=4376, loss=25775.790344925364\n",
      "Current iteration=4377, loss=25775.786112447167\n",
      "Current iteration=4378, loss=25775.78188160346\n",
      "Current iteration=4379, loss=25775.77765239253\n",
      "Current iteration=4380, loss=25775.77342481268\n",
      "Current iteration=4381, loss=25775.769198862206\n",
      "Current iteration=4382, loss=25775.764974539394\n",
      "Current iteration=4383, loss=25775.76075184255\n",
      "Current iteration=4384, loss=25775.756530769973\n",
      "Current iteration=4385, loss=25775.75231131996\n",
      "Current iteration=4386, loss=25775.74809349083\n",
      "Current iteration=4387, loss=25775.743877280875\n",
      "Current iteration=4388, loss=25775.7396626884\n",
      "Current iteration=4389, loss=25775.735449711727\n",
      "Current iteration=4390, loss=25775.731238349163\n",
      "Current iteration=4391, loss=25775.727028599016\n",
      "Current iteration=4392, loss=25775.722820459607\n",
      "Current iteration=4393, loss=25775.718613929246\n",
      "Current iteration=4394, loss=25775.714409006257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=4395, loss=25775.710205688963\n",
      "Current iteration=4396, loss=25775.706003975683\n",
      "Current iteration=4397, loss=25775.70180386474\n",
      "Current iteration=4398, loss=25775.697605354464\n",
      "Current iteration=4399, loss=25775.693408443178\n",
      "Current iteration=4400, loss=25775.68921312921\n",
      "Current iteration=4401, loss=25775.685019410903\n",
      "Current iteration=4402, loss=25775.680827286575\n",
      "Current iteration=4403, loss=25775.67663675457\n",
      "Current iteration=4404, loss=25775.672447813227\n",
      "Current iteration=4405, loss=25775.668260460872\n",
      "Current iteration=4406, loss=25775.664074695866\n",
      "Current iteration=4407, loss=25775.659890516523\n",
      "Current iteration=4408, loss=25775.655707921218\n",
      "Current iteration=4409, loss=25775.651526908274\n",
      "Current iteration=4410, loss=25775.64734747605\n",
      "Current iteration=4411, loss=25775.64316962289\n",
      "Current iteration=4412, loss=25775.63899334715\n",
      "Current iteration=4413, loss=25775.63481864718\n",
      "Current iteration=4414, loss=25775.63064552133\n",
      "Current iteration=4415, loss=25775.62647396797\n",
      "Current iteration=4416, loss=25775.622303985438\n",
      "Current iteration=4417, loss=25775.618135572116\n",
      "Current iteration=4418, loss=25775.61396872635\n",
      "Current iteration=4419, loss=25775.60980344651\n",
      "Current iteration=4420, loss=25775.60563973096\n",
      "Current iteration=4421, loss=25775.60147757807\n",
      "Current iteration=4422, loss=25775.597316986205\n",
      "Current iteration=4423, loss=25775.593157953743\n",
      "Current iteration=4424, loss=25775.58900047904\n",
      "Current iteration=4425, loss=25775.584844560486\n",
      "Current iteration=4426, loss=25775.580690196457\n",
      "Current iteration=4427, loss=25775.57653738532\n",
      "Current iteration=4428, loss=25775.57238612546\n",
      "Current iteration=4429, loss=25775.56823641526\n",
      "Current iteration=4430, loss=25775.5640882531\n",
      "Current iteration=4431, loss=25775.55994163737\n",
      "Current iteration=4432, loss=25775.555796566456\n",
      "Current iteration=4433, loss=25775.551653038732\n",
      "Current iteration=4434, loss=25775.547511052606\n",
      "Current iteration=4435, loss=25775.543370606458\n",
      "Current iteration=4436, loss=25775.53923169869\n",
      "Current iteration=4437, loss=25775.535094327697\n",
      "Current iteration=4438, loss=25775.530958491865\n",
      "Current iteration=4439, loss=25775.5268241896\n",
      "Current iteration=4440, loss=25775.52269141931\n",
      "Current iteration=4441, loss=25775.518560179386\n",
      "Current iteration=4442, loss=25775.514430468233\n",
      "Current iteration=4443, loss=25775.510302284263\n",
      "Current iteration=4444, loss=25775.506175625876\n",
      "Current iteration=4445, loss=25775.502050491483\n",
      "Current iteration=4446, loss=25775.4979268795\n",
      "Current iteration=4447, loss=25775.493804788333\n",
      "Current iteration=4448, loss=25775.4896842164\n",
      "Current iteration=4449, loss=25775.48556516212\n",
      "Current iteration=4450, loss=25775.481447623897\n",
      "Current iteration=4451, loss=25775.477331600167\n",
      "Current iteration=4452, loss=25775.47321708934\n",
      "Current iteration=4453, loss=25775.469104089843\n",
      "Current iteration=4454, loss=25775.464992600104\n",
      "Current iteration=4455, loss=25775.460882618547\n",
      "Current iteration=4456, loss=25775.45677414359\n",
      "Current iteration=4457, loss=25775.452667173675\n",
      "Current iteration=4458, loss=25775.448561707224\n",
      "Current iteration=4459, loss=25775.444457742677\n",
      "Current iteration=4460, loss=25775.440355278468\n",
      "Current iteration=4461, loss=25775.436254313034\n",
      "Current iteration=4462, loss=25775.43215484481\n",
      "Current iteration=4463, loss=25775.42805687223\n",
      "Current iteration=4464, loss=25775.423960393753\n",
      "Current iteration=4465, loss=25775.419865407806\n",
      "Current iteration=4466, loss=25775.415771912838\n",
      "Current iteration=4467, loss=25775.411679907298\n",
      "Current iteration=4468, loss=25775.407589389633\n",
      "Current iteration=4469, loss=25775.40350035829\n",
      "Current iteration=4470, loss=25775.399412811723\n",
      "Current iteration=4471, loss=25775.39532674839\n",
      "Current iteration=4472, loss=25775.391242166734\n",
      "Current iteration=4473, loss=25775.387159065223\n",
      "Current iteration=4474, loss=25775.383077442308\n",
      "Current iteration=4475, loss=25775.37899729646\n",
      "Current iteration=4476, loss=25775.374918626123\n",
      "Current iteration=4477, loss=25775.370841429773\n",
      "Current iteration=4478, loss=25775.366765705865\n",
      "Current iteration=4479, loss=25775.362691452872\n",
      "Current iteration=4480, loss=25775.35861866927\n",
      "Current iteration=4481, loss=25775.354547353512\n",
      "Current iteration=4482, loss=25775.350477504086\n",
      "Current iteration=4483, loss=25775.346409119455\n",
      "Current iteration=4484, loss=25775.34234219809\n",
      "Current iteration=4485, loss=25775.338276738483\n",
      "Current iteration=4486, loss=25775.334212739093\n",
      "Current iteration=4487, loss=25775.330150198417\n",
      "Current iteration=4488, loss=25775.326089114926\n",
      "Current iteration=4489, loss=25775.322029487103\n",
      "Current iteration=4490, loss=25775.317971313434\n",
      "Current iteration=4491, loss=25775.31391459241\n",
      "Current iteration=4492, loss=25775.309859322515\n",
      "Current iteration=4493, loss=25775.305805502234\n",
      "Current iteration=4494, loss=25775.301753130065\n",
      "Current iteration=4495, loss=25775.2977022045\n",
      "Current iteration=4496, loss=25775.293652724038\n",
      "Current iteration=4497, loss=25775.28960468716\n",
      "Current iteration=4498, loss=25775.28555809238\n",
      "Current iteration=4499, loss=25775.28151293818\n",
      "Current iteration=4500, loss=25775.27746922308\n",
      "Current iteration=4501, loss=25775.273426945576\n",
      "Current iteration=4502, loss=25775.269386104163\n",
      "Current iteration=4503, loss=25775.265346697357\n",
      "Current iteration=4504, loss=25775.26130872366\n",
      "Current iteration=4505, loss=25775.257272181585\n",
      "Current iteration=4506, loss=25775.253237069646\n",
      "Current iteration=4507, loss=25775.24920338634\n",
      "Current iteration=4508, loss=25775.245171130195\n",
      "Current iteration=4509, loss=25775.241140299724\n",
      "Current iteration=4510, loss=25775.237110893435\n",
      "Current iteration=4511, loss=25775.233082909865\n",
      "Current iteration=4512, loss=25775.229056347514\n",
      "Current iteration=4513, loss=25775.225031204922\n",
      "Current iteration=4514, loss=25775.221007480595\n",
      "Current iteration=4515, loss=25775.21698517307\n",
      "Current iteration=4516, loss=25775.212964280865\n",
      "Current iteration=4517, loss=25775.208944802518\n",
      "Current iteration=4518, loss=25775.20492673655\n",
      "Current iteration=4519, loss=25775.2009100815\n",
      "Current iteration=4520, loss=25775.1968948359\n",
      "Current iteration=4521, loss=25775.192880998275\n",
      "Current iteration=4522, loss=25775.18886856717\n",
      "Current iteration=4523, loss=25775.184857541124\n",
      "Current iteration=4524, loss=25775.180847918677\n",
      "Current iteration=4525, loss=25775.176839698357\n",
      "Current iteration=4526, loss=25775.17283287872\n",
      "Current iteration=4527, loss=25775.1688274583\n",
      "Current iteration=4528, loss=25775.164823435658\n",
      "Current iteration=4529, loss=25775.16082080932\n",
      "Current iteration=4530, loss=25775.156819577853\n",
      "Current iteration=4531, loss=25775.152819739797\n",
      "Current iteration=4532, loss=25775.14882129371\n",
      "Current iteration=4533, loss=25775.144824238138\n",
      "Current iteration=4534, loss=25775.14082857165\n",
      "Current iteration=4535, loss=25775.13683429278\n",
      "Current iteration=4536, loss=25775.13284140011\n",
      "Current iteration=4537, loss=25775.128849892186\n",
      "Current iteration=4538, loss=25775.124859767566\n",
      "Current iteration=4539, loss=25775.12087102483\n",
      "Current iteration=4540, loss=25775.116883662522\n",
      "Current iteration=4541, loss=25775.112897679213\n",
      "Current iteration=4542, loss=25775.108913073484\n",
      "Current iteration=4543, loss=25775.10492984389\n",
      "Current iteration=4544, loss=25775.100947989005\n",
      "Current iteration=4545, loss=25775.096967507405\n",
      "Current iteration=4546, loss=25775.092988397657\n",
      "Current iteration=4547, loss=25775.089010658343\n",
      "Current iteration=4548, loss=25775.085034288037\n",
      "Current iteration=4549, loss=25775.081059285316\n",
      "Current iteration=4550, loss=25775.077085648758\n",
      "Current iteration=4551, loss=25775.07311337694\n",
      "Current iteration=4552, loss=25775.069142468463\n",
      "Current iteration=4553, loss=25775.065172921888\n",
      "Current iteration=4554, loss=25775.06120473582\n",
      "Current iteration=4555, loss=25775.057237908844\n",
      "Current iteration=4556, loss=25775.053272439534\n",
      "Current iteration=4557, loss=25775.049308326496\n",
      "Current iteration=4558, loss=25775.04534556831\n",
      "Current iteration=4559, loss=25775.04138416358\n",
      "Current iteration=4560, loss=25775.037424110895\n",
      "Current iteration=4561, loss=25775.03346540885\n",
      "Current iteration=4562, loss=25775.029508056054\n",
      "Current iteration=4563, loss=25775.025552051095\n",
      "Current iteration=4564, loss=25775.021597392573\n",
      "Current iteration=4565, loss=25775.01764407909\n",
      "Current iteration=4566, loss=25775.013692109267\n",
      "Current iteration=4567, loss=25775.009741481696\n",
      "Current iteration=4568, loss=25775.00579219498\n",
      "Current iteration=4569, loss=25775.001844247738\n",
      "Current iteration=4570, loss=25774.997897638576\n",
      "Current iteration=4571, loss=25774.993952366098\n",
      "Current iteration=4572, loss=25774.99000842893\n",
      "Current iteration=4573, loss=25774.98606582568\n",
      "Current iteration=4574, loss=25774.98212455496\n",
      "Current iteration=4575, loss=25774.978184615393\n",
      "Current iteration=4576, loss=25774.9742460056\n",
      "Current iteration=4577, loss=25774.9703087242\n",
      "Current iteration=4578, loss=25774.96637276981\n",
      "Current iteration=4579, loss=25774.962438141058\n",
      "Current iteration=4580, loss=25774.95850483657\n",
      "Current iteration=4581, loss=25774.954572854967\n",
      "Current iteration=4582, loss=25774.950642194883\n",
      "Current iteration=4583, loss=25774.94671285494\n",
      "Current iteration=4584, loss=25774.94278483378\n",
      "Current iteration=4585, loss=25774.938858130026\n",
      "Current iteration=4586, loss=25774.934932742315\n",
      "Current iteration=4587, loss=25774.931008669282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=4588, loss=25774.92708590956\n",
      "Current iteration=4589, loss=25774.923164461794\n",
      "Current iteration=4590, loss=25774.91924432462\n",
      "Current iteration=4591, loss=25774.915325496688\n",
      "Current iteration=4592, loss=25774.911407976622\n",
      "Current iteration=4593, loss=25774.907491763086\n",
      "Current iteration=4594, loss=25774.903576854704\n",
      "Current iteration=4595, loss=25774.89966325014\n",
      "Current iteration=4596, loss=25774.89575094804\n",
      "Current iteration=4597, loss=25774.891839947057\n",
      "Current iteration=4598, loss=25774.887930245834\n",
      "Current iteration=4599, loss=25774.88402184302\n",
      "Current iteration=4600, loss=25774.880114737287\n",
      "Current iteration=4601, loss=25774.87620892727\n",
      "Current iteration=4602, loss=25774.872304411645\n",
      "Current iteration=4603, loss=25774.868401189055\n",
      "Current iteration=4604, loss=25774.86449925817\n",
      "Current iteration=4605, loss=25774.860598617648\n",
      "Current iteration=4606, loss=25774.85669926615\n",
      "Current iteration=4607, loss=25774.852801202345\n",
      "Current iteration=4608, loss=25774.848904424893\n",
      "Current iteration=4609, loss=25774.845008932472\n",
      "Current iteration=4610, loss=25774.841114723742\n",
      "Current iteration=4611, loss=25774.837221797377\n",
      "Current iteration=4612, loss=25774.833330152032\n",
      "Current iteration=4613, loss=25774.82943978641\n",
      "Current iteration=4614, loss=25774.825550699166\n",
      "Current iteration=4615, loss=25774.82166288898\n",
      "Current iteration=4616, loss=25774.817776354525\n",
      "Current iteration=4617, loss=25774.81389109449\n",
      "Current iteration=4618, loss=25774.810007107542\n",
      "Current iteration=4619, loss=25774.806124392373\n",
      "Current iteration=4620, loss=25774.80224294767\n",
      "Current iteration=4621, loss=25774.798362772097\n",
      "Current iteration=4622, loss=25774.79448386436\n",
      "Current iteration=4623, loss=25774.790606223134\n",
      "Current iteration=4624, loss=25774.786729847117\n",
      "Current iteration=4625, loss=25774.782854734993\n",
      "Current iteration=4626, loss=25774.77898088546\n",
      "Current iteration=4627, loss=25774.775108297203\n",
      "Current iteration=4628, loss=25774.771236968918\n",
      "Current iteration=4629, loss=25774.767366899305\n",
      "Current iteration=4630, loss=25774.763498087057\n",
      "Current iteration=4631, loss=25774.75963053087\n",
      "Current iteration=4632, loss=25774.755764229445\n",
      "Current iteration=4633, loss=25774.751899181494\n",
      "Current iteration=4634, loss=25774.74803538571\n",
      "Current iteration=4635, loss=25774.744172840794\n",
      "Current iteration=4636, loss=25774.740311545458\n",
      "Current iteration=4637, loss=25774.73645149841\n",
      "Current iteration=4638, loss=25774.732592698354\n",
      "Current iteration=4639, loss=25774.728735144\n",
      "Current iteration=4640, loss=25774.72487883406\n",
      "Current iteration=4641, loss=25774.721023767244\n",
      "Current iteration=4642, loss=25774.717169942276\n",
      "Current iteration=4643, loss=25774.713317357855\n",
      "Current iteration=4644, loss=25774.709466012715\n",
      "Current iteration=4645, loss=25774.705615905554\n",
      "Current iteration=4646, loss=25774.70176703511\n",
      "Current iteration=4647, loss=25774.6979194001\n",
      "Current iteration=4648, loss=25774.694072999228\n",
      "Current iteration=4649, loss=25774.690227831237\n",
      "Current iteration=4650, loss=25774.68638389486\n",
      "Current iteration=4651, loss=25774.6825411888\n",
      "Current iteration=4652, loss=25774.67869971179\n",
      "Current iteration=4653, loss=25774.674859462568\n",
      "Current iteration=4654, loss=25774.67102043986\n",
      "Current iteration=4655, loss=25774.667182642395\n",
      "Current iteration=4656, loss=25774.663346068908\n",
      "Current iteration=4657, loss=25774.659510718135\n",
      "Current iteration=4658, loss=25774.655676588816\n",
      "Current iteration=4659, loss=25774.65184367968\n",
      "Current iteration=4660, loss=25774.648011989466\n",
      "Current iteration=4661, loss=25774.644181516916\n",
      "Current iteration=4662, loss=25774.640352260772\n",
      "Current iteration=4663, loss=25774.636524219775\n",
      "Current iteration=4664, loss=25774.632697392677\n",
      "Current iteration=4665, loss=25774.628871778208\n",
      "Current iteration=4666, loss=25774.625047375128\n",
      "Current iteration=4667, loss=25774.621224182178\n",
      "Current iteration=4668, loss=25774.617402198106\n",
      "Current iteration=4669, loss=25774.61358142167\n",
      "Current iteration=4670, loss=25774.609761851614\n",
      "Current iteration=4671, loss=25774.6059434867\n",
      "Current iteration=4672, loss=25774.602126325677\n",
      "Current iteration=4673, loss=25774.5983103673\n",
      "Current iteration=4674, loss=25774.594495610327\n",
      "Current iteration=4675, loss=25774.59068205352\n",
      "Current iteration=4676, loss=25774.58686969563\n",
      "Current iteration=4677, loss=25774.58305853544\n",
      "Current iteration=4678, loss=25774.579248571685\n",
      "Current iteration=4679, loss=25774.57543980315\n",
      "Current iteration=4680, loss=25774.571632228588\n",
      "Current iteration=4681, loss=25774.567825846774\n",
      "Current iteration=4682, loss=25774.564020656468\n",
      "Current iteration=4683, loss=25774.560216656446\n",
      "Current iteration=4684, loss=25774.556413845476\n",
      "Current iteration=4685, loss=25774.55261222233\n",
      "Current iteration=4686, loss=25774.548811785782\n",
      "Current iteration=4687, loss=25774.545012534607\n",
      "Current iteration=4688, loss=25774.54121446758\n",
      "Current iteration=4689, loss=25774.537417583477\n",
      "Current iteration=4690, loss=25774.533621881073\n",
      "Current iteration=4691, loss=25774.52982735916\n",
      "Current iteration=4692, loss=25774.52603401651\n",
      "Current iteration=4693, loss=25774.52224185191\n",
      "Current iteration=4694, loss=25774.518450864136\n",
      "Current iteration=4695, loss=25774.51466105198\n",
      "Current iteration=4696, loss=25774.510872414226\n",
      "Current iteration=4697, loss=25774.507084949666\n",
      "Current iteration=4698, loss=25774.50329865708\n",
      "Current iteration=4699, loss=25774.499513535273\n",
      "Current iteration=4700, loss=25774.495729583025\n",
      "Current iteration=4701, loss=25774.491946799124\n",
      "Current iteration=4702, loss=25774.488165182378\n",
      "Current iteration=4703, loss=25774.48438473157\n",
      "Current iteration=4704, loss=25774.48060544551\n",
      "Current iteration=4705, loss=25774.476827322993\n",
      "Current iteration=4706, loss=25774.473050362805\n",
      "Current iteration=4707, loss=25774.46927456376\n",
      "Current iteration=4708, loss=25774.46549992465\n",
      "Current iteration=4709, loss=25774.461726444293\n",
      "Current iteration=4710, loss=25774.45795412148\n",
      "Current iteration=4711, loss=25774.454182955025\n",
      "Current iteration=4712, loss=25774.450412943734\n",
      "Current iteration=4713, loss=25774.44664408641\n",
      "Current iteration=4714, loss=25774.442876381865\n",
      "Current iteration=4715, loss=25774.439109828912\n",
      "Current iteration=4716, loss=25774.43534442636\n",
      "Current iteration=4717, loss=25774.431580173034\n",
      "Current iteration=4718, loss=25774.42781706773\n",
      "Current iteration=4719, loss=25774.424055109277\n",
      "Current iteration=4720, loss=25774.420294296488\n",
      "Current iteration=4721, loss=25774.41653462818\n",
      "Current iteration=4722, loss=25774.412776103178\n",
      "Current iteration=4723, loss=25774.409018720304\n",
      "Current iteration=4724, loss=25774.405262478376\n",
      "Current iteration=4725, loss=25774.401507376213\n",
      "Current iteration=4726, loss=25774.397753412653\n",
      "Current iteration=4727, loss=25774.394000586508\n",
      "Current iteration=4728, loss=25774.390248896616\n",
      "Current iteration=4729, loss=25774.386498341803\n",
      "Current iteration=4730, loss=25774.382748920896\n",
      "Current iteration=4731, loss=25774.379000632733\n",
      "Current iteration=4732, loss=25774.37525347613\n",
      "Current iteration=4733, loss=25774.371507449938\n",
      "Current iteration=4734, loss=25774.367762552985\n",
      "Current iteration=4735, loss=25774.364018784112\n",
      "Current iteration=4736, loss=25774.36027614215\n",
      "Current iteration=4737, loss=25774.35653462595\n",
      "Current iteration=4738, loss=25774.35279423433\n",
      "Current iteration=4739, loss=25774.349054966147\n",
      "Current iteration=4740, loss=25774.345316820243\n",
      "Current iteration=4741, loss=25774.34157979546\n",
      "Current iteration=4742, loss=25774.33784389064\n",
      "Current iteration=4743, loss=25774.33410910463\n",
      "Current iteration=4744, loss=25774.330375436282\n",
      "Current iteration=4745, loss=25774.326642884444\n",
      "Current iteration=4746, loss=25774.322911447958\n",
      "Current iteration=4747, loss=25774.319181125677\n",
      "Current iteration=4748, loss=25774.315451916467\n",
      "Current iteration=4749, loss=25774.311723819166\n",
      "Current iteration=4750, loss=25774.30799683263\n",
      "Current iteration=4751, loss=25774.304270955727\n",
      "Current iteration=4752, loss=25774.300546187304\n",
      "Current iteration=4753, loss=25774.296822526216\n",
      "Current iteration=4754, loss=25774.293099971335\n",
      "Current iteration=4755, loss=25774.289378521513\n",
      "Current iteration=4756, loss=25774.285658175617\n",
      "Current iteration=4757, loss=25774.281938932512\n",
      "Current iteration=4758, loss=25774.27822079105\n",
      "Current iteration=4759, loss=25774.27450375011\n",
      "Current iteration=4760, loss=25774.270787808557\n",
      "Current iteration=4761, loss=25774.267072965253\n",
      "Current iteration=4762, loss=25774.26335921908\n",
      "Current iteration=4763, loss=25774.25964656889\n",
      "Current iteration=4764, loss=25774.25593501357\n",
      "Current iteration=4765, loss=25774.252224551987\n",
      "Current iteration=4766, loss=25774.248515183015\n",
      "Current iteration=4767, loss=25774.244806905543\n",
      "Current iteration=4768, loss=25774.241099718423\n",
      "Current iteration=4769, loss=25774.237393620548\n",
      "Current iteration=4770, loss=25774.2336886108\n",
      "Current iteration=4771, loss=25774.229984688056\n",
      "Current iteration=4772, loss=25774.226281851188\n",
      "Current iteration=4773, loss=25774.222580099093\n",
      "Current iteration=4774, loss=25774.218879430646\n",
      "Current iteration=4775, loss=25774.215179844734\n",
      "Current iteration=4776, loss=25774.211481340244\n",
      "Current iteration=4777, loss=25774.207783916067\n",
      "Current iteration=4778, loss=25774.204087571088\n",
      "Current iteration=4779, loss=25774.200392304196\n",
      "Current iteration=4780, loss=25774.19669811428\n",
      "Current iteration=4781, loss=25774.193005000234\n",
      "Current iteration=4782, loss=25774.189312960963\n",
      "Current iteration=4783, loss=25774.185621995348\n",
      "Current iteration=4784, loss=25774.181932102292\n",
      "Current iteration=4785, loss=25774.178243280683\n",
      "Current iteration=4786, loss=25774.174555529426\n",
      "Current iteration=4787, loss=25774.17086884742\n",
      "Current iteration=4788, loss=25774.167183233574\n",
      "Current iteration=4789, loss=25774.16349868677\n",
      "Current iteration=4790, loss=25774.159815205923\n",
      "Current iteration=4791, loss=25774.15613278994\n",
      "Current iteration=4792, loss=25774.15245143772\n",
      "Current iteration=4793, loss=25774.14877114817\n",
      "Current iteration=4794, loss=25774.145091920203\n",
      "Current iteration=4795, loss=25774.14141375272\n",
      "Current iteration=4796, loss=25774.13773664464\n",
      "Current iteration=4797, loss=25774.134060594868\n",
      "Current iteration=4798, loss=25774.13038560232\n",
      "Current iteration=4799, loss=25774.126711665907\n",
      "Current iteration=4800, loss=25774.12303878453\n",
      "Current iteration=4801, loss=25774.119366957137\n",
      "Current iteration=4802, loss=25774.11569618262\n",
      "Current iteration=4803, loss=25774.1120264599\n",
      "Current iteration=4804, loss=25774.108357787907\n",
      "Current iteration=4805, loss=25774.104690165546\n",
      "Current iteration=4806, loss=25774.101023591757\n",
      "Current iteration=4807, loss=25774.097358065446\n",
      "Current iteration=4808, loss=25774.09369358555\n",
      "Current iteration=4809, loss=25774.090030150976\n",
      "Current iteration=4810, loss=25774.086367760672\n",
      "Current iteration=4811, loss=25774.082706413552\n",
      "Current iteration=4812, loss=25774.07904610855\n",
      "Current iteration=4813, loss=25774.075386844594\n",
      "Current iteration=4814, loss=25774.071728620613\n",
      "Current iteration=4815, loss=25774.068071435537\n",
      "Current iteration=4816, loss=25774.06441528831\n",
      "Current iteration=4817, loss=25774.060760177857\n",
      "Current iteration=4818, loss=25774.057106103108\n",
      "Current iteration=4819, loss=25774.053453063014\n",
      "Current iteration=4820, loss=25774.0498010565\n",
      "Current iteration=4821, loss=25774.046150082515\n",
      "Current iteration=4822, loss=25774.042500139993\n",
      "Current iteration=4823, loss=25774.03885122787\n",
      "Current iteration=4824, loss=25774.035203345105\n",
      "Current iteration=4825, loss=25774.031556490627\n",
      "Current iteration=4826, loss=25774.02791066339\n",
      "Current iteration=4827, loss=25774.024265862317\n",
      "Current iteration=4828, loss=25774.020622086387\n",
      "Current iteration=4829, loss=25774.016979334523\n",
      "Current iteration=4830, loss=25774.013337605695\n",
      "Current iteration=4831, loss=25774.00969689883\n",
      "Current iteration=4832, loss=25774.0060572129\n",
      "Current iteration=4833, loss=25774.002418546846\n",
      "Current iteration=4834, loss=25773.998780899616\n",
      "Current iteration=4835, loss=25773.995144270182\n",
      "Current iteration=4836, loss=25773.991508657484\n",
      "Current iteration=4837, loss=25773.987874060484\n",
      "Current iteration=4838, loss=25773.984240478145\n",
      "Current iteration=4839, loss=25773.98060790942\n",
      "Current iteration=4840, loss=25773.976976353275\n",
      "Current iteration=4841, loss=25773.97334580866\n",
      "Current iteration=4842, loss=25773.96971627455\n",
      "Current iteration=4843, loss=25773.96608774991\n",
      "Current iteration=4844, loss=25773.962460233688\n",
      "Current iteration=4845, loss=25773.95883372486\n",
      "Current iteration=4846, loss=25773.9552082224\n",
      "Current iteration=4847, loss=25773.951583725262\n",
      "Current iteration=4848, loss=25773.947960232425\n",
      "Current iteration=4849, loss=25773.944337742854\n",
      "Current iteration=4850, loss=25773.94071625552\n",
      "Current iteration=4851, loss=25773.937095769405\n",
      "Current iteration=4852, loss=25773.93347628347\n",
      "Current iteration=4853, loss=25773.929857796695\n",
      "Current iteration=4854, loss=25773.926240308054\n",
      "Current iteration=4855, loss=25773.922623816525\n",
      "Current iteration=4856, loss=25773.919008321092\n",
      "Current iteration=4857, loss=25773.91539382072\n",
      "Current iteration=4858, loss=25773.911780314404\n",
      "Current iteration=4859, loss=25773.90816780111\n",
      "Current iteration=4860, loss=25773.904556279835\n",
      "Current iteration=4861, loss=25773.900945749552\n",
      "Current iteration=4862, loss=25773.897336209247\n",
      "Current iteration=4863, loss=25773.893727657916\n",
      "Current iteration=4864, loss=25773.89012009453\n",
      "Current iteration=4865, loss=25773.88651351809\n",
      "Current iteration=4866, loss=25773.88290792757\n",
      "Current iteration=4867, loss=25773.87930332197\n",
      "Current iteration=4868, loss=25773.87569970028\n",
      "Current iteration=4869, loss=25773.872097061496\n",
      "Current iteration=4870, loss=25773.868495404604\n",
      "Current iteration=4871, loss=25773.864894728595\n",
      "Current iteration=4872, loss=25773.861295032475\n",
      "Current iteration=4873, loss=25773.857696315237\n",
      "Current iteration=4874, loss=25773.85409857587\n",
      "Current iteration=4875, loss=25773.85050181338\n",
      "Current iteration=4876, loss=25773.84690602677\n",
      "Current iteration=4877, loss=25773.843311215034\n",
      "Current iteration=4878, loss=25773.839717377174\n",
      "Current iteration=4879, loss=25773.836124512196\n",
      "Current iteration=4880, loss=25773.832532619097\n",
      "Current iteration=4881, loss=25773.8289416969\n",
      "Current iteration=4882, loss=25773.82535174458\n",
      "Current iteration=4883, loss=25773.821762761174\n",
      "Current iteration=4884, loss=25773.818174745677\n",
      "Current iteration=4885, loss=25773.814587697096\n",
      "Current iteration=4886, loss=25773.81100161444\n",
      "Current iteration=4887, loss=25773.807416496733\n",
      "Current iteration=4888, loss=25773.80383234297\n",
      "Current iteration=4889, loss=25773.80024915218\n",
      "Current iteration=4890, loss=25773.79666692336\n",
      "Current iteration=4891, loss=25773.793085655547\n",
      "Current iteration=4892, loss=25773.78950534774\n",
      "Current iteration=4893, loss=25773.785925998967\n",
      "Current iteration=4894, loss=25773.782347608245\n",
      "Current iteration=4895, loss=25773.778770174584\n",
      "Current iteration=4896, loss=25773.775193697016\n",
      "Current iteration=4897, loss=25773.771618174553\n",
      "Current iteration=4898, loss=25773.76804360623\n",
      "Current iteration=4899, loss=25773.76446999106\n",
      "Current iteration=4900, loss=25773.760897328073\n",
      "Current iteration=4901, loss=25773.757325616294\n",
      "Current iteration=4902, loss=25773.753754854748\n",
      "Current iteration=4903, loss=25773.75018504247\n",
      "Current iteration=4904, loss=25773.74661617848\n",
      "Current iteration=4905, loss=25773.743048261815\n",
      "Current iteration=4906, loss=25773.739481291504\n",
      "Current iteration=4907, loss=25773.73591526657\n",
      "Current iteration=4908, loss=25773.73235018606\n",
      "Current iteration=4909, loss=25773.728786049003\n",
      "Current iteration=4910, loss=25773.72522285443\n",
      "Current iteration=4911, loss=25773.721660601386\n",
      "Current iteration=4912, loss=25773.718099288904\n",
      "Current iteration=4913, loss=25773.71453891602\n",
      "Current iteration=4914, loss=25773.710979481777\n",
      "Current iteration=4915, loss=25773.707420985207\n",
      "Current iteration=4916, loss=25773.703863425362\n",
      "Current iteration=4917, loss=25773.700306801285\n",
      "Current iteration=4918, loss=25773.69675111201\n",
      "Current iteration=4919, loss=25773.693196356584\n",
      "Current iteration=4920, loss=25773.689642534064\n",
      "Current iteration=4921, loss=25773.68608964348\n",
      "Current iteration=4922, loss=25773.68253768389\n",
      "Current iteration=4923, loss=25773.678986654337\n",
      "Current iteration=4924, loss=25773.67543655388\n",
      "Current iteration=4925, loss=25773.671887381555\n",
      "Current iteration=4926, loss=25773.66833913642\n",
      "Current iteration=4927, loss=25773.664791817533\n",
      "Current iteration=4928, loss=25773.661245423944\n",
      "Current iteration=4929, loss=25773.65769995471\n",
      "Current iteration=4930, loss=25773.65415540888\n",
      "Current iteration=4931, loss=25773.650611785513\n",
      "Current iteration=4932, loss=25773.647069083672\n",
      "Current iteration=4933, loss=25773.643527302414\n",
      "Current iteration=4934, loss=25773.639986440794\n",
      "Current iteration=4935, loss=25773.636446497872\n",
      "Current iteration=4936, loss=25773.632907472722\n",
      "Current iteration=4937, loss=25773.629369364386\n",
      "Current iteration=4938, loss=25773.625832171943\n",
      "Current iteration=4939, loss=25773.622295894456\n",
      "Current iteration=4940, loss=25773.618760530993\n",
      "Current iteration=4941, loss=25773.61522608061\n",
      "Current iteration=4942, loss=25773.61169254239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=4943, loss=25773.608159915384\n",
      "Current iteration=4944, loss=25773.60462819867\n",
      "Current iteration=4945, loss=25773.60109739133\n",
      "Current iteration=4946, loss=25773.597567492412\n",
      "Current iteration=4947, loss=25773.594038501007\n",
      "Current iteration=4948, loss=25773.590510416187\n",
      "Current iteration=4949, loss=25773.586983237015\n",
      "Current iteration=4950, loss=25773.583456962584\n",
      "Current iteration=4951, loss=25773.579931591954\n",
      "Current iteration=4952, loss=25773.576407124216\n",
      "Current iteration=4953, loss=25773.572883558434\n",
      "Current iteration=4954, loss=25773.5693608937\n",
      "Current iteration=4955, loss=25773.565839129093\n",
      "Current iteration=4956, loss=25773.56231826369\n",
      "Current iteration=4957, loss=25773.558798296574\n",
      "Current iteration=4958, loss=25773.55527922683\n",
      "Current iteration=4959, loss=25773.551761053546\n",
      "Current iteration=4960, loss=25773.548243775796\n",
      "Current iteration=4961, loss=25773.54472739268\n",
      "Current iteration=4962, loss=25773.54121190328\n",
      "Current iteration=4963, loss=25773.537697306685\n",
      "Current iteration=4964, loss=25773.534183601987\n",
      "Current iteration=4965, loss=25773.530670788266\n",
      "Current iteration=4966, loss=25773.527158864625\n",
      "Current iteration=4967, loss=25773.523647830145\n",
      "Current iteration=4968, loss=25773.520137683925\n",
      "Current iteration=4969, loss=25773.516628425066\n",
      "Current iteration=4970, loss=25773.513120052652\n",
      "Current iteration=4971, loss=25773.509612565787\n",
      "Current iteration=4972, loss=25773.506105963566\n",
      "Current iteration=4973, loss=25773.502600245083\n",
      "Current iteration=4974, loss=25773.499095409443\n",
      "Current iteration=4975, loss=25773.495591455743\n",
      "Current iteration=4976, loss=25773.49208838308\n",
      "Current iteration=4977, loss=25773.488586190568\n",
      "Current iteration=4978, loss=25773.48508487729\n",
      "Current iteration=4979, loss=25773.481584442376\n",
      "Current iteration=4980, loss=25773.478084884904\n",
      "Current iteration=4981, loss=25773.474586204\n",
      "Current iteration=4982, loss=25773.47108839876\n",
      "Current iteration=4983, loss=25773.467591468296\n",
      "Current iteration=4984, loss=25773.464095411713\n",
      "Current iteration=4985, loss=25773.460600228125\n",
      "Current iteration=4986, loss=25773.457105916626\n",
      "Current iteration=4987, loss=25773.45361247636\n",
      "Current iteration=4988, loss=25773.45011990642\n",
      "Current iteration=4989, loss=25773.446628205915\n",
      "Current iteration=4990, loss=25773.443137373957\n",
      "Current iteration=4991, loss=25773.43964740968\n",
      "Current iteration=4992, loss=25773.436158312183\n",
      "Current iteration=4993, loss=25773.43267008059\n",
      "Current iteration=4994, loss=25773.429182714022\n",
      "Current iteration=4995, loss=25773.425696211594\n",
      "Current iteration=4996, loss=25773.42221057242\n",
      "Current iteration=4997, loss=25773.418725795633\n",
      "Current iteration=4998, loss=25773.415241880342\n",
      "Current iteration=4999, loss=25773.411758825685\n",
      "Current iteration=5000, loss=25773.40827663077\n",
      "Current iteration=5001, loss=25773.404795294726\n",
      "Current iteration=5002, loss=25773.40131481669\n",
      "Current iteration=5003, loss=25773.39783519577\n",
      "Current iteration=5004, loss=25773.394356431116\n",
      "Current iteration=5005, loss=25773.39087852183\n",
      "Current iteration=5006, loss=25773.387401467062\n",
      "Current iteration=5007, loss=25773.38392526593\n",
      "Current iteration=5008, loss=25773.380449917575\n",
      "Current iteration=5009, loss=25773.376975421124\n",
      "Current iteration=5010, loss=25773.373501775706\n",
      "Current iteration=5011, loss=25773.370028980462\n",
      "Current iteration=5012, loss=25773.366557034526\n",
      "Current iteration=5013, loss=25773.36308593703\n",
      "Current iteration=5014, loss=25773.359615687106\n",
      "Current iteration=5015, loss=25773.356146283906\n",
      "Current iteration=5016, loss=25773.352677726558\n",
      "Current iteration=5017, loss=25773.3492100142\n",
      "Current iteration=5018, loss=25773.345743145983\n",
      "Current iteration=5019, loss=25773.34227712103\n",
      "Current iteration=5020, loss=25773.338811938505\n",
      "Current iteration=5021, loss=25773.33534759753\n",
      "Current iteration=5022, loss=25773.33188409727\n",
      "Current iteration=5023, loss=25773.328421436858\n",
      "Current iteration=5024, loss=25773.32495961544\n",
      "Current iteration=5025, loss=25773.32149863216\n",
      "Current iteration=5026, loss=25773.31803848617\n",
      "Current iteration=5027, loss=25773.314579176622\n",
      "Current iteration=5028, loss=25773.31112070266\n",
      "Current iteration=5029, loss=25773.307663063435\n",
      "Current iteration=5030, loss=25773.3042062581\n",
      "Current iteration=5031, loss=25773.300750285813\n",
      "Current iteration=5032, loss=25773.297295145712\n",
      "Current iteration=5033, loss=25773.29384083696\n",
      "Current iteration=5034, loss=25773.290387358713\n",
      "Current iteration=5035, loss=25773.28693471013\n",
      "Current iteration=5036, loss=25773.28348289036\n",
      "Current iteration=5037, loss=25773.280031898557\n",
      "Current iteration=5038, loss=25773.276581733895\n",
      "Current iteration=5039, loss=25773.27313239552\n",
      "Current iteration=5040, loss=25773.269683882594\n",
      "Current iteration=5041, loss=25773.26623619429\n",
      "Current iteration=5042, loss=25773.26278932975\n",
      "Current iteration=5043, loss=25773.25934328815\n",
      "Current iteration=5044, loss=25773.255898068655\n",
      "Current iteration=5045, loss=25773.252453670426\n",
      "Current iteration=5046, loss=25773.24901009263\n",
      "Current iteration=5047, loss=25773.24556733443\n",
      "Current iteration=5048, loss=25773.242125394994\n",
      "Current iteration=5049, loss=25773.2386842735\n",
      "Current iteration=5050, loss=25773.235243969102\n",
      "Current iteration=5051, loss=25773.231804480976\n",
      "Current iteration=5052, loss=25773.2283658083\n",
      "Current iteration=5053, loss=25773.22492795024\n",
      "Current iteration=5054, loss=25773.221490905962\n",
      "Current iteration=5055, loss=25773.218054674653\n",
      "Current iteration=5056, loss=25773.21461925548\n",
      "Current iteration=5057, loss=25773.21118464762\n",
      "Current iteration=5058, loss=25773.207750850244\n",
      "Current iteration=5059, loss=25773.20431786253\n",
      "Current iteration=5060, loss=25773.20088568367\n",
      "Current iteration=5061, loss=25773.19745431283\n",
      "Current iteration=5062, loss=25773.194023749187\n",
      "Current iteration=5063, loss=25773.19059399193\n",
      "Current iteration=5064, loss=25773.187165040235\n",
      "Current iteration=5065, loss=25773.18373689329\n",
      "Current iteration=5066, loss=25773.180309550276\n",
      "Current iteration=5067, loss=25773.176883010372\n",
      "Current iteration=5068, loss=25773.17345727277\n",
      "Current iteration=5069, loss=25773.170032336653\n",
      "Current iteration=5070, loss=25773.166608201205\n",
      "Current iteration=5071, loss=25773.163184865618\n",
      "Current iteration=5072, loss=25773.159762329073\n",
      "Current iteration=5073, loss=25773.156340590773\n",
      "Current iteration=5074, loss=25773.15291964989\n",
      "Current iteration=5075, loss=25773.149499505627\n",
      "Current iteration=5076, loss=25773.146080157177\n",
      "Current iteration=5077, loss=25773.142661603728\n",
      "Current iteration=5078, loss=25773.139243844475\n",
      "Current iteration=5079, loss=25773.135826878617\n",
      "Current iteration=5080, loss=25773.132410705337\n",
      "Current iteration=5081, loss=25773.12899532384\n",
      "Current iteration=5082, loss=25773.125580733325\n",
      "Current iteration=5083, loss=25773.12216693299\n",
      "Current iteration=5084, loss=25773.11875392202\n",
      "Current iteration=5085, loss=25773.115341699635\n",
      "Current iteration=5086, loss=25773.11193026502\n",
      "Current iteration=5087, loss=25773.10851961738\n",
      "Current iteration=5088, loss=25773.10510975592\n",
      "Current iteration=5089, loss=25773.101700679847\n",
      "Current iteration=5090, loss=25773.098292388357\n",
      "Current iteration=5091, loss=25773.09488488066\n",
      "Current iteration=5092, loss=25773.09147815595\n",
      "Current iteration=5093, loss=25773.08807221345\n",
      "Current iteration=5094, loss=25773.084667052357\n",
      "Current iteration=5095, loss=25773.081262671883\n",
      "Current iteration=5096, loss=25773.07785907123\n",
      "Current iteration=5097, loss=25773.07445624962\n",
      "Current iteration=5098, loss=25773.07105420625\n",
      "Current iteration=5099, loss=25773.067652940343\n",
      "Current iteration=5100, loss=25773.064252451102\n",
      "Current iteration=5101, loss=25773.06085273775\n",
      "Current iteration=5102, loss=25773.05745379949\n",
      "Current iteration=5103, loss=25773.054055635548\n",
      "Current iteration=5104, loss=25773.05065824513\n",
      "Current iteration=5105, loss=25773.047261627456\n",
      "Current iteration=5106, loss=25773.043865781743\n",
      "Current iteration=5107, loss=25773.040470707216\n",
      "Current iteration=5108, loss=25773.03707640308\n",
      "Current iteration=5109, loss=25773.03368286856\n",
      "Current iteration=5110, loss=25773.030290102888\n",
      "Current iteration=5111, loss=25773.026898105276\n",
      "Current iteration=5112, loss=25773.023506874935\n",
      "Current iteration=5113, loss=25773.020116411102\n",
      "Current iteration=5114, loss=25773.01672671301\n",
      "Current iteration=5115, loss=25773.013337779863\n",
      "Current iteration=5116, loss=25773.0099496109\n",
      "Current iteration=5117, loss=25773.006562205337\n",
      "Current iteration=5118, loss=25773.00317556241\n",
      "Current iteration=5119, loss=25772.999789681347\n",
      "Current iteration=5120, loss=25772.996404561378\n",
      "Current iteration=5121, loss=25772.993020201724\n",
      "Current iteration=5122, loss=25772.98963660162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=5123, loss=25772.986253760297\n",
      "Current iteration=5124, loss=25772.982871676984\n",
      "Current iteration=5125, loss=25772.979490350925\n",
      "Current iteration=5126, loss=25772.976109781346\n",
      "Current iteration=5127, loss=25772.972729967474\n",
      "Current iteration=5128, loss=25772.969350908563\n",
      "Current iteration=5129, loss=25772.96597260383\n",
      "Current iteration=5130, loss=25772.962595052522\n",
      "Current iteration=5131, loss=25772.95921825388\n",
      "Current iteration=5132, loss=25772.955842207135\n",
      "Current iteration=5133, loss=25772.95246691153\n",
      "Current iteration=5134, loss=25772.949092366303\n",
      "Current iteration=5135, loss=25772.9457185707\n",
      "Current iteration=5136, loss=25772.942345523963\n",
      "Current iteration=5137, loss=25772.938973225322\n",
      "Current iteration=5138, loss=25772.935601674035\n",
      "Current iteration=5139, loss=25772.932230869334\n",
      "Current iteration=5140, loss=25772.928860810483\n",
      "Current iteration=5141, loss=25772.925491496713\n",
      "Current iteration=5142, loss=25772.922122927263\n",
      "Current iteration=5143, loss=25772.9187551014\n",
      "Current iteration=5144, loss=25772.915388018366\n",
      "Current iteration=5145, loss=25772.912021677403\n",
      "Current iteration=5146, loss=25772.908656077758\n",
      "Current iteration=5147, loss=25772.905291218707\n",
      "Current iteration=5148, loss=25772.901927099465\n",
      "Current iteration=5149, loss=25772.89856371931\n",
      "Current iteration=5150, loss=25772.89520107749\n",
      "Current iteration=5151, loss=25772.89183917326\n",
      "Current iteration=5152, loss=25772.888478005858\n",
      "Current iteration=5153, loss=25772.885117574566\n",
      "Current iteration=5154, loss=25772.881757878622\n",
      "Current iteration=5155, loss=25772.878398917288\n",
      "Current iteration=5156, loss=25772.87504068982\n",
      "Current iteration=5157, loss=25772.87168319548\n",
      "Current iteration=5158, loss=25772.868326433527\n",
      "Current iteration=5159, loss=25772.864970403214\n",
      "Current iteration=5160, loss=25772.861615103815\n",
      "Current iteration=5161, loss=25772.85826053458\n",
      "Current iteration=5162, loss=25772.85490669478\n",
      "Current iteration=5163, loss=25772.85155358367\n",
      "Current iteration=5164, loss=25772.84820120052\n",
      "Current iteration=5165, loss=25772.8448495446\n",
      "Current iteration=5166, loss=25772.84149861516\n",
      "Current iteration=5167, loss=25772.838148411473\n",
      "Current iteration=5168, loss=25772.834798932818\n",
      "Current iteration=5169, loss=25772.831450178448\n",
      "Current iteration=5170, loss=25772.828102147643\n",
      "Current iteration=5171, loss=25772.824754839658\n",
      "Current iteration=5172, loss=25772.821408253774\n",
      "Current iteration=5173, loss=25772.81806238927\n",
      "Current iteration=5174, loss=25772.814717245397\n",
      "Current iteration=5175, loss=25772.81137282144\n",
      "Current iteration=5176, loss=25772.808029116677\n",
      "Current iteration=5177, loss=25772.804686130374\n",
      "Current iteration=5178, loss=25772.80134386181\n",
      "Current iteration=5179, loss=25772.798002310254\n",
      "Current iteration=5180, loss=25772.79466147499\n",
      "Current iteration=5181, loss=25772.7913213553\n",
      "Current iteration=5182, loss=25772.787981950445\n",
      "Current iteration=5183, loss=25772.784643259714\n",
      "Current iteration=5184, loss=25772.78130528239\n",
      "Current iteration=5185, loss=25772.777968017752\n",
      "Current iteration=5186, loss=25772.774631465076\n",
      "Current iteration=5187, loss=25772.771295623646\n",
      "Current iteration=5188, loss=25772.767960492743\n",
      "Current iteration=5189, loss=25772.764626071654\n",
      "Current iteration=5190, loss=25772.761292359668\n",
      "Current iteration=5191, loss=25772.75795935605\n",
      "Current iteration=5192, loss=25772.75462706011\n",
      "Current iteration=5193, loss=25772.75129547112\n",
      "Current iteration=5194, loss=25772.747964588372\n",
      "Current iteration=5195, loss=25772.744634411156\n",
      "Current iteration=5196, loss=25772.74130493875\n",
      "Current iteration=5197, loss=25772.73797617045\n",
      "Current iteration=5198, loss=25772.734648105554\n",
      "Current iteration=5199, loss=25772.73132074334\n",
      "Current iteration=5200, loss=25772.727994083118\n",
      "Current iteration=5201, loss=25772.724668124156\n",
      "Current iteration=5202, loss=25772.721342865756\n",
      "Current iteration=5203, loss=25772.718018307227\n",
      "Current iteration=5204, loss=25772.71469444784\n",
      "Current iteration=5205, loss=25772.71137128691\n",
      "Current iteration=5206, loss=25772.708048823726\n",
      "Current iteration=5207, loss=25772.704727057582\n",
      "Current iteration=5208, loss=25772.701405987777\n",
      "Current iteration=5209, loss=25772.69808561361\n",
      "Current iteration=5210, loss=25772.69476593439\n",
      "Current iteration=5211, loss=25772.6914469494\n",
      "Current iteration=5212, loss=25772.688128657945\n",
      "Current iteration=5213, loss=25772.68481105933\n",
      "Current iteration=5214, loss=25772.68149415287\n",
      "Current iteration=5215, loss=25772.67817793784\n",
      "Current iteration=5216, loss=25772.674862413565\n",
      "Current iteration=5217, loss=25772.67154757934\n",
      "Current iteration=5218, loss=25772.668233434477\n",
      "Current iteration=5219, loss=25772.664919978277\n",
      "Current iteration=5220, loss=25772.66160721004\n",
      "Current iteration=5221, loss=25772.658295129087\n",
      "Current iteration=5222, loss=25772.654983734723\n",
      "Current iteration=5223, loss=25772.65167302625\n",
      "Current iteration=5224, loss=25772.648363002976\n",
      "Current iteration=5225, loss=25772.645053664226\n",
      "Current iteration=5226, loss=25772.641745009292\n",
      "Current iteration=5227, loss=25772.6384370375\n",
      "Current iteration=5228, loss=25772.635129748156\n",
      "Current iteration=5229, loss=25772.631823140575\n",
      "Current iteration=5230, loss=25772.628517214067\n",
      "Current iteration=5231, loss=25772.625211967956\n",
      "Current iteration=5232, loss=25772.621907401546\n",
      "Current iteration=5233, loss=25772.618603514158\n",
      "Current iteration=5234, loss=25772.61530030511\n",
      "Current iteration=5235, loss=25772.611997773725\n",
      "Current iteration=5236, loss=25772.608695919313\n",
      "Current iteration=5237, loss=25772.60539474119\n",
      "Current iteration=5238, loss=25772.602094238682\n",
      "Current iteration=5239, loss=25772.598794411115\n",
      "Current iteration=5240, loss=25772.595495257796\n",
      "Current iteration=5241, loss=25772.592196778056\n",
      "Current iteration=5242, loss=25772.58889897121\n",
      "Current iteration=5243, loss=25772.585601836596\n",
      "Current iteration=5244, loss=25772.58230537352\n",
      "Current iteration=5245, loss=25772.579009581324\n",
      "Current iteration=5246, loss=25772.575714459323\n",
      "Current iteration=5247, loss=25772.57242000684\n",
      "Current iteration=5248, loss=25772.569126223214\n",
      "Current iteration=5249, loss=25772.565833107757\n",
      "Current iteration=5250, loss=25772.56254065982\n",
      "Current iteration=5251, loss=25772.559248878708\n",
      "Current iteration=5252, loss=25772.555957763765\n",
      "Current iteration=5253, loss=25772.55266731431\n",
      "Current iteration=5254, loss=25772.549377529685\n",
      "Current iteration=5255, loss=25772.54608840922\n",
      "Current iteration=5256, loss=25772.542799952247\n",
      "Current iteration=5257, loss=25772.539512158095\n",
      "Current iteration=5258, loss=25772.5362250261\n",
      "Current iteration=5259, loss=25772.532938555603\n",
      "Current iteration=5260, loss=25772.52965274593\n",
      "Current iteration=5261, loss=25772.52636759642\n",
      "Current iteration=5262, loss=25772.52308310641\n",
      "Current iteration=5263, loss=25772.519799275244\n",
      "Current iteration=5264, loss=25772.516516102252\n",
      "Current iteration=5265, loss=25772.513233586775\n",
      "Current iteration=5266, loss=25772.509951728152\n",
      "Current iteration=5267, loss=25772.506670525723\n",
      "Current iteration=5268, loss=25772.503389978832\n",
      "Current iteration=5269, loss=25772.50011008682\n",
      "Current iteration=5270, loss=25772.496830849028\n",
      "Current iteration=5271, loss=25772.493552264797\n",
      "Current iteration=5272, loss=25772.490274333482\n",
      "Current iteration=5273, loss=25772.486997054406\n",
      "Current iteration=5274, loss=25772.48372042693\n",
      "Current iteration=5275, loss=25772.480444450404\n",
      "Current iteration=5276, loss=25772.47716912416\n",
      "Current iteration=5277, loss=25772.47389444756\n",
      "Current iteration=5278, loss=25772.470620419943\n",
      "Current iteration=5279, loss=25772.467347040656\n",
      "Current iteration=5280, loss=25772.46407430905\n",
      "Current iteration=5281, loss=25772.46080222448\n",
      "Current iteration=5282, loss=25772.457530786283\n",
      "Current iteration=5283, loss=25772.454259993836\n",
      "Current iteration=5284, loss=25772.450989846464\n",
      "Current iteration=5285, loss=25772.447720343538\n",
      "Current iteration=5286, loss=25772.444451484404\n",
      "Current iteration=5287, loss=25772.441183268416\n",
      "Current iteration=5288, loss=25772.43791569493\n",
      "Current iteration=5289, loss=25772.434648763297\n",
      "Current iteration=5290, loss=25772.43138247288\n",
      "Current iteration=5291, loss=25772.428116823037\n",
      "Current iteration=5292, loss=25772.42485181312\n",
      "Current iteration=5293, loss=25772.421587442484\n",
      "Current iteration=5294, loss=25772.4183237105\n",
      "Current iteration=5295, loss=25772.415060616524\n",
      "Current iteration=5296, loss=25772.411798159897\n",
      "Current iteration=5297, loss=25772.408536340015\n",
      "Current iteration=5298, loss=25772.40527515621\n",
      "Current iteration=5299, loss=25772.402014607855\n",
      "Current iteration=5300, loss=25772.39875469431\n",
      "Current iteration=5301, loss=25772.395495414945\n",
      "Current iteration=5302, loss=25772.392236769127\n",
      "Current iteration=5303, loss=25772.388978756208\n",
      "Current iteration=5304, loss=25772.38572137555\n",
      "Current iteration=5305, loss=25772.382464626546\n",
      "Current iteration=5306, loss=25772.379208508537\n",
      "Current iteration=5307, loss=25772.375953020903\n",
      "Current iteration=5308, loss=25772.37269816301\n",
      "Current iteration=5309, loss=25772.36944393423\n",
      "Current iteration=5310, loss=25772.366190333923\n",
      "Current iteration=5311, loss=25772.36293736147\n",
      "Current iteration=5312, loss=25772.359685016232\n",
      "Current iteration=5313, loss=25772.356433297588\n",
      "Current iteration=5314, loss=25772.35318220491\n",
      "Current iteration=5315, loss=25772.34993173757\n",
      "Current iteration=5316, loss=25772.346681894942\n",
      "Current iteration=5317, loss=25772.343432676404\n",
      "Current iteration=5318, loss=25772.340184081317\n",
      "Current iteration=5319, loss=25772.33693610907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=5320, loss=25772.33368875904\n",
      "Current iteration=5321, loss=25772.33044203059\n",
      "Current iteration=5322, loss=25772.327195923113\n",
      "Current iteration=5323, loss=25772.32395043598\n",
      "Current iteration=5324, loss=25772.320705568578\n",
      "Current iteration=5325, loss=25772.31746132027\n",
      "Current iteration=5326, loss=25772.314217690455\n",
      "Current iteration=5327, loss=25772.310974678498\n",
      "Current iteration=5328, loss=25772.307732283793\n",
      "Current iteration=5329, loss=25772.304490505714\n",
      "Current iteration=5330, loss=25772.301249343647\n",
      "Current iteration=5331, loss=25772.298008796977\n",
      "Current iteration=5332, loss=25772.294768865082\n",
      "Current iteration=5333, loss=25772.291529547358\n",
      "Current iteration=5334, loss=25772.28829084318\n",
      "Current iteration=5335, loss=25772.28505275194\n",
      "Current iteration=5336, loss=25772.281815273018\n",
      "Current iteration=5337, loss=25772.27857840581\n",
      "Current iteration=5338, loss=25772.275342149704\n",
      "Current iteration=5339, loss=25772.27210650408\n",
      "Current iteration=5340, loss=25772.268871468328\n",
      "Current iteration=5341, loss=25772.265637041848\n",
      "Current iteration=5342, loss=25772.262403224024\n",
      "Current iteration=5343, loss=25772.25917001425\n",
      "Current iteration=5344, loss=25772.255937411916\n",
      "Current iteration=5345, loss=25772.252705416413\n",
      "Current iteration=5346, loss=25772.24947402714\n",
      "Current iteration=5347, loss=25772.246243243484\n",
      "Current iteration=5348, loss=25772.243013064843\n",
      "Current iteration=5349, loss=25772.239783490615\n",
      "Current iteration=5350, loss=25772.236554520183\n",
      "Current iteration=5351, loss=25772.233326152964\n",
      "Current iteration=5352, loss=25772.23009838834\n",
      "Current iteration=5353, loss=25772.226871225714\n",
      "Current iteration=5354, loss=25772.22364466448\n",
      "Current iteration=5355, loss=25772.22041870404\n",
      "Current iteration=5356, loss=25772.217193343793\n",
      "Current iteration=5357, loss=25772.213968583143\n",
      "Current iteration=5358, loss=25772.210744421493\n",
      "Current iteration=5359, loss=25772.20752085823\n",
      "Current iteration=5360, loss=25772.20429789277\n",
      "Current iteration=5361, loss=25772.20107552451\n",
      "Current iteration=5362, loss=25772.197853752856\n",
      "Current iteration=5363, loss=25772.19463257721\n",
      "Current iteration=5364, loss=25772.191411996977\n",
      "Current iteration=5365, loss=25772.18819201157\n",
      "Current iteration=5366, loss=25772.18497262038\n",
      "Current iteration=5367, loss=25772.18175382282\n",
      "Current iteration=5368, loss=25772.178535618303\n",
      "Current iteration=5369, loss=25772.175318006233\n",
      "Current iteration=5370, loss=25772.172100986012\n",
      "Current iteration=5371, loss=25772.16888455706\n",
      "Current iteration=5372, loss=25772.165668718786\n",
      "Current iteration=5373, loss=25772.162453470588\n",
      "Current iteration=5374, loss=25772.159238811888\n",
      "Current iteration=5375, loss=25772.156024742093\n",
      "Current iteration=5376, loss=25772.152811260614\n",
      "Current iteration=5377, loss=25772.149598366872\n",
      "Current iteration=5378, loss=25772.146386060274\n",
      "Current iteration=5379, loss=25772.143174340235\n",
      "Current iteration=5380, loss=25772.139963206166\n",
      "Current iteration=5381, loss=25772.13675265749\n",
      "Current iteration=5382, loss=25772.133542693617\n",
      "Current iteration=5383, loss=25772.130333313966\n",
      "Current iteration=5384, loss=25772.127124517952\n",
      "Current iteration=5385, loss=25772.123916304994\n",
      "Current iteration=5386, loss=25772.120708674513\n",
      "Current iteration=5387, loss=25772.117501625926\n",
      "Current iteration=5388, loss=25772.114295158644\n",
      "Current iteration=5389, loss=25772.111089272104\n",
      "Current iteration=5390, loss=25772.10788396572\n",
      "Current iteration=5391, loss=25772.104679238906\n",
      "Current iteration=5392, loss=25772.10147509109\n",
      "Current iteration=5393, loss=25772.098271521696\n",
      "Current iteration=5394, loss=25772.09506853015\n",
      "Current iteration=5395, loss=25772.091866115865\n",
      "Current iteration=5396, loss=25772.08866427827\n",
      "Current iteration=5397, loss=25772.085463016792\n",
      "Current iteration=5398, loss=25772.082262330863\n",
      "Current iteration=5399, loss=25772.079062219902\n",
      "Current iteration=5400, loss=25772.075862683338\n",
      "Current iteration=5401, loss=25772.072663720595\n",
      "Current iteration=5402, loss=25772.069465331104\n",
      "Current iteration=5403, loss=25772.066267514296\n",
      "Current iteration=5404, loss=25772.06307026959\n",
      "Current iteration=5405, loss=25772.059873596427\n",
      "Current iteration=5406, loss=25772.056677494234\n",
      "Current iteration=5407, loss=25772.053481962448\n",
      "Current iteration=5408, loss=25772.05028700049\n",
      "Current iteration=5409, loss=25772.047092607798\n",
      "Current iteration=5410, loss=25772.043898783806\n",
      "Current iteration=5411, loss=25772.040705527943\n",
      "Current iteration=5412, loss=25772.037512839648\n",
      "Current iteration=5413, loss=25772.034320718354\n",
      "Current iteration=5414, loss=25772.031129163493\n",
      "Current iteration=5415, loss=25772.02793817451\n",
      "Current iteration=5416, loss=25772.024747750827\n",
      "Current iteration=5417, loss=25772.021557891894\n",
      "Current iteration=5418, loss=25772.018368597146\n",
      "Current iteration=5419, loss=25772.015179866023\n",
      "Current iteration=5420, loss=25772.01199169795\n",
      "Current iteration=5421, loss=25772.00880409238\n",
      "Current iteration=5422, loss=25772.005617048755\n",
      "Current iteration=5423, loss=25772.002430566507\n",
      "Current iteration=5424, loss=25771.99924464508\n",
      "Current iteration=5425, loss=25771.996059283927\n",
      "Current iteration=5426, loss=25771.99287448247\n",
      "Current iteration=5427, loss=25771.98969024016\n",
      "Current iteration=5428, loss=25771.986506556448\n",
      "Current iteration=5429, loss=25771.98332343077\n",
      "Current iteration=5430, loss=25771.98014086258\n",
      "Current iteration=5431, loss=25771.976958851315\n",
      "Current iteration=5432, loss=25771.97377739642\n",
      "Current iteration=5433, loss=25771.970596497347\n",
      "Current iteration=5434, loss=25771.967416153533\n",
      "Current iteration=5435, loss=25771.96423636445\n",
      "Current iteration=5436, loss=25771.961057129516\n",
      "Current iteration=5437, loss=25771.957878448196\n",
      "Current iteration=5438, loss=25771.954700319944\n",
      "Current iteration=5439, loss=25771.951522744203\n",
      "Current iteration=5440, loss=25771.94834572042\n",
      "Current iteration=5441, loss=25771.945169248047\n",
      "Current iteration=5442, loss=25771.941993326538\n",
      "Current iteration=5443, loss=25771.938817955353\n",
      "Current iteration=5444, loss=25771.935643133933\n",
      "Current iteration=5445, loss=25771.932468861738\n",
      "Current iteration=5446, loss=25771.929295138223\n",
      "Current iteration=5447, loss=25771.92612196284\n",
      "Current iteration=5448, loss=25771.92294933504\n",
      "Current iteration=5449, loss=25771.919777254287\n",
      "Current iteration=5450, loss=25771.91660572004\n",
      "Current iteration=5451, loss=25771.913434731745\n",
      "Current iteration=5452, loss=25771.910264288857\n",
      "Current iteration=5453, loss=25771.907094390845\n",
      "Current iteration=5454, loss=25771.903925037168\n",
      "Current iteration=5455, loss=25771.90075622728\n",
      "Current iteration=5456, loss=25771.89758796064\n",
      "Current iteration=5457, loss=25771.894420236713\n",
      "Current iteration=5458, loss=25771.891253054964\n",
      "Current iteration=5459, loss=25771.888086414838\n",
      "Current iteration=5460, loss=25771.884920315806\n",
      "Current iteration=5461, loss=25771.881754757338\n",
      "Current iteration=5462, loss=25771.878589738884\n",
      "Current iteration=5463, loss=25771.875425259925\n",
      "Current iteration=5464, loss=25771.872261319906\n",
      "Current iteration=5465, loss=25771.869097918305\n",
      "Current iteration=5466, loss=25771.865935054582\n",
      "Current iteration=5467, loss=25771.86277272821\n",
      "Current iteration=5468, loss=25771.85961093865\n",
      "Current iteration=5469, loss=25771.856449685365\n",
      "Current iteration=5470, loss=25771.853288967824\n",
      "Current iteration=5471, loss=25771.850128785503\n",
      "Current iteration=5472, loss=25771.846969137867\n",
      "Current iteration=5473, loss=25771.843810024387\n",
      "Current iteration=5474, loss=25771.840651444527\n",
      "Current iteration=5475, loss=25771.83749339776\n",
      "Current iteration=5476, loss=25771.834335883555\n",
      "Current iteration=5477, loss=25771.831178901393\n",
      "Current iteration=5478, loss=25771.82802245074\n",
      "Current iteration=5479, loss=25771.824866531068\n",
      "Current iteration=5480, loss=25771.821711141856\n",
      "Current iteration=5481, loss=25771.818556282567\n",
      "Current iteration=5482, loss=25771.81540195268\n",
      "Current iteration=5483, loss=25771.81224815168\n",
      "Current iteration=5484, loss=25771.809094879023\n",
      "Current iteration=5485, loss=25771.8059421342\n",
      "Current iteration=5486, loss=25771.802789916692\n",
      "Current iteration=5487, loss=25771.79963822596\n",
      "Current iteration=5488, loss=25771.796487061492\n",
      "Current iteration=5489, loss=25771.79333642276\n",
      "Current iteration=5490, loss=25771.790186309256\n",
      "Current iteration=5491, loss=25771.78703672044\n",
      "Current iteration=5492, loss=25771.78388765581\n",
      "Current iteration=5493, loss=25771.780739114838\n",
      "Current iteration=5494, loss=25771.777591097\n",
      "Current iteration=5495, loss=25771.774443601786\n",
      "Current iteration=5496, loss=25771.771296628678\n",
      "Current iteration=5497, loss=25771.768150177155\n",
      "Current iteration=5498, loss=25771.765004246707\n",
      "Current iteration=5499, loss=25771.761858836802\n",
      "Current iteration=5500, loss=25771.758713946943\n",
      "Current iteration=5501, loss=25771.755569576606\n",
      "Current iteration=5502, loss=25771.752425725273\n",
      "Current iteration=5503, loss=25771.74928239243\n",
      "Current iteration=5504, loss=25771.746139577575\n",
      "Current iteration=5505, loss=25771.742997280184\n",
      "Current iteration=5506, loss=25771.73985549975\n",
      "Current iteration=5507, loss=25771.736714235754\n",
      "Current iteration=5508, loss=25771.73357348769\n",
      "Current iteration=5509, loss=25771.730433255056\n",
      "Current iteration=5510, loss=25771.72729353732\n",
      "Current iteration=5511, loss=25771.72415433399\n",
      "Current iteration=5512, loss=25771.721015644547\n",
      "Current iteration=5513, loss=25771.717877468498\n",
      "Current iteration=5514, loss=25771.714739805313\n",
      "Current iteration=5515, loss=25771.711602654505\n",
      "Current iteration=5516, loss=25771.70846601555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=5517, loss=25771.705329887947\n",
      "Current iteration=5518, loss=25771.702194271198\n",
      "Current iteration=5519, loss=25771.699059164785\n",
      "Current iteration=5520, loss=25771.695924568212\n",
      "Current iteration=5521, loss=25771.69279048097\n",
      "Current iteration=5522, loss=25771.68965690256\n",
      "Current iteration=5523, loss=25771.686523832468\n",
      "Current iteration=5524, loss=25771.68339127021\n",
      "Current iteration=5525, loss=25771.680259215267\n",
      "Current iteration=5526, loss=25771.67712766714\n",
      "Current iteration=5527, loss=25771.673996625334\n",
      "Current iteration=5528, loss=25771.670866089342\n",
      "Current iteration=5529, loss=25771.667736058665\n",
      "Current iteration=5530, loss=25771.66460653281\n",
      "Current iteration=5531, loss=25771.661477511272\n",
      "Current iteration=5532, loss=25771.65834899355\n",
      "Current iteration=5533, loss=25771.65522097915\n",
      "Current iteration=5534, loss=25771.652093467575\n",
      "Current iteration=5535, loss=25771.648966458324\n",
      "Current iteration=5536, loss=25771.645839950907\n",
      "Current iteration=5537, loss=25771.642713944828\n",
      "Current iteration=5538, loss=25771.639588439575\n",
      "Current iteration=5539, loss=25771.636463434676\n",
      "Current iteration=5540, loss=25771.63333892962\n",
      "Current iteration=5541, loss=25771.63021492393\n",
      "Current iteration=5542, loss=25771.627091417096\n",
      "Current iteration=5543, loss=25771.623968408632\n",
      "Current iteration=5544, loss=25771.620845898036\n",
      "Current iteration=5545, loss=25771.617723884836\n",
      "Current iteration=5546, loss=25771.61460236852\n",
      "Current iteration=5547, loss=25771.61148134862\n",
      "Current iteration=5548, loss=25771.608360824623\n",
      "Current iteration=5549, loss=25771.605240796052\n",
      "Current iteration=5550, loss=25771.60212126242\n",
      "Current iteration=5551, loss=25771.599002223225\n",
      "Current iteration=5552, loss=25771.59588367799\n",
      "Current iteration=5553, loss=25771.592765626225\n",
      "Current iteration=5554, loss=25771.589648067442\n",
      "Current iteration=5555, loss=25771.586531001154\n",
      "Current iteration=5556, loss=25771.58341442687\n",
      "Current iteration=5557, loss=25771.58029834412\n",
      "Current iteration=5558, loss=25771.577182752404\n",
      "Current iteration=5559, loss=25771.574067651243\n",
      "Current iteration=5560, loss=25771.570953040147\n",
      "Current iteration=5561, loss=25771.56783891864\n",
      "Current iteration=5562, loss=25771.564725286233\n",
      "Current iteration=5563, loss=25771.561612142454\n",
      "Current iteration=5564, loss=25771.558499486804\n",
      "Current iteration=5565, loss=25771.555387318822\n",
      "Current iteration=5566, loss=25771.552275638005\n",
      "Current iteration=5567, loss=25771.549164443895\n",
      "Current iteration=5568, loss=25771.546053735987\n",
      "Current iteration=5569, loss=25771.54294351383\n",
      "Current iteration=5570, loss=25771.53983377692\n",
      "Current iteration=5571, loss=25771.536724524783\n",
      "Current iteration=5572, loss=25771.533615756955\n",
      "Current iteration=5573, loss=25771.530507472944\n",
      "Current iteration=5574, loss=25771.527399672286\n",
      "Current iteration=5575, loss=25771.52429235449\n",
      "Current iteration=5576, loss=25771.52118551909\n",
      "Current iteration=5577, loss=25771.51807916561\n",
      "Current iteration=5578, loss=25771.514973293564\n",
      "Current iteration=5579, loss=25771.511867902496\n",
      "Current iteration=5580, loss=25771.508762991914\n",
      "Current iteration=5581, loss=25771.505658561357\n",
      "Current iteration=5582, loss=25771.50255461035\n",
      "Current iteration=5583, loss=25771.49945113841\n",
      "Current iteration=5584, loss=25771.496348145083\n",
      "Current iteration=5585, loss=25771.493245629877\n",
      "Current iteration=5586, loss=25771.490143592335\n",
      "Current iteration=5587, loss=25771.487042031982\n",
      "Current iteration=5588, loss=25771.483940948354\n",
      "Current iteration=5589, loss=25771.48084034097\n",
      "Current iteration=5590, loss=25771.477740209368\n",
      "Current iteration=5591, loss=25771.474640553082\n",
      "Current iteration=5592, loss=25771.47154137164\n",
      "Current iteration=5593, loss=25771.468442664573\n",
      "Current iteration=5594, loss=25771.465344431414\n",
      "Current iteration=5595, loss=25771.462246671705\n",
      "Current iteration=5596, loss=25771.459149384962\n",
      "Current iteration=5597, loss=25771.45605257074\n",
      "Current iteration=5598, loss=25771.45295622856\n",
      "Current iteration=5599, loss=25771.449860357963\n",
      "Current iteration=5600, loss=25771.44676495848\n",
      "Current iteration=5601, loss=25771.443670029657\n",
      "Current iteration=5602, loss=25771.44057557102\n",
      "Current iteration=5603, loss=25771.437481582114\n",
      "Current iteration=5604, loss=25771.434388062473\n",
      "Current iteration=5605, loss=25771.431295011636\n",
      "Current iteration=5606, loss=25771.42820242914\n",
      "Current iteration=5607, loss=25771.42511031453\n",
      "Current iteration=5608, loss=25771.422018667334\n",
      "Current iteration=5609, loss=25771.418927487106\n",
      "Current iteration=5610, loss=25771.415836773383\n",
      "Current iteration=5611, loss=25771.4127465257\n",
      "Current iteration=5612, loss=25771.409656743603\n",
      "Current iteration=5613, loss=25771.406567426635\n",
      "Current iteration=5614, loss=25771.403478574335\n",
      "Current iteration=5615, loss=25771.40039018625\n",
      "Current iteration=5616, loss=25771.397302261918\n",
      "Current iteration=5617, loss=25771.39421480089\n",
      "Current iteration=5618, loss=25771.391127802715\n",
      "Current iteration=5619, loss=25771.388041266924\n",
      "Current iteration=5620, loss=25771.38495519307\n",
      "Current iteration=5621, loss=25771.38186958069\n",
      "Current iteration=5622, loss=25771.378784429355\n",
      "Current iteration=5623, loss=25771.375699738583\n",
      "Current iteration=5624, loss=25771.37261550794\n",
      "Current iteration=5625, loss=25771.36953173696\n",
      "Current iteration=5626, loss=25771.36644842521\n",
      "Current iteration=5627, loss=25771.363365572226\n",
      "Current iteration=5628, loss=25771.360283177553\n",
      "Current iteration=5629, loss=25771.35720124075\n",
      "Current iteration=5630, loss=25771.354119761363\n",
      "Current iteration=5631, loss=25771.351038738947\n",
      "Current iteration=5632, loss=25771.347958173053\n",
      "Current iteration=5633, loss=25771.344878063228\n",
      "Current iteration=5634, loss=25771.341798409023\n",
      "Current iteration=5635, loss=25771.338719209998\n",
      "Current iteration=5636, loss=25771.335640465695\n",
      "Current iteration=5637, loss=25771.332562175678\n",
      "Current iteration=5638, loss=25771.329484339498\n",
      "Current iteration=5639, loss=25771.326406956712\n",
      "Current iteration=5640, loss=25771.323330026873\n",
      "Current iteration=5641, loss=25771.32025354953\n",
      "Current iteration=5642, loss=25771.317177524244\n",
      "Current iteration=5643, loss=25771.31410195057\n",
      "Current iteration=5644, loss=25771.311026828072\n",
      "Current iteration=5645, loss=25771.307952156305\n",
      "Current iteration=5646, loss=25771.304877934817\n",
      "Current iteration=5647, loss=25771.301804163173\n",
      "Current iteration=5648, loss=25771.298730840932\n",
      "Current iteration=5649, loss=25771.295657967657\n",
      "Current iteration=5650, loss=25771.29258554289\n",
      "Current iteration=5651, loss=25771.28951356622\n",
      "Current iteration=5652, loss=25771.286442037184\n",
      "Current iteration=5653, loss=25771.283370955352\n",
      "Current iteration=5654, loss=25771.280300320283\n",
      "Current iteration=5655, loss=25771.27723013154\n",
      "Current iteration=5656, loss=25771.274160388683\n",
      "Current iteration=5657, loss=25771.27109109128\n",
      "Current iteration=5658, loss=25771.268022238895\n",
      "Current iteration=5659, loss=25771.264953831083\n",
      "Current iteration=5660, loss=25771.261885867414\n",
      "Current iteration=5661, loss=25771.258818347458\n",
      "Current iteration=5662, loss=25771.25575127077\n",
      "Current iteration=5663, loss=25771.252684636915\n",
      "Current iteration=5664, loss=25771.249618445476\n",
      "Current iteration=5665, loss=25771.246552696\n",
      "Current iteration=5666, loss=25771.243487388063\n",
      "Current iteration=5667, loss=25771.240422521227\n",
      "Current iteration=5668, loss=25771.237358095066\n",
      "Current iteration=5669, loss=25771.23429410915\n",
      "Current iteration=5670, loss=25771.23123056304\n",
      "Current iteration=5671, loss=25771.22816745631\n",
      "Current iteration=5672, loss=25771.22510478852\n",
      "Current iteration=5673, loss=25771.22204255926\n",
      "Current iteration=5674, loss=25771.218980768084\n",
      "Current iteration=5675, loss=25771.21591941457\n",
      "Current iteration=5676, loss=25771.21285849829\n",
      "Current iteration=5677, loss=25771.209798018812\n",
      "Current iteration=5678, loss=25771.206737975706\n",
      "Current iteration=5679, loss=25771.203678368554\n",
      "Current iteration=5680, loss=25771.200619196927\n",
      "Current iteration=5681, loss=25771.197560460387\n",
      "Current iteration=5682, loss=25771.194502158527\n",
      "Current iteration=5683, loss=25771.191444290896\n",
      "Current iteration=5684, loss=25771.188386857102\n",
      "Current iteration=5685, loss=25771.185329856693\n",
      "Current iteration=5686, loss=25771.18227328926\n",
      "Current iteration=5687, loss=25771.179217154368\n",
      "Current iteration=5688, loss=25771.176161451604\n",
      "Current iteration=5689, loss=25771.173106180544\n",
      "Current iteration=5690, loss=25771.17005134076\n",
      "Current iteration=5691, loss=25771.16699693184\n",
      "Current iteration=5692, loss=25771.16394295335\n",
      "Current iteration=5693, loss=25771.160889404877\n",
      "Current iteration=5694, loss=25771.157836285995\n",
      "Current iteration=5695, loss=25771.154783596292\n",
      "Current iteration=5696, loss=25771.151731335343\n",
      "Current iteration=5697, loss=25771.14867950273\n",
      "Current iteration=5698, loss=25771.14562809803\n",
      "Current iteration=5699, loss=25771.142577120838\n",
      "Current iteration=5700, loss=25771.139526570718\n",
      "Current iteration=5701, loss=25771.136476447264\n",
      "Current iteration=5702, loss=25771.133426750057\n",
      "Current iteration=5703, loss=25771.13037747868\n",
      "Current iteration=5704, loss=25771.127328632716\n",
      "Current iteration=5705, loss=25771.12428021175\n",
      "Current iteration=5706, loss=25771.12123221537\n",
      "Current iteration=5707, loss=25771.11818464315\n",
      "Current iteration=5708, loss=25771.115137494693\n",
      "Current iteration=5709, loss=25771.112090769573\n",
      "Current iteration=5710, loss=25771.109044467375\n",
      "Current iteration=5711, loss=25771.105998587696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=5712, loss=25771.10295313011\n",
      "Current iteration=5713, loss=25771.099908094217\n",
      "Current iteration=5714, loss=25771.0968634796\n",
      "Current iteration=5715, loss=25771.09381928585\n",
      "Current iteration=5716, loss=25771.090775512548\n",
      "Current iteration=5717, loss=25771.087732159296\n",
      "Current iteration=5718, loss=25771.084689225674\n",
      "Current iteration=5719, loss=25771.08164671127\n",
      "Current iteration=5720, loss=25771.07860461569\n",
      "Current iteration=5721, loss=25771.075562938517\n",
      "Current iteration=5722, loss=25771.07252167934\n",
      "Current iteration=5723, loss=25771.06948083775\n",
      "Current iteration=5724, loss=25771.06644041334\n",
      "Current iteration=5725, loss=25771.063400405706\n",
      "Current iteration=5726, loss=25771.060360814445\n",
      "Current iteration=5727, loss=25771.05732163915\n",
      "Current iteration=5728, loss=25771.054282879402\n",
      "Current iteration=5729, loss=25771.05124453481\n",
      "Current iteration=5730, loss=25771.048206604966\n",
      "Current iteration=5731, loss=25771.045169089462\n",
      "Current iteration=5732, loss=25771.042131987895\n",
      "Current iteration=5733, loss=25771.039095299857\n",
      "Current iteration=5734, loss=25771.036059024962\n",
      "Current iteration=5735, loss=25771.03302316279\n",
      "Current iteration=5736, loss=25771.029987712936\n",
      "Current iteration=5737, loss=25771.026952675013\n",
      "Current iteration=5738, loss=25771.02391804861\n",
      "Current iteration=5739, loss=25771.02088383333\n",
      "Current iteration=5740, loss=25771.01785002877\n",
      "Current iteration=5741, loss=25771.014816634528\n",
      "Current iteration=5742, loss=25771.01178365021\n",
      "Current iteration=5743, loss=25771.00875107541\n",
      "Current iteration=5744, loss=25771.005718909728\n",
      "Current iteration=5745, loss=25771.002687152777\n",
      "Current iteration=5746, loss=25770.999655804146\n",
      "Current iteration=5747, loss=25770.99662486344\n",
      "Current iteration=5748, loss=25770.993594330266\n",
      "Current iteration=5749, loss=25770.990564204225\n",
      "Current iteration=5750, loss=25770.987534484928\n",
      "Current iteration=5751, loss=25770.984505171957\n",
      "Current iteration=5752, loss=25770.98147626494\n",
      "Current iteration=5753, loss=25770.978447763468\n",
      "Current iteration=5754, loss=25770.975419667153\n",
      "Current iteration=5755, loss=25770.972391975592\n",
      "Current iteration=5756, loss=25770.9693646884\n",
      "Current iteration=5757, loss=25770.966337805185\n",
      "Current iteration=5758, loss=25770.963311325544\n",
      "Current iteration=5759, loss=25770.960285249093\n",
      "Current iteration=5760, loss=25770.95725957543\n",
      "Current iteration=5761, loss=25770.954234304172\n",
      "Current iteration=5762, loss=25770.95120943492\n",
      "Current iteration=5763, loss=25770.9481849673\n",
      "Current iteration=5764, loss=25770.945160900894\n",
      "Current iteration=5765, loss=25770.942137235335\n",
      "Current iteration=5766, loss=25770.939113970213\n",
      "Current iteration=5767, loss=25770.93609110516\n",
      "Current iteration=5768, loss=25770.93306863977\n",
      "Current iteration=5769, loss=25770.930046573667\n",
      "Current iteration=5770, loss=25770.92702490645\n",
      "Current iteration=5771, loss=25770.924003637745\n",
      "Current iteration=5772, loss=25770.92098276715\n",
      "Current iteration=5773, loss=25770.917962294283\n",
      "Current iteration=5774, loss=25770.91494221877\n",
      "Current iteration=5775, loss=25770.91192254021\n",
      "Current iteration=5776, loss=25770.908903258212\n",
      "Current iteration=5777, loss=25770.905884372405\n",
      "Current iteration=5778, loss=25770.902865882403\n",
      "Current iteration=5779, loss=25770.899847787805\n",
      "Current iteration=5780, loss=25770.896830088248\n",
      "Current iteration=5781, loss=25770.89381278334\n",
      "Current iteration=5782, loss=25770.890795872692\n",
      "Current iteration=5783, loss=25770.88777935593\n",
      "Current iteration=5784, loss=25770.88476323266\n",
      "Current iteration=5785, loss=25770.881747502513\n",
      "Current iteration=5786, loss=25770.878732165096\n",
      "Current iteration=5787, loss=25770.875717220028\n",
      "Current iteration=5788, loss=25770.87270266694\n",
      "Current iteration=5789, loss=25770.869688505438\n",
      "Current iteration=5790, loss=25770.866674735153\n",
      "Current iteration=5791, loss=25770.863661355692\n",
      "Current iteration=5792, loss=25770.860648366688\n",
      "Current iteration=5793, loss=25770.85763576775\n",
      "Current iteration=5794, loss=25770.854623558513\n",
      "Current iteration=5795, loss=25770.851611738588\n",
      "Current iteration=5796, loss=25770.848600307596\n",
      "Current iteration=5797, loss=25770.84558926517\n",
      "Current iteration=5798, loss=25770.84257861093\n",
      "Current iteration=5799, loss=25770.839568344494\n",
      "Current iteration=5800, loss=25770.836558465482\n",
      "Current iteration=5801, loss=25770.833548973533\n",
      "Current iteration=5802, loss=25770.83053986826\n",
      "Current iteration=5803, loss=25770.82753114929\n",
      "Current iteration=5804, loss=25770.824522816252\n",
      "Current iteration=5805, loss=25770.821514868763\n",
      "Current iteration=5806, loss=25770.81850730646\n",
      "Current iteration=5807, loss=25770.815500128956\n",
      "Current iteration=5808, loss=25770.812493335896\n",
      "Current iteration=5809, loss=25770.809486926897\n",
      "Current iteration=5810, loss=25770.806480901574\n",
      "Current iteration=5811, loss=25770.803475259578\n",
      "Current iteration=5812, loss=25770.80047000053\n",
      "Current iteration=5813, loss=25770.79746512405\n",
      "Current iteration=5814, loss=25770.79446062977\n",
      "Current iteration=5815, loss=25770.791456517327\n",
      "Current iteration=5816, loss=25770.78845278635\n",
      "Current iteration=5817, loss=25770.785449436466\n",
      "Current iteration=5818, loss=25770.7824464673\n",
      "Current iteration=5819, loss=25770.779443878488\n",
      "Current iteration=5820, loss=25770.776441669666\n",
      "Current iteration=5821, loss=25770.773439840465\n",
      "Current iteration=5822, loss=25770.770438390515\n",
      "Current iteration=5823, loss=25770.76743731944\n",
      "Current iteration=5824, loss=25770.76443662689\n",
      "Current iteration=5825, loss=25770.761436312485\n",
      "Current iteration=5826, loss=25770.75843637587\n",
      "Current iteration=5827, loss=25770.75543681666\n",
      "Current iteration=5828, loss=25770.75243763451\n",
      "Current iteration=5829, loss=25770.749438829043\n",
      "Current iteration=5830, loss=25770.746440399904\n",
      "Current iteration=5831, loss=25770.743442346724\n",
      "Current iteration=5832, loss=25770.740444669133\n",
      "Current iteration=5833, loss=25770.737447366773\n",
      "Current iteration=5834, loss=25770.734450439286\n",
      "Current iteration=5835, loss=25770.731453886296\n",
      "Current iteration=5836, loss=25770.728457707457\n",
      "Current iteration=5837, loss=25770.72546190239\n",
      "Current iteration=5838, loss=25770.722466470746\n",
      "Current iteration=5839, loss=25770.719471412158\n",
      "Current iteration=5840, loss=25770.716476726266\n",
      "Current iteration=5841, loss=25770.713482412713\n",
      "Current iteration=5842, loss=25770.710488471137\n",
      "Current iteration=5843, loss=25770.707494901173\n",
      "Current iteration=5844, loss=25770.70450170247\n",
      "Current iteration=5845, loss=25770.701508874663\n",
      "Current iteration=5846, loss=25770.69851641739\n",
      "Current iteration=5847, loss=25770.69552433031\n",
      "Current iteration=5848, loss=25770.692532613044\n",
      "Current iteration=5849, loss=25770.689541265245\n",
      "Current iteration=5850, loss=25770.686550286555\n",
      "Current iteration=5851, loss=25770.683559676618\n",
      "Current iteration=5852, loss=25770.680569435073\n",
      "Current iteration=5853, loss=25770.67757956157\n",
      "Current iteration=5854, loss=25770.674590055754\n",
      "Current iteration=5855, loss=25770.67160091726\n",
      "Current iteration=5856, loss=25770.668612145746\n",
      "Current iteration=5857, loss=25770.665623740846\n",
      "Current iteration=5858, loss=25770.66263570221\n",
      "Current iteration=5859, loss=25770.659648029494\n",
      "Current iteration=5860, loss=25770.656660722325\n",
      "Current iteration=5861, loss=25770.653673780365\n",
      "Current iteration=5862, loss=25770.650687203262\n",
      "Current iteration=5863, loss=25770.647700990652\n",
      "Current iteration=5864, loss=25770.644715142196\n",
      "Current iteration=5865, loss=25770.641729657535\n",
      "Current iteration=5866, loss=25770.63874453631\n",
      "Current iteration=5867, loss=25770.63575977819\n",
      "Current iteration=5868, loss=25770.632775382808\n",
      "Current iteration=5869, loss=25770.629791349824\n",
      "Current iteration=5870, loss=25770.62680767888\n",
      "Current iteration=5871, loss=25770.623824369635\n",
      "Current iteration=5872, loss=25770.62084142173\n",
      "Current iteration=5873, loss=25770.617858834823\n",
      "Current iteration=5874, loss=25770.614876608568\n",
      "Current iteration=5875, loss=25770.61189474261\n",
      "Current iteration=5876, loss=25770.60891323661\n",
      "Current iteration=5877, loss=25770.605932090213\n",
      "Current iteration=5878, loss=25770.60295130307\n",
      "Current iteration=5879, loss=25770.599970874846\n",
      "Current iteration=5880, loss=25770.596990805185\n",
      "Current iteration=5881, loss=25770.59401109375\n",
      "Current iteration=5882, loss=25770.59103174019\n",
      "Current iteration=5883, loss=25770.58805274416\n",
      "Current iteration=5884, loss=25770.585074105315\n",
      "Current iteration=5885, loss=25770.58209582331\n",
      "Current iteration=5886, loss=25770.5791178978\n",
      "Current iteration=5887, loss=25770.57614032845\n",
      "Current iteration=5888, loss=25770.573163114903\n",
      "Current iteration=5889, loss=25770.570186256835\n",
      "Current iteration=5890, loss=25770.567209753885\n",
      "Current iteration=5891, loss=25770.564233605717\n",
      "Current iteration=5892, loss=25770.561257811994\n",
      "Current iteration=5893, loss=25770.55828237237\n",
      "Current iteration=5894, loss=25770.555307286508\n",
      "Current iteration=5895, loss=25770.55233255406\n",
      "Current iteration=5896, loss=25770.549358174692\n",
      "Current iteration=5897, loss=25770.54638414806\n",
      "Current iteration=5898, loss=25770.54341047383\n",
      "Current iteration=5899, loss=25770.540437151652\n",
      "Current iteration=5900, loss=25770.53746418121\n",
      "Current iteration=5901, loss=25770.534491562128\n",
      "Current iteration=5902, loss=25770.531519294105\n",
      "Current iteration=5903, loss=25770.52854737678\n",
      "Current iteration=5904, loss=25770.525575809825\n",
      "Current iteration=5905, loss=25770.522604592894\n",
      "Current iteration=5906, loss=25770.519633725668\n",
      "Current iteration=5907, loss=25770.516663207793\n",
      "Current iteration=5908, loss=25770.513693038938\n",
      "Current iteration=5909, loss=25770.510723218773\n",
      "Current iteration=5910, loss=25770.50775374696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=5911, loss=25770.504784623154\n",
      "Current iteration=5912, loss=25770.50181584703\n",
      "Current iteration=5913, loss=25770.498847418257\n",
      "Current iteration=5914, loss=25770.49587933649\n",
      "Current iteration=5915, loss=25770.4929116014\n",
      "Current iteration=5916, loss=25770.489944212663\n",
      "Current iteration=5917, loss=25770.48697716993\n",
      "Current iteration=5918, loss=25770.48401047288\n",
      "Current iteration=5919, loss=25770.481044121174\n",
      "Current iteration=5920, loss=25770.47807811449\n",
      "Current iteration=5921, loss=25770.47511245248\n",
      "Current iteration=5922, loss=25770.472147134828\n",
      "Current iteration=5923, loss=25770.469182161196\n",
      "Current iteration=5924, loss=25770.46621753125\n",
      "Current iteration=5925, loss=25770.463253244674\n",
      "Current iteration=5926, loss=25770.460289301118\n",
      "Current iteration=5927, loss=25770.45732570027\n",
      "Current iteration=5928, loss=25770.45436244179\n",
      "Current iteration=5929, loss=25770.451399525355\n",
      "Current iteration=5930, loss=25770.44843695063\n",
      "Current iteration=5931, loss=25770.4454747173\n",
      "Current iteration=5932, loss=25770.442512825022\n",
      "Current iteration=5933, loss=25770.439551273477\n",
      "Current iteration=5934, loss=25770.436590062338\n",
      "Current iteration=5935, loss=25770.433629191273\n",
      "Current iteration=5936, loss=25770.430668659963\n",
      "Current iteration=5937, loss=25770.427708468076\n",
      "Current iteration=5938, loss=25770.42474861528\n",
      "Current iteration=5939, loss=25770.421789101263\n",
      "Current iteration=5940, loss=25770.4188299257\n",
      "Current iteration=5941, loss=25770.41587108825\n",
      "Current iteration=5942, loss=25770.41291258861\n",
      "Current iteration=5943, loss=25770.40995442644\n",
      "Current iteration=5944, loss=25770.40699660142\n",
      "Current iteration=5945, loss=25770.40403911323\n",
      "Current iteration=5946, loss=25770.401081961543\n",
      "Current iteration=5947, loss=25770.39812514604\n",
      "Current iteration=5948, loss=25770.395168666397\n",
      "Current iteration=5949, loss=25770.392212522293\n",
      "Current iteration=5950, loss=25770.3892567134\n",
      "Current iteration=5951, loss=25770.386301239407\n",
      "Current iteration=5952, loss=25770.383346099985\n",
      "Current iteration=5953, loss=25770.380391294817\n",
      "Current iteration=5954, loss=25770.37743682358\n",
      "Current iteration=5955, loss=25770.374482685955\n",
      "Current iteration=5956, loss=25770.371528881624\n",
      "Current iteration=5957, loss=25770.36857541027\n",
      "Current iteration=5958, loss=25770.365622271565\n",
      "Current iteration=5959, loss=25770.3626694652\n",
      "Current iteration=5960, loss=25770.359716990853\n",
      "Current iteration=5961, loss=25770.356764848202\n",
      "Current iteration=5962, loss=25770.353813036934\n",
      "Current iteration=5963, loss=25770.350861556726\n",
      "Current iteration=5964, loss=25770.347910407272\n",
      "Current iteration=5965, loss=25770.344959588245\n",
      "Current iteration=5966, loss=25770.342009099335\n",
      "Current iteration=5967, loss=25770.339058940215\n",
      "Current iteration=5968, loss=25770.336109110583\n",
      "Current iteration=5969, loss=25770.333159610116\n",
      "Current iteration=5970, loss=25770.330210438507\n",
      "Current iteration=5971, loss=25770.32726159543\n",
      "Current iteration=5972, loss=25770.32431308057\n",
      "Current iteration=5973, loss=25770.321364893625\n",
      "Current iteration=5974, loss=25770.318417034276\n",
      "Current iteration=5975, loss=25770.315469502206\n",
      "Current iteration=5976, loss=25770.312522297107\n",
      "Current iteration=5977, loss=25770.309575418658\n",
      "Current iteration=5978, loss=25770.306628866558\n",
      "Current iteration=5979, loss=25770.303682640486\n",
      "Current iteration=5980, loss=25770.300736740133\n",
      "Current iteration=5981, loss=25770.29779116519\n",
      "Current iteration=5982, loss=25770.294845915338\n",
      "Current iteration=5983, loss=25770.291900990273\n",
      "Current iteration=5984, loss=25770.28895638969\n",
      "Current iteration=5985, loss=25770.286012113265\n",
      "Current iteration=5986, loss=25770.2830681607\n",
      "Current iteration=5987, loss=25770.28012453168\n",
      "Current iteration=5988, loss=25770.27718122589\n",
      "Current iteration=5989, loss=25770.274238243033\n",
      "Current iteration=5990, loss=25770.271295582796\n",
      "Current iteration=5991, loss=25770.268353244865\n",
      "Current iteration=5992, loss=25770.26541122894\n",
      "Current iteration=5993, loss=25770.262469534704\n",
      "Current iteration=5994, loss=25770.259528161863\n",
      "Current iteration=5995, loss=25770.2565871101\n",
      "Current iteration=5996, loss=25770.253646379115\n",
      "Current iteration=5997, loss=25770.250705968592\n",
      "Current iteration=5998, loss=25770.247765878234\n",
      "Current iteration=5999, loss=25770.24482610773\n",
      "Current iteration=6000, loss=25770.24188665678\n",
      "Current iteration=6001, loss=25770.23894752507\n",
      "Current iteration=6002, loss=25770.236008712305\n",
      "Current iteration=6003, loss=25770.23307021818\n",
      "Current iteration=6004, loss=25770.23013204238\n",
      "Current iteration=6005, loss=25770.227194184612\n",
      "Current iteration=6006, loss=25770.22425664457\n",
      "Current iteration=6007, loss=25770.22131942195\n",
      "Current iteration=6008, loss=25770.21838251645\n",
      "Current iteration=6009, loss=25770.215445927766\n",
      "Current iteration=6010, loss=25770.212509655594\n",
      "Current iteration=6011, loss=25770.209573699638\n",
      "Current iteration=6012, loss=25770.206638059593\n",
      "Current iteration=6013, loss=25770.203702735154\n",
      "Current iteration=6014, loss=25770.200767726026\n",
      "Current iteration=6015, loss=25770.197833031907\n",
      "Current iteration=6016, loss=25770.194898652495\n",
      "Current iteration=6017, loss=25770.19196458749\n",
      "Current iteration=6018, loss=25770.1890308366\n",
      "Current iteration=6019, loss=25770.186097399506\n",
      "Current iteration=6020, loss=25770.183164275928\n",
      "Current iteration=6021, loss=25770.180231465565\n",
      "Current iteration=6022, loss=25770.177298968112\n",
      "Current iteration=6023, loss=25770.17436678327\n",
      "Current iteration=6024, loss=25770.171434910746\n",
      "Current iteration=6025, loss=25770.168503350244\n",
      "Current iteration=6026, loss=25770.165572101454\n",
      "Current iteration=6027, loss=25770.1626411641\n",
      "Current iteration=6028, loss=25770.159710537868\n",
      "Current iteration=6029, loss=25770.156780222467\n",
      "Current iteration=6030, loss=25770.153850217604\n",
      "Current iteration=6031, loss=25770.15092052299\n",
      "Current iteration=6032, loss=25770.14799113831\n",
      "Current iteration=6033, loss=25770.145062063282\n",
      "Current iteration=6034, loss=25770.142133297602\n",
      "Current iteration=6035, loss=25770.139204840998\n",
      "Current iteration=6036, loss=25770.136276693152\n",
      "Current iteration=6037, loss=25770.13334885378\n",
      "Current iteration=6038, loss=25770.13042132259\n",
      "Current iteration=6039, loss=25770.12749409928\n",
      "Current iteration=6040, loss=25770.12456718356\n",
      "Current iteration=6041, loss=25770.12164057515\n",
      "Current iteration=6042, loss=25770.118714273747\n",
      "Current iteration=6043, loss=25770.115788279058\n",
      "Current iteration=6044, loss=25770.112862590795\n",
      "Current iteration=6045, loss=25770.109937208657\n",
      "Current iteration=6046, loss=25770.10701213237\n",
      "Current iteration=6047, loss=25770.10408736163\n",
      "Current iteration=6048, loss=25770.10116289615\n",
      "Current iteration=6049, loss=25770.09823873564\n",
      "Current iteration=6050, loss=25770.095314879814\n",
      "Current iteration=6051, loss=25770.092391328377\n",
      "Current iteration=6052, loss=25770.08946808104\n",
      "Current iteration=6053, loss=25770.08654513752\n",
      "Current iteration=6054, loss=25770.08362249752\n",
      "Current iteration=6055, loss=25770.080700160757\n",
      "Current iteration=6056, loss=25770.077778126943\n",
      "Current iteration=6057, loss=25770.074856395793\n",
      "Current iteration=6058, loss=25770.071934967007\n",
      "Current iteration=6059, loss=25770.069013840308\n",
      "Current iteration=6060, loss=25770.066093015408\n",
      "Current iteration=6061, loss=25770.06317249202\n",
      "Current iteration=6062, loss=25770.060252269865\n",
      "Current iteration=6063, loss=25770.057332348635\n",
      "Current iteration=6064, loss=25770.054412728066\n",
      "Current iteration=6065, loss=25770.05149340787\n",
      "Current iteration=6066, loss=25770.048574387754\n",
      "Current iteration=6067, loss=25770.04565566743\n",
      "Current iteration=6068, loss=25770.04273724663\n",
      "Current iteration=6069, loss=25770.039819125053\n",
      "Current iteration=6070, loss=25770.03690130242\n",
      "Current iteration=6071, loss=25770.033983778456\n",
      "Current iteration=6072, loss=25770.031066552867\n",
      "Current iteration=6073, loss=25770.02814962538\n",
      "Current iteration=6074, loss=25770.025232995697\n",
      "Current iteration=6075, loss=25770.022316663548\n",
      "Current iteration=6076, loss=25770.01940062865\n",
      "Current iteration=6077, loss=25770.016484890715\n",
      "Current iteration=6078, loss=25770.013569449467\n",
      "Current iteration=6079, loss=25770.01065430462\n",
      "Current iteration=6080, loss=25770.007739455905\n",
      "Current iteration=6081, loss=25770.004824903022\n",
      "Current iteration=6082, loss=25770.0019106457\n",
      "Current iteration=6083, loss=25769.998996683666\n",
      "Current iteration=6084, loss=25769.996083016635\n",
      "Current iteration=6085, loss=25769.993169644324\n",
      "Current iteration=6086, loss=25769.990256566452\n",
      "Current iteration=6087, loss=25769.98734378275\n",
      "Current iteration=6088, loss=25769.98443129293\n",
      "Current iteration=6089, loss=25769.98151909672\n",
      "Current iteration=6090, loss=25769.978607193836\n",
      "Current iteration=6091, loss=25769.975695584002\n",
      "Current iteration=6092, loss=25769.972784266945\n",
      "Current iteration=6093, loss=25769.969873242386\n",
      "Current iteration=6094, loss=25769.966962510043\n",
      "Current iteration=6095, loss=25769.96405206965\n",
      "Current iteration=6096, loss=25769.96114192092\n",
      "Current iteration=6097, loss=25769.958232063575\n",
      "Current iteration=6098, loss=25769.955322497357\n",
      "Current iteration=6099, loss=25769.952413221967\n",
      "Current iteration=6100, loss=25769.94950423714\n",
      "Current iteration=6101, loss=25769.946595542613\n",
      "Current iteration=6102, loss=25769.943687138097\n",
      "Current iteration=6103, loss=25769.940779023324\n",
      "Current iteration=6104, loss=25769.937871198006\n",
      "Current iteration=6105, loss=25769.93496366189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=6106, loss=25769.93205641469\n",
      "Current iteration=6107, loss=25769.92914945614\n",
      "Current iteration=6108, loss=25769.926242785954\n",
      "Current iteration=6109, loss=25769.923336403877\n",
      "Current iteration=6110, loss=25769.920430309623\n",
      "Current iteration=6111, loss=25769.917524502933\n",
      "Current iteration=6112, loss=25769.914618983516\n",
      "Current iteration=6113, loss=25769.911713751124\n",
      "Current iteration=6114, loss=25769.908808805463\n",
      "Current iteration=6115, loss=25769.905904146275\n",
      "Current iteration=6116, loss=25769.90299977329\n",
      "Current iteration=6117, loss=25769.900095686233\n",
      "Current iteration=6118, loss=25769.897191884833\n",
      "Current iteration=6119, loss=25769.894288368825\n",
      "Current iteration=6120, loss=25769.891385137937\n",
      "Current iteration=6121, loss=25769.8884821919\n",
      "Current iteration=6122, loss=25769.885579530448\n",
      "Current iteration=6123, loss=25769.8826771533\n",
      "Current iteration=6124, loss=25769.879775060203\n",
      "Current iteration=6125, loss=25769.876873250883\n",
      "Current iteration=6126, loss=25769.873971725065\n",
      "Current iteration=6127, loss=25769.871070482495\n",
      "Current iteration=6128, loss=25769.8681695229\n",
      "Current iteration=6129, loss=25769.865268846006\n",
      "Current iteration=6130, loss=25769.86236845155\n",
      "Current iteration=6131, loss=25769.859468339277\n",
      "Current iteration=6132, loss=25769.8565685089\n",
      "Current iteration=6133, loss=25769.85366896018\n",
      "Current iteration=6134, loss=25769.850769692825\n",
      "Current iteration=6135, loss=25769.847870706584\n",
      "Current iteration=6136, loss=25769.844972001185\n",
      "Current iteration=6137, loss=25769.84207357637\n",
      "Current iteration=6138, loss=25769.839175431865\n",
      "Current iteration=6139, loss=25769.836277567414\n",
      "Current iteration=6140, loss=25769.833379982756\n",
      "Current iteration=6141, loss=25769.830482677615\n",
      "Current iteration=6142, loss=25769.827585651743\n",
      "Current iteration=6143, loss=25769.82468890486\n",
      "Current iteration=6144, loss=25769.82179243672\n",
      "Current iteration=6145, loss=25769.818896247045\n",
      "Current iteration=6146, loss=25769.81600033558\n",
      "Current iteration=6147, loss=25769.813104702065\n",
      "Current iteration=6148, loss=25769.810209346237\n",
      "Current iteration=6149, loss=25769.80731426783\n",
      "Current iteration=6150, loss=25769.804419466585\n",
      "Current iteration=6151, loss=25769.801524942246\n",
      "Current iteration=6152, loss=25769.798630694546\n",
      "Current iteration=6153, loss=25769.795736723223\n",
      "Current iteration=6154, loss=25769.79284302803\n",
      "Current iteration=6155, loss=25769.789949608687\n",
      "Current iteration=6156, loss=25769.787056464953\n",
      "Current iteration=6157, loss=25769.78416359656\n",
      "Current iteration=6158, loss=25769.781271003245\n",
      "Current iteration=6159, loss=25769.77837868475\n",
      "Current iteration=6160, loss=25769.775486640825\n",
      "Current iteration=6161, loss=25769.77259487121\n",
      "Current iteration=6162, loss=25769.769703375634\n",
      "Current iteration=6163, loss=25769.766812153855\n",
      "Current iteration=6164, loss=25769.763921205616\n",
      "Current iteration=6165, loss=25769.761030530648\n",
      "Current iteration=6166, loss=25769.758140128695\n",
      "Current iteration=6167, loss=25769.75524999951\n",
      "Current iteration=6168, loss=25769.75236014283\n",
      "Current iteration=6169, loss=25769.749470558396\n",
      "Current iteration=6170, loss=25769.74658124596\n",
      "Current iteration=6171, loss=25769.743692205262\n",
      "Current iteration=6172, loss=25769.740803436045\n",
      "Current iteration=6173, loss=25769.737914938058\n",
      "Current iteration=6174, loss=25769.735026711045\n",
      "Current iteration=6175, loss=25769.73213875475\n",
      "Current iteration=6176, loss=25769.72925106892\n",
      "Current iteration=6177, loss=25769.72636365329\n",
      "Current iteration=6178, loss=25769.723476507632\n",
      "Current iteration=6179, loss=25769.720589631666\n",
      "Current iteration=6180, loss=25769.71770302515\n",
      "Current iteration=6181, loss=25769.71481668784\n",
      "Current iteration=6182, loss=25769.711930619465\n",
      "Current iteration=6183, loss=25769.709044819792\n",
      "Current iteration=6184, loss=25769.706159288547\n",
      "Current iteration=6185, loss=25769.70327402549\n",
      "Current iteration=6186, loss=25769.700389030375\n",
      "Current iteration=6187, loss=25769.697504302945\n",
      "Current iteration=6188, loss=25769.694619842936\n",
      "Current iteration=6189, loss=25769.691735650114\n",
      "Current iteration=6190, loss=25769.688851724233\n",
      "Current iteration=6191, loss=25769.68596806503\n",
      "Current iteration=6192, loss=25769.68308467225\n",
      "Current iteration=6193, loss=25769.680201545656\n",
      "Current iteration=6194, loss=25769.677318684993\n",
      "Current iteration=6195, loss=25769.674436090016\n",
      "Current iteration=6196, loss=25769.67155376047\n",
      "Current iteration=6197, loss=25769.668671696105\n",
      "Current iteration=6198, loss=25769.665789896673\n",
      "Current iteration=6199, loss=25769.662908361934\n",
      "Current iteration=6200, loss=25769.660027091635\n",
      "Current iteration=6201, loss=25769.657146085523\n",
      "Current iteration=6202, loss=25769.65426534336\n",
      "Current iteration=6203, loss=25769.651384864886\n",
      "Current iteration=6204, loss=25769.64850464987\n",
      "Current iteration=6205, loss=25769.645624698052\n",
      "Current iteration=6206, loss=25769.64274500919\n",
      "Current iteration=6207, loss=25769.63986558304\n",
      "Current iteration=6208, loss=25769.63698641936\n",
      "Current iteration=6209, loss=25769.634107517886\n",
      "Current iteration=6210, loss=25769.631228878396\n",
      "Current iteration=6211, loss=25769.62835050063\n",
      "Current iteration=6212, loss=25769.625472384338\n",
      "Current iteration=6213, loss=25769.622594529294\n",
      "Current iteration=6214, loss=25769.619716935245\n",
      "Current iteration=6215, loss=25769.616839601935\n",
      "Current iteration=6216, loss=25769.613962529136\n",
      "Current iteration=6217, loss=25769.6110857166\n",
      "Current iteration=6218, loss=25769.60820916408\n",
      "Current iteration=6219, loss=25769.60533287133\n",
      "Current iteration=6220, loss=25769.602456838118\n",
      "Current iteration=6221, loss=25769.599581064198\n",
      "Current iteration=6222, loss=25769.596705549313\n",
      "Current iteration=6223, loss=25769.593830293245\n",
      "Current iteration=6224, loss=25769.59095529573\n",
      "Current iteration=6225, loss=25769.588080556543\n",
      "Current iteration=6226, loss=25769.585206075426\n",
      "Current iteration=6227, loss=25769.582331852154\n",
      "Current iteration=6228, loss=25769.57945788648\n",
      "Current iteration=6229, loss=25769.57658417816\n",
      "Current iteration=6230, loss=25769.573710726952\n",
      "Current iteration=6231, loss=25769.57083753263\n",
      "Current iteration=6232, loss=25769.567964594935\n",
      "Current iteration=6233, loss=25769.565091913635\n",
      "Current iteration=6234, loss=25769.5622194885\n",
      "Current iteration=6235, loss=25769.55934731927\n",
      "Current iteration=6236, loss=25769.55647540573\n",
      "Current iteration=6237, loss=25769.55360374763\n",
      "Current iteration=6238, loss=25769.55073234472\n",
      "Current iteration=6239, loss=25769.54786119678\n",
      "Current iteration=6240, loss=25769.544990303562\n",
      "Current iteration=6241, loss=25769.542119664835\n",
      "Current iteration=6242, loss=25769.53924928036\n",
      "Current iteration=6243, loss=25769.53637914989\n",
      "Current iteration=6244, loss=25769.5335092732\n",
      "Current iteration=6245, loss=25769.530639650045\n",
      "Current iteration=6246, loss=25769.527770280194\n",
      "Current iteration=6247, loss=25769.524901163408\n",
      "Current iteration=6248, loss=25769.522032299457\n",
      "Current iteration=6249, loss=25769.519163688095\n",
      "Current iteration=6250, loss=25769.516295329093\n",
      "Current iteration=6251, loss=25769.513427222213\n",
      "Current iteration=6252, loss=25769.510559367223\n",
      "Current iteration=6253, loss=25769.50769176389\n",
      "Current iteration=6254, loss=25769.504824411968\n",
      "Current iteration=6255, loss=25769.501957311233\n",
      "Current iteration=6256, loss=25769.49909046145\n",
      "Current iteration=6257, loss=25769.496223862385\n",
      "Current iteration=6258, loss=25769.493357513802\n",
      "Current iteration=6259, loss=25769.490491415476\n",
      "Current iteration=6260, loss=25769.48762556715\n",
      "Current iteration=6261, loss=25769.484759968618\n",
      "Current iteration=6262, loss=25769.48189461964\n",
      "Current iteration=6263, loss=25769.47902951997\n",
      "Current iteration=6264, loss=25769.4761646694\n",
      "Current iteration=6265, loss=25769.473300067675\n",
      "Current iteration=6266, loss=25769.470435714575\n",
      "Current iteration=6267, loss=25769.467571609868\n",
      "Current iteration=6268, loss=25769.464707753326\n",
      "Current iteration=6269, loss=25769.461844144706\n",
      "Current iteration=6270, loss=25769.45898078379\n",
      "Current iteration=6271, loss=25769.45611767034\n",
      "Current iteration=6272, loss=25769.453254804124\n",
      "Current iteration=6273, loss=25769.450392184917\n",
      "Current iteration=6274, loss=25769.447529812496\n",
      "Current iteration=6275, loss=25769.44466768662\n",
      "Current iteration=6276, loss=25769.441805807066\n",
      "Current iteration=6277, loss=25769.4389441736\n",
      "Current iteration=6278, loss=25769.436082785993\n",
      "Current iteration=6279, loss=25769.433221644016\n",
      "Current iteration=6280, loss=25769.430360747454\n",
      "Current iteration=6281, loss=25769.427500096062\n",
      "Current iteration=6282, loss=25769.42463968962\n",
      "Current iteration=6283, loss=25769.421779527904\n",
      "Current iteration=6284, loss=25769.418919610678\n",
      "Current iteration=6285, loss=25769.416059937717\n",
      "Current iteration=6286, loss=25769.413200508796\n",
      "Current iteration=6287, loss=25769.410341323695\n",
      "Current iteration=6288, loss=25769.407482382172\n",
      "Current iteration=6289, loss=25769.404623684015\n",
      "Current iteration=6290, loss=25769.40176522899\n",
      "Current iteration=6291, loss=25769.39890701688\n",
      "Current iteration=6292, loss=25769.396049047442\n",
      "Current iteration=6293, loss=25769.393191320465\n",
      "Current iteration=6294, loss=25769.390333835723\n",
      "Current iteration=6295, loss=25769.387476592994\n",
      "Current iteration=6296, loss=25769.384619592045\n",
      "Current iteration=6297, loss=25769.38176283265\n",
      "Current iteration=6298, loss=25769.378906314596\n",
      "Current iteration=6299, loss=25769.376050037652\n",
      "Current iteration=6300, loss=25769.3731940016\n",
      "Current iteration=6301, loss=25769.370338206205\n",
      "Current iteration=6302, loss=25769.367482651258\n",
      "Current iteration=6303, loss=25769.364627336523\n",
      "Current iteration=6304, loss=25769.361772261782\n",
      "Current iteration=6305, loss=25769.35891742682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=6306, loss=25769.356062831404\n",
      "Current iteration=6307, loss=25769.353208475313\n",
      "Current iteration=6308, loss=25769.350354358336\n",
      "Current iteration=6309, loss=25769.34750048024\n",
      "Current iteration=6310, loss=25769.34464684081\n",
      "Current iteration=6311, loss=25769.341793439824\n",
      "Current iteration=6312, loss=25769.338940277055\n",
      "Current iteration=6313, loss=25769.336087352287\n",
      "Current iteration=6314, loss=25769.3332346653\n",
      "Current iteration=6315, loss=25769.330382215885\n",
      "Current iteration=6316, loss=25769.327530003797\n",
      "Current iteration=6317, loss=25769.324678028832\n",
      "Current iteration=6318, loss=25769.32182629077\n",
      "Current iteration=6319, loss=25769.318974789392\n",
      "Current iteration=6320, loss=25769.31612352447\n",
      "Current iteration=6321, loss=25769.3132724958\n",
      "Current iteration=6322, loss=25769.310421703154\n",
      "Current iteration=6323, loss=25769.307571146306\n",
      "Current iteration=6324, loss=25769.304720825054\n",
      "Current iteration=6325, loss=25769.301870739175\n",
      "Current iteration=6326, loss=25769.299020888444\n",
      "Current iteration=6327, loss=25769.29617127265\n",
      "Current iteration=6328, loss=25769.29332189158\n",
      "Current iteration=6329, loss=25769.290472745004\n",
      "Current iteration=6330, loss=25769.28762383271\n",
      "Current iteration=6331, loss=25769.28477515449\n",
      "Current iteration=6332, loss=25769.281926710122\n",
      "Current iteration=6333, loss=25769.279078499385\n",
      "Current iteration=6334, loss=25769.27623052207\n",
      "Current iteration=6335, loss=25769.27338277796\n",
      "Current iteration=6336, loss=25769.270535266834\n",
      "Current iteration=6337, loss=25769.26768798848\n",
      "Current iteration=6338, loss=25769.26484094269\n",
      "Current iteration=6339, loss=25769.261994129243\n",
      "Current iteration=6340, loss=25769.259147547924\n",
      "Current iteration=6341, loss=25769.25630119852\n",
      "Current iteration=6342, loss=25769.25345508081\n",
      "Current iteration=6343, loss=25769.250609194587\n",
      "Current iteration=6344, loss=25769.247763539643\n",
      "Current iteration=6345, loss=25769.24491811575\n",
      "Current iteration=6346, loss=25769.242072922705\n",
      "Current iteration=6347, loss=25769.239227960294\n",
      "Current iteration=6348, loss=25769.236383228304\n",
      "Current iteration=6349, loss=25769.23353872652\n",
      "Current iteration=6350, loss=25769.23069445473\n",
      "Current iteration=6351, loss=25769.227850412724\n",
      "Current iteration=6352, loss=25769.22500660029\n",
      "Current iteration=6353, loss=25769.22216301721\n",
      "Current iteration=6354, loss=25769.21931966328\n",
      "Current iteration=6355, loss=25769.21647653829\n",
      "Current iteration=6356, loss=25769.21363364202\n",
      "Current iteration=6357, loss=25769.210790974266\n",
      "Current iteration=6358, loss=25769.207948534815\n",
      "Current iteration=6359, loss=25769.205106323458\n",
      "Current iteration=6360, loss=25769.202264339983\n",
      "Current iteration=6361, loss=25769.199422584185\n",
      "Current iteration=6362, loss=25769.19658105585\n",
      "Current iteration=6363, loss=25769.19373975476\n",
      "Current iteration=6364, loss=25769.190898680718\n",
      "Current iteration=6365, loss=25769.18805783352\n",
      "Current iteration=6366, loss=25769.185217212937\n",
      "Current iteration=6367, loss=25769.18237681878\n",
      "Current iteration=6368, loss=25769.179536650823\n",
      "Current iteration=6369, loss=25769.176696708873\n",
      "Current iteration=6370, loss=25769.17385699271\n",
      "Current iteration=6371, loss=25769.171017502144\n",
      "Current iteration=6372, loss=25769.16817823695\n",
      "Current iteration=6373, loss=25769.165339196923\n",
      "Current iteration=6374, loss=25769.16250038186\n",
      "Current iteration=6375, loss=25769.15966179155\n",
      "Current iteration=6376, loss=25769.156823425797\n",
      "Current iteration=6377, loss=25769.15398528438\n",
      "Current iteration=6378, loss=25769.151147367098\n",
      "Current iteration=6379, loss=25769.14830967375\n",
      "Current iteration=6380, loss=25769.145472204127\n",
      "Current iteration=6381, loss=25769.14263495803\n",
      "Current iteration=6382, loss=25769.139797935226\n",
      "Current iteration=6383, loss=25769.136961135544\n",
      "Current iteration=6384, loss=25769.13412455876\n",
      "Current iteration=6385, loss=25769.131288204673\n",
      "Current iteration=6386, loss=25769.128452073088\n",
      "Current iteration=6387, loss=25769.125616163783\n",
      "Current iteration=6388, loss=25769.122780476562\n",
      "Current iteration=6389, loss=25769.119945011225\n",
      "Current iteration=6390, loss=25769.117109767565\n",
      "Current iteration=6391, loss=25769.11427474538\n",
      "Current iteration=6392, loss=25769.111439944463\n",
      "Current iteration=6393, loss=25769.108605364618\n",
      "Current iteration=6394, loss=25769.105771005627\n",
      "Current iteration=6395, loss=25769.102936867297\n",
      "Current iteration=6396, loss=25769.100102949433\n",
      "Current iteration=6397, loss=25769.097269251823\n",
      "Current iteration=6398, loss=25769.094435774266\n",
      "Current iteration=6399, loss=25769.09160251656\n",
      "Current iteration=6400, loss=25769.088769478505\n",
      "Current iteration=6401, loss=25769.085936659903\n",
      "Current iteration=6402, loss=25769.083104060544\n",
      "Current iteration=6403, loss=25769.080271680235\n",
      "Current iteration=6404, loss=25769.077439518765\n",
      "Current iteration=6405, loss=25769.07460757595\n",
      "Current iteration=6406, loss=25769.071775851575\n",
      "Current iteration=6407, loss=25769.068944345447\n",
      "Current iteration=6408, loss=25769.066113057357\n",
      "Current iteration=6409, loss=25769.06328198711\n",
      "Current iteration=6410, loss=25769.060451134515\n",
      "Current iteration=6411, loss=25769.057620499367\n",
      "Current iteration=6412, loss=25769.054790081464\n",
      "Current iteration=6413, loss=25769.051959880602\n",
      "Current iteration=6414, loss=25769.049129896597\n",
      "Current iteration=6415, loss=25769.046300129234\n",
      "Current iteration=6416, loss=25769.043470578326\n",
      "Current iteration=6417, loss=25769.04064124367\n",
      "Current iteration=6418, loss=25769.037812125072\n",
      "Current iteration=6419, loss=25769.034983222333\n",
      "Current iteration=6420, loss=25769.03215453525\n",
      "Current iteration=6421, loss=25769.02932606363\n",
      "Current iteration=6422, loss=25769.026497807277\n",
      "Current iteration=6423, loss=25769.023669765993\n",
      "Current iteration=6424, loss=25769.020841939582\n",
      "Current iteration=6425, loss=25769.01801432784\n",
      "Current iteration=6426, loss=25769.01518693058\n",
      "Current iteration=6427, loss=25769.012359747605\n",
      "Current iteration=6428, loss=25769.009532778713\n",
      "Current iteration=6429, loss=25769.006706023712\n",
      "Current iteration=6430, loss=25769.00387948241\n",
      "Current iteration=6431, loss=25769.001053154607\n",
      "Current iteration=6432, loss=25768.99822704011\n",
      "Current iteration=6433, loss=25768.995401138716\n",
      "Current iteration=6434, loss=25768.99257545024\n",
      "Current iteration=6435, loss=25768.989749974487\n",
      "Current iteration=6436, loss=25768.986924711262\n",
      "Current iteration=6437, loss=25768.984099660367\n",
      "Current iteration=6438, loss=25768.981274821614\n",
      "Current iteration=6439, loss=25768.978450194794\n",
      "Current iteration=6440, loss=25768.975625779734\n",
      "Current iteration=6441, loss=25768.97280157623\n",
      "Current iteration=6442, loss=25768.969977584093\n",
      "Current iteration=6443, loss=25768.967153803125\n",
      "Current iteration=6444, loss=25768.96433023314\n",
      "Current iteration=6445, loss=25768.961506873933\n",
      "Current iteration=6446, loss=25768.958683725323\n",
      "Current iteration=6447, loss=25768.95586078712\n",
      "Current iteration=6448, loss=25768.953038059117\n",
      "Current iteration=6449, loss=25768.95021554114\n",
      "Current iteration=6450, loss=25768.947393232982\n",
      "Current iteration=6451, loss=25768.944571134467\n",
      "Current iteration=6452, loss=25768.941749245387\n",
      "Current iteration=6453, loss=25768.93892756557\n",
      "Current iteration=6454, loss=25768.936106094814\n",
      "Current iteration=6455, loss=25768.93328483292\n",
      "Current iteration=6456, loss=25768.930463779718\n",
      "Current iteration=6457, loss=25768.927642935003\n",
      "Current iteration=6458, loss=25768.92482229859\n",
      "Current iteration=6459, loss=25768.922001870284\n",
      "Current iteration=6460, loss=25768.9191816499\n",
      "Current iteration=6461, loss=25768.916361637253\n",
      "Current iteration=6462, loss=25768.91354183215\n",
      "Current iteration=6463, loss=25768.910722234396\n",
      "Current iteration=6464, loss=25768.907902843806\n",
      "Current iteration=6465, loss=25768.905083660196\n",
      "Current iteration=6466, loss=25768.902264683376\n",
      "Current iteration=6467, loss=25768.899445913154\n",
      "Current iteration=6468, loss=25768.896627349342\n",
      "Current iteration=6469, loss=25768.893808991757\n",
      "Current iteration=6470, loss=25768.89099084021\n",
      "Current iteration=6471, loss=25768.888172894513\n",
      "Current iteration=6472, loss=25768.885355154474\n",
      "Current iteration=6473, loss=25768.882537619906\n",
      "Current iteration=6474, loss=25768.879720290628\n",
      "Current iteration=6475, loss=25768.876903166456\n",
      "Current iteration=6476, loss=25768.874086247193\n",
      "Current iteration=6477, loss=25768.87126953267\n",
      "Current iteration=6478, loss=25768.86845302267\n",
      "Current iteration=6479, loss=25768.865636717033\n",
      "Current iteration=6480, loss=25768.862820615574\n",
      "Current iteration=6481, loss=25768.860004718095\n",
      "Current iteration=6482, loss=25768.85718902442\n",
      "Current iteration=6483, loss=25768.854373534352\n",
      "Current iteration=6484, loss=25768.85155824772\n",
      "Current iteration=6485, loss=25768.848743164326\n",
      "Current iteration=6486, loss=25768.845928284\n",
      "Current iteration=6487, loss=25768.843113606545\n",
      "Current iteration=6488, loss=25768.84029913178\n",
      "Current iteration=6489, loss=25768.837484859523\n",
      "Current iteration=6490, loss=25768.834670789598\n",
      "Current iteration=6491, loss=25768.831856921803\n",
      "Current iteration=6492, loss=25768.82904325597\n",
      "Current iteration=6493, loss=25768.82622979191\n",
      "Current iteration=6494, loss=25768.82341652944\n",
      "Current iteration=6495, loss=25768.820603468375\n",
      "Current iteration=6496, loss=25768.817790608544\n",
      "Current iteration=6497, loss=25768.81497794974\n",
      "Current iteration=6498, loss=25768.812165491807\n",
      "Current iteration=6499, loss=25768.80935323455\n",
      "Current iteration=6500, loss=25768.80654117779\n",
      "Current iteration=6501, loss=25768.80372932134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=6502, loss=25768.800917665027\n",
      "Current iteration=6503, loss=25768.79810620866\n",
      "Current iteration=6504, loss=25768.795294952062\n",
      "Current iteration=6505, loss=25768.792483895064\n",
      "Current iteration=6506, loss=25768.789673037463\n",
      "Current iteration=6507, loss=25768.786862379093\n",
      "Current iteration=6508, loss=25768.78405191977\n",
      "Current iteration=6509, loss=25768.78124165932\n",
      "Current iteration=6510, loss=25768.778431597548\n",
      "Current iteration=6511, loss=25768.77562173428\n",
      "Current iteration=6512, loss=25768.77281206935\n",
      "Current iteration=6513, loss=25768.77000260256\n",
      "Current iteration=6514, loss=25768.76719333374\n",
      "Current iteration=6515, loss=25768.764384262715\n",
      "Current iteration=6516, loss=25768.761575389293\n",
      "Current iteration=6517, loss=25768.7587667133\n",
      "Current iteration=6518, loss=25768.75595823457\n",
      "Current iteration=6519, loss=25768.753149952907\n",
      "Current iteration=6520, loss=25768.750341868144\n",
      "Current iteration=6521, loss=25768.747533980095\n",
      "Current iteration=6522, loss=25768.74472628859\n",
      "Current iteration=6523, loss=25768.741918793443\n",
      "Current iteration=6524, loss=25768.739111494488\n",
      "Current iteration=6525, loss=25768.736304391532\n",
      "Current iteration=6526, loss=25768.733497484416\n",
      "Current iteration=6527, loss=25768.730690772947\n",
      "Current iteration=6528, loss=25768.727884256958\n",
      "Current iteration=6529, loss=25768.725077936266\n",
      "Current iteration=6530, loss=25768.7222718107\n",
      "Current iteration=6531, loss=25768.719465880087\n",
      "Current iteration=6532, loss=25768.716660144233\n",
      "Current iteration=6533, loss=25768.713854602986\n",
      "Current iteration=6534, loss=25768.711049256155\n",
      "Current iteration=6535, loss=25768.708244103567\n",
      "Current iteration=6536, loss=25768.70543914505\n",
      "Current iteration=6537, loss=25768.702634380425\n",
      "Current iteration=6538, loss=25768.69982980952\n",
      "Current iteration=6539, loss=25768.697025432164\n",
      "Current iteration=6540, loss=25768.694221248174\n",
      "Current iteration=6541, loss=25768.691417257374\n",
      "Current iteration=6542, loss=25768.688613459602\n",
      "Current iteration=6543, loss=25768.685809854673\n",
      "Current iteration=6544, loss=25768.68300644242\n",
      "Current iteration=6545, loss=25768.680203222666\n",
      "Current iteration=6546, loss=25768.677400195236\n",
      "Current iteration=6547, loss=25768.67459735996\n",
      "Current iteration=6548, loss=25768.671794716665\n",
      "Current iteration=6549, loss=25768.66899226518\n",
      "Current iteration=6550, loss=25768.666190005322\n",
      "Current iteration=6551, loss=25768.66338793693\n",
      "Current iteration=6552, loss=25768.660586059814\n",
      "Current iteration=6553, loss=25768.657784373827\n",
      "Current iteration=6554, loss=25768.65498287878\n",
      "Current iteration=6555, loss=25768.6521815745\n",
      "Current iteration=6556, loss=25768.649380460833\n",
      "Current iteration=6557, loss=25768.64657953758\n",
      "Current iteration=6558, loss=25768.643778804595\n",
      "Current iteration=6559, loss=25768.640978261697\n",
      "Current iteration=6560, loss=25768.638177908706\n",
      "Current iteration=6561, loss=25768.635377745464\n",
      "Current iteration=6562, loss=25768.632577771794\n",
      "Current iteration=6563, loss=25768.629777987524\n",
      "Current iteration=6564, loss=25768.62697839249\n",
      "Current iteration=6565, loss=25768.624178986523\n",
      "Current iteration=6566, loss=25768.62137976944\n",
      "Current iteration=6567, loss=25768.618580741084\n",
      "Current iteration=6568, loss=25768.61578190128\n",
      "Current iteration=6569, loss=25768.612983249863\n",
      "Current iteration=6570, loss=25768.61018478666\n",
      "Current iteration=6571, loss=25768.6073865115\n",
      "Current iteration=6572, loss=25768.604588424216\n",
      "Current iteration=6573, loss=25768.601790524637\n",
      "Current iteration=6574, loss=25768.598992812604\n",
      "Current iteration=6575, loss=25768.596195287933\n",
      "Current iteration=6576, loss=25768.59339795047\n",
      "Current iteration=6577, loss=25768.59060080004\n",
      "Current iteration=6578, loss=25768.58780383647\n",
      "Current iteration=6579, loss=25768.585007059606\n",
      "Current iteration=6580, loss=25768.582210469274\n",
      "Current iteration=6581, loss=25768.579414065298\n",
      "Current iteration=6582, loss=25768.57661784752\n",
      "Current iteration=6583, loss=25768.573821815775\n",
      "Current iteration=6584, loss=25768.57102596989\n",
      "Current iteration=6585, loss=25768.568230309702\n",
      "Current iteration=6586, loss=25768.565434835036\n",
      "Current iteration=6587, loss=25768.562639545744\n",
      "Current iteration=6588, loss=25768.55984444164\n",
      "Current iteration=6589, loss=25768.55704952257\n",
      "Current iteration=6590, loss=25768.554254788363\n",
      "Current iteration=6591, loss=25768.551460238858\n",
      "Current iteration=6592, loss=25768.54866587388\n",
      "Current iteration=6593, loss=25768.545871693277\n",
      "Current iteration=6594, loss=25768.543077696875\n",
      "Current iteration=6595, loss=25768.540283884508\n",
      "Current iteration=6596, loss=25768.53749025602\n",
      "Current iteration=6597, loss=25768.534696811235\n",
      "Current iteration=6598, loss=25768.531903549996\n",
      "Current iteration=6599, loss=25768.529110472133\n",
      "Current iteration=6600, loss=25768.526317577485\n",
      "Current iteration=6601, loss=25768.523524865894\n",
      "Current iteration=6602, loss=25768.520732337187\n",
      "Current iteration=6603, loss=25768.517939991205\n",
      "Current iteration=6604, loss=25768.51514782778\n",
      "Current iteration=6605, loss=25768.512355846753\n",
      "Current iteration=6606, loss=25768.50956404797\n",
      "Current iteration=6607, loss=25768.506772431247\n",
      "Current iteration=6608, loss=25768.50398099643\n",
      "Current iteration=6609, loss=25768.50118974337\n",
      "Current iteration=6610, loss=25768.498398671887\n",
      "Current iteration=6611, loss=25768.49560778182\n",
      "Current iteration=6612, loss=25768.49281707302\n",
      "Current iteration=6613, loss=25768.490026545307\n",
      "Current iteration=6614, loss=25768.48723619854\n",
      "Current iteration=6615, loss=25768.48444603254\n",
      "Current iteration=6616, loss=25768.481656047148\n",
      "Current iteration=6617, loss=25768.478866242207\n",
      "Current iteration=6618, loss=25768.476076617564\n",
      "Current iteration=6619, loss=25768.473287173045\n",
      "Current iteration=6620, loss=25768.470497908485\n",
      "Current iteration=6621, loss=25768.46770882374\n",
      "Current iteration=6622, loss=25768.46491991864\n",
      "Current iteration=6623, loss=25768.462131193028\n",
      "Current iteration=6624, loss=25768.459342646736\n",
      "Current iteration=6625, loss=25768.45655427961\n",
      "Current iteration=6626, loss=25768.453766091494\n",
      "Current iteration=6627, loss=25768.45097808222\n",
      "Current iteration=6628, loss=25768.448190251638\n",
      "Current iteration=6629, loss=25768.44540259958\n",
      "Current iteration=6630, loss=25768.442615125892\n",
      "Current iteration=6631, loss=25768.439827830414\n",
      "Current iteration=6632, loss=25768.437040712983\n",
      "Current iteration=6633, loss=25768.43425377344\n",
      "Current iteration=6634, loss=25768.431467011636\n",
      "Current iteration=6635, loss=25768.42868042741\n",
      "Current iteration=6636, loss=25768.425894020595\n",
      "Current iteration=6637, loss=25768.423107791037\n",
      "Current iteration=6638, loss=25768.42032173858\n",
      "Current iteration=6639, loss=25768.417535863067\n",
      "Current iteration=6640, loss=25768.41475016434\n",
      "Current iteration=6641, loss=25768.411964642244\n",
      "Current iteration=6642, loss=25768.409179296614\n",
      "Current iteration=6643, loss=25768.406394127298\n",
      "Current iteration=6644, loss=25768.40360913414\n",
      "Current iteration=6645, loss=25768.40082431698\n",
      "Current iteration=6646, loss=25768.398039675667\n",
      "Current iteration=6647, loss=25768.39525521004\n",
      "Current iteration=6648, loss=25768.392470919942\n",
      "Current iteration=6649, loss=25768.38968680522\n",
      "Current iteration=6650, loss=25768.386902865714\n",
      "Current iteration=6651, loss=25768.384119101273\n",
      "Current iteration=6652, loss=25768.381335511735\n",
      "Current iteration=6653, loss=25768.378552096954\n",
      "Current iteration=6654, loss=25768.37576885677\n",
      "Current iteration=6655, loss=25768.372985791026\n",
      "Current iteration=6656, loss=25768.37020289957\n",
      "Current iteration=6657, loss=25768.367420182236\n",
      "Current iteration=6658, loss=25768.364637638886\n",
      "Current iteration=6659, loss=25768.361855269362\n",
      "Current iteration=6660, loss=25768.359073073498\n",
      "Current iteration=6661, loss=25768.356291051154\n",
      "Current iteration=6662, loss=25768.353509202163\n",
      "Current iteration=6663, loss=25768.35072752638\n",
      "Current iteration=6664, loss=25768.347946023656\n",
      "Current iteration=6665, loss=25768.345164693827\n",
      "Current iteration=6666, loss=25768.342383536736\n",
      "Current iteration=6667, loss=25768.339602552245\n",
      "Current iteration=6668, loss=25768.33682174019\n",
      "Current iteration=6669, loss=25768.33404110042\n",
      "Current iteration=6670, loss=25768.331260632785\n",
      "Current iteration=6671, loss=25768.328480337128\n",
      "Current iteration=6672, loss=25768.325700213296\n",
      "Current iteration=6673, loss=25768.322920261147\n",
      "Current iteration=6674, loss=25768.320140480515\n",
      "Current iteration=6675, loss=25768.31736087126\n",
      "Current iteration=6676, loss=25768.31458143322\n",
      "Current iteration=6677, loss=25768.311802166252\n",
      "Current iteration=6678, loss=25768.309023070196\n",
      "Current iteration=6679, loss=25768.306244144907\n",
      "Current iteration=6680, loss=25768.303465390236\n",
      "Current iteration=6681, loss=25768.300686806026\n",
      "Current iteration=6682, loss=25768.29790839212\n",
      "Current iteration=6683, loss=25768.295130148384\n",
      "Current iteration=6684, loss=25768.292352074655\n",
      "Current iteration=6685, loss=25768.289574170783\n",
      "Current iteration=6686, loss=25768.286796436627\n",
      "Current iteration=6687, loss=25768.284018872026\n",
      "Current iteration=6688, loss=25768.281241476838\n",
      "Current iteration=6689, loss=25768.27846425091\n",
      "Current iteration=6690, loss=25768.275687194084\n",
      "Current iteration=6691, loss=25768.27291030623\n",
      "Current iteration=6692, loss=25768.270133587175\n",
      "Current iteration=6693, loss=25768.267357036795\n",
      "Current iteration=6694, loss=25768.26458065492\n",
      "Current iteration=6695, loss=25768.261804441412\n",
      "Current iteration=6696, loss=25768.259028396118\n",
      "Current iteration=6697, loss=25768.256252518884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=6698, loss=25768.253476809583\n",
      "Current iteration=6699, loss=25768.250701268033\n",
      "Current iteration=6700, loss=25768.24792589412\n",
      "Current iteration=6701, loss=25768.245150687675\n",
      "Current iteration=6702, loss=25768.24237564856\n",
      "Current iteration=6703, loss=25768.239600776615\n",
      "Current iteration=6704, loss=25768.23682607171\n",
      "Current iteration=6705, loss=25768.23405153368\n",
      "Current iteration=6706, loss=25768.23127716239\n",
      "Current iteration=6707, loss=25768.22850295769\n",
      "Current iteration=6708, loss=25768.225728919428\n",
      "Current iteration=6709, loss=25768.222955047462\n",
      "Current iteration=6710, loss=25768.220181341647\n",
      "Current iteration=6711, loss=25768.217407801832\n",
      "Current iteration=6712, loss=25768.214634427874\n",
      "Current iteration=6713, loss=25768.211861219625\n",
      "Current iteration=6714, loss=25768.20908817694\n",
      "Current iteration=6715, loss=25768.20631529967\n",
      "Current iteration=6716, loss=25768.20354258767\n",
      "Current iteration=6717, loss=25768.2007700408\n",
      "Current iteration=6718, loss=25768.197997658906\n",
      "Current iteration=6719, loss=25768.195225441857\n",
      "Current iteration=6720, loss=25768.192453389496\n",
      "Current iteration=6721, loss=25768.189681501673\n",
      "Current iteration=6722, loss=25768.186909778255\n",
      "Current iteration=6723, loss=25768.18413821909\n",
      "Current iteration=6724, loss=25768.18136682404\n",
      "Current iteration=6725, loss=25768.17859559296\n",
      "Current iteration=6726, loss=25768.175824525697\n",
      "Current iteration=6727, loss=25768.173053622108\n",
      "Current iteration=6728, loss=25768.17028288206\n",
      "Current iteration=6729, loss=25768.1675123054\n",
      "Current iteration=6730, loss=25768.164741891986\n",
      "Current iteration=6731, loss=25768.16197164168\n",
      "Current iteration=6732, loss=25768.159201554332\n",
      "Current iteration=6733, loss=25768.1564316298\n",
      "Current iteration=6734, loss=25768.15366186794\n",
      "Current iteration=6735, loss=25768.15089226861\n",
      "Current iteration=6736, loss=25768.148122831673\n",
      "Current iteration=6737, loss=25768.145353556974\n",
      "Current iteration=6738, loss=25768.142584444387\n",
      "Current iteration=6739, loss=25768.13981549376\n",
      "Current iteration=6740, loss=25768.137046704942\n",
      "Current iteration=6741, loss=25768.134278077807\n",
      "Current iteration=6742, loss=25768.131509612205\n",
      "Current iteration=6743, loss=25768.128741307995\n",
      "Current iteration=6744, loss=25768.125973165035\n",
      "Current iteration=6745, loss=25768.12320518319\n",
      "Current iteration=6746, loss=25768.120437362308\n",
      "Current iteration=6747, loss=25768.117669702253\n",
      "Current iteration=6748, loss=25768.114902202884\n",
      "Current iteration=6749, loss=25768.112134864063\n",
      "Current iteration=6750, loss=25768.109367685644\n",
      "Current iteration=6751, loss=25768.106600667485\n",
      "Current iteration=6752, loss=25768.103833809455\n",
      "Current iteration=6753, loss=25768.101067111405\n",
      "Current iteration=6754, loss=25768.098300573194\n",
      "Current iteration=6755, loss=25768.09553419469\n",
      "Current iteration=6756, loss=25768.092767975744\n",
      "Current iteration=6757, loss=25768.090001916225\n",
      "Current iteration=6758, loss=25768.087236015992\n",
      "Current iteration=6759, loss=25768.084470274902\n",
      "Current iteration=6760, loss=25768.081704692813\n",
      "Current iteration=6761, loss=25768.07893926959\n",
      "Current iteration=6762, loss=25768.076174005095\n",
      "Current iteration=6763, loss=25768.073408899185\n",
      "Current iteration=6764, loss=25768.070643951723\n",
      "Current iteration=6765, loss=25768.067879162576\n",
      "Current iteration=6766, loss=25768.0651145316\n",
      "Current iteration=6767, loss=25768.062350058648\n",
      "Current iteration=6768, loss=25768.059585743602\n",
      "Current iteration=6769, loss=25768.056821586306\n",
      "Current iteration=6770, loss=25768.054057586633\n",
      "Current iteration=6771, loss=25768.05129374444\n",
      "Current iteration=6772, loss=25768.04853005959\n",
      "Current iteration=6773, loss=25768.045766531948\n",
      "Current iteration=6774, loss=25768.043003161376\n",
      "Current iteration=6775, loss=25768.040239947735\n",
      "Current iteration=6776, loss=25768.03747689088\n",
      "Current iteration=6777, loss=25768.03471399069\n",
      "Current iteration=6778, loss=25768.031951247027\n",
      "Current iteration=6779, loss=25768.029188659733\n",
      "Current iteration=6780, loss=25768.026426228706\n",
      "Current iteration=6781, loss=25768.023663953772\n",
      "Current iteration=6782, loss=25768.02090183482\n",
      "Current iteration=6783, loss=25768.018139871707\n",
      "Current iteration=6784, loss=25768.0153780643\n",
      "Current iteration=6785, loss=25768.01261641246\n",
      "Current iteration=6786, loss=25768.009854916047\n",
      "Current iteration=6787, loss=25768.007093574935\n",
      "Current iteration=6788, loss=25768.00433238898\n",
      "Current iteration=6789, loss=25768.00157135805\n",
      "Current iteration=6790, loss=25767.998810482015\n",
      "Current iteration=6791, loss=25767.99604976073\n",
      "Current iteration=6792, loss=25767.99328919407\n",
      "Current iteration=6793, loss=25767.99052878189\n",
      "Current iteration=6794, loss=25767.987768524068\n",
      "Current iteration=6795, loss=25767.98500842046\n",
      "Current iteration=6796, loss=25767.98224847093\n",
      "Current iteration=6797, loss=25767.979488675355\n",
      "Current iteration=6798, loss=25767.97672903359\n",
      "Current iteration=6799, loss=25767.973969545514\n",
      "Current iteration=6800, loss=25767.971210210973\n",
      "Current iteration=6801, loss=25767.96845102985\n",
      "Current iteration=6802, loss=25767.965692002013\n",
      "Current iteration=6803, loss=25767.96293312731\n",
      "Current iteration=6804, loss=25767.960174405627\n",
      "Current iteration=6805, loss=25767.957415836827\n",
      "Current iteration=6806, loss=25767.95465742077\n",
      "Current iteration=6807, loss=25767.951899157328\n",
      "Current iteration=6808, loss=25767.949141046367\n",
      "Current iteration=6809, loss=25767.946383087754\n",
      "Current iteration=6810, loss=25767.94362528136\n",
      "Current iteration=6811, loss=25767.940867627054\n",
      "Current iteration=6812, loss=25767.938110124695\n",
      "Current iteration=6813, loss=25767.935352774155\n",
      "Current iteration=6814, loss=25767.932595575312\n",
      "Current iteration=6815, loss=25767.929838528016\n",
      "Current iteration=6816, loss=25767.92708163215\n",
      "Current iteration=6817, loss=25767.924324887586\n",
      "Current iteration=6818, loss=25767.921568294172\n",
      "Current iteration=6819, loss=25767.918811851792\n",
      "Current iteration=6820, loss=25767.916055560312\n",
      "Current iteration=6821, loss=25767.913299419604\n",
      "Current iteration=6822, loss=25767.91054342954\n",
      "Current iteration=6823, loss=25767.907787589975\n",
      "Current iteration=6824, loss=25767.90503190079\n",
      "Current iteration=6825, loss=25767.90227636186\n",
      "Current iteration=6826, loss=25767.89952097304\n",
      "Current iteration=6827, loss=25767.896765734207\n",
      "Current iteration=6828, loss=25767.89401064523\n",
      "Current iteration=6829, loss=25767.89125570598\n",
      "Current iteration=6830, loss=25767.88850091633\n",
      "Current iteration=6831, loss=25767.885746276148\n",
      "Current iteration=6832, loss=25767.882991785307\n",
      "Current iteration=6833, loss=25767.880237443667\n",
      "Current iteration=6834, loss=25767.87748325111\n",
      "Current iteration=6835, loss=25767.87472920751\n",
      "Current iteration=6836, loss=25767.871975312722\n",
      "Current iteration=6837, loss=25767.869221566634\n",
      "Current iteration=6838, loss=25767.866467969106\n",
      "Current iteration=6839, loss=25767.86371452002\n",
      "Current iteration=6840, loss=25767.860961219238\n",
      "Current iteration=6841, loss=25767.85820806663\n",
      "Current iteration=6842, loss=25767.85545506208\n",
      "Current iteration=6843, loss=25767.852702205448\n",
      "Current iteration=6844, loss=25767.84994949662\n",
      "Current iteration=6845, loss=25767.847196935454\n",
      "Current iteration=6846, loss=25767.84444452183\n",
      "Current iteration=6847, loss=25767.84169225562\n",
      "Current iteration=6848, loss=25767.838940136695\n",
      "Current iteration=6849, loss=25767.83618816492\n",
      "Current iteration=6850, loss=25767.83343634018\n",
      "Current iteration=6851, loss=25767.83068466235\n",
      "Current iteration=6852, loss=25767.827933131295\n",
      "Current iteration=6853, loss=25767.825181746888\n",
      "Current iteration=6854, loss=25767.822430509004\n",
      "Current iteration=6855, loss=25767.819679417524\n",
      "Current iteration=6856, loss=25767.816928472304\n",
      "Current iteration=6857, loss=25767.81417767324\n",
      "Current iteration=6858, loss=25767.811427020188\n",
      "Current iteration=6859, loss=25767.80867651303\n",
      "Current iteration=6860, loss=25767.80592615164\n",
      "Current iteration=6861, loss=25767.80317593589\n",
      "Current iteration=6862, loss=25767.80042586566\n",
      "Current iteration=6863, loss=25767.79767594082\n",
      "Current iteration=6864, loss=25767.79492616124\n",
      "Current iteration=6865, loss=25767.792176526804\n",
      "Current iteration=6866, loss=25767.78942703739\n",
      "Current iteration=6867, loss=25767.786677692853\n",
      "Current iteration=6868, loss=25767.78392849309\n",
      "Current iteration=6869, loss=25767.781179437963\n",
      "Current iteration=6870, loss=25767.77843052736\n",
      "Current iteration=6871, loss=25767.775681761144\n",
      "Current iteration=6872, loss=25767.7729331392\n",
      "Current iteration=6873, loss=25767.77018466139\n",
      "Current iteration=6874, loss=25767.76743632761\n",
      "Current iteration=6875, loss=25767.76468813772\n",
      "Current iteration=6876, loss=25767.7619400916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=6877, loss=25767.759192189133\n",
      "Current iteration=6878, loss=25767.756444430186\n",
      "Current iteration=6879, loss=25767.75369681464\n",
      "Current iteration=6880, loss=25767.750949342382\n",
      "Current iteration=6881, loss=25767.74820201327\n",
      "Current iteration=6882, loss=25767.745454827196\n",
      "Current iteration=6883, loss=25767.742707784026\n",
      "Current iteration=6884, loss=25767.739960883646\n",
      "Current iteration=6885, loss=25767.73721412593\n",
      "Current iteration=6886, loss=25767.73446751075\n",
      "Current iteration=6887, loss=25767.731721037995\n",
      "Current iteration=6888, loss=25767.72897470753\n",
      "Current iteration=6889, loss=25767.72622851924\n",
      "Current iteration=6890, loss=25767.723482473015\n",
      "Current iteration=6891, loss=25767.720736568706\n",
      "Current iteration=6892, loss=25767.717990806217\n",
      "Current iteration=6893, loss=25767.71524518541\n",
      "Current iteration=6894, loss=25767.712499706166\n",
      "Current iteration=6895, loss=25767.70975436837\n",
      "Current iteration=6896, loss=25767.7070091719\n",
      "Current iteration=6897, loss=25767.70426411663\n",
      "Current iteration=6898, loss=25767.701519202434\n",
      "Current iteration=6899, loss=25767.69877442921\n",
      "Current iteration=6900, loss=25767.696029796818\n",
      "Current iteration=6901, loss=25767.693285305148\n",
      "Current iteration=6902, loss=25767.69054095407\n",
      "Current iteration=6903, loss=25767.68779674348\n",
      "Current iteration=6904, loss=25767.685052673245\n",
      "Current iteration=6905, loss=25767.682308743242\n",
      "Current iteration=6906, loss=25767.679564953356\n",
      "Current iteration=6907, loss=25767.676821303474\n",
      "Current iteration=6908, loss=25767.674077793465\n",
      "Current iteration=6909, loss=25767.671334423212\n",
      "Current iteration=6910, loss=25767.668591192603\n",
      "Current iteration=6911, loss=25767.66584810151\n",
      "Current iteration=6912, loss=25767.66310514982\n",
      "Current iteration=6913, loss=25767.660362337407\n",
      "Current iteration=6914, loss=25767.657619664158\n",
      "Current iteration=6915, loss=25767.654877129953\n",
      "Current iteration=6916, loss=25767.65213473467\n",
      "Current iteration=6917, loss=25767.64939247819\n",
      "Current iteration=6918, loss=25767.646650360402\n",
      "Current iteration=6919, loss=25767.643908381175\n",
      "Current iteration=6920, loss=25767.6411665404\n",
      "Current iteration=6921, loss=25767.638424837958\n",
      "Current iteration=6922, loss=25767.63568327373\n",
      "Current iteration=6923, loss=25767.632941847605\n",
      "Current iteration=6924, loss=25767.630200559448\n",
      "Current iteration=6925, loss=25767.627459409152\n",
      "Current iteration=6926, loss=25767.6247183966\n",
      "Current iteration=6927, loss=25767.62197752167\n",
      "Current iteration=6928, loss=25767.61923678425\n",
      "Current iteration=6929, loss=25767.616496184222\n",
      "Current iteration=6930, loss=25767.613755721468\n",
      "Current iteration=6931, loss=25767.61101539587\n",
      "Current iteration=6932, loss=25767.608275207313\n",
      "Current iteration=6933, loss=25767.60553515567\n",
      "Current iteration=6934, loss=25767.60279524084\n",
      "Current iteration=6935, loss=25767.600055462695\n",
      "Current iteration=6936, loss=25767.597315821127\n",
      "Current iteration=6937, loss=25767.59457631602\n",
      "Current iteration=6938, loss=25767.591836947242\n",
      "Current iteration=6939, loss=25767.589097714696\n",
      "Current iteration=6940, loss=25767.58635861826\n",
      "Current iteration=6941, loss=25767.58361965781\n",
      "Current iteration=6942, loss=25767.58088083324\n",
      "Current iteration=6943, loss=25767.57814214443\n",
      "Current iteration=6944, loss=25767.575403591272\n",
      "Current iteration=6945, loss=25767.57266517364\n",
      "Current iteration=6946, loss=25767.56992689142\n",
      "Current iteration=6947, loss=25767.567188744506\n",
      "Current iteration=6948, loss=25767.564450732778\n",
      "Current iteration=6949, loss=25767.561712856117\n",
      "Current iteration=6950, loss=25767.55897511441\n",
      "Current iteration=6951, loss=25767.55623750755\n",
      "Current iteration=6952, loss=25767.553500035414\n",
      "Current iteration=6953, loss=25767.550762697887\n",
      "Current iteration=6954, loss=25767.548025494867\n",
      "Current iteration=6955, loss=25767.54528842622\n",
      "Current iteration=6956, loss=25767.542551491846\n",
      "Current iteration=6957, loss=25767.539814691627\n",
      "Current iteration=6958, loss=25767.53707802545\n",
      "Current iteration=6959, loss=25767.5343414932\n",
      "Current iteration=6960, loss=25767.531605094766\n",
      "Current iteration=6961, loss=25767.528868830035\n",
      "Current iteration=6962, loss=25767.52613269889\n",
      "Current iteration=6963, loss=25767.523396701217\n",
      "Current iteration=6964, loss=25767.52066083691\n",
      "Current iteration=6965, loss=25767.517925105843\n",
      "Current iteration=6966, loss=25767.515189507918\n",
      "Current iteration=6967, loss=25767.512454043015\n",
      "Current iteration=6968, loss=25767.50971871102\n",
      "Current iteration=6969, loss=25767.506983511823\n",
      "Current iteration=6970, loss=25767.50424844531\n",
      "Current iteration=6971, loss=25767.50151351137\n",
      "Current iteration=6972, loss=25767.49877870989\n",
      "Current iteration=6973, loss=25767.49604404076\n",
      "Current iteration=6974, loss=25767.493309503865\n",
      "Current iteration=6975, loss=25767.490575099095\n",
      "Current iteration=6976, loss=25767.487840826332\n",
      "Current iteration=6977, loss=25767.48510668548\n",
      "Current iteration=6978, loss=25767.48237267641\n",
      "Current iteration=6979, loss=25767.479638799017\n",
      "Current iteration=6980, loss=25767.476905053198\n",
      "Current iteration=6981, loss=25767.47417143883\n",
      "Current iteration=6982, loss=25767.4714379558\n",
      "Current iteration=6983, loss=25767.468704604005\n",
      "Current iteration=6984, loss=25767.465971383343\n",
      "Current iteration=6985, loss=25767.463238293687\n",
      "Current iteration=6986, loss=25767.46050533493\n",
      "Current iteration=6987, loss=25767.45777250696\n",
      "Current iteration=6988, loss=25767.455039809676\n",
      "Current iteration=6989, loss=25767.452307242966\n",
      "Current iteration=6990, loss=25767.449574806702\n",
      "Current iteration=6991, loss=25767.446842500798\n",
      "Current iteration=6992, loss=25767.44411032513\n",
      "Current iteration=6993, loss=25767.441378279593\n",
      "Current iteration=6994, loss=25767.438646364077\n",
      "Current iteration=6995, loss=25767.435914578473\n",
      "Current iteration=6996, loss=25767.433182922672\n",
      "Current iteration=6997, loss=25767.43045139655\n",
      "Current iteration=6998, loss=25767.42772000002\n",
      "Current iteration=6999, loss=25767.424988732964\n",
      "Current iteration=7000, loss=25767.422257595266\n",
      "Current iteration=7001, loss=25767.419526586826\n",
      "Current iteration=7002, loss=25767.416795707533\n",
      "Current iteration=7003, loss=25767.41406495728\n",
      "Current iteration=7004, loss=25767.41133433595\n",
      "Current iteration=7005, loss=25767.408603843443\n",
      "Current iteration=7006, loss=25767.40587347965\n",
      "Current iteration=7007, loss=25767.40314324446\n",
      "Current iteration=7008, loss=25767.400413137766\n",
      "Current iteration=7009, loss=25767.397683159452\n",
      "Current iteration=7010, loss=25767.394953309427\n",
      "Current iteration=7011, loss=25767.39222358757\n",
      "Current iteration=7012, loss=25767.389493993775\n",
      "Current iteration=7013, loss=25767.386764527935\n",
      "Current iteration=7014, loss=25767.38403518995\n",
      "Current iteration=7015, loss=25767.381305979696\n",
      "Current iteration=7016, loss=25767.378576897085\n",
      "Current iteration=7017, loss=25767.375847942\n",
      "Current iteration=7018, loss=25767.37311911433\n",
      "Current iteration=7019, loss=25767.370390413973\n",
      "Current iteration=7020, loss=25767.367661840828\n",
      "Current iteration=7021, loss=25767.364933394776\n",
      "Current iteration=7022, loss=25767.36220507571\n",
      "Current iteration=7023, loss=25767.359476883546\n",
      "Current iteration=7024, loss=25767.35674881815\n",
      "Current iteration=7025, loss=25767.354020879433\n",
      "Current iteration=7026, loss=25767.351293067277\n",
      "Current iteration=7027, loss=25767.348565381588\n",
      "Current iteration=7028, loss=25767.34583782225\n",
      "Current iteration=7029, loss=25767.343110389153\n",
      "Current iteration=7030, loss=25767.340383082214\n",
      "Current iteration=7031, loss=25767.337655901305\n",
      "Current iteration=7032, loss=25767.334928846325\n",
      "Current iteration=7033, loss=25767.332201917172\n",
      "Current iteration=7034, loss=25767.329475113744\n",
      "Current iteration=7035, loss=25767.32674843593\n",
      "Current iteration=7036, loss=25767.32402188363\n",
      "Current iteration=7037, loss=25767.321295456728\n",
      "Current iteration=7038, loss=25767.31856915513\n",
      "Current iteration=7039, loss=25767.31584297873\n",
      "Current iteration=7040, loss=25767.313116927417\n",
      "Current iteration=7041, loss=25767.31039100109\n",
      "Current iteration=7042, loss=25767.307665199645\n",
      "Current iteration=7043, loss=25767.30493952298\n",
      "Current iteration=7044, loss=25767.302213970986\n",
      "Current iteration=7045, loss=25767.29948854357\n",
      "Current iteration=7046, loss=25767.296763240607\n",
      "Current iteration=7047, loss=25767.294038062006\n",
      "Current iteration=7048, loss=25767.29131300767\n",
      "Current iteration=7049, loss=25767.28858807748\n",
      "Current iteration=7050, loss=25767.285863271347\n",
      "Current iteration=7051, loss=25767.28313858915\n",
      "Current iteration=7052, loss=25767.2804140308\n",
      "Current iteration=7053, loss=25767.27768959619\n",
      "Current iteration=7054, loss=25767.274965285218\n",
      "Current iteration=7055, loss=25767.272241097773\n",
      "Current iteration=7056, loss=25767.269517033757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=7057, loss=25767.266793093077\n",
      "Current iteration=7058, loss=25767.264069275618\n",
      "Current iteration=7059, loss=25767.261345581283\n",
      "Current iteration=7060, loss=25767.258622009955\n",
      "Current iteration=7061, loss=25767.25589856155\n",
      "Current iteration=7062, loss=25767.253175235957\n",
      "Current iteration=7063, loss=25767.25045203308\n",
      "Current iteration=7064, loss=25767.24772895281\n",
      "Current iteration=7065, loss=25767.24500599505\n",
      "Current iteration=7066, loss=25767.242283159692\n",
      "Current iteration=7067, loss=25767.239560446636\n",
      "Current iteration=7068, loss=25767.23683785578\n",
      "Current iteration=7069, loss=25767.23411538703\n",
      "Current iteration=7070, loss=25767.231393040274\n",
      "Current iteration=7071, loss=25767.228670815413\n",
      "Current iteration=7072, loss=25767.225948712356\n",
      "Current iteration=7073, loss=25767.223226730985\n",
      "Current iteration=7074, loss=25767.22050487121\n",
      "Current iteration=7075, loss=25767.217783132925\n",
      "Current iteration=7076, loss=25767.215061516035\n",
      "Current iteration=7077, loss=25767.212340020433\n",
      "Current iteration=7078, loss=25767.20961864602\n",
      "Current iteration=7079, loss=25767.20689739269\n",
      "Current iteration=7080, loss=25767.20417626036\n",
      "Current iteration=7081, loss=25767.201455248913\n",
      "Current iteration=7082, loss=25767.19873435825\n",
      "Current iteration=7083, loss=25767.19601358827\n",
      "Current iteration=7084, loss=25767.193292938882\n",
      "Current iteration=7085, loss=25767.190572409985\n",
      "Current iteration=7086, loss=25767.187852001476\n",
      "Current iteration=7087, loss=25767.185131713242\n",
      "Current iteration=7088, loss=25767.182411545204\n",
      "Current iteration=7089, loss=25767.179691497255\n",
      "Current iteration=7090, loss=25767.17697156929\n",
      "Current iteration=7091, loss=25767.17425176122\n",
      "Current iteration=7092, loss=25767.17153207293\n",
      "Current iteration=7093, loss=25767.16881250434\n",
      "Current iteration=7094, loss=25767.166093055333\n",
      "Current iteration=7095, loss=25767.163373725823\n",
      "Current iteration=7096, loss=25767.160654515705\n",
      "Current iteration=7097, loss=25767.15793542488\n",
      "Current iteration=7098, loss=25767.155216453248\n",
      "Current iteration=7099, loss=25767.152497600717\n",
      "Current iteration=7100, loss=25767.149778867184\n",
      "Current iteration=7101, loss=25767.14706025255\n",
      "Current iteration=7102, loss=25767.14434175672\n",
      "Current iteration=7103, loss=25767.141623379594\n",
      "Current iteration=7104, loss=25767.138905121068\n",
      "Current iteration=7105, loss=25767.136186981053\n",
      "Current iteration=7106, loss=25767.133468959444\n",
      "Current iteration=7107, loss=25767.130751056153\n",
      "Current iteration=7108, loss=25767.128033271063\n",
      "Current iteration=7109, loss=25767.1253156041\n",
      "Current iteration=7110, loss=25767.122598055154\n",
      "Current iteration=7111, loss=25767.119880624126\n",
      "Current iteration=7112, loss=25767.117163310922\n",
      "Current iteration=7113, loss=25767.114446115447\n",
      "Current iteration=7114, loss=25767.1117290376\n",
      "Current iteration=7115, loss=25767.109012077286\n",
      "Current iteration=7116, loss=25767.106295234404\n",
      "Current iteration=7117, loss=25767.10357850886\n",
      "Current iteration=7118, loss=25767.100861900566\n",
      "Current iteration=7119, loss=25767.098145409407\n",
      "Current iteration=7120, loss=25767.09542903531\n",
      "Current iteration=7121, loss=25767.09271277815\n",
      "Current iteration=7122, loss=25767.089996637857\n",
      "Current iteration=7123, loss=25767.087280614323\n",
      "Current iteration=7124, loss=25767.084564707442\n",
      "Current iteration=7125, loss=25767.081848917132\n",
      "Current iteration=7126, loss=25767.0791332433\n",
      "Current iteration=7127, loss=25767.076417685836\n",
      "Current iteration=7128, loss=25767.073702244656\n",
      "Current iteration=7129, loss=25767.07098691966\n",
      "Current iteration=7130, loss=25767.068271710752\n",
      "Current iteration=7131, loss=25767.065556617832\n",
      "Current iteration=7132, loss=25767.062841640818\n",
      "Current iteration=7133, loss=25767.060126779597\n",
      "Current iteration=7134, loss=25767.05741203409\n",
      "Current iteration=7135, loss=25767.054697404194\n",
      "Current iteration=7136, loss=25767.051982889814\n",
      "Current iteration=7137, loss=25767.049268490853\n",
      "Current iteration=7138, loss=25767.046554207227\n",
      "Current iteration=7139, loss=25767.043840038823\n",
      "Current iteration=7140, loss=25767.04112598556\n",
      "Current iteration=7141, loss=25767.038412047343\n",
      "Current iteration=7142, loss=25767.035698224077\n",
      "Current iteration=7143, loss=25767.032984515663\n",
      "Current iteration=7144, loss=25767.030270922012\n",
      "Current iteration=7145, loss=25767.027557443023\n",
      "Current iteration=7146, loss=25767.024844078605\n",
      "Current iteration=7147, loss=25767.02213082867\n",
      "Current iteration=7148, loss=25767.019417693118\n",
      "Current iteration=7149, loss=25767.016704671856\n",
      "Current iteration=7150, loss=25767.013991764794\n",
      "Current iteration=7151, loss=25767.011278971833\n",
      "Current iteration=7152, loss=25767.00856629288\n",
      "Current iteration=7153, loss=25767.00585372785\n",
      "Current iteration=7154, loss=25767.003141276633\n",
      "Current iteration=7155, loss=25767.00042893915\n",
      "Current iteration=7156, loss=25766.997716715312\n",
      "Current iteration=7157, loss=25766.995004605003\n",
      "Current iteration=7158, loss=25766.992292608156\n",
      "Current iteration=7159, loss=25766.98958072467\n",
      "Current iteration=7160, loss=25766.98686895444\n",
      "Current iteration=7161, loss=25766.984157297386\n",
      "Current iteration=7162, loss=25766.981445753412\n",
      "Current iteration=7163, loss=25766.97873432243\n",
      "Current iteration=7164, loss=25766.976023004343\n",
      "Current iteration=7165, loss=25766.973311799054\n",
      "Current iteration=7166, loss=25766.970600706485\n",
      "Current iteration=7167, loss=25766.96788972653\n",
      "Current iteration=7168, loss=25766.9651788591\n",
      "Current iteration=7169, loss=25766.96246810411\n",
      "Current iteration=7170, loss=25766.95975746146\n",
      "Current iteration=7171, loss=25766.957046931064\n",
      "Current iteration=7172, loss=25766.954336512823\n",
      "Current iteration=7173, loss=25766.951626206657\n",
      "Current iteration=7174, loss=25766.948916012465\n",
      "Current iteration=7175, loss=25766.946205930162\n",
      "Current iteration=7176, loss=25766.94349595965\n",
      "Current iteration=7177, loss=25766.940786100844\n",
      "Current iteration=7178, loss=25766.93807635365\n",
      "Current iteration=7179, loss=25766.935366717982\n",
      "Current iteration=7180, loss=25766.93265719374\n",
      "Current iteration=7181, loss=25766.929947780838\n",
      "Current iteration=7182, loss=25766.927238479188\n",
      "Current iteration=7183, loss=25766.92452928869\n",
      "Current iteration=7184, loss=25766.92182020927\n",
      "Current iteration=7185, loss=25766.919111240823\n",
      "Current iteration=7186, loss=25766.91640238326\n",
      "Current iteration=7187, loss=25766.913693636503\n",
      "Current iteration=7188, loss=25766.910985000442\n",
      "Current iteration=7189, loss=25766.908276475006\n",
      "Current iteration=7190, loss=25766.905568060098\n",
      "Current iteration=7191, loss=25766.902859755624\n",
      "Current iteration=7192, loss=25766.900151561502\n",
      "Current iteration=7193, loss=25766.897443477632\n",
      "Current iteration=7194, loss=25766.89473550393\n",
      "Current iteration=7195, loss=25766.892027640315\n",
      "Current iteration=7196, loss=25766.889319886683\n",
      "Current iteration=7197, loss=25766.886612242954\n",
      "Current iteration=7198, loss=25766.883904709033\n",
      "Current iteration=7199, loss=25766.881197284838\n",
      "Current iteration=7200, loss=25766.878489970273\n",
      "Current iteration=7201, loss=25766.875782765248\n",
      "Current iteration=7202, loss=25766.873075669682\n",
      "Current iteration=7203, loss=25766.87036868349\n",
      "Current iteration=7204, loss=25766.867661806562\n",
      "Current iteration=7205, loss=25766.86495503883\n",
      "Current iteration=7206, loss=25766.862248380196\n",
      "Current iteration=7207, loss=25766.859541830578\n",
      "Current iteration=7208, loss=25766.856835389877\n",
      "Current iteration=7209, loss=25766.854129058014\n",
      "Current iteration=7210, loss=25766.8514228349\n",
      "Current iteration=7211, loss=25766.84871672045\n",
      "Current iteration=7212, loss=25766.846010714562\n",
      "Current iteration=7213, loss=25766.843304817165\n",
      "Current iteration=7214, loss=25766.840599028164\n",
      "Current iteration=7215, loss=25766.837893347467\n",
      "Current iteration=7216, loss=25766.83518777499\n",
      "Current iteration=7217, loss=25766.832482310645\n",
      "Current iteration=7218, loss=25766.82977695435\n",
      "Current iteration=7219, loss=25766.82707170601\n",
      "Current iteration=7220, loss=25766.82436656554\n",
      "Current iteration=7221, loss=25766.821661532853\n",
      "Current iteration=7222, loss=25766.81895660787\n",
      "Current iteration=7223, loss=25766.81625179049\n",
      "Current iteration=7224, loss=25766.813547080634\n",
      "Current iteration=7225, loss=25766.81084247822\n",
      "Current iteration=7226, loss=25766.808137983142\n",
      "Current iteration=7227, loss=25766.805433595327\n",
      "Current iteration=7228, loss=25766.802729314702\n",
      "Current iteration=7229, loss=25766.800025141158\n",
      "Current iteration=7230, loss=25766.79732107462\n",
      "Current iteration=7231, loss=25766.79461711499\n",
      "Current iteration=7232, loss=25766.791913262205\n",
      "Current iteration=7233, loss=25766.789209516155\n",
      "Current iteration=7234, loss=25766.78650587676\n",
      "Current iteration=7235, loss=25766.78380234395\n",
      "Current iteration=7236, loss=25766.781098917618\n",
      "Current iteration=7237, loss=25766.77839559769\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-83b5b6c0e25f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Execute for data originally with no NaNs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdtmp_tr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtmp_te\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcross_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_no_nans\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_no_nans\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunction_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'logistic_regression'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mmeanacc_dtest_lr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtmp_te\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mmeanacc_dtrain_lr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtmp_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\College\\EPFL\\3rd_semester\\ML\\Project_1\\cross_validation.py\u001b[0m in \u001b[0;36mcross_validation\u001b[1;34m(y, x, k_fold, function_name, lambda_, max_iters, gamma, seed)\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;31m# calculate the error for train and test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\College\\EPFL\\3rd_semester\\ML\\Project_1\\logistic_regression.py\u001b[0m in \u001b[0;36mlogistic_regression\u001b[1;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;31m# log info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m#if iter % 100 == 0:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Current iteration={i}, loss={l}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;31m# converge criterion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, string)\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m             \u001b[1;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m                 \u001b[1;31m# mp.Pool cannot be trusted to flush promptly (or ever),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;31m# wake event thread (message content is ignored)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\zmq\\sugar\\socket.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[0;32m    398\u001b[0m                                  copy_threshold=self.copy_threshold)\n\u001b[0;32m    399\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSocket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msend_multipart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg_parts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "meanacc_dtest_lr = []\n",
    "meanacc_dtrain_lr = []\n",
    "\n",
    "gamma = 0.0000000005\n",
    "\n",
    "# Execute for data originally with no NaNs\n",
    "dtmp_tr,dtmp_te=cross_validation(Y_no_nans,X_no_nans,k_fold=3,seed=1, function_name='logistic_regression', max_iters=10000,gamma=gamma)\n",
    "meanacc_dtest_lr.append(dtmp_te)\n",
    "meanacc_dtrain_lr.append(dtmp_tr)\n",
    "\n",
    "# Execute for data originally with NaNs\n",
    "dtmp_tr,dtmp_te=cross_validation(Y_cleaned,X_cleaned,k_fold=3,seed=1, function_name='logistic_regression', max_iters=10000,gamma=gamma)\n",
    "meanacc_dtest_lr.append(dtmp_te)\n",
    "meanacc_dtrain_lr.append(dtmp_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No NaNs accuracy:\n",
      "Test: 0.6844608879492601 Data: 0.6841599130843316\n",
      "\n",
      "Cleaned NaNs accuracy:\n",
      "Test: 0.7420058938664204 Data: 0.7420361329198829\n"
     ]
    }
   ],
   "source": [
    "print(\"No NaNs accuracy:\")\n",
    "print(\"Test:\",meanacc_dtest_lr[0],\"Data:\", meanacc_dtrain_lr[0])\n",
    "print(\"\\nCleaned NaNs accuracy:\", )\n",
    "print(\"Test:\",meanacc_dtest_lr[1],\"Data:\", meanacc_dtrain_lr[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reg Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-74b3d1ebd299>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0000000005\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Execute for data originally with no NaNs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mdtmp_tr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtmp_te\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcross_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_no_nans\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_no_nans\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunction_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'reg_logistic_regression'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "meanacc_dtest_rlr = []\n",
    "meanacc_dtrain_rlr = []\n",
    "\n",
    "gamma = 0.0000000005\n",
    "\n",
    "for lambda_ in np.logspace(-5,0,15):\n",
    "    # Execute for data originally with no NaNs\n",
    "    dtmp_tr,dtmp_te=cross_validation(Y_no_nans,X_no_nans,k_fold=3,seed=1, function_name='reg_logistic_regression', max_iters=10000,gamma=gamma,lambda_=lambda_)\n",
    "    meanacc_dtest_rlr.append(dtmp_te)\n",
    "    meanacc_dtrain_rlr.append(dtmp_tr)\n",
    "\n",
    "    # Execute for data originally with NaNs\n",
    "    dtmp_tr,dtmp_te=cross_validation(Y_cleaned,X_cleaned,k_fold=3,seed=1, function_name='reg_logistic_regression', max_iters=10000,gamma=gamma,lambda_=lambda_)\n",
    "    meanacc_dtest_rlr.append(dtmp_te)\n",
    "    meanacc_dtrain_rlr.append(dtmp_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too slow! Find good lambda before running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "Try with polynomial expansion and standardization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
