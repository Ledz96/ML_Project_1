{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from helpers import*\n",
    "from build_polynomial import*\n",
    "from utils_predictions_manipulation import*\n",
    "from utils_nans_manipulation import*\n",
    "from cross_validation import*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model testing on training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = \"i8,S5,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,i8,f8,f8,f8,f8,f8,f8,f8\"\n",
    "with open('Data/train.csv', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    headerstrain = next(reader)\n",
    "datatrain = np.genfromtxt('Data/train.csv', delimiter=\",\",names=True, dtype=dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format training data, add fake feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: cleanup to avoid using lists\n",
    "\n",
    "\n",
    "#training (s=1 b=0)\n",
    "y=(datatrain['Prediction']==b's').astype(int)\n",
    "#transforming features X into a list of list\n",
    "trainxlist=[]\n",
    "for i in datatrain:\n",
    "    trainsublist=[]\n",
    "    for x in i:\n",
    "        trainsublist.append(x)\n",
    "    trainsublist=trainsublist[2:]\n",
    "    trainxlist.append(trainsublist)\n",
    "\n",
    "#adding column of 1 \n",
    "for x in trainxlist:\n",
    "    x.insert(0,1)\n",
    "#trainxlist is the extended X matrix\n",
    "\n",
    "#transform into np.array\n",
    "Y_total=np.array(y)\n",
    "X_total=np.array(trainxlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating data points with and without NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing undefined data with NaNs\n",
    "\n",
    "X_nans = replace_bad_data_with_nans(X_total, -999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset based on presence of NaNs\n",
    "\n",
    "X_no_nans, Y_no_nans = delete_nan_points(X_nans, Y_total)\n",
    "X_with_nans, Y_with_nans = store_nan_points(X_nans, Y_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing NaNs with median\n",
    "\n",
    "X_cleaned = replace_nans_with_median(X_with_nans, threshold=0.5)\n",
    "Y_cleaned = Y_with_nans  # As we do not delete any data point, Y is the same for the cleaned data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaned Data names in the cell below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68114, 31) (68114,)\n",
      "(181886, 21) (181886,)\n"
     ]
    }
   ],
   "source": [
    "print(X_no_nans.shape, Y_no_nans.shape)\n",
    "print(X_cleaned.shape, Y_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999): loss=0.2344471850285804\n",
      "Gradient Descent(1/999): loss=0.22297924209834177\n",
      "Gradient Descent(2/999): loss=0.2127135262756751\n",
      "Gradient Descent(3/999): loss=0.2035237988318852\n",
      "Gradient Descent(4/999): loss=0.19529707756213635\n",
      "Gradient Descent(5/999): loss=0.18793224468320563\n",
      "Gradient Descent(6/999): loss=0.181338800919803\n",
      "Gradient Descent(7/999): loss=0.1754357504277822\n",
      "Gradient Descent(8/999): loss=0.17015060281468786\n",
      "Gradient Descent(9/999): loss=0.16541847996091408\n",
      "Gradient Descent(10/999): loss=0.16118131663606572\n",
      "Gradient Descent(11/999): loss=0.15738714506082038\n",
      "Gradient Descent(12/999): loss=0.15398945459893632\n",
      "Gradient Descent(13/999): loss=0.15094661868977616\n",
      "Gradient Descent(14/999): loss=0.14822138196022944\n",
      "Gradient Descent(15/999): loss=0.1457804011964236\n",
      "Gradient Descent(16/999): loss=0.143593834519256\n",
      "Gradient Descent(17/999): loss=0.14163497370172534\n",
      "Gradient Descent(18/999): loss=0.13987991509762168\n",
      "Gradient Descent(19/999): loss=0.13830726512688576\n",
      "Gradient Descent(20/999): loss=0.13689787668874492\n",
      "Gradient Descent(21/999): loss=0.13563461325481216\n",
      "Gradient Descent(22/999): loss=0.13450213773539735\n",
      "Gradient Descent(23/999): loss=0.13348672351752566\n",
      "Gradient Descent(24/999): loss=0.13257608534635032\n",
      "Gradient Descent(25/999): loss=0.13175922796614808\n",
      "Gradient Descent(26/999): loss=0.1310263106559152\n",
      "Gradient Descent(27/999): loss=0.13036852599042587\n",
      "Gradient Descent(28/999): loss=0.12977799133289877\n",
      "Gradient Descent(29/999): loss=0.12924765172228886\n",
      "Gradient Descent(30/999): loss=0.12877119295862424\n",
      "Gradient Descent(31/999): loss=0.12834296381546215\n",
      "Gradient Descent(32/999): loss=0.12795790642100024\n",
      "Gradient Descent(33/999): loss=0.12761149395003013\n",
      "Gradient Descent(34/999): loss=0.1272996748590016\n",
      "Gradient Descent(35/999): loss=0.12701882297708705\n",
      "Gradient Descent(36/999): loss=0.12676569283829234\n",
      "Gradient Descent(37/999): loss=0.12653737970423598\n",
      "Gradient Descent(38/999): loss=0.12633128378501898\n",
      "Gradient Descent(39/999): loss=0.1261450782173303\n",
      "Gradient Descent(40/999): loss=0.1259766804052321\n",
      "Gradient Descent(41/999): loss=0.12582422637050017\n",
      "Gradient Descent(42/999): loss=0.125686047796478\n",
      "Gradient Descent(43/999): loss=0.12556065148259174\n",
      "Gradient Descent(44/999): loss=0.12544670095637656\n",
      "Gradient Descent(45/999): loss=0.12534300001644794\n",
      "Gradient Descent(46/999): loss=0.1252484780036445\n",
      "Gradient Descent(47/999): loss=0.12516217661886347\n",
      "Gradient Descent(48/999): loss=0.12508323812516564\n",
      "Gradient Descent(49/999): loss=0.1250108947887846\n",
      "Gradient Descent(50/999): loss=0.12494445942894067\n",
      "Gradient Descent(51/999): loss=0.12488331696001989\n",
      "Gradient Descent(52/999): loss=0.12482691682190862\n",
      "Gradient Descent(53/999): loss=0.12477476620521594\n",
      "Gradient Descent(54/999): loss=0.12472642398791096\n",
      "Gradient Descent(55/999): loss=0.12468149530866793\n",
      "Gradient Descent(56/999): loss=0.12463962671005697\n",
      "Gradient Descent(57/999): loss=0.12460050179173969\n",
      "Gradient Descent(58/999): loss=0.12456383732011332\n",
      "Gradient Descent(59/999): loss=0.12452937974647056\n",
      "Gradient Descent(60/999): loss=0.12449690209077621\n",
      "Gradient Descent(61/999): loss=0.12446620115266675\n",
      "Gradient Descent(62/999): loss=0.12443709501531028\n",
      "Gradient Descent(63/999): loss=0.1244094208113736\n",
      "Gradient Descent(64/999): loss=0.12438303272357158\n",
      "Gradient Descent(65/999): loss=0.12435780019516562\n",
      "Gradient Descent(66/999): loss=0.12433360632836406\n",
      "Gradient Descent(67/999): loss=0.12431034645089235\n",
      "Gradient Descent(68/999): loss=0.12428792683307405\n",
      "Gradient Descent(69/999): loss=0.12426626353961702\n",
      "Gradient Descent(70/999): loss=0.12424528140195933\n",
      "Gradient Descent(71/999): loss=0.12422491309851513\n",
      "Gradient Descent(72/999): loss=0.12420509833148984\n",
      "Gradient Descent(73/999): loss=0.12418578309012383\n",
      "Gradient Descent(74/999): loss=0.12416691899128908\n",
      "Gradient Descent(75/999): loss=0.1241484626893157\n",
      "Gradient Descent(76/999): loss=0.12413037534777911\n",
      "Gradient Descent(77/999): loss=0.1241126221667409\n",
      "Gradient Descent(78/999): loss=0.12409517195962083\n",
      "Gradient Descent(79/999): loss=0.1240779967744881\n",
      "Gradient Descent(80/999): loss=0.1240610715551076\n",
      "Gradient Descent(81/999): loss=0.12404437383756652\n",
      "Gradient Descent(82/999): loss=0.12402788347874584\n",
      "Gradient Descent(83/999): loss=0.12401158241329158\n",
      "Gradient Descent(84/999): loss=0.12399545443609464\n",
      "Gradient Descent(85/999): loss=0.12397948500759982\n",
      "Gradient Descent(86/999): loss=0.12396366107954726\n",
      "Gradient Descent(87/999): loss=0.12394797093900063\n",
      "Gradient Descent(88/999): loss=0.12393240406874265\n",
      "Gradient Descent(89/999): loss=0.12391695102231824\n",
      "Gradient Descent(90/999): loss=0.12390160331218861\n",
      "Gradient Descent(91/999): loss=0.12388635330961881\n",
      "Gradient Descent(92/999): loss=0.12387119415506695\n",
      "Gradient Descent(93/999): loss=0.12385611967797303\n",
      "Gradient Descent(94/999): loss=0.12384112432495975\n",
      "Gradient Descent(95/999): loss=0.12382620309556273\n",
      "Gradient Descent(96/999): loss=0.12381135148469938\n",
      "Gradient Descent(97/999): loss=0.12379656543116908\n",
      "Gradient Descent(98/999): loss=0.12378184127155134\n",
      "Gradient Descent(99/999): loss=0.12376717569893565\n",
      "Gradient Descent(100/999): loss=0.12375256572597532\n",
      "Gradient Descent(101/999): loss=0.12373800865181181\n",
      "Gradient Descent(102/999): loss=0.12372350203246325\n",
      "Gradient Descent(103/999): loss=0.12370904365431319\n",
      "Gradient Descent(104/999): loss=0.12369463151037467\n",
      "Gradient Descent(105/999): loss=0.12368026377903808\n",
      "Gradient Descent(106/999): loss=0.12366593880504208\n",
      "Gradient Descent(107/999): loss=0.12365165508243459\n",
      "Gradient Descent(108/999): loss=0.12363741123931461\n",
      "Gradient Descent(109/999): loss=0.12362320602416856\n",
      "Gradient Descent(110/999): loss=0.12360903829363314\n",
      "Gradient Descent(111/999): loss=0.123594907001536\n",
      "Gradient Descent(112/999): loss=0.1235808111890791\n",
      "Gradient Descent(113/999): loss=0.1235667499760459\n",
      "Gradient Descent(114/999): loss=0.1235527225529243\n",
      "Gradient Descent(115/999): loss=0.12353872817384977\n",
      "Gradient Descent(116/999): loss=0.12352476615028232\n",
      "Gradient Descent(117/999): loss=0.1235108358453407\n",
      "Gradient Descent(118/999): loss=0.12349693666872466\n",
      "Gradient Descent(119/999): loss=0.12348306807216397\n",
      "Gradient Descent(120/999): loss=0.12346922954533869\n",
      "Gradient Descent(121/999): loss=0.12345542061222148\n",
      "Gradient Descent(122/999): loss=0.12344164082779772\n",
      "Gradient Descent(123/999): loss=0.12342788977512421\n",
      "Gradient Descent(124/999): loss=0.12341416706269016\n",
      "Gradient Descent(125/999): loss=0.12340047232204987\n",
      "Gradient Descent(126/999): loss=0.1233868052056981\n",
      "Gradient Descent(127/999): loss=0.1233731653851627\n",
      "Gradient Descent(128/999): loss=0.12335955254929211\n",
      "Gradient Descent(129/999): loss=0.12334596640271729\n",
      "Gradient Descent(130/999): loss=0.12333240666446965\n",
      "Gradient Descent(131/999): loss=0.1233188730667389\n",
      "Gradient Descent(132/999): loss=0.12330536535375629\n",
      "Gradient Descent(133/999): loss=0.1232918832807899\n",
      "Gradient Descent(134/999): loss=0.12327842661324069\n",
      "Gradient Descent(135/999): loss=0.12326499512582838\n",
      "Gradient Descent(136/999): loss=0.12325158860185835\n",
      "Gradient Descent(137/999): loss=0.12323820683256036\n",
      "Gradient Descent(138/999): loss=0.12322484961649294\n",
      "Gradient Descent(139/999): loss=0.12321151675900481\n",
      "Gradient Descent(140/999): loss=0.12319820807174936\n",
      "Gradient Descent(141/999): loss=0.12318492337224528\n",
      "Gradient Descent(142/999): loss=0.1231716624834792\n",
      "Gradient Descent(143/999): loss=0.12315842523354598\n",
      "Gradient Descent(144/999): loss=0.12314521145532269\n",
      "Gradient Descent(145/999): loss=0.12313202098617264\n",
      "Gradient Descent(146/999): loss=0.12311885366767693\n",
      "Gradient Descent(147/999): loss=0.1231057093453899\n",
      "Gradient Descent(148/999): loss=0.12309258786861695\n",
      "Gradient Descent(149/999): loss=0.12307948909021169\n",
      "Gradient Descent(150/999): loss=0.12306641286639078\n",
      "Gradient Descent(151/999): loss=0.1230533590565649\n",
      "Gradient Descent(152/999): loss=0.12304032752318364\n",
      "Gradient Descent(153/999): loss=0.12302731813159365\n",
      "Gradient Descent(154/999): loss=0.12301433074990792\n",
      "Gradient Descent(155/999): loss=0.12300136524888587\n",
      "Gradient Descent(156/999): loss=0.1229884215018226\n",
      "Gradient Descent(157/999): loss=0.12297549938444653\n",
      "Gradient Descent(158/999): loss=0.12296259877482513\n",
      "Gradient Descent(159/999): loss=0.12294971955327687\n",
      "Gradient Descent(160/999): loss=0.12293686160229002\n",
      "Gradient Descent(161/999): loss=0.12292402480644662\n",
      "Gradient Descent(162/999): loss=0.1229112090523519\n",
      "Gradient Descent(163/999): loss=0.12289841422856801\n",
      "Gradient Descent(164/999): loss=0.12288564022555232\n",
      "Gradient Descent(165/999): loss=0.12287288693559899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(166/999): loss=0.12286015425278453\n",
      "Gradient Descent(167/999): loss=0.12284744207291617\n",
      "Gradient Descent(168/999): loss=0.12283475029348327\n",
      "Gradient Descent(169/999): loss=0.12282207881361115\n",
      "Gradient Descent(170/999): loss=0.12280942753401744\n",
      "Gradient Descent(171/999): loss=0.12279679635697077\n",
      "Gradient Descent(172/999): loss=0.12278418518625102\n",
      "Gradient Descent(173/999): loss=0.12277159392711184\n",
      "Gradient Descent(174/999): loss=0.12275902248624464\n",
      "Gradient Descent(175/999): loss=0.12274647077174416\n",
      "Gradient Descent(176/999): loss=0.12273393869307563\n",
      "Gradient Descent(177/999): loss=0.12272142616104283\n",
      "Gradient Descent(178/999): loss=0.12270893308775797\n",
      "Gradient Descent(179/999): loss=0.12269645938661228\n",
      "Gradient Descent(180/999): loss=0.1226840049722478\n",
      "Gradient Descent(181/999): loss=0.12267156976053006\n",
      "Gradient Descent(182/999): loss=0.12265915366852195\n",
      "Gradient Descent(183/999): loss=0.12264675661445812\n",
      "Gradient Descent(184/999): loss=0.12263437851772037\n",
      "Gradient Descent(185/999): loss=0.12262201929881383\n",
      "Gradient Descent(186/999): loss=0.12260967887934371\n",
      "Gradient Descent(187/999): loss=0.1225973571819929\n",
      "Gradient Descent(188/999): loss=0.1225850541305\n",
      "Gradient Descent(189/999): loss=0.12257276964963816\n",
      "Gradient Descent(190/999): loss=0.12256050366519439\n",
      "Gradient Descent(191/999): loss=0.12254825610394944\n",
      "Gradient Descent(192/999): loss=0.12253602689365813\n",
      "Gradient Descent(193/999): loss=0.12252381596303023\n",
      "Gradient Descent(194/999): loss=0.12251162324171201\n",
      "Gradient Descent(195/999): loss=0.12249944866026777\n",
      "Gradient Descent(196/999): loss=0.12248729215016235\n",
      "Gradient Descent(197/999): loss=0.12247515364374355\n",
      "Gradient Descent(198/999): loss=0.12246303307422536\n",
      "Gradient Descent(199/999): loss=0.12245093037567142\n",
      "Gradient Descent(200/999): loss=0.12243884548297883\n",
      "Gradient Descent(201/999): loss=0.12242677833186216\n",
      "Gradient Descent(202/999): loss=0.12241472885883842\n",
      "Gradient Descent(203/999): loss=0.12240269700121138\n",
      "Gradient Descent(204/999): loss=0.12239068269705723\n",
      "Gradient Descent(205/999): loss=0.12237868588520984\n",
      "Gradient Descent(206/999): loss=0.12236670650524674\n",
      "Gradient Descent(207/999): loss=0.12235474449747512\n",
      "Gradient Descent(208/999): loss=0.12234279980291832\n",
      "Gradient Descent(209/999): loss=0.12233087236330253\n",
      "Gradient Descent(210/999): loss=0.1223189621210437\n",
      "Gradient Descent(211/999): loss=0.12230706901923488\n",
      "Gradient Descent(212/999): loss=0.12229519300163358\n",
      "Gradient Descent(213/999): loss=0.1222833340126497\n",
      "Gradient Descent(214/999): loss=0.12227149199733334\n",
      "Gradient Descent(215/999): loss=0.12225966690136321\n",
      "Gradient Descent(216/999): loss=0.12224785867103503\n",
      "Gradient Descent(217/999): loss=0.12223606725325022\n",
      "Gradient Descent(218/999): loss=0.1222242925955049\n",
      "Gradient Descent(219/999): loss=0.12221253464587901\n",
      "Gradient Descent(220/999): loss=0.1222007933530257\n",
      "Gradient Descent(221/999): loss=0.12218906866616082\n",
      "Gradient Descent(222/999): loss=0.12217736053505283\n",
      "Gradient Descent(223/999): loss=0.12216566891001276\n",
      "Gradient Descent(224/999): loss=0.12215399374188439\n",
      "Gradient Descent(225/999): loss=0.12214233498203449\n",
      "Gradient Descent(226/999): loss=0.12213069258234363\n",
      "Gradient Descent(227/999): loss=0.12211906649519681\n",
      "Gradient Descent(228/999): loss=0.12210745667347427\n",
      "Gradient Descent(229/999): loss=0.12209586307054283\n",
      "Gradient Descent(230/999): loss=0.12208428564024701\n",
      "Gradient Descent(231/999): loss=0.12207272433690056\n",
      "Gradient Descent(232/999): loss=0.12206117911527796\n",
      "Gradient Descent(233/999): loss=0.1220496499306064\n",
      "Gradient Descent(234/999): loss=0.12203813673855753\n",
      "Gradient Descent(235/999): loss=0.12202663949523966\n",
      "Gradient Descent(236/999): loss=0.12201515815719001\n",
      "Gradient Descent(237/999): loss=0.12200369268136699\n",
      "Gradient Descent(238/999): loss=0.12199224302514289\n",
      "Gradient Descent(239/999): loss=0.12198080914629658\n",
      "Gradient Descent(240/999): loss=0.12196939100300616\n",
      "Gradient Descent(241/999): loss=0.12195798855384216\n",
      "Gradient Descent(242/999): loss=0.12194660175776041\n",
      "Gradient Descent(243/999): loss=0.12193523057409553\n",
      "Gradient Descent(244/999): loss=0.12192387496255412\n",
      "Gradient Descent(245/999): loss=0.12191253488320827\n",
      "Gradient Descent(246/999): loss=0.12190121029648932\n",
      "Gradient Descent(247/999): loss=0.12188990116318142\n",
      "Gradient Descent(248/999): loss=0.1218786074444155\n",
      "Gradient Descent(249/999): loss=0.12186732910166331\n",
      "Gradient Descent(250/999): loss=0.12185606609673134\n",
      "Gradient Descent(251/999): loss=0.12184481839175525\n",
      "Gradient Descent(252/999): loss=0.121833585949194\n",
      "Gradient Descent(253/999): loss=0.12182236873182442\n",
      "Gradient Descent(254/999): loss=0.12181116670273573\n",
      "Gradient Descent(255/999): loss=0.12179997982532402\n",
      "Gradient Descent(256/999): loss=0.12178880806328729\n",
      "Gradient Descent(257/999): loss=0.12177765138062008\n",
      "Gradient Descent(258/999): loss=0.12176650974160844\n",
      "Gradient Descent(259/999): loss=0.12175538311082512\n",
      "Gradient Descent(260/999): loss=0.12174427145312455\n",
      "Gradient Descent(261/999): loss=0.12173317473363805\n",
      "Gradient Descent(262/999): loss=0.12172209291776934\n",
      "Gradient Descent(263/999): loss=0.12171102597118987\n",
      "Gradient Descent(264/999): loss=0.12169997385983417\n",
      "Gradient Descent(265/999): loss=0.12168893654989572\n",
      "Gradient Descent(266/999): loss=0.12167791400782234\n",
      "Gradient Descent(267/999): loss=0.12166690620031215\n",
      "Gradient Descent(268/999): loss=0.12165591309430933\n",
      "Gradient Descent(269/999): loss=0.12164493465699996\n",
      "Gradient Descent(270/999): loss=0.12163397085580825\n",
      "Gradient Descent(271/999): loss=0.12162302165839228\n",
      "Gradient Descent(272/999): loss=0.12161208703264044\n",
      "Gradient Descent(273/999): loss=0.12160116694666746\n",
      "Gradient Descent(274/999): loss=0.12159026136881082\n",
      "Gradient Descent(275/999): loss=0.12157937026762705\n",
      "Gradient Descent(276/999): loss=0.12156849361188812\n",
      "Gradient Descent(277/999): loss=0.12155763137057808\n",
      "Gradient Descent(278/999): loss=0.1215467835128895\n",
      "Gradient Descent(279/999): loss=0.12153595000822015\n",
      "Gradient Descent(280/999): loss=0.12152513082616967\n",
      "Gradient Descent(281/999): loss=0.12151432593653638\n",
      "Gradient Descent(282/999): loss=0.12150353530931404\n",
      "Gradient Descent(283/999): loss=0.12149275891468884\n",
      "Gradient Descent(284/999): loss=0.12148199672303614\n",
      "Gradient Descent(285/999): loss=0.12147124870491773\n",
      "Gradient Descent(286/999): loss=0.12146051483107863\n",
      "Gradient Descent(287/999): loss=0.12144979507244436\n",
      "Gradient Descent(288/999): loss=0.1214390894001181\n",
      "Gradient Descent(289/999): loss=0.12142839778537791\n",
      "Gradient Descent(290/999): loss=0.12141772019967381\n",
      "Gradient Descent(291/999): loss=0.1214070566146255\n",
      "Gradient Descent(292/999): loss=0.12139640700201931\n",
      "Gradient Descent(293/999): loss=0.12138577133380593\n",
      "Gradient Descent(294/999): loss=0.12137514958209769\n",
      "Gradient Descent(295/999): loss=0.12136454171916627\n",
      "Gradient Descent(296/999): loss=0.12135394771744011\n",
      "Gradient Descent(297/999): loss=0.12134336754950205\n",
      "Gradient Descent(298/999): loss=0.12133280118808712\n",
      "Gradient Descent(299/999): loss=0.12132224860608007\n",
      "Gradient Descent(300/999): loss=0.12131170977651318\n",
      "Gradient Descent(301/999): loss=0.12130118467256412\n",
      "Gradient Descent(302/999): loss=0.12129067326755377\n",
      "Gradient Descent(303/999): loss=0.12128017553494391\n",
      "Gradient Descent(304/999): loss=0.12126969144833545\n",
      "Gradient Descent(305/999): loss=0.12125922098146609\n",
      "Gradient Descent(306/999): loss=0.12124876410820844\n",
      "Gradient Descent(307/999): loss=0.12123832080256806\n",
      "Gradient Descent(308/999): loss=0.12122789103868159\n",
      "Gradient Descent(309/999): loss=0.12121747479081461\n",
      "Gradient Descent(310/999): loss=0.1212070720333601\n",
      "Gradient Descent(311/999): loss=0.12119668274083627\n",
      "Gradient Descent(312/999): loss=0.12118630688788516\n",
      "Gradient Descent(313/999): loss=0.12117594444927048\n",
      "Gradient Descent(314/999): loss=0.12116559539987623\n",
      "Gradient Descent(315/999): loss=0.12115525971470471\n",
      "Gradient Descent(316/999): loss=0.12114493736887517\n",
      "Gradient Descent(317/999): loss=0.1211346283376218\n",
      "Gradient Descent(318/999): loss=0.12112433259629254\n",
      "Gradient Descent(319/999): loss=0.1211140501203472\n",
      "Gradient Descent(320/999): loss=0.12110378088535606\n",
      "Gradient Descent(321/999): loss=0.1210935248669983\n",
      "Gradient Descent(322/999): loss=0.12108328204106063\n",
      "Gradient Descent(323/999): loss=0.12107305238343562\n",
      "Gradient Descent(324/999): loss=0.12106283587012052\n",
      "Gradient Descent(325/999): loss=0.12105263247721566\n",
      "Gradient Descent(326/999): loss=0.1210424421809232\n",
      "Gradient Descent(327/999): loss=0.1210322649575457\n",
      "Gradient Descent(328/999): loss=0.12102210078348483\n",
      "Gradient Descent(329/999): loss=0.1210119496352401\n",
      "Gradient Descent(330/999): loss=0.12100181148940749\n",
      "Gradient Descent(331/999): loss=0.12099168632267834\n",
      "Gradient Descent(332/999): loss=0.12098157411183796\n",
      "Gradient Descent(333/999): loss=0.12097147483376448\n",
      "Gradient Descent(334/999): loss=0.12096138846542781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(335/999): loss=0.12095131498388825\n",
      "Gradient Descent(336/999): loss=0.12094125436629546\n",
      "Gradient Descent(337/999): loss=0.12093120658988733\n",
      "Gradient Descent(338/999): loss=0.12092117163198894\n",
      "Gradient Descent(339/999): loss=0.12091114947001136\n",
      "Gradient Descent(340/999): loss=0.12090114008145064\n",
      "Gradient Descent(341/999): loss=0.12089114344388686\n",
      "Gradient Descent(342/999): loss=0.12088115953498299\n",
      "Gradient Descent(343/999): loss=0.12087118833248389\n",
      "Gradient Descent(344/999): loss=0.12086122981421543\n",
      "Gradient Descent(345/999): loss=0.1208512839580834\n",
      "Gradient Descent(346/999): loss=0.1208413507420727\n",
      "Gradient Descent(347/999): loss=0.12083143014424624\n",
      "Gradient Descent(348/999): loss=0.12082152214274415\n",
      "Gradient Descent(349/999): loss=0.12081162671578295\n",
      "Gradient Descent(350/999): loss=0.1208017438416544\n",
      "Gradient Descent(351/999): loss=0.1207918734987249\n",
      "Gradient Descent(352/999): loss=0.12078201566543453\n",
      "Gradient Descent(353/999): loss=0.12077217032029629\n",
      "Gradient Descent(354/999): loss=0.12076233744189512\n",
      "Gradient Descent(355/999): loss=0.12075251700888721\n",
      "Gradient Descent(356/999): loss=0.1207427089999993\n",
      "Gradient Descent(357/999): loss=0.1207329133940277\n",
      "Gradient Descent(358/999): loss=0.12072313016983774\n",
      "Gradient Descent(359/999): loss=0.1207133593063628\n",
      "Gradient Descent(360/999): loss=0.12070360078260375\n",
      "Gradient Descent(361/999): loss=0.12069385457762818\n",
      "Gradient Descent(362/999): loss=0.1206841206705697\n",
      "Gradient Descent(363/999): loss=0.1206743990406272\n",
      "Gradient Descent(364/999): loss=0.12066468966706417\n",
      "Gradient Descent(365/999): loss=0.12065499252920817\n",
      "Gradient Descent(366/999): loss=0.12064530760644988\n",
      "Gradient Descent(367/999): loss=0.12063563487824285\n",
      "Gradient Descent(368/999): loss=0.12062597432410244\n",
      "Gradient Descent(369/999): loss=0.1206163259236055\n",
      "Gradient Descent(370/999): loss=0.12060668965638972\n",
      "Gradient Descent(371/999): loss=0.12059706550215286\n",
      "Gradient Descent(372/999): loss=0.12058745344065235\n",
      "Gradient Descent(373/999): loss=0.12057785345170455\n",
      "Gradient Descent(374/999): loss=0.1205682655151843\n",
      "Gradient Descent(375/999): loss=0.12055868961102435\n",
      "Gradient Descent(376/999): loss=0.12054912571921474\n",
      "Gradient Descent(377/999): loss=0.1205395738198022\n",
      "Gradient Descent(378/999): loss=0.1205300338928899\n",
      "Gradient Descent(379/999): loss=0.12052050591863664\n",
      "Gradient Descent(380/999): loss=0.1205109898772564\n",
      "Gradient Descent(381/999): loss=0.12050148574901794\n",
      "Gradient Descent(382/999): loss=0.1204919935142442\n",
      "Gradient Descent(383/999): loss=0.12048251315331183\n",
      "Gradient Descent(384/999): loss=0.12047304464665086\n",
      "Gradient Descent(385/999): loss=0.1204635879747439\n",
      "Gradient Descent(386/999): loss=0.12045414311812605\n",
      "Gradient Descent(387/999): loss=0.12044471005738418\n",
      "Gradient Descent(388/999): loss=0.12043528877315665\n",
      "Gradient Descent(389/999): loss=0.12042587924613275\n",
      "Gradient Descent(390/999): loss=0.1204164814570523\n",
      "Gradient Descent(391/999): loss=0.12040709538670533\n",
      "Gradient Descent(392/999): loss=0.1203977210159315\n",
      "Gradient Descent(393/999): loss=0.1203883583256198\n",
      "Gradient Descent(394/999): loss=0.12037900729670811\n",
      "Gradient Descent(395/999): loss=0.12036966791018289\n",
      "Gradient Descent(396/999): loss=0.12036034014707854\n",
      "Gradient Descent(397/999): loss=0.1203510239884774\n",
      "Gradient Descent(398/999): loss=0.12034171941550896\n",
      "Gradient Descent(399/999): loss=0.12033242640934987\n",
      "Gradient Descent(400/999): loss=0.12032314495122329\n",
      "Gradient Descent(401/999): loss=0.12031387502239871\n",
      "Gradient Descent(402/999): loss=0.12030461660419146\n",
      "Gradient Descent(403/999): loss=0.12029536967796256\n",
      "Gradient Descent(404/999): loss=0.1202861342251181\n",
      "Gradient Descent(405/999): loss=0.1202769102271092\n",
      "Gradient Descent(406/999): loss=0.12026769766543147\n",
      "Gradient Descent(407/999): loss=0.12025849652162478\n",
      "Gradient Descent(408/999): loss=0.12024930677727291\n",
      "Gradient Descent(409/999): loss=0.1202401284140033\n",
      "Gradient Descent(410/999): loss=0.1202309614134866\n",
      "Gradient Descent(411/999): loss=0.1202218057574366\n",
      "Gradient Descent(412/999): loss=0.12021266142760964\n",
      "Gradient Descent(413/999): loss=0.12020352840580459\n",
      "Gradient Descent(414/999): loss=0.1201944066738623\n",
      "Gradient Descent(415/999): loss=0.12018529621366567\n",
      "Gradient Descent(416/999): loss=0.12017619700713894\n",
      "Gradient Descent(417/999): loss=0.12016710903624775\n",
      "Gradient Descent(418/999): loss=0.12015803228299878\n",
      "Gradient Descent(419/999): loss=0.12014896672943935\n",
      "Gradient Descent(420/999): loss=0.12013991235765736\n",
      "Gradient Descent(421/999): loss=0.12013086914978084\n",
      "Gradient Descent(422/999): loss=0.12012183708797798\n",
      "Gradient Descent(423/999): loss=0.12011281615445647\n",
      "Gradient Descent(424/999): loss=0.12010380633146361\n",
      "Gradient Descent(425/999): loss=0.12009480760128591\n",
      "Gradient Descent(426/999): loss=0.12008581994624891\n",
      "Gradient Descent(427/999): loss=0.12007684334871682\n",
      "Gradient Descent(428/999): loss=0.12006787779109244\n",
      "Gradient Descent(429/999): loss=0.120058923255817\n",
      "Gradient Descent(430/999): loss=0.12004997972536958\n",
      "Gradient Descent(431/999): loss=0.1200410471822673\n",
      "Gradient Descent(432/999): loss=0.1200321256090648\n",
      "Gradient Descent(433/999): loss=0.12002321498835423\n",
      "Gradient Descent(434/999): loss=0.12001431530276506\n",
      "Gradient Descent(435/999): loss=0.12000542653496349\n",
      "Gradient Descent(436/999): loss=0.11999654866765282\n",
      "Gradient Descent(437/999): loss=0.1199876816835728\n",
      "Gradient Descent(438/999): loss=0.11997882556549962\n",
      "Gradient Descent(439/999): loss=0.11996998029624568\n",
      "Gradient Descent(440/999): loss=0.11996114585865945\n",
      "Gradient Descent(441/999): loss=0.11995232223562517\n",
      "Gradient Descent(442/999): loss=0.11994350941006275\n",
      "Gradient Descent(443/999): loss=0.11993470736492759\n",
      "Gradient Descent(444/999): loss=0.11992591608321031\n",
      "Gradient Descent(445/999): loss=0.11991713554793672\n",
      "Gradient Descent(446/999): loss=0.11990836574216741\n",
      "Gradient Descent(447/999): loss=0.11989960664899788\n",
      "Gradient Descent(448/999): loss=0.1198908582515581\n",
      "Gradient Descent(449/999): loss=0.11988212053301255\n",
      "Gradient Descent(450/999): loss=0.1198733934765598\n",
      "Gradient Descent(451/999): loss=0.11986467706543268\n",
      "Gradient Descent(452/999): loss=0.11985597128289784\n",
      "Gradient Descent(453/999): loss=0.11984727611225564\n",
      "Gradient Descent(454/999): loss=0.11983859153684016\n",
      "Gradient Descent(455/999): loss=0.11982991754001886\n",
      "Gradient Descent(456/999): loss=0.11982125410519245\n",
      "Gradient Descent(457/999): loss=0.11981260121579489\n",
      "Gradient Descent(458/999): loss=0.11980395885529303\n",
      "Gradient Descent(459/999): loss=0.11979532700718665\n",
      "Gradient Descent(460/999): loss=0.11978670565500815\n",
      "Gradient Descent(461/999): loss=0.11977809478232254\n",
      "Gradient Descent(462/999): loss=0.11976949437272723\n",
      "Gradient Descent(463/999): loss=0.11976090440985195\n",
      "Gradient Descent(464/999): loss=0.1197523248773584\n",
      "Gradient Descent(465/999): loss=0.11974375575894054\n",
      "Gradient Descent(466/999): loss=0.11973519703832398\n",
      "Gradient Descent(467/999): loss=0.1197266486992662\n",
      "Gradient Descent(468/999): loss=0.11971811072555626\n",
      "Gradient Descent(469/999): loss=0.11970958310101458\n",
      "Gradient Descent(470/999): loss=0.11970106580949315\n",
      "Gradient Descent(471/999): loss=0.11969255883487497\n",
      "Gradient Descent(472/999): loss=0.11968406216107425\n",
      "Gradient Descent(473/999): loss=0.1196755757720362\n",
      "Gradient Descent(474/999): loss=0.11966709965173675\n",
      "Gradient Descent(475/999): loss=0.11965863378418279\n",
      "Gradient Descent(476/999): loss=0.11965017815341156\n",
      "Gradient Descent(477/999): loss=0.11964173274349103\n",
      "Gradient Descent(478/999): loss=0.11963329753851944\n",
      "Gradient Descent(479/999): loss=0.11962487252262533\n",
      "Gradient Descent(480/999): loss=0.11961645767996743\n",
      "Gradient Descent(481/999): loss=0.11960805299473441\n",
      "Gradient Descent(482/999): loss=0.11959965845114504\n",
      "Gradient Descent(483/999): loss=0.11959127403344781\n",
      "Gradient Descent(484/999): loss=0.11958289972592095\n",
      "Gradient Descent(485/999): loss=0.11957453551287231\n",
      "Gradient Descent(486/999): loss=0.1195661813786393\n",
      "Gradient Descent(487/999): loss=0.11955783730758868\n",
      "Gradient Descent(488/999): loss=0.11954950328411648\n",
      "Gradient Descent(489/999): loss=0.11954117929264808\n",
      "Gradient Descent(490/999): loss=0.11953286531763781\n",
      "Gradient Descent(491/999): loss=0.11952456134356908\n",
      "Gradient Descent(492/999): loss=0.11951626735495417\n",
      "Gradient Descent(493/999): loss=0.11950798333633426\n",
      "Gradient Descent(494/999): loss=0.11949970927227915\n",
      "Gradient Descent(495/999): loss=0.11949144514738728\n",
      "Gradient Descent(496/999): loss=0.11948319094628558\n",
      "Gradient Descent(497/999): loss=0.11947494665362952\n",
      "Gradient Descent(498/999): loss=0.11946671225410281\n",
      "Gradient Descent(499/999): loss=0.11945848773241746\n",
      "Gradient Descent(500/999): loss=0.11945027307331363\n",
      "Gradient Descent(501/999): loss=0.11944206826155951\n",
      "Gradient Descent(502/999): loss=0.11943387328195133\n",
      "Gradient Descent(503/999): loss=0.11942568811931319\n",
      "Gradient Descent(504/999): loss=0.11941751275849698\n",
      "Gradient Descent(505/999): loss=0.11940934718438236\n",
      "Gradient Descent(506/999): loss=0.11940119138187656\n",
      "Gradient Descent(507/999): loss=0.11939304533591445\n",
      "Gradient Descent(508/999): loss=0.11938490903145828\n",
      "Gradient Descent(509/999): loss=0.11937678245349775\n",
      "Gradient Descent(510/999): loss=0.11936866558704987\n",
      "Gradient Descent(511/999): loss=0.11936055841715883\n",
      "Gradient Descent(512/999): loss=0.11935246092889598\n",
      "Gradient Descent(513/999): loss=0.11934437310735979\n",
      "Gradient Descent(514/999): loss=0.11933629493767565\n",
      "Gradient Descent(515/999): loss=0.11932822640499592\n",
      "Gradient Descent(516/999): loss=0.11932016749449972\n",
      "Gradient Descent(517/999): loss=0.11931211819139302\n",
      "Gradient Descent(518/999): loss=0.11930407848090836\n",
      "Gradient Descent(519/999): loss=0.11929604834830504\n",
      "Gradient Descent(520/999): loss=0.11928802777886875\n",
      "Gradient Descent(521/999): loss=0.1192800167579117\n",
      "Gradient Descent(522/999): loss=0.11927201527077244\n",
      "Gradient Descent(523/999): loss=0.11926402330281595\n",
      "Gradient Descent(524/999): loss=0.11925604083943335\n",
      "Gradient Descent(525/999): loss=0.11924806786604193\n",
      "Gradient Descent(526/999): loss=0.1192401043680851\n",
      "Gradient Descent(527/999): loss=0.11923215033103235\n",
      "Gradient Descent(528/999): loss=0.11922420574037906\n",
      "Gradient Descent(529/999): loss=0.11921627058164651\n",
      "Gradient Descent(530/999): loss=0.11920834484038188\n",
      "Gradient Descent(531/999): loss=0.11920042850215799\n",
      "Gradient Descent(532/999): loss=0.11919252155257344\n",
      "Gradient Descent(533/999): loss=0.11918462397725241\n",
      "Gradient Descent(534/999): loss=0.11917673576184465\n",
      "Gradient Descent(535/999): loss=0.11916885689202536\n",
      "Gradient Descent(536/999): loss=0.11916098735349526\n",
      "Gradient Descent(537/999): loss=0.11915312713198037\n",
      "Gradient Descent(538/999): loss=0.11914527621323191\n",
      "Gradient Descent(539/999): loss=0.11913743458302652\n",
      "Gradient Descent(540/999): loss=0.11912960222716591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(541/999): loss=0.11912177913147684\n",
      "Gradient Descent(542/999): loss=0.11911396528181128\n",
      "Gradient Descent(543/999): loss=0.11910616066404599\n",
      "Gradient Descent(544/999): loss=0.11909836526408278\n",
      "Gradient Descent(545/999): loss=0.1190905790678483\n",
      "Gradient Descent(546/999): loss=0.11908280206129397\n",
      "Gradient Descent(547/999): loss=0.11907503423039591\n",
      "Gradient Descent(548/999): loss=0.11906727556115503\n",
      "Gradient Descent(549/999): loss=0.1190595260395968\n",
      "Gradient Descent(550/999): loss=0.11905178565177121\n",
      "Gradient Descent(551/999): loss=0.11904405438375283\n",
      "Gradient Descent(552/999): loss=0.11903633222164063\n",
      "Gradient Descent(553/999): loss=0.11902861915155795\n",
      "Gradient Descent(554/999): loss=0.1190209151596525\n",
      "Gradient Descent(555/999): loss=0.11901322023209626\n",
      "Gradient Descent(556/999): loss=0.11900553435508542\n",
      "Gradient Descent(557/999): loss=0.1189978575148403\n",
      "Gradient Descent(558/999): loss=0.11899018969760536\n",
      "Gradient Descent(559/999): loss=0.11898253088964911\n",
      "Gradient Descent(560/999): loss=0.118974881077264\n",
      "Gradient Descent(561/999): loss=0.11896724024676647\n",
      "Gradient Descent(562/999): loss=0.11895960838449685\n",
      "Gradient Descent(563/999): loss=0.11895198547681925\n",
      "Gradient Descent(564/999): loss=0.11894437151012158\n",
      "Gradient Descent(565/999): loss=0.1189367664708155\n",
      "Gradient Descent(566/999): loss=0.1189291703453363\n",
      "Gradient Descent(567/999): loss=0.11892158312014287\n",
      "Gradient Descent(568/999): loss=0.11891400478171772\n",
      "Gradient Descent(569/999): loss=0.11890643531656685\n",
      "Gradient Descent(570/999): loss=0.11889887471121967\n",
      "Gradient Descent(571/999): loss=0.11889132295222911\n",
      "Gradient Descent(572/999): loss=0.11888378002617127\n",
      "Gradient Descent(573/999): loss=0.11887624591964573\n",
      "Gradient Descent(574/999): loss=0.11886872061927523\n",
      "Gradient Descent(575/999): loss=0.11886120411170575\n",
      "Gradient Descent(576/999): loss=0.11885369638360636\n",
      "Gradient Descent(577/999): loss=0.11884619742166935\n",
      "Gradient Descent(578/999): loss=0.11883870721260996\n",
      "Gradient Descent(579/999): loss=0.11883122574316644\n",
      "Gradient Descent(580/999): loss=0.11882375300010002\n",
      "Gradient Descent(581/999): loss=0.11881628897019483\n",
      "Gradient Descent(582/999): loss=0.11880883364025784\n",
      "Gradient Descent(583/999): loss=0.11880138699711883\n",
      "Gradient Descent(584/999): loss=0.11879394902763037\n",
      "Gradient Descent(585/999): loss=0.11878651971866767\n",
      "Gradient Descent(586/999): loss=0.11877909905712863\n",
      "Gradient Descent(587/999): loss=0.11877168702993383\n",
      "Gradient Descent(588/999): loss=0.11876428362402632\n",
      "Gradient Descent(589/999): loss=0.11875688882637167\n",
      "Gradient Descent(590/999): loss=0.11874950262395799\n",
      "Gradient Descent(591/999): loss=0.11874212500379579\n",
      "Gradient Descent(592/999): loss=0.11873475595291795\n",
      "Gradient Descent(593/999): loss=0.11872739545837965\n",
      "Gradient Descent(594/999): loss=0.11872004350725837\n",
      "Gradient Descent(595/999): loss=0.1187127000866539\n",
      "Gradient Descent(596/999): loss=0.1187053651836881\n",
      "Gradient Descent(597/999): loss=0.11869803878550508\n",
      "Gradient Descent(598/999): loss=0.118690720879271\n",
      "Gradient Descent(599/999): loss=0.11868341145217408\n",
      "Gradient Descent(600/999): loss=0.11867611049142457\n",
      "Gradient Descent(601/999): loss=0.11866881798425469\n",
      "Gradient Descent(602/999): loss=0.11866153391791853\n",
      "Gradient Descent(603/999): loss=0.11865425827969216\n",
      "Gradient Descent(604/999): loss=0.11864699105687335\n",
      "Gradient Descent(605/999): loss=0.11863973223678177\n",
      "Gradient Descent(606/999): loss=0.11863248180675878\n",
      "Gradient Descent(607/999): loss=0.11862523975416746\n",
      "Gradient Descent(608/999): loss=0.11861800606639253\n",
      "Gradient Descent(609/999): loss=0.11861078073084036\n",
      "Gradient Descent(610/999): loss=0.11860356373493887\n",
      "Gradient Descent(611/999): loss=0.11859635506613748\n",
      "Gradient Descent(612/999): loss=0.11858915471190716\n",
      "Gradient Descent(613/999): loss=0.11858196265974023\n",
      "Gradient Descent(614/999): loss=0.11857477889715053\n",
      "Gradient Descent(615/999): loss=0.11856760341167313\n",
      "Gradient Descent(616/999): loss=0.11856043619086451\n",
      "Gradient Descent(617/999): loss=0.1185532772223024\n",
      "Gradient Descent(618/999): loss=0.1185461264935857\n",
      "Gradient Descent(619/999): loss=0.11853898399233463\n",
      "Gradient Descent(620/999): loss=0.1185318497061904\n",
      "Gradient Descent(621/999): loss=0.11852472362281545\n",
      "Gradient Descent(622/999): loss=0.11851760572989323\n",
      "Gradient Descent(623/999): loss=0.1185104960151282\n",
      "Gradient Descent(624/999): loss=0.11850339446624585\n",
      "Gradient Descent(625/999): loss=0.11849630107099256\n",
      "Gradient Descent(626/999): loss=0.11848921581713565\n",
      "Gradient Descent(627/999): loss=0.1184821386924633\n",
      "Gradient Descent(628/999): loss=0.1184750696847845\n",
      "Gradient Descent(629/999): loss=0.11846800878192897\n",
      "Gradient Descent(630/999): loss=0.11846095597174722\n",
      "Gradient Descent(631/999): loss=0.1184539112421105\n",
      "Gradient Descent(632/999): loss=0.11844687458091063\n",
      "Gradient Descent(633/999): loss=0.11843984597606012\n",
      "Gradient Descent(634/999): loss=0.11843282541549202\n",
      "Gradient Descent(635/999): loss=0.11842581288715986\n",
      "Gradient Descent(636/999): loss=0.11841880837903786\n",
      "Gradient Descent(637/999): loss=0.11841181187912046\n",
      "Gradient Descent(638/999): loss=0.1184048233754228\n",
      "Gradient Descent(639/999): loss=0.11839784285598004\n",
      "Gradient Descent(640/999): loss=0.11839087030884803\n",
      "Gradient Descent(641/999): loss=0.11838390572210278\n",
      "Gradient Descent(642/999): loss=0.11837694908384044\n",
      "Gradient Descent(643/999): loss=0.11837000038217767\n",
      "Gradient Descent(644/999): loss=0.11836305960525106\n",
      "Gradient Descent(645/999): loss=0.11835612674121751\n",
      "Gradient Descent(646/999): loss=0.11834920177825393\n",
      "Gradient Descent(647/999): loss=0.1183422847045574\n",
      "Gradient Descent(648/999): loss=0.11833537550834498\n",
      "Gradient Descent(649/999): loss=0.11832847417785368\n",
      "Gradient Descent(650/999): loss=0.11832158070134058\n",
      "Gradient Descent(651/999): loss=0.11831469506708259\n",
      "Gradient Descent(652/999): loss=0.11830781726337655\n",
      "Gradient Descent(653/999): loss=0.11830094727853915\n",
      "Gradient Descent(654/999): loss=0.11829408510090689\n",
      "Gradient Descent(655/999): loss=0.11828723071883601\n",
      "Gradient Descent(656/999): loss=0.11828038412070248\n",
      "Gradient Descent(657/999): loss=0.11827354529490204\n",
      "Gradient Descent(658/999): loss=0.11826671422985002\n",
      "Gradient Descent(659/999): loss=0.11825989091398142\n",
      "Gradient Descent(660/999): loss=0.11825307533575082\n",
      "Gradient Descent(661/999): loss=0.11824626748363229\n",
      "Gradient Descent(662/999): loss=0.11823946734611952\n",
      "Gradient Descent(663/999): loss=0.11823267491172558\n",
      "Gradient Descent(664/999): loss=0.11822589016898304\n",
      "Gradient Descent(665/999): loss=0.1182191131064439\n",
      "Gradient Descent(666/999): loss=0.11821234371267939\n",
      "Gradient Descent(667/999): loss=0.11820558197628024\n",
      "Gradient Descent(668/999): loss=0.11819882788585644\n",
      "Gradient Descent(669/999): loss=0.11819208143003716\n",
      "Gradient Descent(670/999): loss=0.11818534259747082\n",
      "Gradient Descent(671/999): loss=0.11817861137682512\n",
      "Gradient Descent(672/999): loss=0.11817188775678682\n",
      "Gradient Descent(673/999): loss=0.11816517172606181\n",
      "Gradient Descent(674/999): loss=0.11815846327337512\n",
      "Gradient Descent(675/999): loss=0.11815176238747079\n",
      "Gradient Descent(676/999): loss=0.1181450690571118\n",
      "Gradient Descent(677/999): loss=0.11813838327108026\n",
      "Gradient Descent(678/999): loss=0.1181317050181771\n",
      "Gradient Descent(679/999): loss=0.11812503428722225\n",
      "Gradient Descent(680/999): loss=0.11811837106705438\n",
      "Gradient Descent(681/999): loss=0.11811171534653113\n",
      "Gradient Descent(682/999): loss=0.11810506711452894\n",
      "Gradient Descent(683/999): loss=0.11809842635994282\n",
      "Gradient Descent(684/999): loss=0.11809179307168682\n",
      "Gradient Descent(685/999): loss=0.11808516723869346\n",
      "Gradient Descent(686/999): loss=0.118078548849914\n",
      "Gradient Descent(687/999): loss=0.11807193789431836\n",
      "Gradient Descent(688/999): loss=0.11806533436089496\n",
      "Gradient Descent(689/999): loss=0.11805873823865093\n",
      "Gradient Descent(690/999): loss=0.11805214951661178\n",
      "Gradient Descent(691/999): loss=0.1180455681838216\n",
      "Gradient Descent(692/999): loss=0.11803899422934296\n",
      "Gradient Descent(693/999): loss=0.11803242764225673\n",
      "Gradient Descent(694/999): loss=0.11802586841166232\n",
      "Gradient Descent(695/999): loss=0.11801931652667742\n",
      "Gradient Descent(696/999): loss=0.11801277197643804\n",
      "Gradient Descent(697/999): loss=0.11800623475009857\n",
      "Gradient Descent(698/999): loss=0.1179997048368315\n",
      "Gradient Descent(699/999): loss=0.11799318222582772\n",
      "Gradient Descent(700/999): loss=0.1179866669062962\n",
      "Gradient Descent(701/999): loss=0.11798015886746407\n",
      "Gradient Descent(702/999): loss=0.11797365809857664\n",
      "Gradient Descent(703/999): loss=0.11796716458889728\n",
      "Gradient Descent(704/999): loss=0.11796067832770742\n",
      "Gradient Descent(705/999): loss=0.11795419930430652\n",
      "Gradient Descent(706/999): loss=0.11794772750801207\n",
      "Gradient Descent(707/999): loss=0.11794126292815942\n",
      "Gradient Descent(708/999): loss=0.11793480555410193\n",
      "Gradient Descent(709/999): loss=0.11792835537521092\n",
      "Gradient Descent(710/999): loss=0.11792191238087535\n",
      "Gradient Descent(711/999): loss=0.11791547656050227\n",
      "Gradient Descent(712/999): loss=0.11790904790351639\n",
      "Gradient Descent(713/999): loss=0.11790262639936017\n",
      "Gradient Descent(714/999): loss=0.11789621203749384\n",
      "Gradient Descent(715/999): loss=0.11788980480739537\n",
      "Gradient Descent(716/999): loss=0.11788340469856033\n",
      "Gradient Descent(717/999): loss=0.11787701170050198\n",
      "Gradient Descent(718/999): loss=0.11787062580275115\n",
      "Gradient Descent(719/999): loss=0.11786424699485626\n",
      "Gradient Descent(720/999): loss=0.11785787526638325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(721/999): loss=0.1178515106069156\n",
      "Gradient Descent(722/999): loss=0.11784515300605428\n",
      "Gradient Descent(723/999): loss=0.11783880245341764\n",
      "Gradient Descent(724/999): loss=0.11783245893864151\n",
      "Gradient Descent(725/999): loss=0.11782612245137908\n",
      "Gradient Descent(726/999): loss=0.1178197929813009\n",
      "Gradient Descent(727/999): loss=0.11781347051809479\n",
      "Gradient Descent(728/999): loss=0.11780715505146597\n",
      "Gradient Descent(729/999): loss=0.11780084657113676\n",
      "Gradient Descent(730/999): loss=0.11779454506684689\n",
      "Gradient Descent(731/999): loss=0.11778825052835315\n",
      "Gradient Descent(732/999): loss=0.1177819629454295\n",
      "Gradient Descent(733/999): loss=0.11777568230786718\n",
      "Gradient Descent(734/999): loss=0.11776940860547432\n",
      "Gradient Descent(735/999): loss=0.11776314182807622\n",
      "Gradient Descent(736/999): loss=0.11775688196551533\n",
      "Gradient Descent(737/999): loss=0.11775062900765089\n",
      "Gradient Descent(738/999): loss=0.11774438294435931\n",
      "Gradient Descent(739/999): loss=0.11773814376553386\n",
      "Gradient Descent(740/999): loss=0.11773191146108472\n",
      "Gradient Descent(741/999): loss=0.11772568602093905\n",
      "Gradient Descent(742/999): loss=0.1177194674350407\n",
      "Gradient Descent(743/999): loss=0.11771325569335053\n",
      "Gradient Descent(744/999): loss=0.11770705078584606\n",
      "Gradient Descent(745/999): loss=0.11770085270252172\n",
      "Gradient Descent(746/999): loss=0.1176946614333885\n",
      "Gradient Descent(747/999): loss=0.11768847696847423\n",
      "Gradient Descent(748/999): loss=0.11768229929782335\n",
      "Gradient Descent(749/999): loss=0.117676128411497\n",
      "Gradient Descent(750/999): loss=0.11766996429957284\n",
      "Gradient Descent(751/999): loss=0.11766380695214529\n",
      "Gradient Descent(752/999): loss=0.11765765635932512\n",
      "Gradient Descent(753/999): loss=0.1176515125112398\n",
      "Gradient Descent(754/999): loss=0.1176453753980332\n",
      "Gradient Descent(755/999): loss=0.11763924500986565\n",
      "Gradient Descent(756/999): loss=0.11763312133691403\n",
      "Gradient Descent(757/999): loss=0.1176270043693715\n",
      "Gradient Descent(758/999): loss=0.11762089409744766\n",
      "Gradient Descent(759/999): loss=0.11761479051136847\n",
      "Gradient Descent(760/999): loss=0.11760869360137613\n",
      "Gradient Descent(761/999): loss=0.1176026033577293\n",
      "Gradient Descent(762/999): loss=0.11759651977070272\n",
      "Gradient Descent(763/999): loss=0.11759044283058748\n",
      "Gradient Descent(764/999): loss=0.11758437252769077\n",
      "Gradient Descent(765/999): loss=0.11757830885233611\n",
      "Gradient Descent(766/999): loss=0.11757225179486305\n",
      "Gradient Descent(767/999): loss=0.11756620134562727\n",
      "Gradient Descent(768/999): loss=0.11756015749500057\n",
      "Gradient Descent(769/999): loss=0.11755412023337074\n",
      "Gradient Descent(770/999): loss=0.11754808955114175\n",
      "Gradient Descent(771/999): loss=0.11754206543873343\n",
      "Gradient Descent(772/999): loss=0.11753604788658159\n",
      "Gradient Descent(773/999): loss=0.1175300368851381\n",
      "Gradient Descent(774/999): loss=0.11752403242487061\n",
      "Gradient Descent(775/999): loss=0.11751803449626277\n",
      "Gradient Descent(776/999): loss=0.11751204308981401\n",
      "Gradient Descent(777/999): loss=0.11750605819603967\n",
      "Gradient Descent(778/999): loss=0.11750007980547077\n",
      "Gradient Descent(779/999): loss=0.11749410790865424\n",
      "Gradient Descent(780/999): loss=0.11748814249615265\n",
      "Gradient Descent(781/999): loss=0.11748218355854441\n",
      "Gradient Descent(782/999): loss=0.11747623108642348\n",
      "Gradient Descent(783/999): loss=0.11747028507039958\n",
      "Gradient Descent(784/999): loss=0.11746434550109805\n",
      "Gradient Descent(785/999): loss=0.11745841236915976\n",
      "Gradient Descent(786/999): loss=0.11745248566524127\n",
      "Gradient Descent(787/999): loss=0.11744656538001463\n",
      "Gradient Descent(788/999): loss=0.11744065150416742\n",
      "Gradient Descent(789/999): loss=0.11743474402840275\n",
      "Gradient Descent(790/999): loss=0.1174288429434391\n",
      "Gradient Descent(791/999): loss=0.11742294824001052\n",
      "Gradient Descent(792/999): loss=0.1174170599088664\n",
      "Gradient Descent(793/999): loss=0.1174111779407715\n",
      "Gradient Descent(794/999): loss=0.11740530232650595\n",
      "Gradient Descent(795/999): loss=0.11739943305686529\n",
      "Gradient Descent(796/999): loss=0.11739357012266025\n",
      "Gradient Descent(797/999): loss=0.1173877135147169\n",
      "Gradient Descent(798/999): loss=0.11738186322387653\n",
      "Gradient Descent(799/999): loss=0.11737601924099567\n",
      "Gradient Descent(800/999): loss=0.11737018155694601\n",
      "Gradient Descent(801/999): loss=0.1173643501626145\n",
      "Gradient Descent(802/999): loss=0.11735852504890314\n",
      "Gradient Descent(803/999): loss=0.11735270620672904\n",
      "Gradient Descent(804/999): loss=0.11734689362702441\n",
      "Gradient Descent(805/999): loss=0.1173410873007366\n",
      "Gradient Descent(806/999): loss=0.11733528721882788\n",
      "Gradient Descent(807/999): loss=0.1173294933722756\n",
      "Gradient Descent(808/999): loss=0.11732370575207204\n",
      "Gradient Descent(809/999): loss=0.1173179243492245\n",
      "Gradient Descent(810/999): loss=0.1173121491547551\n",
      "Gradient Descent(811/999): loss=0.11730638015970096\n",
      "Gradient Descent(812/999): loss=0.11730061735511402\n",
      "Gradient Descent(813/999): loss=0.11729486073206108\n",
      "Gradient Descent(814/999): loss=0.11728911028162375\n",
      "Gradient Descent(815/999): loss=0.11728336599489851\n",
      "Gradient Descent(816/999): loss=0.11727762786299646\n",
      "Gradient Descent(817/999): loss=0.11727189587704354\n",
      "Gradient Descent(818/999): loss=0.11726617002818042\n",
      "Gradient Descent(819/999): loss=0.11726045030756238\n",
      "Gradient Descent(820/999): loss=0.11725473670635944\n",
      "Gradient Descent(821/999): loss=0.11724902921575626\n",
      "Gradient Descent(822/999): loss=0.11724332782695202\n",
      "Gradient Descent(823/999): loss=0.11723763253116057\n",
      "Gradient Descent(824/999): loss=0.11723194331961023\n",
      "Gradient Descent(825/999): loss=0.117226260183544\n",
      "Gradient Descent(826/999): loss=0.11722058311421925\n",
      "Gradient Descent(827/999): loss=0.1172149121029079\n",
      "Gradient Descent(828/999): loss=0.11720924714089631\n",
      "Gradient Descent(829/999): loss=0.11720358821948526\n",
      "Gradient Descent(830/999): loss=0.11719793532998991\n",
      "Gradient Descent(831/999): loss=0.11719228846373986\n",
      "Gradient Descent(832/999): loss=0.117186647612079\n",
      "Gradient Descent(833/999): loss=0.11718101276636567\n",
      "Gradient Descent(834/999): loss=0.11717538391797232\n",
      "Gradient Descent(835/999): loss=0.11716976105828583\n",
      "Gradient Descent(836/999): loss=0.11716414417870728\n",
      "Gradient Descent(837/999): loss=0.11715853327065193\n",
      "Gradient Descent(838/999): loss=0.11715292832554935\n",
      "Gradient Descent(839/999): loss=0.11714732933484322\n",
      "Gradient Descent(840/999): loss=0.1171417362899913\n",
      "Gradient Descent(841/999): loss=0.11713614918246566\n",
      "Gradient Descent(842/999): loss=0.11713056800375224\n",
      "Gradient Descent(843/999): loss=0.1171249927453513\n",
      "Gradient Descent(844/999): loss=0.11711942339877694\n",
      "Gradient Descent(845/999): loss=0.1171138599555574\n",
      "Gradient Descent(846/999): loss=0.1171083024072349\n",
      "Gradient Descent(847/999): loss=0.11710275074536557\n",
      "Gradient Descent(848/999): loss=0.11709720496151961\n",
      "Gradient Descent(849/999): loss=0.11709166504728112\n",
      "Gradient Descent(850/999): loss=0.11708613099424799\n",
      "Gradient Descent(851/999): loss=0.11708060279403207\n",
      "Gradient Descent(852/999): loss=0.11707508043825912\n",
      "Gradient Descent(853/999): loss=0.1170695639185686\n",
      "Gradient Descent(854/999): loss=0.1170640532266139\n",
      "Gradient Descent(855/999): loss=0.11705854835406207\n",
      "Gradient Descent(856/999): loss=0.117053049292594\n",
      "Gradient Descent(857/999): loss=0.1170475560339043\n",
      "Gradient Descent(858/999): loss=0.1170420685697012\n",
      "Gradient Descent(859/999): loss=0.11703658689170675\n",
      "Gradient Descent(860/999): loss=0.11703111099165658\n",
      "Gradient Descent(861/999): loss=0.11702564086129992\n",
      "Gradient Descent(862/999): loss=0.1170201764923997\n",
      "Gradient Descent(863/999): loss=0.11701471787673233\n",
      "Gradient Descent(864/999): loss=0.11700926500608788\n",
      "Gradient Descent(865/999): loss=0.11700381787226985\n",
      "Gradient Descent(866/999): loss=0.1169983764670954\n",
      "Gradient Descent(867/999): loss=0.11699294078239507\n",
      "Gradient Descent(868/999): loss=0.11698751081001284\n",
      "Gradient Descent(869/999): loss=0.1169820865418062\n",
      "Gradient Descent(870/999): loss=0.11697666796964608\n",
      "Gradient Descent(871/999): loss=0.11697125508541671\n",
      "Gradient Descent(872/999): loss=0.11696584788101577\n",
      "Gradient Descent(873/999): loss=0.11696044634835427\n",
      "Gradient Descent(874/999): loss=0.1169550504793565\n",
      "Gradient Descent(875/999): loss=0.11694966026596007\n",
      "Gradient Descent(876/999): loss=0.11694427570011591\n",
      "Gradient Descent(877/999): loss=0.11693889677378815\n",
      "Gradient Descent(878/999): loss=0.11693352347895418\n",
      "Gradient Descent(879/999): loss=0.11692815580760459\n",
      "Gradient Descent(880/999): loss=0.11692279375174311\n",
      "Gradient Descent(881/999): loss=0.1169174373033867\n",
      "Gradient Descent(882/999): loss=0.11691208645456538\n",
      "Gradient Descent(883/999): loss=0.11690674119732235\n",
      "Gradient Descent(884/999): loss=0.11690140152371385\n",
      "Gradient Descent(885/999): loss=0.11689606742580924\n",
      "Gradient Descent(886/999): loss=0.11689073889569088\n",
      "Gradient Descent(887/999): loss=0.11688541592545412\n",
      "Gradient Descent(888/999): loss=0.11688009850720733\n",
      "Gradient Descent(889/999): loss=0.11687478663307199\n",
      "Gradient Descent(890/999): loss=0.11686948029518225\n",
      "Gradient Descent(891/999): loss=0.11686417948568546\n",
      "Gradient Descent(892/999): loss=0.11685888419674174\n",
      "Gradient Descent(893/999): loss=0.11685359442052409\n",
      "Gradient Descent(894/999): loss=0.11684831014921843\n",
      "Gradient Descent(895/999): loss=0.11684303137502343\n",
      "Gradient Descent(896/999): loss=0.11683775809015066\n",
      "Gradient Descent(897/999): loss=0.11683249028682445\n",
      "Gradient Descent(898/999): loss=0.11682722795728191\n",
      "Gradient Descent(899/999): loss=0.11682197109377286\n",
      "Gradient Descent(900/999): loss=0.11681671968855985\n",
      "Gradient Descent(901/999): loss=0.11681147373391822\n",
      "Gradient Descent(902/999): loss=0.11680623322213582\n",
      "Gradient Descent(903/999): loss=0.11680099814551333\n",
      "Gradient Descent(904/999): loss=0.11679576849636394\n",
      "Gradient Descent(905/999): loss=0.11679054426701353\n",
      "Gradient Descent(906/999): loss=0.11678532544980055\n",
      "Gradient Descent(907/999): loss=0.116780112037076\n",
      "Gradient Descent(908/999): loss=0.11677490402120339\n",
      "Gradient Descent(909/999): loss=0.11676970139455888\n",
      "Gradient Descent(910/999): loss=0.11676450414953096\n",
      "Gradient Descent(911/999): loss=0.11675931227852074\n",
      "Gradient Descent(912/999): loss=0.11675412577394172\n",
      "Gradient Descent(913/999): loss=0.11674894462821986\n",
      "Gradient Descent(914/999): loss=0.11674376883379353\n",
      "Gradient Descent(915/999): loss=0.11673859838311341\n",
      "Gradient Descent(916/999): loss=0.11673343326864273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(917/999): loss=0.11672827348285693\n",
      "Gradient Descent(918/999): loss=0.11672311901824375\n",
      "Gradient Descent(919/999): loss=0.11671796986730333\n",
      "Gradient Descent(920/999): loss=0.11671282602254808\n",
      "Gradient Descent(921/999): loss=0.11670768747650262\n",
      "Gradient Descent(922/999): loss=0.11670255422170378\n",
      "Gradient Descent(923/999): loss=0.11669742625070074\n",
      "Gradient Descent(924/999): loss=0.11669230355605477\n",
      "Gradient Descent(925/999): loss=0.11668718613033935\n",
      "Gradient Descent(926/999): loss=0.11668207396614008\n",
      "Gradient Descent(927/999): loss=0.11667696705605474\n",
      "Gradient Descent(928/999): loss=0.11667186539269317\n",
      "Gradient Descent(929/999): loss=0.11666676896867734\n",
      "Gradient Descent(930/999): loss=0.11666167777664128\n",
      "Gradient Descent(931/999): loss=0.11665659180923107\n",
      "Gradient Descent(932/999): loss=0.11665151105910476\n",
      "Gradient Descent(933/999): loss=0.11664643551893249\n",
      "Gradient Descent(934/999): loss=0.11664136518139631\n",
      "Gradient Descent(935/999): loss=0.11663630003919032\n",
      "Gradient Descent(936/999): loss=0.11663124008502042\n",
      "Gradient Descent(937/999): loss=0.11662618531160454\n",
      "Gradient Descent(938/999): loss=0.11662113571167254\n",
      "Gradient Descent(939/999): loss=0.11661609127796604\n",
      "Gradient Descent(940/999): loss=0.1166110520032386\n",
      "Gradient Descent(941/999): loss=0.11660601788025558\n",
      "Gradient Descent(942/999): loss=0.11660098890179418\n",
      "Gradient Descent(943/999): loss=0.11659596506064338\n",
      "Gradient Descent(944/999): loss=0.11659094634960393\n",
      "Gradient Descent(945/999): loss=0.1165859327614883\n",
      "Gradient Descent(946/999): loss=0.1165809242891208\n",
      "Gradient Descent(947/999): loss=0.11657592092533733\n",
      "Gradient Descent(948/999): loss=0.11657092266298553\n",
      "Gradient Descent(949/999): loss=0.11656592949492477\n",
      "Gradient Descent(950/999): loss=0.11656094141402597\n",
      "Gradient Descent(951/999): loss=0.11655595841317172\n",
      "Gradient Descent(952/999): loss=0.11655098048525621\n",
      "Gradient Descent(953/999): loss=0.11654600762318526\n",
      "Gradient Descent(954/999): loss=0.1165410398198762\n",
      "Gradient Descent(955/999): loss=0.11653607706825794\n",
      "Gradient Descent(956/999): loss=0.11653111936127092\n",
      "Gradient Descent(957/999): loss=0.11652616669186705\n",
      "Gradient Descent(958/999): loss=0.11652121905300981\n",
      "Gradient Descent(959/999): loss=0.11651627643767405\n",
      "Gradient Descent(960/999): loss=0.1165113388388461\n",
      "Gradient Descent(961/999): loss=0.11650640624952374\n",
      "Gradient Descent(962/999): loss=0.11650147866271615\n",
      "Gradient Descent(963/999): loss=0.11649655607144387\n",
      "Gradient Descent(964/999): loss=0.11649163846873883\n",
      "Gradient Descent(965/999): loss=0.11648672584764433\n",
      "Gradient Descent(966/999): loss=0.1164818182012149\n",
      "Gradient Descent(967/999): loss=0.11647691552251646\n",
      "Gradient Descent(968/999): loss=0.11647201780462624\n",
      "Gradient Descent(969/999): loss=0.11646712504063268\n",
      "Gradient Descent(970/999): loss=0.11646223722363543\n",
      "Gradient Descent(971/999): loss=0.11645735434674548\n",
      "Gradient Descent(972/999): loss=0.11645247640308494\n",
      "Gradient Descent(973/999): loss=0.11644760338578711\n",
      "Gradient Descent(974/999): loss=0.11644273528799652\n",
      "Gradient Descent(975/999): loss=0.1164378721028688\n",
      "Gradient Descent(976/999): loss=0.1164330138235707\n",
      "Gradient Descent(977/999): loss=0.11642816044328012\n",
      "Gradient Descent(978/999): loss=0.116423311955186\n",
      "Gradient Descent(979/999): loss=0.11641846835248838\n",
      "Gradient Descent(980/999): loss=0.11641362962839838\n",
      "Gradient Descent(981/999): loss=0.11640879577613811\n",
      "Gradient Descent(982/999): loss=0.11640396678894072\n",
      "Gradient Descent(983/999): loss=0.11639914266005026\n",
      "Gradient Descent(984/999): loss=0.11639432338272189\n",
      "Gradient Descent(985/999): loss=0.11638950895022165\n",
      "Gradient Descent(986/999): loss=0.1163846993558265\n",
      "Gradient Descent(987/999): loss=0.11637989459282436\n",
      "Gradient Descent(988/999): loss=0.11637509465451402\n",
      "Gradient Descent(989/999): loss=0.11637029953420522\n",
      "Gradient Descent(990/999): loss=0.1163655092252184\n",
      "Gradient Descent(991/999): loss=0.11636072372088503\n",
      "Gradient Descent(992/999): loss=0.11635594301454716\n",
      "Gradient Descent(993/999): loss=0.11635116709955795\n",
      "Gradient Descent(994/999): loss=0.11634639596928106\n",
      "Gradient Descent(995/999): loss=0.11634162961709112\n",
      "Gradient Descent(996/999): loss=0.11633686803637332\n",
      "Gradient Descent(997/999): loss=0.11633211122052373\n",
      "Gradient Descent(998/999): loss=0.11632735916294906\n",
      "Gradient Descent(999/999): loss=0.11632261185706673\n",
      "Gradient Descent(0/999): loss=0.23409482421110328\n",
      "Gradient Descent(1/999): loss=0.22268728515151393\n",
      "Gradient Descent(2/999): loss=0.21247076730859876\n",
      "Gradient Descent(3/999): loss=0.20332072804792875\n",
      "Gradient Descent(4/999): loss=0.19512564864397453\n",
      "Gradient Descent(5/999): loss=0.18778567230875923\n",
      "Gradient Descent(6/999): loss=0.18121138464845063\n",
      "Gradient Descent(7/999): loss=0.17532272165351004\n",
      "Gradient Descent(8/999): loss=0.17004799188559228\n",
      "Gradient Descent(9/999): loss=0.16532300091908717\n",
      "Gradient Descent(10/999): loss=0.16109026734403603\n",
      "Gradient Descent(11/999): loss=0.15729832075540587\n",
      "Gradient Descent(12/999): loss=0.15390107315500726\n",
      "Gradient Descent(13/999): loss=0.15085725608894024\n",
      "Gradient Descent(14/999): loss=0.14812991664628292\n",
      "Gradient Descent(15/999): loss=0.14568596616361756\n",
      "Gradient Descent(16/999): loss=0.14349577612368733\n",
      "Gradient Descent(17/999): loss=0.14153281631286516\n",
      "Gradient Descent(18/999): loss=0.1397733308182246\n",
      "Gradient Descent(19/999): loss=0.1381960479071421\n",
      "Gradient Descent(20/999): loss=0.13678192024616972\n",
      "Gradient Descent(21/999): loss=0.13551389228645355\n",
      "Gradient Descent(22/999): loss=0.1343766919747603\n",
      "Gradient Descent(23/999): loss=0.13335664424626517\n",
      "Gradient Descent(24/999): loss=0.1324415040212771\n",
      "Gradient Descent(25/999): loss=0.13162030666627944\n",
      "Gradient Descent(26/999): loss=0.1308832340929586\n",
      "Gradient Descent(27/999): loss=0.1302214948598804\n",
      "Gradient Descent(28/999): loss=0.12962721681248981\n",
      "Gradient Descent(29/999): loss=0.12909335095024044\n",
      "Gradient Descent(30/999): loss=0.1286135853467792\n",
      "Gradient Descent(31/999): loss=0.12818226807188965\n",
      "Gradient Descent(32/999): loss=0.12779433817383654\n",
      "Gradient Descent(33/999): loss=0.12744526387919636\n",
      "Gradient Descent(34/999): loss=0.1271309872554079\n",
      "Gradient Descent(35/999): loss=0.12684787466020306\n",
      "Gradient Descent(36/999): loss=0.12659267237275879\n",
      "Gradient Descent(37/999): loss=0.12636246686469038\n",
      "Gradient Descent(38/999): loss=0.12615464922567723\n",
      "Gradient Descent(39/999): loss=0.12596688330925082\n",
      "Gradient Descent(40/999): loss=0.12579707720971003\n",
      "Gradient Descent(41/999): loss=0.1256433577218118\n",
      "Gradient Descent(42/999): loss=0.12550404747131444\n",
      "Gradient Descent(43/999): loss=0.1253776444370699\n",
      "Gradient Descent(44/999): loss=0.12526280361456948\n",
      "Gradient Descent(45/999): loss=0.1251583205970008\n",
      "Gradient Descent(46/999): loss=0.1250631168732938\n",
      "Gradient Descent(47/999): loss=0.12497622666360124\n",
      "Gradient Descent(48/999): loss=0.12489678513143773\n",
      "Gradient Descent(49/999): loss=0.12482401782851375\n",
      "Gradient Descent(50/999): loss=0.12475723124335562\n",
      "Gradient Descent(51/999): loss=0.12469580433828421\n",
      "Gradient Descent(52/999): loss=0.12463918097139451\n",
      "Gradient Descent(53/999): loss=0.12458686311098834\n",
      "Gradient Descent(54/999): loss=0.124538404759589\n",
      "Gradient Descent(55/999): loss=0.12449340651333485\n",
      "Gradient Descent(56/999): loss=0.12445151069030612\n",
      "Gradient Descent(57/999): loss=0.12441239696829046\n",
      "Gradient Descent(58/999): loss=0.12437577847871213\n",
      "Gradient Descent(59/999): loss=0.12434139830902258\n",
      "Gradient Descent(60/999): loss=0.12430902637083699\n",
      "Gradient Descent(61/999): loss=0.12427845659557071\n",
      "Gradient Descent(62/999): loss=0.12424950442332594\n",
      "Gradient Descent(63/999): loss=0.1242220045543636\n",
      "Gradient Descent(64/999): loss=0.12419580893570022\n",
      "Gradient Descent(65/999): loss=0.12417078495824244\n",
      "Gradient Descent(66/999): loss=0.1241468138424417\n",
      "Gradient Descent(67/999): loss=0.12412378919275581\n",
      "Gradient Descent(68/999): loss=0.12410161570326401\n",
      "Gradient Descent(69/999): loss=0.12408020799862933\n",
      "Gradient Descent(70/999): loss=0.12405948959625433\n",
      "Gradient Descent(71/999): loss=0.12403939197695675\n",
      "Gradient Descent(72/999): loss=0.12401985375281713\n",
      "Gradient Descent(73/999): loss=0.12400081992203632\n",
      "Gradient Descent(74/999): loss=0.1239822412017047\n",
      "Gradient Descent(75/999): loss=0.12396407343033515\n",
      "Gradient Descent(76/999): loss=0.123946277032865\n",
      "Gradient Descent(77/999): loss=0.12392881654159409\n",
      "Gradient Descent(78/999): loss=0.1239116601672098\n",
      "Gradient Descent(79/999): loss=0.12389477941466152\n",
      "Gradient Descent(80/999): loss=0.1238781487391943\n",
      "Gradient Descent(81/999): loss=0.12386174523834273\n",
      "Gradient Descent(82/999): loss=0.12384554837612448\n",
      "Gradient Descent(83/999): loss=0.12382953973606603\n",
      "Gradient Descent(84/999): loss=0.12381370280004653\n",
      "Gradient Descent(85/999): loss=0.1237980227502591\n",
      "Gradient Descent(86/999): loss=0.12378248629187324\n",
      "Gradient Descent(87/999): loss=0.12376708149423256\n",
      "Gradient Descent(88/999): loss=0.12375179764865073\n",
      "Gradient Descent(89/999): loss=0.12373662514106938\n",
      "Gradient Descent(90/999): loss=0.12372155533802477\n",
      "Gradient Descent(91/999): loss=0.12370658048452997\n",
      "Gradient Descent(92/999): loss=0.12369169361262883\n",
      "Gradient Descent(93/999): loss=0.1236768884595042\n",
      "Gradient Descent(94/999): loss=0.12366215939414221\n",
      "Gradient Descent(95/999): loss=0.12364750135165777\n",
      "Gradient Descent(96/999): loss=0.12363290977448044\n",
      "Gradient Descent(97/999): loss=0.12361838055968317\n",
      "Gradient Descent(98/999): loss=0.12360391001181191\n",
      "Gradient Descent(99/999): loss=0.12358949480064073\n",
      "Gradient Descent(100/999): loss=0.12357513192333776\n",
      "Gradient Descent(101/999): loss=0.12356081867058064\n",
      "Gradient Descent(102/999): loss=0.12354655259620866\n",
      "Gradient Descent(103/999): loss=0.12353233149004184\n",
      "Gradient Descent(104/999): loss=0.123518153353536\n",
      "Gradient Descent(105/999): loss=0.12350401637797724\n",
      "Gradient Descent(106/999): loss=0.12348991892495056\n",
      "Gradient Descent(107/999): loss=0.12347585950884463\n",
      "Gradient Descent(108/999): loss=0.1234618367811802\n",
      "Gradient Descent(109/999): loss=0.12344784951657162\n",
      "Gradient Descent(110/999): loss=0.12343389660015035\n",
      "Gradient Descent(111/999): loss=0.12341997701629799\n",
      "Gradient Descent(112/999): loss=0.12340608983855235\n",
      "Gradient Descent(113/999): loss=0.12339223422056309\n",
      "Gradient Descent(114/999): loss=0.12337840938798832\n",
      "Gradient Descent(115/999): loss=0.12336461463123288\n",
      "Gradient Descent(116/999): loss=0.12335084929894137\n",
      "Gradient Descent(117/999): loss=0.12333711279216598\n",
      "Gradient Descent(118/999): loss=0.12332340455914002\n",
      "Gradient Descent(119/999): loss=0.12330972409059242\n",
      "Gradient Descent(120/999): loss=0.12329607091554795\n",
      "Gradient Descent(121/999): loss=0.12328244459756164\n",
      "Gradient Descent(122/999): loss=0.12326884473134281\n",
      "Gradient Descent(123/999): loss=0.12325527093972712\n",
      "Gradient Descent(124/999): loss=0.12324172287096122\n",
      "Gradient Descent(125/999): loss=0.12322820019626718\n",
      "Gradient Descent(126/999): loss=0.12321470260765681\n",
      "Gradient Descent(127/999): loss=0.1232012298159711\n",
      "Gradient Descent(128/999): loss=0.12318778154912005\n",
      "Gradient Descent(129/999): loss=0.12317435755050289\n",
      "Gradient Descent(130/999): loss=0.12316095757758956\n",
      "Gradient Descent(131/999): loss=0.12314758140064656\n",
      "Gradient Descent(132/999): loss=0.12313422880159237\n",
      "Gradient Descent(133/999): loss=0.12312089957296897\n",
      "Gradient Descent(134/999): loss=0.12310759351701715\n",
      "Gradient Descent(135/999): loss=0.12309431044484494\n",
      "Gradient Descent(136/999): loss=0.12308105017567976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(137/999): loss=0.12306781253619518\n",
      "Gradient Descent(138/999): loss=0.12305459735990469\n",
      "Gradient Descent(139/999): loss=0.12304140448661578\n",
      "Gradient Descent(140/999): loss=0.12302823376193774\n",
      "Gradient Descent(141/999): loss=0.12301508503683789\n",
      "Gradient Descent(142/999): loss=0.12300195816724101\n",
      "Gradient Descent(143/999): loss=0.12298885301366776\n",
      "Gradient Descent(144/999): loss=0.12297576944090773\n",
      "Gradient Descent(145/999): loss=0.12296270731772417\n",
      "Gradient Descent(146/999): loss=0.12294966651658623\n",
      "Gradient Descent(147/999): loss=0.12293664691342683\n",
      "Gradient Descent(148/999): loss=0.12292364838742277\n",
      "Gradient Descent(149/999): loss=0.12291067082079545\n",
      "Gradient Descent(150/999): loss=0.12289771409862955\n",
      "Gradient Descent(151/999): loss=0.12288477810870826\n",
      "Gradient Descent(152/999): loss=0.12287186274136297\n",
      "Gradient Descent(153/999): loss=0.12285896788933645\n",
      "Gradient Descent(154/999): loss=0.12284609344765776\n",
      "Gradient Descent(155/999): loss=0.12283323931352787\n",
      "Gradient Descent(156/999): loss=0.12282040538621479\n",
      "Gradient Descent(157/999): loss=0.1228075915669577\n",
      "Gradient Descent(158/999): loss=0.12279479775887837\n",
      "Gradient Descent(159/999): loss=0.12278202386690001\n",
      "Gradient Descent(160/999): loss=0.12276926979767228\n",
      "Gradient Descent(161/999): loss=0.12275653545950208\n",
      "Gradient Descent(162/999): loss=0.12274382076228929\n",
      "Gradient Descent(163/999): loss=0.12273112561746759\n",
      "Gradient Descent(164/999): loss=0.12271844993794899\n",
      "Gradient Descent(165/999): loss=0.12270579363807262\n",
      "Gradient Descent(166/999): loss=0.12269315663355655\n",
      "Gradient Descent(167/999): loss=0.12268053884145312\n",
      "Gradient Descent(168/999): loss=0.12266794018010685\n",
      "Gradient Descent(169/999): loss=0.12265536056911493\n",
      "Gradient Descent(170/999): loss=0.12264279992929038\n",
      "Gradient Descent(171/999): loss=0.12263025818262675\n",
      "Gradient Descent(172/999): loss=0.12261773525226537\n",
      "Gradient Descent(173/999): loss=0.12260523106246392\n",
      "Gradient Descent(174/999): loss=0.12259274553856692\n",
      "Gradient Descent(175/999): loss=0.12258027860697751\n",
      "Gradient Descent(176/999): loss=0.12256783019513073\n",
      "Gradient Descent(177/999): loss=0.12255540023146792\n",
      "Gradient Descent(178/999): loss=0.12254298864541227\n",
      "Gradient Descent(179/999): loss=0.12253059536734565\n",
      "Gradient Descent(180/999): loss=0.12251822032858604\n",
      "Gradient Descent(181/999): loss=0.12250586346136627\n",
      "Gradient Descent(182/999): loss=0.12249352469881324\n",
      "Gradient Descent(183/999): loss=0.12248120397492814\n",
      "Gradient Descent(184/999): loss=0.1224689012245673\n",
      "Gradient Descent(185/999): loss=0.12245661638342371\n",
      "Gradient Descent(186/999): loss=0.12244434938800926\n",
      "Gradient Descent(187/999): loss=0.12243210017563741\n",
      "Gradient Descent(188/999): loss=0.12241986868440653\n",
      "Gradient Descent(189/999): loss=0.12240765485318376\n",
      "Gradient Descent(190/999): loss=0.12239545862158925\n",
      "Gradient Descent(191/999): loss=0.12238327992998096\n",
      "Gradient Descent(192/999): loss=0.12237111871943991\n",
      "Gradient Descent(193/999): loss=0.12235897493175575\n",
      "Gradient Descent(194/999): loss=0.12234684850941265\n",
      "Gradient Descent(195/999): loss=0.12233473939557597\n",
      "Gradient Descent(196/999): loss=0.1223226475340787\n",
      "Gradient Descent(197/999): loss=0.12231057286940865\n",
      "Gradient Descent(198/999): loss=0.12229851534669586\n",
      "Gradient Descent(199/999): loss=0.12228647491170022\n",
      "Gradient Descent(200/999): loss=0.12227445151079958\n",
      "Gradient Descent(201/999): loss=0.12226244509097788\n",
      "Gradient Descent(202/999): loss=0.1222504555998138\n",
      "Gradient Descent(203/999): loss=0.12223848298546958\n",
      "Gradient Descent(204/999): loss=0.12222652719668\n",
      "Gradient Descent(205/999): loss=0.1222145881827418\n",
      "Gradient Descent(206/999): loss=0.12220266589350318\n",
      "Gradient Descent(207/999): loss=0.12219076027935352\n",
      "Gradient Descent(208/999): loss=0.12217887129121348\n",
      "Gradient Descent(209/999): loss=0.12216699888052517\n",
      "Gradient Descent(210/999): loss=0.12215514299924254\n",
      "Gradient Descent(211/999): loss=0.12214330359982213\n",
      "Gradient Descent(212/999): loss=0.12213148063521362\n",
      "Gradient Descent(213/999): loss=0.12211967405885123\n",
      "Gradient Descent(214/999): loss=0.12210788382464453\n",
      "Gradient Descent(215/999): loss=0.12209610988697017\n",
      "Gradient Descent(216/999): loss=0.12208435220066315\n",
      "Gradient Descent(217/999): loss=0.12207261072100874\n",
      "Gradient Descent(218/999): loss=0.12206088540373425\n",
      "Gradient Descent(219/999): loss=0.12204917620500118\n",
      "Gradient Descent(220/999): loss=0.12203748308139742\n",
      "Gradient Descent(221/999): loss=0.1220258059899296\n",
      "Gradient Descent(222/999): loss=0.12201414488801567\n",
      "Gradient Descent(223/999): loss=0.12200249973347746\n",
      "Gradient Descent(224/999): loss=0.12199087048453372\n",
      "Gradient Descent(225/999): loss=0.1219792570997928\n",
      "Gradient Descent(226/999): loss=0.1219676595382461\n",
      "Gradient Descent(227/999): loss=0.12195607775926093\n",
      "Gradient Descent(228/999): loss=0.12194451172257414\n",
      "Gradient Descent(229/999): loss=0.12193296138828566\n",
      "Gradient Descent(230/999): loss=0.12192142671685192\n",
      "Gradient Descent(231/999): loss=0.12190990766907976\n",
      "Gradient Descent(232/999): loss=0.12189840420612028\n",
      "Gradient Descent(233/999): loss=0.12188691628946279\n",
      "Gradient Descent(234/999): loss=0.12187544388092901\n",
      "Gradient Descent(235/999): loss=0.1218639869426672\n",
      "Gradient Descent(236/999): loss=0.12185254543714662\n",
      "Gradient Descent(237/999): loss=0.1218411193271519\n",
      "Gradient Descent(238/999): loss=0.12182970857577764\n",
      "Gradient Descent(239/999): loss=0.12181831314642298\n",
      "Gradient Descent(240/999): loss=0.12180693300278655\n",
      "Gradient Descent(241/999): loss=0.12179556810886118\n",
      "Gradient Descent(242/999): loss=0.1217842184289289\n",
      "Gradient Descent(243/999): loss=0.12177288392755614\n",
      "Gradient Descent(244/999): loss=0.12176156456958874\n",
      "Gradient Descent(245/999): loss=0.1217502603201472\n",
      "Gradient Descent(246/999): loss=0.1217389711446222\n",
      "Gradient Descent(247/999): loss=0.12172769700866982\n",
      "Gradient Descent(248/999): loss=0.1217164378782072\n",
      "Gradient Descent(249/999): loss=0.12170519371940816\n",
      "Gradient Descent(250/999): loss=0.1216939644986988\n",
      "Gradient Descent(251/999): loss=0.12168275018275342\n",
      "Gradient Descent(252/999): loss=0.12167155073849026\n",
      "Gradient Descent(253/999): loss=0.12166036613306742\n",
      "Gradient Descent(254/999): loss=0.12164919633387904\n",
      "Gradient Descent(255/999): loss=0.12163804130855121\n",
      "Gradient Descent(256/999): loss=0.12162690102493837\n",
      "Gradient Descent(257/999): loss=0.12161577545111918\n",
      "Gradient Descent(258/999): loss=0.12160466455539326\n",
      "Gradient Descent(259/999): loss=0.12159356830627731\n",
      "Gradient Descent(260/999): loss=0.12158248667250163\n",
      "Gradient Descent(261/999): loss=0.12157141962300672\n",
      "Gradient Descent(262/999): loss=0.12156036712693971\n",
      "Gradient Descent(263/999): loss=0.12154932915365114\n",
      "Gradient Descent(264/999): loss=0.1215383056726917\n",
      "Gradient Descent(265/999): loss=0.1215272966538089\n",
      "Gradient Descent(266/999): loss=0.12151630206694401\n",
      "Gradient Descent(267/999): loss=0.1215053218822289\n",
      "Gradient Descent(268/999): loss=0.12149435606998313\n",
      "Gradient Descent(269/999): loss=0.12148340460071076\n",
      "Gradient Descent(270/999): loss=0.12147246744509756\n",
      "Gradient Descent(271/999): loss=0.12146154457400823\n",
      "Gradient Descent(272/999): loss=0.12145063595848339\n",
      "Gradient Descent(273/999): loss=0.12143974156973697\n",
      "Gradient Descent(274/999): loss=0.12142886137915347\n",
      "Gradient Descent(275/999): loss=0.12141799535828526\n",
      "Gradient Descent(276/999): loss=0.12140714347885002\n",
      "Gradient Descent(277/999): loss=0.12139630571272822\n",
      "Gradient Descent(278/999): loss=0.12138548203196055\n",
      "Gradient Descent(279/999): loss=0.12137467240874548\n",
      "Gradient Descent(280/999): loss=0.1213638768154369\n",
      "Gradient Descent(281/999): loss=0.12135309522454164\n",
      "Gradient Descent(282/999): loss=0.12134232760871737\n",
      "Gradient Descent(283/999): loss=0.12133157394077\n",
      "Gradient Descent(284/999): loss=0.12132083419365176\n",
      "Gradient Descent(285/999): loss=0.12131010834045888\n",
      "Gradient Descent(286/999): loss=0.1212993963544294\n",
      "Gradient Descent(287/999): loss=0.12128869820894114\n",
      "Gradient Descent(288/999): loss=0.12127801387750956\n",
      "Gradient Descent(289/999): loss=0.12126734333378579\n",
      "Gradient Descent(290/999): loss=0.12125668655155462\n",
      "Gradient Descent(291/999): loss=0.12124604350473263\n",
      "Gradient Descent(292/999): loss=0.12123541416736604\n",
      "Gradient Descent(293/999): loss=0.12122479851362915\n",
      "Gradient Descent(294/999): loss=0.12121419651782221\n",
      "Gradient Descent(295/999): loss=0.1212036081543698\n",
      "Gradient Descent(296/999): loss=0.12119303339781892\n",
      "Gradient Descent(297/999): loss=0.12118247222283747\n",
      "Gradient Descent(298/999): loss=0.1211719246042122\n",
      "Gradient Descent(299/999): loss=0.12116139051684735\n",
      "Gradient Descent(300/999): loss=0.12115086993576288\n",
      "Gradient Descent(301/999): loss=0.12114036283609278\n",
      "Gradient Descent(302/999): loss=0.12112986919308359\n",
      "Gradient Descent(303/999): loss=0.12111938898209292\n",
      "Gradient Descent(304/999): loss=0.12110892217858765\n",
      "Gradient Descent(305/999): loss=0.12109846875814273\n",
      "Gradient Descent(306/999): loss=0.12108802869643959\n",
      "Gradient Descent(307/999): loss=0.12107760196926463\n",
      "Gradient Descent(308/999): loss=0.12106718855250795\n",
      "Gradient Descent(309/999): loss=0.12105678842216186\n",
      "Gradient Descent(310/999): loss=0.12104640155431952\n",
      "Gradient Descent(311/999): loss=0.12103602792517375\n",
      "Gradient Descent(312/999): loss=0.12102566751101541\n",
      "Gradient Descent(313/999): loss=0.12101532028823249\n",
      "Gradient Descent(314/999): loss=0.12100498623330856\n",
      "Gradient Descent(315/999): loss=0.12099466532282167\n",
      "Gradient Descent(316/999): loss=0.12098435753344312\n",
      "Gradient Descent(317/999): loss=0.12097406284193622\n",
      "Gradient Descent(318/999): loss=0.12096378122515515\n",
      "Gradient Descent(319/999): loss=0.12095351266004381\n",
      "Gradient Descent(320/999): loss=0.12094325712363467\n",
      "Gradient Descent(321/999): loss=0.12093301459304774\n",
      "Gradient Descent(322/999): loss=0.12092278504548931\n",
      "Gradient Descent(323/999): loss=0.12091256845825119\n",
      "Gradient Descent(324/999): loss=0.12090236480870928\n",
      "Gradient Descent(325/999): loss=0.1208921740743228\n",
      "Gradient Descent(326/999): loss=0.12088199623263332\n",
      "Gradient Descent(327/999): loss=0.12087183126126348\n",
      "Gradient Descent(328/999): loss=0.12086167913791633\n",
      "Gradient Descent(329/999): loss=0.12085153984037424\n",
      "Gradient Descent(330/999): loss=0.12084141334649792\n",
      "Gradient Descent(331/999): loss=0.12083129963422556\n",
      "Gradient Descent(332/999): loss=0.12082119868157207\n",
      "Gradient Descent(333/999): loss=0.1208111104666278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(334/999): loss=0.1208010349675581\n",
      "Gradient Descent(335/999): loss=0.12079097216260222\n",
      "Gradient Descent(336/999): loss=0.12078092203007254\n",
      "Gradient Descent(337/999): loss=0.12077088454835373\n",
      "Gradient Descent(338/999): loss=0.12076085969590199\n",
      "Gradient Descent(339/999): loss=0.12075084745124415\n",
      "Gradient Descent(340/999): loss=0.12074084779297707\n",
      "Gradient Descent(341/999): loss=0.12073086069976666\n",
      "Gradient Descent(342/999): loss=0.12072088615034729\n",
      "Gradient Descent(343/999): loss=0.12071092412352098\n",
      "Gradient Descent(344/999): loss=0.1207009745981567\n",
      "Gradient Descent(345/999): loss=0.12069103755318963\n",
      "Gradient Descent(346/999): loss=0.12068111296762057\n",
      "Gradient Descent(347/999): loss=0.12067120082051509\n",
      "Gradient Descent(348/999): loss=0.1206613010910029\n",
      "Gradient Descent(349/999): loss=0.12065141375827733\n",
      "Gradient Descent(350/999): loss=0.12064153880159449\n",
      "Gradient Descent(351/999): loss=0.12063167620027275\n",
      "Gradient Descent(352/999): loss=0.12062182593369214\n",
      "Gradient Descent(353/999): loss=0.12061198798129354\n",
      "Gradient Descent(354/999): loss=0.12060216232257831\n",
      "Gradient Descent(355/999): loss=0.12059234893710755\n",
      "Gradient Descent(356/999): loss=0.12058254780450163\n",
      "Gradient Descent(357/999): loss=0.1205727589044395\n",
      "Gradient Descent(358/999): loss=0.12056298221665818\n",
      "Gradient Descent(359/999): loss=0.1205532177209522\n",
      "Gradient Descent(360/999): loss=0.1205434653971731\n",
      "Gradient Descent(361/999): loss=0.12053372522522882\n",
      "Gradient Descent(362/999): loss=0.12052399718508326\n",
      "Gradient Descent(363/999): loss=0.12051428125675567\n",
      "Gradient Descent(364/999): loss=0.12050457742032021\n",
      "Gradient Descent(365/999): loss=0.12049488565590549\n",
      "Gradient Descent(366/999): loss=0.12048520594369393\n",
      "Gradient Descent(367/999): loss=0.12047553826392149\n",
      "Gradient Descent(368/999): loss=0.12046588259687703\n",
      "Gradient Descent(369/999): loss=0.12045623892290193\n",
      "Gradient Descent(370/999): loss=0.1204466072223895\n",
      "Gradient Descent(371/999): loss=0.12043698747578487\n",
      "Gradient Descent(372/999): loss=0.12042737966358405\n",
      "Gradient Descent(373/999): loss=0.12041778376633404\n",
      "Gradient Descent(374/999): loss=0.1204081997646319\n",
      "Gradient Descent(375/999): loss=0.12039862763912476\n",
      "Gradient Descent(376/999): loss=0.12038906737050908\n",
      "Gradient Descent(377/999): loss=0.12037951893953046\n",
      "Gradient Descent(378/999): loss=0.12036998232698314\n",
      "Gradient Descent(379/999): loss=0.12036045751370965\n",
      "Gradient Descent(380/999): loss=0.12035094448060041\n",
      "Gradient Descent(381/999): loss=0.12034144320859334\n",
      "Gradient Descent(382/999): loss=0.12033195367867355\n",
      "Gradient Descent(383/999): loss=0.12032247587187292\n",
      "Gradient Descent(384/999): loss=0.1203130097692697\n",
      "Gradient Descent(385/999): loss=0.12030355535198833\n",
      "Gradient Descent(386/999): loss=0.1202941126011989\n",
      "Gradient Descent(387/999): loss=0.12028468149811686\n",
      "Gradient Descent(388/999): loss=0.12027526202400281\n",
      "Gradient Descent(389/999): loss=0.12026585416016197\n",
      "Gradient Descent(390/999): loss=0.12025645788794403\n",
      "Gradient Descent(391/999): loss=0.12024707318874274\n",
      "Gradient Descent(392/999): loss=0.12023770004399562\n",
      "Gradient Descent(393/999): loss=0.12022833843518362\n",
      "Gradient Descent(394/999): loss=0.1202189883438309\n",
      "Gradient Descent(395/999): loss=0.1202096497515045\n",
      "Gradient Descent(396/999): loss=0.1202003226398139\n",
      "Gradient Descent(397/999): loss=0.120191006990411\n",
      "Gradient Descent(398/999): loss=0.12018170278498963\n",
      "Gradient Descent(399/999): loss=0.12017241000528534\n",
      "Gradient Descent(400/999): loss=0.1201631286330751\n",
      "Gradient Descent(401/999): loss=0.12015385865017707\n",
      "Gradient Descent(402/999): loss=0.12014460003845037\n",
      "Gradient Descent(403/999): loss=0.12013535277979462\n",
      "Gradient Descent(404/999): loss=0.12012611685614996\n",
      "Gradient Descent(405/999): loss=0.12011689224949659\n",
      "Gradient Descent(406/999): loss=0.12010767894185462\n",
      "Gradient Descent(407/999): loss=0.1200984769152838\n",
      "Gradient Descent(408/999): loss=0.12008928615188327\n",
      "Gradient Descent(409/999): loss=0.12008010663379126\n",
      "Gradient Descent(410/999): loss=0.12007093834318501\n",
      "Gradient Descent(411/999): loss=0.12006178126228037\n",
      "Gradient Descent(412/999): loss=0.12005263537333173\n",
      "Gradient Descent(413/999): loss=0.12004350065863172\n",
      "Gradient Descent(414/999): loss=0.12003437710051086\n",
      "Gradient Descent(415/999): loss=0.12002526468133767\n",
      "Gradient Descent(416/999): loss=0.12001616338351807\n",
      "Gradient Descent(417/999): loss=0.12000707318949548\n",
      "Gradient Descent(418/999): loss=0.11999799408175044\n",
      "Gradient Descent(419/999): loss=0.11998892604280044\n",
      "Gradient Descent(420/999): loss=0.11997986905519978\n",
      "Gradient Descent(421/999): loss=0.1199708231015393\n",
      "Gradient Descent(422/999): loss=0.11996178816444623\n",
      "Gradient Descent(423/999): loss=0.11995276422658396\n",
      "Gradient Descent(424/999): loss=0.11994375127065185\n",
      "Gradient Descent(425/999): loss=0.11993474927938512\n",
      "Gradient Descent(426/999): loss=0.11992575823555456\n",
      "Gradient Descent(427/999): loss=0.11991677812196637\n",
      "Gradient Descent(428/999): loss=0.11990780892146213\n",
      "Gradient Descent(429/999): loss=0.11989885061691839\n",
      "Gradient Descent(430/999): loss=0.11988990319124662\n",
      "Gradient Descent(431/999): loss=0.11988096662739306\n",
      "Gradient Descent(432/999): loss=0.11987204090833854\n",
      "Gradient Descent(433/999): loss=0.11986312601709823\n",
      "Gradient Descent(434/999): loss=0.11985422193672154\n",
      "Gradient Descent(435/999): loss=0.11984532865029206\n",
      "Gradient Descent(436/999): loss=0.1198364461409272\n",
      "Gradient Descent(437/999): loss=0.11982757439177812\n",
      "Gradient Descent(438/999): loss=0.11981871338602963\n",
      "Gradient Descent(439/999): loss=0.11980986310690002\n",
      "Gradient Descent(440/999): loss=0.11980102353764077\n",
      "Gradient Descent(441/999): loss=0.11979219466153668\n",
      "Gradient Descent(442/999): loss=0.11978337646190534\n",
      "Gradient Descent(443/999): loss=0.11977456892209742\n",
      "Gradient Descent(444/999): loss=0.11976577202549618\n",
      "Gradient Descent(445/999): loss=0.11975698575551745\n",
      "Gradient Descent(446/999): loss=0.11974821009560956\n",
      "Gradient Descent(447/999): loss=0.11973944502925311\n",
      "Gradient Descent(448/999): loss=0.11973069053996085\n",
      "Gradient Descent(449/999): loss=0.11972194661127761\n",
      "Gradient Descent(450/999): loss=0.11971321322678001\n",
      "Gradient Descent(451/999): loss=0.1197044903700766\n",
      "Gradient Descent(452/999): loss=0.11969577802480744\n",
      "Gradient Descent(453/999): loss=0.11968707617464414\n",
      "Gradient Descent(454/999): loss=0.11967838480328971\n",
      "Gradient Descent(455/999): loss=0.11966970389447841\n",
      "Gradient Descent(456/999): loss=0.11966103343197565\n",
      "Gradient Descent(457/999): loss=0.11965237339957784\n",
      "Gradient Descent(458/999): loss=0.11964372378111231\n",
      "Gradient Descent(459/999): loss=0.11963508456043716\n",
      "Gradient Descent(460/999): loss=0.11962645572144119\n",
      "Gradient Descent(461/999): loss=0.11961783724804373\n",
      "Gradient Descent(462/999): loss=0.11960922912419446\n",
      "Gradient Descent(463/999): loss=0.11960063133387355\n",
      "Gradient Descent(464/999): loss=0.11959204386109125\n",
      "Gradient Descent(465/999): loss=0.11958346668988802\n",
      "Gradient Descent(466/999): loss=0.11957489980433422\n",
      "Gradient Descent(467/999): loss=0.11956634318853011\n",
      "Gradient Descent(468/999): loss=0.1195577968266058\n",
      "Gradient Descent(469/999): loss=0.11954926070272104\n",
      "Gradient Descent(470/999): loss=0.11954073480106514\n",
      "Gradient Descent(471/999): loss=0.1195322191058569\n",
      "Gradient Descent(472/999): loss=0.11952371360134448\n",
      "Gradient Descent(473/999): loss=0.11951521827180532\n",
      "Gradient Descent(474/999): loss=0.11950673310154604\n",
      "Gradient Descent(475/999): loss=0.11949825807490229\n",
      "Gradient Descent(476/999): loss=0.11948979317623878\n",
      "Gradient Descent(477/999): loss=0.11948133838994898\n",
      "Gradient Descent(478/999): loss=0.11947289370045532\n",
      "Gradient Descent(479/999): loss=0.11946445909220872\n",
      "Gradient Descent(480/999): loss=0.11945603454968885\n",
      "Gradient Descent(481/999): loss=0.11944762005740385\n",
      "Gradient Descent(482/999): loss=0.11943921559989024\n",
      "Gradient Descent(483/999): loss=0.1194308211617129\n",
      "Gradient Descent(484/999): loss=0.11942243672746498\n",
      "Gradient Descent(485/999): loss=0.11941406228176768\n",
      "Gradient Descent(486/999): loss=0.11940569780927039\n",
      "Gradient Descent(487/999): loss=0.11939734329465039\n",
      "Gradient Descent(488/999): loss=0.11938899872261287\n",
      "Gradient Descent(489/999): loss=0.11938066407789086\n",
      "Gradient Descent(490/999): loss=0.11937233934524508\n",
      "Gradient Descent(491/999): loss=0.11936402450946389\n",
      "Gradient Descent(492/999): loss=0.11935571955536324\n",
      "Gradient Descent(493/999): loss=0.11934742446778651\n",
      "Gradient Descent(494/999): loss=0.11933913923160452\n",
      "Gradient Descent(495/999): loss=0.11933086383171541\n",
      "Gradient Descent(496/999): loss=0.11932259825304452\n",
      "Gradient Descent(497/999): loss=0.11931434248054434\n",
      "Gradient Descent(498/999): loss=0.1193060964991945\n",
      "Gradient Descent(499/999): loss=0.11929786029400163\n",
      "Gradient Descent(500/999): loss=0.11928963384999919\n",
      "Gradient Descent(501/999): loss=0.11928141715224766\n",
      "Gradient Descent(502/999): loss=0.11927321018583416\n",
      "Gradient Descent(503/999): loss=0.11926501293587255\n",
      "Gradient Descent(504/999): loss=0.11925682538750335\n",
      "Gradient Descent(505/999): loss=0.1192486475258936\n",
      "Gradient Descent(506/999): loss=0.11924047933623685\n",
      "Gradient Descent(507/999): loss=0.1192323208037531\n",
      "Gradient Descent(508/999): loss=0.11922417191368863\n",
      "Gradient Descent(509/999): loss=0.11921603265131596\n",
      "Gradient Descent(510/999): loss=0.11920790300193392\n",
      "Gradient Descent(511/999): loss=0.11919978295086742\n",
      "Gradient Descent(512/999): loss=0.11919167248346743\n",
      "Gradient Descent(513/999): loss=0.11918357158511089\n",
      "Gradient Descent(514/999): loss=0.11917548024120074\n",
      "Gradient Descent(515/999): loss=0.11916739843716574\n",
      "Gradient Descent(516/999): loss=0.11915932615846042\n",
      "Gradient Descent(517/999): loss=0.11915126339056507\n",
      "Gradient Descent(518/999): loss=0.11914321011898563\n",
      "Gradient Descent(519/999): loss=0.11913516632925365\n",
      "Gradient Descent(520/999): loss=0.11912713200692623\n",
      "Gradient Descent(521/999): loss=0.11911910713758588\n",
      "Gradient Descent(522/999): loss=0.11911109170684057\n",
      "Gradient Descent(523/999): loss=0.11910308570032363\n",
      "Gradient Descent(524/999): loss=0.11909508910369357\n",
      "Gradient Descent(525/999): loss=0.11908710190263423\n",
      "Gradient Descent(526/999): loss=0.11907912408285456\n",
      "Gradient Descent(527/999): loss=0.11907115563008856\n",
      "Gradient Descent(528/999): loss=0.11906319653009538\n",
      "Gradient Descent(529/999): loss=0.11905524676865903\n",
      "Gradient Descent(530/999): loss=0.11904730633158846\n",
      "Gradient Descent(531/999): loss=0.11903937520471752\n",
      "Gradient Descent(532/999): loss=0.11903145337390476\n",
      "Gradient Descent(533/999): loss=0.1190235408250336\n",
      "Gradient Descent(534/999): loss=0.11901563754401205\n",
      "Gradient Descent(535/999): loss=0.11900774351677271\n",
      "Gradient Descent(536/999): loss=0.11899985872927282\n",
      "Gradient Descent(537/999): loss=0.11899198316749406\n",
      "Gradient Descent(538/999): loss=0.1189841168174426\n",
      "Gradient Descent(539/999): loss=0.11897625966514899\n",
      "Gradient Descent(540/999): loss=0.1189684116966681\n",
      "Gradient Descent(541/999): loss=0.1189605728980791\n",
      "Gradient Descent(542/999): loss=0.11895274325548533\n",
      "Gradient Descent(543/999): loss=0.11894492275501435\n",
      "Gradient Descent(544/999): loss=0.11893711138281783\n",
      "Gradient Descent(545/999): loss=0.11892930912507149\n",
      "Gradient Descent(546/999): loss=0.11892151596797501\n",
      "Gradient Descent(547/999): loss=0.11891373189775208\n",
      "Gradient Descent(548/999): loss=0.11890595690065026\n",
      "Gradient Descent(549/999): loss=0.11889819096294095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(550/999): loss=0.11889043407091933\n",
      "Gradient Descent(551/999): loss=0.11888268621090439\n",
      "Gradient Descent(552/999): loss=0.11887494736923869\n",
      "Gradient Descent(553/999): loss=0.1188672175322885\n",
      "Gradient Descent(554/999): loss=0.11885949668644363\n",
      "Gradient Descent(555/999): loss=0.11885178481811742\n",
      "Gradient Descent(556/999): loss=0.1188440819137468\n",
      "Gradient Descent(557/999): loss=0.11883638795979191\n",
      "Gradient Descent(558/999): loss=0.11882870294273647\n",
      "Gradient Descent(559/999): loss=0.1188210268490874\n",
      "Gradient Descent(560/999): loss=0.11881335966537496\n",
      "Gradient Descent(561/999): loss=0.11880570137815258\n",
      "Gradient Descent(562/999): loss=0.11879805197399693\n",
      "Gradient Descent(563/999): loss=0.11879041143950778\n",
      "Gradient Descent(564/999): loss=0.11878277976130795\n",
      "Gradient Descent(565/999): loss=0.11877515692604332\n",
      "Gradient Descent(566/999): loss=0.11876754292038275\n",
      "Gradient Descent(567/999): loss=0.118759937731018\n",
      "Gradient Descent(568/999): loss=0.11875234134466373\n",
      "Gradient Descent(569/999): loss=0.11874475374805747\n",
      "Gradient Descent(570/999): loss=0.11873717492795946\n",
      "Gradient Descent(571/999): loss=0.11872960487115275\n",
      "Gradient Descent(572/999): loss=0.11872204356444307\n",
      "Gradient Descent(573/999): loss=0.11871449099465874\n",
      "Gradient Descent(574/999): loss=0.11870694714865074\n",
      "Gradient Descent(575/999): loss=0.11869941201329261\n",
      "Gradient Descent(576/999): loss=0.11869188557548033\n",
      "Gradient Descent(577/999): loss=0.11868436782213235\n",
      "Gradient Descent(578/999): loss=0.11867685874018963\n",
      "Gradient Descent(579/999): loss=0.11866935831661533\n",
      "Gradient Descent(580/999): loss=0.11866186653839508\n",
      "Gradient Descent(581/999): loss=0.11865438339253673\n",
      "Gradient Descent(582/999): loss=0.11864690886607035\n",
      "Gradient Descent(583/999): loss=0.1186394429460482\n",
      "Gradient Descent(584/999): loss=0.11863198561954469\n",
      "Gradient Descent(585/999): loss=0.11862453687365632\n",
      "Gradient Descent(586/999): loss=0.11861709669550159\n",
      "Gradient Descent(587/999): loss=0.11860966507222113\n",
      "Gradient Descent(588/999): loss=0.11860224199097749\n",
      "Gradient Descent(589/999): loss=0.11859482743895498\n",
      "Gradient Descent(590/999): loss=0.11858742140336\n",
      "Gradient Descent(591/999): loss=0.11858002387142072\n",
      "Gradient Descent(592/999): loss=0.11857263483038699\n",
      "Gradient Descent(593/999): loss=0.11856525426753053\n",
      "Gradient Descent(594/999): loss=0.11855788217014473\n",
      "Gradient Descent(595/999): loss=0.11855051852554457\n",
      "Gradient Descent(596/999): loss=0.11854316332106678\n",
      "Gradient Descent(597/999): loss=0.11853581654406953\n",
      "Gradient Descent(598/999): loss=0.11852847818193257\n",
      "Gradient Descent(599/999): loss=0.11852114822205717\n",
      "Gradient Descent(600/999): loss=0.11851382665186597\n",
      "Gradient Descent(601/999): loss=0.1185065134588031\n",
      "Gradient Descent(602/999): loss=0.11849920863033403\n",
      "Gradient Descent(603/999): loss=0.11849191215394547\n",
      "Gradient Descent(604/999): loss=0.11848462401714552\n",
      "Gradient Descent(605/999): loss=0.1184773442074634\n",
      "Gradient Descent(606/999): loss=0.1184700727124497\n",
      "Gradient Descent(607/999): loss=0.11846280951967594\n",
      "Gradient Descent(608/999): loss=0.11845555461673495\n",
      "Gradient Descent(609/999): loss=0.11844830799124052\n",
      "Gradient Descent(610/999): loss=0.11844106963082754\n",
      "Gradient Descent(611/999): loss=0.11843383952315178\n",
      "Gradient Descent(612/999): loss=0.11842661765589012\n",
      "Gradient Descent(613/999): loss=0.1184194040167402\n",
      "Gradient Descent(614/999): loss=0.11841219859342064\n",
      "Gradient Descent(615/999): loss=0.11840500137367081\n",
      "Gradient Descent(616/999): loss=0.11839781234525094\n",
      "Gradient Descent(617/999): loss=0.11839063149594198\n",
      "Gradient Descent(618/999): loss=0.11838345881354556\n",
      "Gradient Descent(619/999): loss=0.118376294285884\n",
      "Gradient Descent(620/999): loss=0.11836913790080032\n",
      "Gradient Descent(621/999): loss=0.118361989646158\n",
      "Gradient Descent(622/999): loss=0.11835484950984121\n",
      "Gradient Descent(623/999): loss=0.11834771747975453\n",
      "Gradient Descent(624/999): loss=0.11834059354382306\n",
      "Gradient Descent(625/999): loss=0.11833347768999233\n",
      "Gradient Descent(626/999): loss=0.1183263699062283\n",
      "Gradient Descent(627/999): loss=0.11831927018051726\n",
      "Gradient Descent(628/999): loss=0.11831217850086577\n",
      "Gradient Descent(629/999): loss=0.11830509485530073\n",
      "Gradient Descent(630/999): loss=0.11829801923186935\n",
      "Gradient Descent(631/999): loss=0.11829095161863884\n",
      "Gradient Descent(632/999): loss=0.11828389200369684\n",
      "Gradient Descent(633/999): loss=0.11827684037515089\n",
      "Gradient Descent(634/999): loss=0.11826979672112879\n",
      "Gradient Descent(635/999): loss=0.11826276102977828\n",
      "Gradient Descent(636/999): loss=0.1182557332892672\n",
      "Gradient Descent(637/999): loss=0.11824871348778326\n",
      "Gradient Descent(638/999): loss=0.1182417016135343\n",
      "Gradient Descent(639/999): loss=0.11823469765474784\n",
      "Gradient Descent(640/999): loss=0.11822770159967143\n",
      "Gradient Descent(641/999): loss=0.11822071343657235\n",
      "Gradient Descent(642/999): loss=0.11821373315373777\n",
      "Gradient Descent(643/999): loss=0.11820676073947452\n",
      "Gradient Descent(644/999): loss=0.11819979618210924\n",
      "Gradient Descent(645/999): loss=0.11819283946998817\n",
      "Gradient Descent(646/999): loss=0.11818589059147723\n",
      "Gradient Descent(647/999): loss=0.11817894953496197\n",
      "Gradient Descent(648/999): loss=0.11817201628884748\n",
      "Gradient Descent(649/999): loss=0.11816509084155846\n",
      "Gradient Descent(650/999): loss=0.11815817318153901\n",
      "Gradient Descent(651/999): loss=0.11815126329725269\n",
      "Gradient Descent(652/999): loss=0.1181443611771826\n",
      "Gradient Descent(653/999): loss=0.11813746680983121\n",
      "Gradient Descent(654/999): loss=0.11813058018372023\n",
      "Gradient Descent(655/999): loss=0.1181237012873908\n",
      "Gradient Descent(656/999): loss=0.11811683010940333\n",
      "Gradient Descent(657/999): loss=0.11810996663833741\n",
      "Gradient Descent(658/999): loss=0.11810311086279196\n",
      "Gradient Descent(659/999): loss=0.11809626277138502\n",
      "Gradient Descent(660/999): loss=0.11808942235275374\n",
      "Gradient Descent(661/999): loss=0.11808258959555443\n",
      "Gradient Descent(662/999): loss=0.11807576448846245\n",
      "Gradient Descent(663/999): loss=0.1180689470201722\n",
      "Gradient Descent(664/999): loss=0.11806213717939713\n",
      "Gradient Descent(665/999): loss=0.11805533495486958\n",
      "Gradient Descent(666/999): loss=0.11804854033534087\n",
      "Gradient Descent(667/999): loss=0.1180417533095812\n",
      "Gradient Descent(668/999): loss=0.11803497386637966\n",
      "Gradient Descent(669/999): loss=0.11802820199454414\n",
      "Gradient Descent(670/999): loss=0.11802143768290135\n",
      "Gradient Descent(671/999): loss=0.11801468092029682\n",
      "Gradient Descent(672/999): loss=0.11800793169559463\n",
      "Gradient Descent(673/999): loss=0.11800118999767777\n",
      "Gradient Descent(674/999): loss=0.1179944558154477\n",
      "Gradient Descent(675/999): loss=0.11798772913782461\n",
      "Gradient Descent(676/999): loss=0.11798100995374729\n",
      "Gradient Descent(677/999): loss=0.11797429825217302\n",
      "Gradient Descent(678/999): loss=0.11796759402207771\n",
      "Gradient Descent(679/999): loss=0.11796089725245558\n",
      "Gradient Descent(680/999): loss=0.11795420793231952\n",
      "Gradient Descent(681/999): loss=0.11794752605070066\n",
      "Gradient Descent(682/999): loss=0.11794085159664867\n",
      "Gradient Descent(683/999): loss=0.11793418455923145\n",
      "Gradient Descent(684/999): loss=0.11792752492753528\n",
      "Gradient Descent(685/999): loss=0.11792087269066473\n",
      "Gradient Descent(686/999): loss=0.11791422783774264\n",
      "Gradient Descent(687/999): loss=0.11790759035791003\n",
      "Gradient Descent(688/999): loss=0.11790096024032612\n",
      "Gradient Descent(689/999): loss=0.11789433747416832\n",
      "Gradient Descent(690/999): loss=0.11788772204863217\n",
      "Gradient Descent(691/999): loss=0.1178811139529312\n",
      "Gradient Descent(692/999): loss=0.11787451317629714\n",
      "Gradient Descent(693/999): loss=0.11786791970797965\n",
      "Gradient Descent(694/999): loss=0.11786133353724643\n",
      "Gradient Descent(695/999): loss=0.11785475465338303\n",
      "Gradient Descent(696/999): loss=0.11784818304569314\n",
      "Gradient Descent(697/999): loss=0.11784161870349817\n",
      "Gradient Descent(698/999): loss=0.11783506161613744\n",
      "Gradient Descent(699/999): loss=0.11782851177296812\n",
      "Gradient Descent(700/999): loss=0.11782196916336517\n",
      "Gradient Descent(701/999): loss=0.1178154337767213\n",
      "Gradient Descent(702/999): loss=0.11780890560244696\n",
      "Gradient Descent(703/999): loss=0.11780238462997035\n",
      "Gradient Descent(704/999): loss=0.11779587084873724\n",
      "Gradient Descent(705/999): loss=0.11778936424821117\n",
      "Gradient Descent(706/999): loss=0.11778286481787316\n",
      "Gradient Descent(707/999): loss=0.11777637254722191\n",
      "Gradient Descent(708/999): loss=0.11776988742577357\n",
      "Gradient Descent(709/999): loss=0.1177634094430619\n",
      "Gradient Descent(710/999): loss=0.11775693858863805\n",
      "Gradient Descent(711/999): loss=0.11775047485207067\n",
      "Gradient Descent(712/999): loss=0.1177440182229458\n",
      "Gradient Descent(713/999): loss=0.11773756869086692\n",
      "Gradient Descent(714/999): loss=0.11773112624545481\n",
      "Gradient Descent(715/999): loss=0.11772469087634757\n",
      "Gradient Descent(716/999): loss=0.11771826257320067\n",
      "Gradient Descent(717/999): loss=0.11771184132568674\n",
      "Gradient Descent(718/999): loss=0.11770542712349573\n",
      "Gradient Descent(719/999): loss=0.11769901995633472\n",
      "Gradient Descent(720/999): loss=0.11769261981392798\n",
      "Gradient Descent(721/999): loss=0.11768622668601693\n",
      "Gradient Descent(722/999): loss=0.11767984056236012\n",
      "Gradient Descent(723/999): loss=0.11767346143273316\n",
      "Gradient Descent(724/999): loss=0.11766708928692869\n",
      "Gradient Descent(725/999): loss=0.11766072411475632\n",
      "Gradient Descent(726/999): loss=0.11765436590604277\n",
      "Gradient Descent(727/999): loss=0.1176480146506316\n",
      "Gradient Descent(728/999): loss=0.11764167033838341\n",
      "Gradient Descent(729/999): loss=0.11763533295917554\n",
      "Gradient Descent(730/999): loss=0.1176290025029023\n",
      "Gradient Descent(731/999): loss=0.11762267895947487\n",
      "Gradient Descent(732/999): loss=0.11761636231882111\n",
      "Gradient Descent(733/999): loss=0.11761005257088573\n",
      "Gradient Descent(734/999): loss=0.11760374970563027\n",
      "Gradient Descent(735/999): loss=0.11759745371303273\n",
      "Gradient Descent(736/999): loss=0.11759116458308805\n",
      "Gradient Descent(737/999): loss=0.11758488230580773\n",
      "Gradient Descent(738/999): loss=0.11757860687121986\n",
      "Gradient Descent(739/999): loss=0.11757233826936916\n",
      "Gradient Descent(740/999): loss=0.11756607649031693\n",
      "Gradient Descent(741/999): loss=0.11755982152414099\n",
      "Gradient Descent(742/999): loss=0.11755357336093564\n",
      "Gradient Descent(743/999): loss=0.11754733199081172\n",
      "Gradient Descent(744/999): loss=0.11754109740389644\n",
      "Gradient Descent(745/999): loss=0.11753486959033346\n",
      "Gradient Descent(746/999): loss=0.11752864854028287\n",
      "Gradient Descent(747/999): loss=0.11752243424392107\n",
      "Gradient Descent(748/999): loss=0.11751622669144077\n",
      "Gradient Descent(749/999): loss=0.11751002587305111\n",
      "Gradient Descent(750/999): loss=0.11750383177897729\n",
      "Gradient Descent(751/999): loss=0.11749764439946093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(752/999): loss=0.11749146372475981\n",
      "Gradient Descent(753/999): loss=0.1174852897451479\n",
      "Gradient Descent(754/999): loss=0.11747912245091528\n",
      "Gradient Descent(755/999): loss=0.11747296183236819\n",
      "Gradient Descent(756/999): loss=0.11746680787982901\n",
      "Gradient Descent(757/999): loss=0.11746066058363618\n",
      "Gradient Descent(758/999): loss=0.11745451993414406\n",
      "Gradient Descent(759/999): loss=0.11744838592172319\n",
      "Gradient Descent(760/999): loss=0.11744225853676002\n",
      "Gradient Descent(761/999): loss=0.11743613776965693\n",
      "Gradient Descent(762/999): loss=0.11743002361083228\n",
      "Gradient Descent(763/999): loss=0.11742391605072032\n",
      "Gradient Descent(764/999): loss=0.11741781507977109\n",
      "Gradient Descent(765/999): loss=0.11741172068845063\n",
      "Gradient Descent(766/999): loss=0.11740563286724066\n",
      "Gradient Descent(767/999): loss=0.11739955160663874\n",
      "Gradient Descent(768/999): loss=0.1173934768971582\n",
      "Gradient Descent(769/999): loss=0.11738740872932805\n",
      "Gradient Descent(770/999): loss=0.11738134709369305\n",
      "Gradient Descent(771/999): loss=0.11737529198081365\n",
      "Gradient Descent(772/999): loss=0.11736924338126585\n",
      "Gradient Descent(773/999): loss=0.11736320128564141\n",
      "Gradient Descent(774/999): loss=0.11735716568454754\n",
      "Gradient Descent(775/999): loss=0.11735113656860716\n",
      "Gradient Descent(776/999): loss=0.11734511392845856\n",
      "Gradient Descent(777/999): loss=0.11733909775475573\n",
      "Gradient Descent(778/999): loss=0.1173330880381679\n",
      "Gradient Descent(779/999): loss=0.11732708476938003\n",
      "Gradient Descent(780/999): loss=0.11732108793909231\n",
      "Gradient Descent(781/999): loss=0.11731509753802036\n",
      "Gradient Descent(782/999): loss=0.11730911355689524\n",
      "Gradient Descent(783/999): loss=0.11730313598646329\n",
      "Gradient Descent(784/999): loss=0.11729716481748621\n",
      "Gradient Descent(785/999): loss=0.11729120004074094\n",
      "Gradient Descent(786/999): loss=0.11728524164701971\n",
      "Gradient Descent(787/999): loss=0.11727928962713005\n",
      "Gradient Descent(788/999): loss=0.11727334397189457\n",
      "Gradient Descent(789/999): loss=0.11726740467215117\n",
      "Gradient Descent(790/999): loss=0.11726147171875284\n",
      "Gradient Descent(791/999): loss=0.11725554510256773\n",
      "Gradient Descent(792/999): loss=0.11724962481447906\n",
      "Gradient Descent(793/999): loss=0.1172437108453852\n",
      "Gradient Descent(794/999): loss=0.11723780318619945\n",
      "Gradient Descent(795/999): loss=0.11723190182785025\n",
      "Gradient Descent(796/999): loss=0.11722600676128092\n",
      "Gradient Descent(797/999): loss=0.11722011797744988\n",
      "Gradient Descent(798/999): loss=0.11721423546733042\n",
      "Gradient Descent(799/999): loss=0.11720835922191067\n",
      "Gradient Descent(800/999): loss=0.11720248923219374\n",
      "Gradient Descent(801/999): loss=0.11719662548919768\n",
      "Gradient Descent(802/999): loss=0.11719076798395517\n",
      "Gradient Descent(803/999): loss=0.11718491670751385\n",
      "Gradient Descent(804/999): loss=0.1171790716509361\n",
      "Gradient Descent(805/999): loss=0.11717323280529912\n",
      "Gradient Descent(806/999): loss=0.1171674001616947\n",
      "Gradient Descent(807/999): loss=0.11716157371122946\n",
      "Gradient Descent(808/999): loss=0.11715575344502463\n",
      "Gradient Descent(809/999): loss=0.1171499393542162\n",
      "Gradient Descent(810/999): loss=0.11714413142995461\n",
      "Gradient Descent(811/999): loss=0.11713832966340502\n",
      "Gradient Descent(812/999): loss=0.1171325340457472\n",
      "Gradient Descent(813/999): loss=0.11712674456817536\n",
      "Gradient Descent(814/999): loss=0.11712096122189831\n",
      "Gradient Descent(815/999): loss=0.11711518399813935\n",
      "Gradient Descent(816/999): loss=0.11710941288813623\n",
      "Gradient Descent(817/999): loss=0.1171036478831411\n",
      "Gradient Descent(818/999): loss=0.11709788897442068\n",
      "Gradient Descent(819/999): loss=0.11709213615325595\n",
      "Gradient Descent(820/999): loss=0.1170863894109423\n",
      "Gradient Descent(821/999): loss=0.11708064873878951\n",
      "Gradient Descent(822/999): loss=0.11707491412812158\n",
      "Gradient Descent(823/999): loss=0.11706918557027693\n",
      "Gradient Descent(824/999): loss=0.11706346305660816\n",
      "Gradient Descent(825/999): loss=0.11705774657848213\n",
      "Gradient Descent(826/999): loss=0.11705203612727992\n",
      "Gradient Descent(827/999): loss=0.11704633169439685\n",
      "Gradient Descent(828/999): loss=0.11704063327124233\n",
      "Gradient Descent(829/999): loss=0.11703494084923997\n",
      "Gradient Descent(830/999): loss=0.11702925441982752\n",
      "Gradient Descent(831/999): loss=0.11702357397445676\n",
      "Gradient Descent(832/999): loss=0.11701789950459357\n",
      "Gradient Descent(833/999): loss=0.11701223100171786\n",
      "Gradient Descent(834/999): loss=0.11700656845732359\n",
      "Gradient Descent(835/999): loss=0.11700091186291868\n",
      "Gradient Descent(836/999): loss=0.11699526121002507\n",
      "Gradient Descent(837/999): loss=0.11698961649017854\n",
      "Gradient Descent(838/999): loss=0.11698397769492896\n",
      "Gradient Descent(839/999): loss=0.11697834481583996\n",
      "Gradient Descent(840/999): loss=0.11697271784448905\n",
      "Gradient Descent(841/999): loss=0.11696709677246764\n",
      "Gradient Descent(842/999): loss=0.11696148159138096\n",
      "Gradient Descent(843/999): loss=0.11695587229284803\n",
      "Gradient Descent(844/999): loss=0.11695026886850157\n",
      "Gradient Descent(845/999): loss=0.11694467130998819\n",
      "Gradient Descent(846/999): loss=0.1169390796089681\n",
      "Gradient Descent(847/999): loss=0.1169334937571153\n",
      "Gradient Descent(848/999): loss=0.1169279137461174\n",
      "Gradient Descent(849/999): loss=0.11692233956767568\n",
      "Gradient Descent(850/999): loss=0.11691677121350512\n",
      "Gradient Descent(851/999): loss=0.11691120867533421\n",
      "Gradient Descent(852/999): loss=0.11690565194490503\n",
      "Gradient Descent(853/999): loss=0.11690010101397329\n",
      "Gradient Descent(854/999): loss=0.11689455587430818\n",
      "Gradient Descent(855/999): loss=0.1168890165176924\n",
      "Gradient Descent(856/999): loss=0.11688348293592214\n",
      "Gradient Descent(857/999): loss=0.11687795512080706\n",
      "Gradient Descent(858/999): loss=0.11687243306417028\n",
      "Gradient Descent(859/999): loss=0.1168669167578483\n",
      "Gradient Descent(860/999): loss=0.11686140619369097\n",
      "Gradient Descent(861/999): loss=0.11685590136356161\n",
      "Gradient Descent(862/999): loss=0.11685040225933681\n",
      "Gradient Descent(863/999): loss=0.11684490887290651\n",
      "Gradient Descent(864/999): loss=0.11683942119617395\n",
      "Gradient Descent(865/999): loss=0.11683393922105562\n",
      "Gradient Descent(866/999): loss=0.11682846293948126\n",
      "Gradient Descent(867/999): loss=0.11682299234339386\n",
      "Gradient Descent(868/999): loss=0.11681752742474961\n",
      "Gradient Descent(869/999): loss=0.1168120681755179\n",
      "Gradient Descent(870/999): loss=0.11680661458768121\n",
      "Gradient Descent(871/999): loss=0.11680116665323517\n",
      "Gradient Descent(872/999): loss=0.11679572436418859\n",
      "Gradient Descent(873/999): loss=0.11679028771256332\n",
      "Gradient Descent(874/999): loss=0.11678485669039428\n",
      "Gradient Descent(875/999): loss=0.1167794312897294\n",
      "Gradient Descent(876/999): loss=0.11677401150262966\n",
      "Gradient Descent(877/999): loss=0.11676859732116907\n",
      "Gradient Descent(878/999): loss=0.11676318873743459\n",
      "Gradient Descent(879/999): loss=0.11675778574352605\n",
      "Gradient Descent(880/999): loss=0.11675238833155634\n",
      "Gradient Descent(881/999): loss=0.11674699649365114\n",
      "Gradient Descent(882/999): loss=0.11674161022194908\n",
      "Gradient Descent(883/999): loss=0.11673622950860164\n",
      "Gradient Descent(884/999): loss=0.11673085434577317\n",
      "Gradient Descent(885/999): loss=0.1167254847256407\n",
      "Gradient Descent(886/999): loss=0.1167201206403942\n",
      "Gradient Descent(887/999): loss=0.11671476208223634\n",
      "Gradient Descent(888/999): loss=0.11670940904338259\n",
      "Gradient Descent(889/999): loss=0.11670406151606104\n",
      "Gradient Descent(890/999): loss=0.11669871949251258\n",
      "Gradient Descent(891/999): loss=0.11669338296499075\n",
      "Gradient Descent(892/999): loss=0.11668805192576175\n",
      "Gradient Descent(893/999): loss=0.1166827263671044\n",
      "Gradient Descent(894/999): loss=0.11667740628131011\n",
      "Gradient Descent(895/999): loss=0.11667209166068297\n",
      "Gradient Descent(896/999): loss=0.11666678249753955\n",
      "Gradient Descent(897/999): loss=0.116661478784209\n",
      "Gradient Descent(898/999): loss=0.11665618051303302\n",
      "Gradient Descent(899/999): loss=0.11665088767636574\n",
      "Gradient Descent(900/999): loss=0.11664560026657386\n",
      "Gradient Descent(901/999): loss=0.11664031827603645\n",
      "Gradient Descent(902/999): loss=0.1166350416971451\n",
      "Gradient Descent(903/999): loss=0.11662977052230378\n",
      "Gradient Descent(904/999): loss=0.11662450474392881\n",
      "Gradient Descent(905/999): loss=0.11661924435444898\n",
      "Gradient Descent(906/999): loss=0.1166139893463053\n",
      "Gradient Descent(907/999): loss=0.11660873971195125\n",
      "Gradient Descent(908/999): loss=0.11660349544385251\n",
      "Gradient Descent(909/999): loss=0.1165982565344871\n",
      "Gradient Descent(910/999): loss=0.11659302297634525\n",
      "Gradient Descent(911/999): loss=0.1165877947619295\n",
      "Gradient Descent(912/999): loss=0.11658257188375454\n",
      "Gradient Descent(913/999): loss=0.11657735433434735\n",
      "Gradient Descent(914/999): loss=0.11657214210624701\n",
      "Gradient Descent(915/999): loss=0.11656693519200474\n",
      "Gradient Descent(916/999): loss=0.116561733584184\n",
      "Gradient Descent(917/999): loss=0.11655653727536025\n",
      "Gradient Descent(918/999): loss=0.11655134625812111\n",
      "Gradient Descent(919/999): loss=0.11654616052506622\n",
      "Gradient Descent(920/999): loss=0.11654098006880732\n",
      "Gradient Descent(921/999): loss=0.1165358048819682\n",
      "Gradient Descent(922/999): loss=0.11653063495718456\n",
      "Gradient Descent(923/999): loss=0.11652547028710418\n",
      "Gradient Descent(924/999): loss=0.11652031086438673\n",
      "Gradient Descent(925/999): loss=0.11651515668170388\n",
      "Gradient Descent(926/999): loss=0.11651000773173925\n",
      "Gradient Descent(927/999): loss=0.11650486400718826\n",
      "Gradient Descent(928/999): loss=0.1164997255007583\n",
      "Gradient Descent(929/999): loss=0.11649459220516857\n",
      "Gradient Descent(930/999): loss=0.11648946411315018\n",
      "Gradient Descent(931/999): loss=0.116484341217446\n",
      "Gradient Descent(932/999): loss=0.11647922351081064\n",
      "Gradient Descent(933/999): loss=0.11647411098601067\n",
      "Gradient Descent(934/999): loss=0.11646900363582423\n",
      "Gradient Descent(935/999): loss=0.11646390145304131\n",
      "Gradient Descent(936/999): loss=0.11645880443046353\n",
      "Gradient Descent(937/999): loss=0.11645371256090434\n",
      "Gradient Descent(938/999): loss=0.11644862583718872\n",
      "Gradient Descent(939/999): loss=0.11644354425215336\n",
      "Gradient Descent(940/999): loss=0.11643846779864664\n",
      "Gradient Descent(941/999): loss=0.11643339646952847\n",
      "Gradient Descent(942/999): loss=0.1164283302576704\n",
      "Gradient Descent(943/999): loss=0.11642326915595552\n",
      "Gradient Descent(944/999): loss=0.11641821315727854\n",
      "Gradient Descent(945/999): loss=0.11641316225454562\n",
      "Gradient Descent(946/999): loss=0.11640811644067448\n",
      "Gradient Descent(947/999): loss=0.11640307570859434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(948/999): loss=0.11639804005124586\n",
      "Gradient Descent(949/999): loss=0.11639300946158118\n",
      "Gradient Descent(950/999): loss=0.11638798393256386\n",
      "Gradient Descent(951/999): loss=0.11638296345716884\n",
      "Gradient Descent(952/999): loss=0.11637794802838251\n",
      "Gradient Descent(953/999): loss=0.11637293763920263\n",
      "Gradient Descent(954/999): loss=0.11636793228263828\n",
      "Gradient Descent(955/999): loss=0.11636293195170987\n",
      "Gradient Descent(956/999): loss=0.11635793663944913\n",
      "Gradient Descent(957/999): loss=0.11635294633889912\n",
      "Gradient Descent(958/999): loss=0.1163479610431141\n",
      "Gradient Descent(959/999): loss=0.11634298074515965\n",
      "Gradient Descent(960/999): loss=0.11633800543811258\n",
      "Gradient Descent(961/999): loss=0.11633303511506085\n",
      "Gradient Descent(962/999): loss=0.1163280697691037\n",
      "Gradient Descent(963/999): loss=0.1163231093933515\n",
      "Gradient Descent(964/999): loss=0.11631815398092575\n",
      "Gradient Descent(965/999): loss=0.11631320352495914\n",
      "Gradient Descent(966/999): loss=0.11630825801859543\n",
      "Gradient Descent(967/999): loss=0.11630331745498954\n",
      "Gradient Descent(968/999): loss=0.11629838182730735\n",
      "Gradient Descent(969/999): loss=0.116293451128726\n",
      "Gradient Descent(970/999): loss=0.11628852535243342\n",
      "Gradient Descent(971/999): loss=0.11628360449162875\n",
      "Gradient Descent(972/999): loss=0.11627868853952204\n",
      "Gradient Descent(973/999): loss=0.11627377748933436\n",
      "Gradient Descent(974/999): loss=0.11626887133429772\n",
      "Gradient Descent(975/999): loss=0.11626397006765508\n",
      "Gradient Descent(976/999): loss=0.11625907368266035\n",
      "Gradient Descent(977/999): loss=0.1162541821725783\n",
      "Gradient Descent(978/999): loss=0.11624929553068458\n",
      "Gradient Descent(979/999): loss=0.11624441375026576\n",
      "Gradient Descent(980/999): loss=0.11623953682461924\n",
      "Gradient Descent(981/999): loss=0.1162346647470532\n",
      "Gradient Descent(982/999): loss=0.11622979751088668\n",
      "Gradient Descent(983/999): loss=0.11622493510944952\n",
      "Gradient Descent(984/999): loss=0.11622007753608231\n",
      "Gradient Descent(985/999): loss=0.11621522478413635\n",
      "Gradient Descent(986/999): loss=0.11621037684697375\n",
      "Gradient Descent(987/999): loss=0.11620553371796734\n",
      "Gradient Descent(988/999): loss=0.11620069539050053\n",
      "Gradient Descent(989/999): loss=0.11619586185796756\n",
      "Gradient Descent(990/999): loss=0.11619103311377321\n",
      "Gradient Descent(991/999): loss=0.11618620915133299\n",
      "Gradient Descent(992/999): loss=0.11618138996407293\n",
      "Gradient Descent(993/999): loss=0.11617657554542982\n",
      "Gradient Descent(994/999): loss=0.11617176588885088\n",
      "Gradient Descent(995/999): loss=0.11616696098779396\n",
      "Gradient Descent(996/999): loss=0.11616216083572746\n",
      "Gradient Descent(997/999): loss=0.11615736542613034\n",
      "Gradient Descent(998/999): loss=0.11615257475249201\n",
      "Gradient Descent(999/999): loss=0.11614778880831238\n",
      "Gradient Descent(0/999): loss=0.23422206561741446\n",
      "Gradient Descent(1/999): loss=0.22274826816031182\n",
      "Gradient Descent(2/999): loss=0.212478664539309\n",
      "Gradient Descent(3/999): loss=0.2032866717495318\n",
      "Gradient Descent(4/999): loss=0.19505901383849528\n",
      "Gradient Descent(5/999): loss=0.18769432298846023\n",
      "Gradient Descent(6/999): loss=0.18110188766168775\n",
      "Gradient Descent(7/999): loss=0.17520053234842226\n",
      "Gradient Descent(8/999): loss=0.16991761508270403\n",
      "Gradient Descent(9/999): loss=0.1651881303455249\n",
      "Gradient Descent(10/999): loss=0.16095390627635042\n",
      "Gradient Descent(11/999): loss=0.15716288627872857\n",
      "Gradient Descent(12/999): loss=0.15376848614795066\n",
      "Gradient Descent(13/999): loss=0.15072901878141748\n",
      "Gradient Descent(14/999): loss=0.14800717936699515\n",
      "Gradient Descent(15/999): loss=0.14556958469153897\n",
      "Gradient Descent(16/999): loss=0.14338636088013496\n",
      "Gradient Descent(17/999): loss=0.14143077447472235\n",
      "Gradient Descent(18/999): loss=0.1396789022959906\n",
      "Gradient Descent(19/999): loss=0.13810933601141231\n",
      "Gradient Descent(20/999): loss=0.13670291776088797\n",
      "Gradient Descent(21/999): loss=0.13544250357503188\n",
      "Gradient Descent(22/999): loss=0.13431275166436593\n",
      "Gradient Descent(23/999): loss=0.13329993296483575\n",
      "Gradient Descent(24/999): loss=0.13239176159992708\n",
      "Gradient Descent(25/999): loss=0.13157724316562583\n",
      "Gradient Descent(26/999): loss=0.1308465389645734\n",
      "Gradient Descent(27/999): loss=0.1301908445127402\n",
      "Gradient Descent(28/999): loss=0.12960228081820066\n",
      "Gradient Descent(29/999): loss=0.12907379708932806\n",
      "Gradient Descent(30/999): loss=0.12859908367087833\n",
      "Gradient Descent(31/999): loss=0.12817249413274273\n",
      "Gradient Descent(32/999): loss=0.12778897554918575\n",
      "Gradient Descent(33/999): loss=0.12744400610753295\n",
      "Gradient Descent(34/999): loss=0.1271335392757924\n",
      "Gradient Descent(35/999): loss=0.12685395383969397\n",
      "Gradient Descent(36/999): loss=0.12660200919211773\n",
      "Gradient Descent(37/999): loss=0.12637480532274717\n",
      "Gradient Descent(38/999): loss=0.1261697470138322\n",
      "Gradient Descent(39/999): loss=0.1259845117998885\n",
      "Gradient Descent(40/999): loss=0.1258170212956466\n",
      "Gradient Descent(41/999): loss=0.1256654155381586\n",
      "Gradient Descent(42/999): loss=0.12552803002619753\n",
      "Gradient Descent(43/999): loss=0.12540337517339165\n",
      "Gradient Descent(44/999): loss=0.1252901179213491\n",
      "Gradient Descent(45/999): loss=0.1251870652857007\n",
      "Gradient Descent(46/999): loss=0.1250931496318614\n",
      "Gradient Descent(47/999): loss=0.12500741549867167\n",
      "Gradient Descent(48/999): loss=0.12492900780719715\n",
      "Gradient Descent(49/999): loss=0.12485716130906946\n",
      "Gradient Descent(50/999): loss=0.1247911911440618\n",
      "Gradient Descent(51/999): loss=0.12473048439028832\n",
      "Gradient Descent(52/999): loss=0.12467449250267874\n",
      "Gradient Descent(53/999): loss=0.12462272454634588\n",
      "Gradient Descent(54/999): loss=0.1245747411412838\n",
      "Gradient Descent(55/999): loss=0.12453014904361659\n",
      "Gradient Descent(56/999): loss=0.12448859629648011\n",
      "Gradient Descent(57/999): loss=0.1244497678906544\n",
      "Gradient Descent(58/999): loss=0.12441338188135792\n",
      "Gradient Descent(59/999): loss=0.12437918591325021\n",
      "Gradient Descent(60/999): loss=0.12434695411072953\n",
      "Gradient Descent(61/999): loss=0.12431648429512401\n",
      "Gradient Descent(62/999): loss=0.1242875954944113\n",
      "Gradient Descent(63/999): loss=0.12426012571471481\n",
      "Gradient Descent(64/999): loss=0.1242339299460572\n",
      "Gradient Descent(65/999): loss=0.12420887837774515\n",
      "Gradient Descent(66/999): loss=0.1241848548013474\n",
      "Gradient Descent(67/999): loss=0.12416175518154604\n",
      "Gradient Descent(68/999): loss=0.12413948637721306\n",
      "Gradient Descent(69/999): loss=0.12411796499692002\n",
      "Gradient Descent(70/999): loss=0.12409711637474867\n",
      "Gradient Descent(71/999): loss=0.12407687365375601\n",
      "Gradient Descent(72/999): loss=0.12405717696577673\n",
      "Gradient Descent(73/999): loss=0.12403797269743579\n",
      "Gradient Descent(74/999): loss=0.12401921283330845\n",
      "Gradient Descent(75/999): loss=0.12400085436811784\n",
      "Gradient Descent(76/999): loss=0.12398285878071258\n",
      "Gradient Descent(77/999): loss=0.12396519156333002\n",
      "Gradient Descent(78/999): loss=0.12394782180033355\n",
      "Gradient Descent(79/999): loss=0.123930721791223\n",
      "Gradient Descent(80/999): loss=0.12391386671326421\n",
      "Gradient Descent(81/999): loss=0.123897234319573\n",
      "Gradient Descent(82/999): loss=0.12388080466892656\n",
      "Gradient Descent(83/999): loss=0.12386455988396709\n",
      "Gradient Descent(84/999): loss=0.12384848393481329\n",
      "Gradient Descent(85/999): loss=0.12383256244540865\n",
      "Gradient Descent(86/999): loss=0.12381678252021655\n",
      "Gradient Descent(87/999): loss=0.12380113258912404\n",
      "Gradient Descent(88/999): loss=0.12378560226863888\n",
      "Gradient Descent(89/999): loss=0.12377018223766893\n",
      "Gradient Descent(90/999): loss=0.12375486412634976\n",
      "Gradient Descent(91/999): loss=0.12373964041654956\n",
      "Gradient Descent(92/999): loss=0.12372450435282387\n",
      "Gradient Descent(93/999): loss=0.12370944986272187\n",
      "Gradient Descent(94/999): loss=0.12369447148546099\n",
      "Gradient Descent(95/999): loss=0.12367956430809063\n",
      "Gradient Descent(96/999): loss=0.12366472390835771\n",
      "Gradient Descent(97/999): loss=0.12364994630356983\n",
      "Gradient Descent(98/999): loss=0.12363522790482537\n",
      "Gradient Descent(99/999): loss=0.12362056547604697\n",
      "Gradient Descent(100/999): loss=0.12360595609731308\n",
      "Gradient Descent(101/999): loss=0.12359139713203648\n",
      "Gradient Descent(102/999): loss=0.12357688619758499\n",
      "Gradient Descent(103/999): loss=0.12356242113898275\n",
      "Gradient Descent(104/999): loss=0.12354800000536888\n",
      "Gradient Descent(105/999): loss=0.12353362102892304\n",
      "Gradient Descent(106/999): loss=0.12351928260599929\n",
      "Gradient Descent(107/999): loss=0.12350498328023594\n",
      "Gradient Descent(108/999): loss=0.12349072172743383\n",
      "Gradient Descent(109/999): loss=0.12347649674201754\n",
      "Gradient Descent(110/999): loss=0.12346230722491278\n",
      "Gradient Descent(111/999): loss=0.12344815217269169\n",
      "Gradient Descent(112/999): loss=0.12343403066785259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(113/999): loss=0.12341994187011512\n",
      "Gradient Descent(114/999): loss=0.1234058850086243\n",
      "Gradient Descent(115/999): loss=0.12339185937496779\n",
      "Gradient Descent(116/999): loss=0.12337786431692124\n",
      "Gradient Descent(117/999): loss=0.12336389923284519\n",
      "Gradient Descent(118/999): loss=0.12334996356666515\n",
      "Gradient Descent(119/999): loss=0.12333605680337383\n",
      "Gradient Descent(120/999): loss=0.12332217846500029\n",
      "Gradient Descent(121/999): loss=0.12330832810699756\n",
      "Gradient Descent(122/999): loss=0.12329450531500467\n",
      "Gradient Descent(123/999): loss=0.12328070970194352\n",
      "Gradient Descent(124/999): loss=0.12326694090541605\n",
      "Gradient Descent(125/999): loss=0.12325319858536977\n",
      "Gradient Descent(126/999): loss=0.12323948242200371\n",
      "Gradient Descent(127/999): loss=0.1232257921138898\n",
      "Gradient Descent(128/999): loss=0.12321212737628673\n",
      "Gradient Descent(129/999): loss=0.12319848793962644\n",
      "Gradient Descent(130/999): loss=0.12318487354815524\n",
      "Gradient Descent(131/999): loss=0.12317128395871306\n",
      "Gradient Descent(132/999): loss=0.12315771893963663\n",
      "Gradient Descent(133/999): loss=0.1231441782697739\n",
      "Gradient Descent(134/999): loss=0.1231306617375974\n",
      "Gradient Descent(135/999): loss=0.12311716914040688\n",
      "Gradient Descent(136/999): loss=0.12310370028361164\n",
      "Gradient Descent(137/999): loss=0.12309025498008423\n",
      "Gradient Descent(138/999): loss=0.12307683304957828\n",
      "Gradient Descent(139/999): loss=0.12306343431820346\n",
      "Gradient Descent(140/999): loss=0.12305005861795214\n",
      "Gradient Descent(141/999): loss=0.12303670578627174\n",
      "Gradient Descent(142/999): loss=0.12302337566567867\n",
      "Gradient Descent(143/999): loss=0.12301006810340931\n",
      "Gradient Descent(144/999): loss=0.12299678295110399\n",
      "Gradient Descent(145/999): loss=0.12298352006452073\n",
      "Gradient Descent(146/999): loss=0.12297027930327604\n",
      "Gradient Descent(147/999): loss=0.12295706053060938\n",
      "Gradient Descent(148/999): loss=0.12294386361316906\n",
      "Gradient Descent(149/999): loss=0.12293068842081792\n",
      "Gradient Descent(150/999): loss=0.12291753482645588\n",
      "Gradient Descent(151/999): loss=0.12290440270585826\n",
      "Gradient Descent(152/999): loss=0.1228912919375281\n",
      "Gradient Descent(153/999): loss=0.12287820240256114\n",
      "Gradient Descent(154/999): loss=0.1228651339845221\n",
      "Gradient Descent(155/999): loss=0.12285208656933094\n",
      "Gradient Descent(156/999): loss=0.1228390600451587\n",
      "Gradient Descent(157/999): loss=0.12282605430233114\n",
      "Gradient Descent(158/999): loss=0.1228130692332402\n",
      "Gradient Descent(159/999): loss=0.12280010473226209\n",
      "Gradient Descent(160/999): loss=0.12278716069568123\n",
      "Gradient Descent(161/999): loss=0.12277423702161995\n",
      "Gradient Descent(162/999): loss=0.12276133360997295\n",
      "Gradient Descent(163/999): loss=0.1227484503623463\n",
      "Gradient Descent(164/999): loss=0.12273558718200052\n",
      "Gradient Descent(165/999): loss=0.1227227439737973\n",
      "Gradient Descent(166/999): loss=0.12270992064414947\n",
      "Gradient Descent(167/999): loss=0.12269711710097421\n",
      "Gradient Descent(168/999): loss=0.12268433325364876\n",
      "Gradient Descent(169/999): loss=0.1226715690129687\n",
      "Gradient Descent(170/999): loss=0.12265882429110883\n",
      "Gradient Descent(171/999): loss=0.12264609900158553\n",
      "Gradient Descent(172/999): loss=0.12263339305922152\n",
      "Gradient Descent(173/999): loss=0.12262070638011226\n",
      "Gradient Descent(174/999): loss=0.12260803888159363\n",
      "Gradient Descent(175/999): loss=0.12259539048221174\n",
      "Gradient Descent(176/999): loss=0.1225827611016933\n",
      "Gradient Descent(177/999): loss=0.12257015066091786\n",
      "Gradient Descent(178/999): loss=0.1225575590818908\n",
      "Gradient Descent(179/999): loss=0.12254498628771766\n",
      "Gradient Descent(180/999): loss=0.1225324322025793\n",
      "Gradient Descent(181/999): loss=0.12251989675170807\n",
      "Gradient Descent(182/999): loss=0.12250737986136459\n",
      "Gradient Descent(183/999): loss=0.1224948814588158\n",
      "Gradient Descent(184/999): loss=0.12248240147231321\n",
      "Gradient Descent(185/999): loss=0.12246993983107224\n",
      "Gradient Descent(186/999): loss=0.12245749646525206\n",
      "Gradient Descent(187/999): loss=0.12244507130593603\n",
      "Gradient Descent(188/999): loss=0.12243266428511271\n",
      "Gradient Descent(189/999): loss=0.12242027533565757\n",
      "Gradient Descent(190/999): loss=0.12240790439131506\n",
      "Gradient Descent(191/999): loss=0.1223955513866811\n",
      "Gradient Descent(192/999): loss=0.1223832162571863\n",
      "Gradient Descent(193/999): loss=0.12237089893907943\n",
      "Gradient Descent(194/999): loss=0.12235859936941128\n",
      "Gradient Descent(195/999): loss=0.12234631748601914\n",
      "Gradient Descent(196/999): loss=0.12233405322751133\n",
      "Gradient Descent(197/999): loss=0.12232180653325253\n",
      "Gradient Descent(198/999): loss=0.12230957734334902\n",
      "Gradient Descent(199/999): loss=0.1222973655986346\n",
      "Gradient Descent(200/999): loss=0.1222851712406567\n",
      "Gradient Descent(201/999): loss=0.1222729942116627\n",
      "Gradient Descent(202/999): loss=0.12226083445458684\n",
      "Gradient Descent(203/999): loss=0.12224869191303715\n",
      "Gradient Descent(204/999): loss=0.12223656653128283\n",
      "Gradient Descent(205/999): loss=0.12222445825424175\n",
      "Gradient Descent(206/999): loss=0.12221236702746846\n",
      "Gradient Descent(207/999): loss=0.12220029279714224\n",
      "Gradient Descent(208/999): loss=0.12218823551005542\n",
      "Gradient Descent(209/999): loss=0.1221761951136021\n",
      "Gradient Descent(210/999): loss=0.12216417155576703\n",
      "Gradient Descent(211/999): loss=0.12215216478511456\n",
      "Gradient Descent(212/999): loss=0.12214017475077815\n",
      "Gradient Descent(213/999): loss=0.12212820140244969\n",
      "Gradient Descent(214/999): loss=0.12211624469036947\n",
      "Gradient Descent(215/999): loss=0.12210430456531599\n",
      "Gradient Descent(216/999): loss=0.12209238097859619\n",
      "Gradient Descent(217/999): loss=0.12208047388203579\n",
      "Gradient Descent(218/999): loss=0.1220685832279699\n",
      "Gradient Descent(219/999): loss=0.12205670896923362\n",
      "Gradient Descent(220/999): loss=0.12204485105915318\n",
      "Gradient Descent(221/999): loss=0.1220330094515369\n",
      "Gradient Descent(222/999): loss=0.1220211841006665\n",
      "Gradient Descent(223/999): loss=0.12200937496128858\n",
      "Gradient Descent(224/999): loss=0.1219975819886064\n",
      "Gradient Descent(225/999): loss=0.12198580513827131\n",
      "Gradient Descent(226/999): loss=0.1219740443663751\n",
      "Gradient Descent(227/999): loss=0.12196229962944184\n",
      "Gradient Descent(228/999): loss=0.12195057088442035\n",
      "Gradient Descent(229/999): loss=0.12193885808867645\n",
      "Gradient Descent(230/999): loss=0.12192716119998571\n",
      "Gradient Descent(231/999): loss=0.12191548017652591\n",
      "Gradient Descent(232/999): loss=0.12190381497687025\n",
      "Gradient Descent(233/999): loss=0.12189216555998005\n",
      "Gradient Descent(234/999): loss=0.12188053188519794\n",
      "Gradient Descent(235/999): loss=0.12186891391224132\n",
      "Gradient Descent(236/999): loss=0.12185731160119552\n",
      "Gradient Descent(237/999): loss=0.12184572491250749\n",
      "Gradient Descent(238/999): loss=0.12183415380697932\n",
      "Gradient Descent(239/999): loss=0.12182259824576218\n",
      "Gradient Descent(240/999): loss=0.12181105819035007\n",
      "Gradient Descent(241/999): loss=0.12179953360257391\n",
      "Gradient Descent(242/999): loss=0.12178802444459572\n",
      "Gradient Descent(243/999): loss=0.1217765306789027\n",
      "Gradient Descent(244/999): loss=0.12176505226830178\n",
      "Gradient Descent(245/999): loss=0.121753589175914\n",
      "Gradient Descent(246/999): loss=0.121742141365169\n",
      "Gradient Descent(247/999): loss=0.1217307087997999\n",
      "Gradient Descent(248/999): loss=0.12171929144383795\n",
      "Gradient Descent(249/999): loss=0.12170788926160735\n",
      "Gradient Descent(250/999): loss=0.12169650221772038\n",
      "Gradient Descent(251/999): loss=0.12168513027707242\n",
      "Gradient Descent(252/999): loss=0.1216737734048371\n",
      "Gradient Descent(253/999): loss=0.12166243156646155\n",
      "Gradient Descent(254/999): loss=0.12165110472766183\n",
      "Gradient Descent(255/999): loss=0.12163979285441838\n",
      "Gradient Descent(256/999): loss=0.12162849591297145\n",
      "Gradient Descent(257/999): loss=0.1216172138698168\n",
      "Gradient Descent(258/999): loss=0.12160594669170136\n",
      "Gradient Descent(259/999): loss=0.12159469434561916\n",
      "Gradient Descent(260/999): loss=0.121583456798807\n",
      "Gradient Descent(261/999): loss=0.12157223401874048\n",
      "Gradient Descent(262/999): loss=0.1215610259731301\n",
      "Gradient Descent(263/999): loss=0.12154983262991724\n",
      "Gradient Descent(264/999): loss=0.12153865395727044\n",
      "Gradient Descent(265/999): loss=0.12152748992358157\n",
      "Gradient Descent(266/999): loss=0.12151634049746225\n",
      "Gradient Descent(267/999): loss=0.12150520564774013\n",
      "Gradient Descent(268/999): loss=0.12149408534345549\n",
      "Gradient Descent(269/999): loss=0.12148297955385749\n",
      "Gradient Descent(270/999): loss=0.12147188824840126\n",
      "Gradient Descent(271/999): loss=0.12146081139674404\n",
      "Gradient Descent(272/999): loss=0.12144974896874229\n",
      "Gradient Descent(273/999): loss=0.12143870093444821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(274/999): loss=0.12142766726410675\n",
      "Gradient Descent(275/999): loss=0.12141664792815256\n",
      "Gradient Descent(276/999): loss=0.12140564289720676\n",
      "Gradient Descent(277/999): loss=0.12139465214207414\n",
      "Gradient Descent(278/999): loss=0.12138367563374022\n",
      "Gradient Descent(279/999): loss=0.12137271334336834\n",
      "Gradient Descent(280/999): loss=0.1213617652422969\n",
      "Gradient Descent(281/999): loss=0.12135083130203658\n",
      "Gradient Descent(282/999): loss=0.12133991149426761\n",
      "Gradient Descent(283/999): loss=0.12132900579083725\n",
      "Gradient Descent(284/999): loss=0.12131811416375707\n",
      "Gradient Descent(285/999): loss=0.12130723658520041\n",
      "Gradient Descent(286/999): loss=0.12129637302749995\n",
      "Gradient Descent(287/999): loss=0.12128552346314522\n",
      "Gradient Descent(288/999): loss=0.12127468786478018\n",
      "Gradient Descent(289/999): loss=0.12126386620520095\n",
      "Gradient Descent(290/999): loss=0.1212530584573533\n",
      "Gradient Descent(291/999): loss=0.12124226459433066\n",
      "Gradient Descent(292/999): loss=0.12123148458937165\n",
      "Gradient Descent(293/999): loss=0.12122071841585792\n",
      "Gradient Descent(294/999): loss=0.12120996604731225\n",
      "Gradient Descent(295/999): loss=0.12119922745739616\n",
      "Gradient Descent(296/999): loss=0.121188502619908\n",
      "Gradient Descent(297/999): loss=0.12117779150878082\n",
      "Gradient Descent(298/999): loss=0.1211670940980806\n",
      "Gradient Descent(299/999): loss=0.12115641036200402\n",
      "Gradient Descent(300/999): loss=0.1211457402748767\n",
      "Gradient Descent(301/999): loss=0.12113508381115139\n",
      "Gradient Descent(302/999): loss=0.12112444094540602\n",
      "Gradient Descent(303/999): loss=0.1211138116523419\n",
      "Gradient Descent(304/999): loss=0.12110319590678201\n",
      "Gradient Descent(305/999): loss=0.12109259368366926\n",
      "Gradient Descent(306/999): loss=0.12108200495806469\n",
      "Gradient Descent(307/999): loss=0.12107142970514595\n",
      "Gradient Descent(308/999): loss=0.12106086790020555\n",
      "Gradient Descent(309/999): loss=0.12105031951864922\n",
      "Gradient Descent(310/999): loss=0.12103978453599451\n",
      "Gradient Descent(311/999): loss=0.12102926292786902\n",
      "Gradient Descent(312/999): loss=0.12101875467000907\n",
      "Gradient Descent(313/999): loss=0.12100825973825802\n",
      "Gradient Descent(314/999): loss=0.120997778108565\n",
      "Gradient Descent(315/999): loss=0.12098730975698335\n",
      "Gradient Descent(316/999): loss=0.12097685465966924\n",
      "Gradient Descent(317/999): loss=0.1209664127928803\n",
      "Gradient Descent(318/999): loss=0.12095598413297419\n",
      "Gradient Descent(319/999): loss=0.12094556865640745\n",
      "Gradient Descent(320/999): loss=0.12093516633973402\n",
      "Gradient Descent(321/999): loss=0.12092477715960395\n",
      "Gradient Descent(322/999): loss=0.12091440109276232\n",
      "Gradient Descent(323/999): loss=0.12090403811604779\n",
      "Gradient Descent(324/999): loss=0.12089368820639157\n",
      "Gradient Descent(325/999): loss=0.12088335134081613\n",
      "Gradient Descent(326/999): loss=0.12087302749643396\n",
      "Gradient Descent(327/999): loss=0.12086271665044669\n",
      "Gradient Descent(328/999): loss=0.1208524187801436\n",
      "Gradient Descent(329/999): loss=0.1208421338629009\n",
      "Gradient Descent(330/999): loss=0.12083186187618027\n",
      "Gradient Descent(331/999): loss=0.12082160279752813\n",
      "Gradient Descent(332/999): loss=0.1208113566045744\n",
      "Gradient Descent(333/999): loss=0.12080112327503152\n",
      "Gradient Descent(334/999): loss=0.12079090278669345\n",
      "Gradient Descent(335/999): loss=0.12078069511743465\n",
      "Gradient Descent(336/999): loss=0.12077050024520927\n",
      "Gradient Descent(337/999): loss=0.12076031814804991\n",
      "Gradient Descent(338/999): loss=0.120750148804067\n",
      "Gradient Descent(339/999): loss=0.12073999219144763\n",
      "Gradient Descent(340/999): loss=0.12072984828845483\n",
      "Gradient Descent(341/999): loss=0.1207197170734266\n",
      "Gradient Descent(342/999): loss=0.120709598524775\n",
      "Gradient Descent(343/999): loss=0.12069949262098546\n",
      "Gradient Descent(344/999): loss=0.12068939934061579\n",
      "Gradient Descent(345/999): loss=0.12067931866229537\n",
      "Gradient Descent(346/999): loss=0.12066925056472447\n",
      "Gradient Descent(347/999): loss=0.12065919502667335\n",
      "Gradient Descent(348/999): loss=0.12064915202698148\n",
      "Gradient Descent(349/999): loss=0.12063912154455689\n",
      "Gradient Descent(350/999): loss=0.12062910355837522\n",
      "Gradient Descent(351/999): loss=0.12061909804747925\n",
      "Gradient Descent(352/999): loss=0.1206091049909779\n",
      "Gradient Descent(353/999): loss=0.12059912436804578\n",
      "Gradient Descent(354/999): loss=0.12058915615792234\n",
      "Gradient Descent(355/999): loss=0.12057920033991117\n",
      "Gradient Descent(356/999): loss=0.1205692568933794\n",
      "Gradient Descent(357/999): loss=0.12055932579775709\n",
      "Gradient Descent(358/999): loss=0.12054940703253642\n",
      "Gradient Descent(359/999): loss=0.12053950057727125\n",
      "Gradient Descent(360/999): loss=0.12052960641157631\n",
      "Gradient Descent(361/999): loss=0.12051972451512669\n",
      "Gradient Descent(362/999): loss=0.12050985486765728\n",
      "Gradient Descent(363/999): loss=0.12049999744896206\n",
      "Gradient Descent(364/999): loss=0.12049015223889363\n",
      "Gradient Descent(365/999): loss=0.12048031921736255\n",
      "Gradient Descent(366/999): loss=0.12047049836433686\n",
      "Gradient Descent(367/999): loss=0.12046068965984146\n",
      "Gradient Descent(368/999): loss=0.12045089308395762\n",
      "Gradient Descent(369/999): loss=0.12044110861682242\n",
      "Gradient Descent(370/999): loss=0.1204313362386282\n",
      "Gradient Descent(371/999): loss=0.12042157592962223\n",
      "Gradient Descent(372/999): loss=0.12041182767010589\n",
      "Gradient Descent(373/999): loss=0.1204020914404344\n",
      "Gradient Descent(374/999): loss=0.12039236722101639\n",
      "Gradient Descent(375/999): loss=0.12038265499231317\n",
      "Gradient Descent(376/999): loss=0.12037295473483849\n",
      "Gradient Descent(377/999): loss=0.12036326642915791\n",
      "Gradient Descent(378/999): loss=0.12035359005588857\n",
      "Gradient Descent(379/999): loss=0.12034392559569851\n",
      "Gradient Descent(380/999): loss=0.12033427302930634\n",
      "Gradient Descent(381/999): loss=0.12032463233748081\n",
      "Gradient Descent(382/999): loss=0.12031500350104038\n",
      "Gradient Descent(383/999): loss=0.1203053865008528\n",
      "Gradient Descent(384/999): loss=0.12029578131783474\n",
      "Gradient Descent(385/999): loss=0.1202861879329512\n",
      "Gradient Descent(386/999): loss=0.12027660632721555\n",
      "Gradient Descent(387/999): loss=0.12026703648168857\n",
      "Gradient Descent(388/999): loss=0.12025747837747854\n",
      "Gradient Descent(389/999): loss=0.1202479319957406\n",
      "Gradient Descent(390/999): loss=0.12023839731767655\n",
      "Gradient Descent(391/999): loss=0.12022887432453429\n",
      "Gradient Descent(392/999): loss=0.12021936299760766\n",
      "Gradient Descent(393/999): loss=0.12020986331823603\n",
      "Gradient Descent(394/999): loss=0.12020037526780385\n",
      "Gradient Descent(395/999): loss=0.12019089882774045\n",
      "Gradient Descent(396/999): loss=0.12018143397951966\n",
      "Gradient Descent(397/999): loss=0.12017198070465947\n",
      "Gradient Descent(398/999): loss=0.12016253898472165\n",
      "Gradient Descent(399/999): loss=0.12015310880131161\n",
      "Gradient Descent(400/999): loss=0.12014369013607792\n",
      "Gradient Descent(401/999): loss=0.12013428297071205\n",
      "Gradient Descent(402/999): loss=0.12012488728694808\n",
      "Gradient Descent(403/999): loss=0.12011550306656242\n",
      "Gradient Descent(404/999): loss=0.1201061302913735\n",
      "Gradient Descent(405/999): loss=0.12009676894324153\n",
      "Gradient Descent(406/999): loss=0.12008741900406805\n",
      "Gradient Descent(407/999): loss=0.12007808045579588\n",
      "Gradient Descent(408/999): loss=0.12006875328040877\n",
      "Gradient Descent(409/999): loss=0.12005943745993095\n",
      "Gradient Descent(410/999): loss=0.1200501329764272\n",
      "Gradient Descent(411/999): loss=0.12004083981200224\n",
      "Gradient Descent(412/999): loss=0.12003155794880083\n",
      "Gradient Descent(413/999): loss=0.12002228736900718\n",
      "Gradient Descent(414/999): loss=0.12001302805484496\n",
      "Gradient Descent(415/999): loss=0.12000377998857681\n",
      "Gradient Descent(416/999): loss=0.1199945431525044\n",
      "Gradient Descent(417/999): loss=0.11998531752896793\n",
      "Gradient Descent(418/999): loss=0.11997610310034601\n",
      "Gradient Descent(419/999): loss=0.11996689984905551\n",
      "Gradient Descent(420/999): loss=0.11995770775755113\n",
      "Gradient Descent(421/999): loss=0.11994852680832534\n",
      "Gradient Descent(422/999): loss=0.11993935698390809\n",
      "Gradient Descent(423/999): loss=0.11993019826686668\n",
      "Gradient Descent(424/999): loss=0.11992105063980549\n",
      "Gradient Descent(425/999): loss=0.11991191408536564\n",
      "Gradient Descent(426/999): loss=0.11990278858622506\n",
      "Gradient Descent(427/999): loss=0.11989367412509808\n",
      "Gradient Descent(428/999): loss=0.11988457068473528\n",
      "Gradient Descent(429/999): loss=0.11987547824792331\n",
      "Gradient Descent(430/999): loss=0.11986639679748469\n",
      "Gradient Descent(431/999): loss=0.11985732631627766\n",
      "Gradient Descent(432/999): loss=0.11984826678719587\n",
      "Gradient Descent(433/999): loss=0.11983921819316831\n",
      "Gradient Descent(434/999): loss=0.11983018051715917\n",
      "Gradient Descent(435/999): loss=0.11982115374216748\n",
      "Gradient Descent(436/999): loss=0.11981213785122707\n",
      "Gradient Descent(437/999): loss=0.11980313282740636\n",
      "Gradient Descent(438/999): loss=0.1197941386538082\n",
      "Gradient Descent(439/999): loss=0.11978515531356965\n",
      "Gradient Descent(440/999): loss=0.11977618278986195\n",
      "Gradient Descent(441/999): loss=0.11976722106589015\n",
      "Gradient Descent(442/999): loss=0.11975827012489308\n",
      "Gradient Descent(443/999): loss=0.11974932995014324\n",
      "Gradient Descent(444/999): loss=0.11974040052494644\n",
      "Gradient Descent(445/999): loss=0.11973148183264183\n",
      "Gradient Descent(446/999): loss=0.11972257385660175\n",
      "Gradient Descent(447/999): loss=0.11971367658023144\n",
      "Gradient Descent(448/999): loss=0.11970478998696894\n",
      "Gradient Descent(449/999): loss=0.11969591406028503\n",
      "Gradient Descent(450/999): loss=0.11968704878368301\n",
      "Gradient Descent(451/999): loss=0.11967819414069857\n",
      "Gradient Descent(452/999): loss=0.11966935011489958\n",
      "Gradient Descent(453/999): loss=0.11966051668988612\n",
      "Gradient Descent(454/999): loss=0.11965169384929018\n",
      "Gradient Descent(455/999): loss=0.11964288157677556\n",
      "Gradient Descent(456/999): loss=0.11963407985603787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(457/999): loss=0.11962528867080412\n",
      "Gradient Descent(458/999): loss=0.1196165080048329\n",
      "Gradient Descent(459/999): loss=0.119607737841914\n",
      "Gradient Descent(460/999): loss=0.11959897816586847\n",
      "Gradient Descent(461/999): loss=0.11959022896054837\n",
      "Gradient Descent(462/999): loss=0.11958149020983672\n",
      "Gradient Descent(463/999): loss=0.11957276189764729\n",
      "Gradient Descent(464/999): loss=0.11956404400792459\n",
      "Gradient Descent(465/999): loss=0.11955533652464365\n",
      "Gradient Descent(466/999): loss=0.11954663943181\n",
      "Gradient Descent(467/999): loss=0.11953795271345947\n",
      "Gradient Descent(468/999): loss=0.11952927635365806\n",
      "Gradient Descent(469/999): loss=0.11952061033650194\n",
      "Gradient Descent(470/999): loss=0.11951195464611729\n",
      "Gradient Descent(471/999): loss=0.11950330926666003\n",
      "Gradient Descent(472/999): loss=0.11949467418231602\n",
      "Gradient Descent(473/999): loss=0.11948604937730062\n",
      "Gradient Descent(474/999): loss=0.11947743483585888\n",
      "Gradient Descent(475/999): loss=0.1194688305422652\n",
      "Gradient Descent(476/999): loss=0.11946023648082334\n",
      "Gradient Descent(477/999): loss=0.11945165263586635\n",
      "Gradient Descent(478/999): loss=0.11944307899175638\n",
      "Gradient Descent(479/999): loss=0.11943451553288455\n",
      "Gradient Descent(480/999): loss=0.11942596224367101\n",
      "Gradient Descent(481/999): loss=0.11941741910856471\n",
      "Gradient Descent(482/999): loss=0.1194088861120433\n",
      "Gradient Descent(483/999): loss=0.11940036323861314\n",
      "Gradient Descent(484/999): loss=0.11939185047280902\n",
      "Gradient Descent(485/999): loss=0.11938334779919432\n",
      "Gradient Descent(486/999): loss=0.11937485520236073\n",
      "Gradient Descent(487/999): loss=0.11936637266692812\n",
      "Gradient Descent(488/999): loss=0.11935790017754462\n",
      "Gradient Descent(489/999): loss=0.11934943771888641\n",
      "Gradient Descent(490/999): loss=0.11934098527565763\n",
      "Gradient Descent(491/999): loss=0.11933254283259045\n",
      "Gradient Descent(492/999): loss=0.11932411037444467\n",
      "Gradient Descent(493/999): loss=0.11931568788600788\n",
      "Gradient Descent(494/999): loss=0.11930727535209544\n",
      "Gradient Descent(495/999): loss=0.11929887275755009\n",
      "Gradient Descent(496/999): loss=0.11929048008724213\n",
      "Gradient Descent(497/999): loss=0.11928209732606926\n",
      "Gradient Descent(498/999): loss=0.11927372445895638\n",
      "Gradient Descent(499/999): loss=0.11926536147085574\n",
      "Gradient Descent(500/999): loss=0.11925700834674668\n",
      "Gradient Descent(501/999): loss=0.11924866507163562\n",
      "Gradient Descent(502/999): loss=0.11924033163055588\n",
      "Gradient Descent(503/999): loss=0.11923200800856779\n",
      "Gradient Descent(504/999): loss=0.11922369419075844\n",
      "Gradient Descent(505/999): loss=0.11921539016224171\n",
      "Gradient Descent(506/999): loss=0.11920709590815812\n",
      "Gradient Descent(507/999): loss=0.11919881141367476\n",
      "Gradient Descent(508/999): loss=0.11919053666398532\n",
      "Gradient Descent(509/999): loss=0.1191822716443099\n",
      "Gradient Descent(510/999): loss=0.11917401633989488\n",
      "Gradient Descent(511/999): loss=0.11916577073601312\n",
      "Gradient Descent(512/999): loss=0.11915753481796353\n",
      "Gradient Descent(513/999): loss=0.11914930857107123\n",
      "Gradient Descent(514/999): loss=0.1191410919806875\n",
      "Gradient Descent(515/999): loss=0.11913288503218955\n",
      "Gradient Descent(516/999): loss=0.11912468771098052\n",
      "Gradient Descent(517/999): loss=0.1191165000024895\n",
      "Gradient Descent(518/999): loss=0.11910832189217128\n",
      "Gradient Descent(519/999): loss=0.11910015336550647\n",
      "Gradient Descent(520/999): loss=0.11909199440800136\n",
      "Gradient Descent(521/999): loss=0.11908384500518766\n",
      "Gradient Descent(522/999): loss=0.11907570514262289\n",
      "Gradient Descent(523/999): loss=0.11906757480588981\n",
      "Gradient Descent(524/999): loss=0.11905945398059668\n",
      "Gradient Descent(525/999): loss=0.11905134265237707\n",
      "Gradient Descent(526/999): loss=0.11904324080688988\n",
      "Gradient Descent(527/999): loss=0.11903514842981908\n",
      "Gradient Descent(528/999): loss=0.11902706550687388\n",
      "Gradient Descent(529/999): loss=0.11901899202378864\n",
      "Gradient Descent(530/999): loss=0.11901092796632255\n",
      "Gradient Descent(531/999): loss=0.1190028733202599\n",
      "Gradient Descent(532/999): loss=0.11899482807140982\n",
      "Gradient Descent(533/999): loss=0.11898679220560629\n",
      "Gradient Descent(534/999): loss=0.11897876570870806\n",
      "Gradient Descent(535/999): loss=0.11897074856659857\n",
      "Gradient Descent(536/999): loss=0.11896274076518587\n",
      "Gradient Descent(537/999): loss=0.11895474229040273\n",
      "Gradient Descent(538/999): loss=0.11894675312820632\n",
      "Gradient Descent(539/999): loss=0.11893877326457834\n",
      "Gradient Descent(540/999): loss=0.1189308026855249\n",
      "Gradient Descent(541/999): loss=0.11892284137707645\n",
      "Gradient Descent(542/999): loss=0.11891488932528776\n",
      "Gradient Descent(543/999): loss=0.11890694651623783\n",
      "Gradient Descent(544/999): loss=0.11889901293602982\n",
      "Gradient Descent(545/999): loss=0.11889108857079106\n",
      "Gradient Descent(546/999): loss=0.11888317340667293\n",
      "Gradient Descent(547/999): loss=0.11887526742985079\n",
      "Gradient Descent(548/999): loss=0.11886737062652405\n",
      "Gradient Descent(549/999): loss=0.11885948298291596\n",
      "Gradient Descent(550/999): loss=0.11885160448527357\n",
      "Gradient Descent(551/999): loss=0.11884373511986784\n",
      "Gradient Descent(552/999): loss=0.11883587487299345\n",
      "Gradient Descent(553/999): loss=0.11882802373096862\n",
      "Gradient Descent(554/999): loss=0.1188201816801354\n",
      "Gradient Descent(555/999): loss=0.11881234870685936\n",
      "Gradient Descent(556/999): loss=0.11880452479752954\n",
      "Gradient Descent(557/999): loss=0.11879670993855845\n",
      "Gradient Descent(558/999): loss=0.11878890411638213\n",
      "Gradient Descent(559/999): loss=0.11878110731745992\n",
      "Gradient Descent(560/999): loss=0.11877331952827448\n",
      "Gradient Descent(561/999): loss=0.1187655407353317\n",
      "Gradient Descent(562/999): loss=0.11875777092516082\n",
      "Gradient Descent(563/999): loss=0.1187500100843141\n",
      "Gradient Descent(564/999): loss=0.11874225819936703\n",
      "Gradient Descent(565/999): loss=0.11873451525691804\n",
      "Gradient Descent(566/999): loss=0.11872678124358871\n",
      "Gradient Descent(567/999): loss=0.11871905614602349\n",
      "Gradient Descent(568/999): loss=0.11871133995088981\n",
      "Gradient Descent(569/999): loss=0.11870363264487789\n",
      "Gradient Descent(570/999): loss=0.1186959342147009\n",
      "Gradient Descent(571/999): loss=0.11868824464709464\n",
      "Gradient Descent(572/999): loss=0.11868056392881772\n",
      "Gradient Descent(573/999): loss=0.11867289204665138\n",
      "Gradient Descent(574/999): loss=0.11866522898739952\n",
      "Gradient Descent(575/999): loss=0.11865757473788857\n",
      "Gradient Descent(576/999): loss=0.11864992928496758\n",
      "Gradient Descent(577/999): loss=0.11864229261550795\n",
      "Gradient Descent(578/999): loss=0.11863466471640363\n",
      "Gradient Descent(579/999): loss=0.1186270455745709\n",
      "Gradient Descent(580/999): loss=0.11861943517694844\n",
      "Gradient Descent(581/999): loss=0.11861183351049714\n",
      "Gradient Descent(582/999): loss=0.1186042405622002\n",
      "Gradient Descent(583/999): loss=0.11859665631906305\n",
      "Gradient Descent(584/999): loss=0.11858908076811323\n",
      "Gradient Descent(585/999): loss=0.11858151389640037\n",
      "Gradient Descent(586/999): loss=0.11857395569099623\n",
      "Gradient Descent(587/999): loss=0.11856640613899455\n",
      "Gradient Descent(588/999): loss=0.11855886522751109\n",
      "Gradient Descent(589/999): loss=0.11855133294368347\n",
      "Gradient Descent(590/999): loss=0.11854380927467133\n",
      "Gradient Descent(591/999): loss=0.11853629420765595\n",
      "Gradient Descent(592/999): loss=0.11852878772984063\n",
      "Gradient Descent(593/999): loss=0.11852128982845025\n",
      "Gradient Descent(594/999): loss=0.11851380049073156\n",
      "Gradient Descent(595/999): loss=0.11850631970395284\n",
      "Gradient Descent(596/999): loss=0.11849884745540405\n",
      "Gradient Descent(597/999): loss=0.1184913837323967\n",
      "Gradient Descent(598/999): loss=0.11848392852226398\n",
      "Gradient Descent(599/999): loss=0.11847648181236037\n",
      "Gradient Descent(600/999): loss=0.1184690435900619\n",
      "Gradient Descent(601/999): loss=0.11846161384276606\n",
      "Gradient Descent(602/999): loss=0.1184541925578916\n",
      "Gradient Descent(603/999): loss=0.11844677972287868\n",
      "Gradient Descent(604/999): loss=0.11843937532518871\n",
      "Gradient Descent(605/999): loss=0.11843197935230437\n",
      "Gradient Descent(606/999): loss=0.11842459179172947\n",
      "Gradient Descent(607/999): loss=0.11841721263098907\n",
      "Gradient Descent(608/999): loss=0.11840984185762932\n",
      "Gradient Descent(609/999): loss=0.11840247945921736\n",
      "Gradient Descent(610/999): loss=0.11839512542334144\n",
      "Gradient Descent(611/999): loss=0.11838777973761079\n",
      "Gradient Descent(612/999): loss=0.11838044238965564\n",
      "Gradient Descent(613/999): loss=0.1183731133671271\n",
      "Gradient Descent(614/999): loss=0.11836579265769706\n",
      "Gradient Descent(615/999): loss=0.11835848024905835\n",
      "Gradient Descent(616/999): loss=0.11835117612892458\n",
      "Gradient Descent(617/999): loss=0.11834388028503004\n",
      "Gradient Descent(618/999): loss=0.1183365927051298\n",
      "Gradient Descent(619/999): loss=0.11832931337699959\n",
      "Gradient Descent(620/999): loss=0.11832204228843574\n",
      "Gradient Descent(621/999): loss=0.11831477942725514\n",
      "Gradient Descent(622/999): loss=0.11830752478129536\n",
      "Gradient Descent(623/999): loss=0.11830027833841435\n",
      "Gradient Descent(624/999): loss=0.11829304008649057\n",
      "Gradient Descent(625/999): loss=0.11828581001342295\n",
      "Gradient Descent(626/999): loss=0.11827858810713075\n",
      "Gradient Descent(627/999): loss=0.11827137435555365\n",
      "Gradient Descent(628/999): loss=0.11826416874665156\n",
      "Gradient Descent(629/999): loss=0.1182569712684048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(630/999): loss=0.11824978190881386\n",
      "Gradient Descent(631/999): loss=0.1182426006558993\n",
      "Gradient Descent(632/999): loss=0.11823542749770208\n",
      "Gradient Descent(633/999): loss=0.11822826242228313\n",
      "Gradient Descent(634/999): loss=0.1182211054177235\n",
      "Gradient Descent(635/999): loss=0.11821395647212429\n",
      "Gradient Descent(636/999): loss=0.11820681557360663\n",
      "Gradient Descent(637/999): loss=0.11819968271031159\n",
      "Gradient Descent(638/999): loss=0.11819255787040019\n",
      "Gradient Descent(639/999): loss=0.11818544104205334\n",
      "Gradient Descent(640/999): loss=0.11817833221347185\n",
      "Gradient Descent(641/999): loss=0.11817123137287629\n",
      "Gradient Descent(642/999): loss=0.118164138508507\n",
      "Gradient Descent(643/999): loss=0.1181570536086242\n",
      "Gradient Descent(644/999): loss=0.11814997666150767\n",
      "Gradient Descent(645/999): loss=0.118142907655457\n",
      "Gradient Descent(646/999): loss=0.11813584657879125\n",
      "Gradient Descent(647/999): loss=0.11812879341984923\n",
      "Gradient Descent(648/999): loss=0.1181217481669893\n",
      "Gradient Descent(649/999): loss=0.11811471080858921\n",
      "Gradient Descent(650/999): loss=0.11810768133304639\n",
      "Gradient Descent(651/999): loss=0.11810065972877758\n",
      "Gradient Descent(652/999): loss=0.11809364598421906\n",
      "Gradient Descent(653/999): loss=0.11808664008782635\n",
      "Gradient Descent(654/999): loss=0.11807964202807444\n",
      "Gradient Descent(655/999): loss=0.11807265179345759\n",
      "Gradient Descent(656/999): loss=0.11806566937248937\n",
      "Gradient Descent(657/999): loss=0.11805869475370245\n",
      "Gradient Descent(658/999): loss=0.1180517279256489\n",
      "Gradient Descent(659/999): loss=0.11804476887689982\n",
      "Gradient Descent(660/999): loss=0.11803781759604552\n",
      "Gradient Descent(661/999): loss=0.11803087407169535\n",
      "Gradient Descent(662/999): loss=0.1180239382924778\n",
      "Gradient Descent(663/999): loss=0.11801701024704028\n",
      "Gradient Descent(664/999): loss=0.11801008992404927\n",
      "Gradient Descent(665/999): loss=0.11800317731219023\n",
      "Gradient Descent(666/999): loss=0.11799627240016743\n",
      "Gradient Descent(667/999): loss=0.11798937517670417\n",
      "Gradient Descent(668/999): loss=0.11798248563054248\n",
      "Gradient Descent(669/999): loss=0.11797560375044332\n",
      "Gradient Descent(670/999): loss=0.11796872952518633\n",
      "Gradient Descent(671/999): loss=0.11796186294356997\n",
      "Gradient Descent(672/999): loss=0.11795500399441142\n",
      "Gradient Descent(673/999): loss=0.11794815266654644\n",
      "Gradient Descent(674/999): loss=0.11794130894882958\n",
      "Gradient Descent(675/999): loss=0.11793447283013393\n",
      "Gradient Descent(676/999): loss=0.11792764429935111\n",
      "Gradient Descent(677/999): loss=0.11792082334539139\n",
      "Gradient Descent(678/999): loss=0.11791400995718351\n",
      "Gradient Descent(679/999): loss=0.11790720412367463\n",
      "Gradient Descent(680/999): loss=0.11790040583383044\n",
      "Gradient Descent(681/999): loss=0.117893615076635\n",
      "Gradient Descent(682/999): loss=0.11788683184109075\n",
      "Gradient Descent(683/999): loss=0.11788005611621845\n",
      "Gradient Descent(684/999): loss=0.1178732878910572\n",
      "Gradient Descent(685/999): loss=0.11786652715466438\n",
      "Gradient Descent(686/999): loss=0.11785977389611561\n",
      "Gradient Descent(687/999): loss=0.11785302810450474\n",
      "Gradient Descent(688/999): loss=0.1178462897689437\n",
      "Gradient Descent(689/999): loss=0.11783955887856268\n",
      "Gradient Descent(690/999): loss=0.11783283542250993\n",
      "Gradient Descent(691/999): loss=0.11782611938995176\n",
      "Gradient Descent(692/999): loss=0.1178194107700726\n",
      "Gradient Descent(693/999): loss=0.11781270955207479\n",
      "Gradient Descent(694/999): loss=0.11780601572517876\n",
      "Gradient Descent(695/999): loss=0.11779932927862281\n",
      "Gradient Descent(696/999): loss=0.11779265020166313\n",
      "Gradient Descent(697/999): loss=0.11778597848357389\n",
      "Gradient Descent(698/999): loss=0.11777931411364706\n",
      "Gradient Descent(699/999): loss=0.11777265708119242\n",
      "Gradient Descent(700/999): loss=0.11776600737553752\n",
      "Gradient Descent(701/999): loss=0.11775936498602775\n",
      "Gradient Descent(702/999): loss=0.11775272990202611\n",
      "Gradient Descent(703/999): loss=0.11774610211291335\n",
      "Gradient Descent(704/999): loss=0.1177394816080879\n",
      "Gradient Descent(705/999): loss=0.11773286837696577\n",
      "Gradient Descent(706/999): loss=0.11772626240898056\n",
      "Gradient Descent(707/999): loss=0.1177196636935835\n",
      "Gradient Descent(708/999): loss=0.11771307222024327\n",
      "Gradient Descent(709/999): loss=0.11770648797844609\n",
      "Gradient Descent(710/999): loss=0.11769991095769564\n",
      "Gradient Descent(711/999): loss=0.11769334114751305\n",
      "Gradient Descent(712/999): loss=0.11768677853743684\n",
      "Gradient Descent(713/999): loss=0.1176802231170229\n",
      "Gradient Descent(714/999): loss=0.11767367487584456\n",
      "Gradient Descent(715/999): loss=0.11766713380349225\n",
      "Gradient Descent(716/999): loss=0.11766059988957385\n",
      "Gradient Descent(717/999): loss=0.1176540731237145\n",
      "Gradient Descent(718/999): loss=0.11764755349555643\n",
      "Gradient Descent(719/999): loss=0.11764104099475918\n",
      "Gradient Descent(720/999): loss=0.11763453561099937\n",
      "Gradient Descent(721/999): loss=0.11762803733397081\n",
      "Gradient Descent(722/999): loss=0.11762154615338435\n",
      "Gradient Descent(723/999): loss=0.11761506205896792\n",
      "Gradient Descent(724/999): loss=0.11760858504046652\n",
      "Gradient Descent(725/999): loss=0.1176021150876421\n",
      "Gradient Descent(726/999): loss=0.11759565219027361\n",
      "Gradient Descent(727/999): loss=0.11758919633815693\n",
      "Gradient Descent(728/999): loss=0.1175827475211049\n",
      "Gradient Descent(729/999): loss=0.11757630572894719\n",
      "Gradient Descent(730/999): loss=0.11756987095153032\n",
      "Gradient Descent(731/999): loss=0.1175634431787177\n",
      "Gradient Descent(732/999): loss=0.11755702240038941\n",
      "Gradient Descent(733/999): loss=0.11755060860644244\n",
      "Gradient Descent(734/999): loss=0.11754420178679041\n",
      "Gradient Descent(735/999): loss=0.11753780193136365\n",
      "Gradient Descent(736/999): loss=0.11753140903010922\n",
      "Gradient Descent(737/999): loss=0.11752502307299072\n",
      "Gradient Descent(738/999): loss=0.1175186440499885\n",
      "Gradient Descent(739/999): loss=0.1175122719510994\n",
      "Gradient Descent(740/999): loss=0.11750590676633682\n",
      "Gradient Descent(741/999): loss=0.11749954848573071\n",
      "Gradient Descent(742/999): loss=0.1174931970993275\n",
      "Gradient Descent(743/999): loss=0.11748685259719008\n",
      "Gradient Descent(744/999): loss=0.11748051496939782\n",
      "Gradient Descent(745/999): loss=0.11747418420604641\n",
      "Gradient Descent(746/999): loss=0.11746786029724803\n",
      "Gradient Descent(747/999): loss=0.11746154323313111\n",
      "Gradient Descent(748/999): loss=0.11745523300384045\n",
      "Gradient Descent(749/999): loss=0.1174489295995371\n",
      "Gradient Descent(750/999): loss=0.11744263301039842\n",
      "Gradient Descent(751/999): loss=0.11743634322661797\n",
      "Gradient Descent(752/999): loss=0.11743006023840556\n",
      "Gradient Descent(753/999): loss=0.1174237840359871\n",
      "Gradient Descent(754/999): loss=0.1174175146096047\n",
      "Gradient Descent(755/999): loss=0.11741125194951657\n",
      "Gradient Descent(756/999): loss=0.117404996045997\n",
      "Gradient Descent(757/999): loss=0.11739874688933634\n",
      "Gradient Descent(758/999): loss=0.117392504469841\n",
      "Gradient Descent(759/999): loss=0.11738626877783336\n",
      "Gradient Descent(760/999): loss=0.1173800398036518\n",
      "Gradient Descent(761/999): loss=0.11737381753765064\n",
      "Gradient Descent(762/999): loss=0.11736760197020008\n",
      "Gradient Descent(763/999): loss=0.11736139309168622\n",
      "Gradient Descent(764/999): loss=0.11735519089251109\n",
      "Gradient Descent(765/999): loss=0.1173489953630925\n",
      "Gradient Descent(766/999): loss=0.11734280649386403\n",
      "Gradient Descent(767/999): loss=0.1173366242752751\n",
      "Gradient Descent(768/999): loss=0.11733044869779087\n",
      "Gradient Descent(769/999): loss=0.11732427975189215\n",
      "Gradient Descent(770/999): loss=0.11731811742807555\n",
      "Gradient Descent(771/999): loss=0.11731196171685328\n",
      "Gradient Descent(772/999): loss=0.11730581260875317\n",
      "Gradient Descent(773/999): loss=0.11729967009431869\n",
      "Gradient Descent(774/999): loss=0.11729353416410895\n",
      "Gradient Descent(775/999): loss=0.11728740480869847\n",
      "Gradient Descent(776/999): loss=0.11728128201867742\n",
      "Gradient Descent(777/999): loss=0.11727516578465148\n",
      "Gradient Descent(778/999): loss=0.11726905609724167\n",
      "Gradient Descent(779/999): loss=0.11726295294708457\n",
      "Gradient Descent(780/999): loss=0.11725685632483213\n",
      "Gradient Descent(781/999): loss=0.11725076622115173\n",
      "Gradient Descent(782/999): loss=0.11724468262672605\n",
      "Gradient Descent(783/999): loss=0.1172386055322532\n",
      "Gradient Descent(784/999): loss=0.11723253492844644\n",
      "Gradient Descent(785/999): loss=0.11722647080603449\n",
      "Gradient Descent(786/999): loss=0.11722041315576125\n",
      "Gradient Descent(787/999): loss=0.11721436196838576\n",
      "Gradient Descent(788/999): loss=0.11720831723468238\n",
      "Gradient Descent(789/999): loss=0.11720227894544062\n",
      "Gradient Descent(790/999): loss=0.1171962470914651\n",
      "Gradient Descent(791/999): loss=0.11719022166357558\n",
      "Gradient Descent(792/999): loss=0.11718420265260693\n",
      "Gradient Descent(793/999): loss=0.11717819004940903\n",
      "Gradient Descent(794/999): loss=0.11717218384484684\n",
      "Gradient Descent(795/999): loss=0.11716618402980035\n",
      "Gradient Descent(796/999): loss=0.11716019059516455\n",
      "Gradient Descent(797/999): loss=0.11715420353184923\n",
      "Gradient Descent(798/999): loss=0.11714822283077937\n",
      "Gradient Descent(799/999): loss=0.11714224848289466\n",
      "Gradient Descent(800/999): loss=0.11713628047914972\n",
      "Gradient Descent(801/999): loss=0.1171303188105141\n",
      "Gradient Descent(802/999): loss=0.11712436346797205\n",
      "Gradient Descent(803/999): loss=0.11711841444252274\n",
      "Gradient Descent(804/999): loss=0.11711247172518004\n",
      "Gradient Descent(805/999): loss=0.11710653530697257\n",
      "Gradient Descent(806/999): loss=0.11710060517894375\n",
      "Gradient Descent(807/999): loss=0.11709468133215162\n",
      "Gradient Descent(808/999): loss=0.11708876375766888\n",
      "Gradient Descent(809/999): loss=0.11708285244658298\n",
      "Gradient Descent(810/999): loss=0.11707694738999587\n",
      "Gradient Descent(811/999): loss=0.11707104857902416\n",
      "Gradient Descent(812/999): loss=0.11706515600479903\n",
      "Gradient Descent(813/999): loss=0.11705926965846616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(814/999): loss=0.1170533895311858\n",
      "Gradient Descent(815/999): loss=0.11704751561413267\n",
      "Gradient Descent(816/999): loss=0.11704164789849593\n",
      "Gradient Descent(817/999): loss=0.11703578637547916\n",
      "Gradient Descent(818/999): loss=0.11702993103630052\n",
      "Gradient Descent(819/999): loss=0.11702408187219229\n",
      "Gradient Descent(820/999): loss=0.11701823887440133\n",
      "Gradient Descent(821/999): loss=0.11701240203418878\n",
      "Gradient Descent(822/999): loss=0.11700657134283005\n",
      "Gradient Descent(823/999): loss=0.11700074679161492\n",
      "Gradient Descent(824/999): loss=0.1169949283718473\n",
      "Gradient Descent(825/999): loss=0.1169891160748455\n",
      "Gradient Descent(826/999): loss=0.11698330989194188\n",
      "Gradient Descent(827/999): loss=0.11697750981448314\n",
      "Gradient Descent(828/999): loss=0.11697171583383006\n",
      "Gradient Descent(829/999): loss=0.11696592794135757\n",
      "Gradient Descent(830/999): loss=0.1169601461284547\n",
      "Gradient Descent(831/999): loss=0.1169543703865246\n",
      "Gradient Descent(832/999): loss=0.11694860070698444\n",
      "Gradient Descent(833/999): loss=0.11694283708126547\n",
      "Gradient Descent(834/999): loss=0.11693707950081297\n",
      "Gradient Descent(835/999): loss=0.1169313279570861\n",
      "Gradient Descent(836/999): loss=0.11692558244155814\n",
      "Gradient Descent(837/999): loss=0.11691984294571618\n",
      "Gradient Descent(838/999): loss=0.11691410946106127\n",
      "Gradient Descent(839/999): loss=0.11690838197910841\n",
      "Gradient Descent(840/999): loss=0.11690266049138637\n",
      "Gradient Descent(841/999): loss=0.11689694498943783\n",
      "Gradient Descent(842/999): loss=0.11689123546481923\n",
      "Gradient Descent(843/999): loss=0.11688553190910089\n",
      "Gradient Descent(844/999): loss=0.11687983431386678\n",
      "Gradient Descent(845/999): loss=0.11687414267071473\n",
      "Gradient Descent(846/999): loss=0.11686845697125622\n",
      "Gradient Descent(847/999): loss=0.11686277720711645\n",
      "Gradient Descent(848/999): loss=0.11685710336993428\n",
      "Gradient Descent(849/999): loss=0.11685143545136228\n",
      "Gradient Descent(850/999): loss=0.11684577344306654\n",
      "Gradient Descent(851/999): loss=0.1168401173367268\n",
      "Gradient Descent(852/999): loss=0.11683446712403643\n",
      "Gradient Descent(853/999): loss=0.11682882279670223\n",
      "Gradient Descent(854/999): loss=0.11682318434644468\n",
      "Gradient Descent(855/999): loss=0.11681755176499761\n",
      "Gradient Descent(856/999): loss=0.11681192504410848\n",
      "Gradient Descent(857/999): loss=0.11680630417553807\n",
      "Gradient Descent(858/999): loss=0.11680068915106072\n",
      "Gradient Descent(859/999): loss=0.11679507996246412\n",
      "Gradient Descent(860/999): loss=0.11678947660154929\n",
      "Gradient Descent(861/999): loss=0.11678387906013069\n",
      "Gradient Descent(862/999): loss=0.11677828733003612\n",
      "Gradient Descent(863/999): loss=0.1167727014031067\n",
      "Gradient Descent(864/999): loss=0.11676712127119673\n",
      "Gradient Descent(865/999): loss=0.11676154692617395\n",
      "Gradient Descent(866/999): loss=0.11675597835991927\n",
      "Gradient Descent(867/999): loss=0.11675041556432676\n",
      "Gradient Descent(868/999): loss=0.1167448585313038\n",
      "Gradient Descent(869/999): loss=0.11673930725277086\n",
      "Gradient Descent(870/999): loss=0.11673376172066165\n",
      "Gradient Descent(871/999): loss=0.11672822192692293\n",
      "Gradient Descent(872/999): loss=0.11672268786351457\n",
      "Gradient Descent(873/999): loss=0.11671715952240957\n",
      "Gradient Descent(874/999): loss=0.116711636895594\n",
      "Gradient Descent(875/999): loss=0.11670611997506693\n",
      "Gradient Descent(876/999): loss=0.11670060875284044\n",
      "Gradient Descent(877/999): loss=0.11669510322093964\n",
      "Gradient Descent(878/999): loss=0.11668960337140262\n",
      "Gradient Descent(879/999): loss=0.1166841091962803\n",
      "Gradient Descent(880/999): loss=0.1166786206876367\n",
      "Gradient Descent(881/999): loss=0.11667313783754861\n",
      "Gradient Descent(882/999): loss=0.11666766063810578\n",
      "Gradient Descent(883/999): loss=0.11666218908141075\n",
      "Gradient Descent(884/999): loss=0.11665672315957892\n",
      "Gradient Descent(885/999): loss=0.11665126286473855\n",
      "Gradient Descent(886/999): loss=0.11664580818903061\n",
      "Gradient Descent(887/999): loss=0.11664035912460886\n",
      "Gradient Descent(888/999): loss=0.11663491566363987\n",
      "Gradient Descent(889/999): loss=0.11662947779830285\n",
      "Gradient Descent(890/999): loss=0.11662404552078977\n",
      "Gradient Descent(891/999): loss=0.11661861882330517\n",
      "Gradient Descent(892/999): loss=0.11661319769806641\n",
      "Gradient Descent(893/999): loss=0.11660778213730334\n",
      "Gradient Descent(894/999): loss=0.11660237213325853\n",
      "Gradient Descent(895/999): loss=0.11659696767818704\n",
      "Gradient Descent(896/999): loss=0.11659156876435657\n",
      "Gradient Descent(897/999): loss=0.11658617538404732\n",
      "Gradient Descent(898/999): loss=0.11658078752955203\n",
      "Gradient Descent(899/999): loss=0.11657540519317598\n",
      "Gradient Descent(900/999): loss=0.11657002836723686\n",
      "Gradient Descent(901/999): loss=0.1165646570440648\n",
      "Gradient Descent(902/999): loss=0.11655929121600253\n",
      "Gradient Descent(903/999): loss=0.116553930875405\n",
      "Gradient Descent(904/999): loss=0.11654857601463962\n",
      "Gradient Descent(905/999): loss=0.11654322662608625\n",
      "Gradient Descent(906/999): loss=0.11653788270213693\n",
      "Gradient Descent(907/999): loss=0.1165325442351962\n",
      "Gradient Descent(908/999): loss=0.11652721121768085\n",
      "Gradient Descent(909/999): loss=0.11652188364201986\n",
      "Gradient Descent(910/999): loss=0.11651656150065459\n",
      "Gradient Descent(911/999): loss=0.11651124478603862\n",
      "Gradient Descent(912/999): loss=0.11650593349063763\n",
      "Gradient Descent(913/999): loss=0.11650062760692972\n",
      "Gradient Descent(914/999): loss=0.116495327127405\n",
      "Gradient Descent(915/999): loss=0.11649003204456573\n",
      "Gradient Descent(916/999): loss=0.11648474235092639\n",
      "Gradient Descent(917/999): loss=0.11647945803901355\n",
      "Gradient Descent(918/999): loss=0.11647417910136583\n",
      "Gradient Descent(919/999): loss=0.11646890553053395\n",
      "Gradient Descent(920/999): loss=0.1164636373190807\n",
      "Gradient Descent(921/999): loss=0.11645837445958088\n",
      "Gradient Descent(922/999): loss=0.11645311694462124\n",
      "Gradient Descent(923/999): loss=0.11644786476680061\n",
      "Gradient Descent(924/999): loss=0.11644261791872977\n",
      "Gradient Descent(925/999): loss=0.11643737639303137\n",
      "Gradient Descent(926/999): loss=0.11643214018234004\n",
      "Gradient Descent(927/999): loss=0.11642690927930231\n",
      "Gradient Descent(928/999): loss=0.11642168367657661\n",
      "Gradient Descent(929/999): loss=0.11641646336683319\n",
      "Gradient Descent(930/999): loss=0.11641124834275418\n",
      "Gradient Descent(931/999): loss=0.11640603859703348\n",
      "Gradient Descent(932/999): loss=0.11640083412237684\n",
      "Gradient Descent(933/999): loss=0.11639563491150176\n",
      "Gradient Descent(934/999): loss=0.1163904409571375\n",
      "Gradient Descent(935/999): loss=0.11638525225202508\n",
      "Gradient Descent(936/999): loss=0.11638006878891723\n",
      "Gradient Descent(937/999): loss=0.11637489056057833\n",
      "Gradient Descent(938/999): loss=0.11636971755978445\n",
      "Gradient Descent(939/999): loss=0.11636454977932342\n",
      "Gradient Descent(940/999): loss=0.11635938721199456\n",
      "Gradient Descent(941/999): loss=0.11635422985060892\n",
      "Gradient Descent(942/999): loss=0.11634907768798904\n",
      "Gradient Descent(943/999): loss=0.11634393071696912\n",
      "Gradient Descent(944/999): loss=0.11633878893039486\n",
      "Gradient Descent(945/999): loss=0.1163336523211235\n",
      "Gradient Descent(946/999): loss=0.11632852088202388\n",
      "Gradient Descent(947/999): loss=0.11632339460597622\n",
      "Gradient Descent(948/999): loss=0.11631827348587223\n",
      "Gradient Descent(949/999): loss=0.11631315751461518\n",
      "Gradient Descent(950/999): loss=0.11630804668511958\n",
      "Gradient Descent(951/999): loss=0.11630294099031159\n",
      "Gradient Descent(952/999): loss=0.11629784042312857\n",
      "Gradient Descent(953/999): loss=0.11629274497651935\n",
      "Gradient Descent(954/999): loss=0.11628765464344405\n",
      "Gradient Descent(955/999): loss=0.11628256941687425\n",
      "Gradient Descent(956/999): loss=0.1162774892897927\n",
      "Gradient Descent(957/999): loss=0.11627241425519351\n",
      "Gradient Descent(958/999): loss=0.11626734430608207\n",
      "Gradient Descent(959/999): loss=0.11626227943547504\n",
      "Gradient Descent(960/999): loss=0.11625721963640022\n",
      "Gradient Descent(961/999): loss=0.11625216490189676\n",
      "Gradient Descent(962/999): loss=0.11624711522501495\n",
      "Gradient Descent(963/999): loss=0.11624207059881622\n",
      "Gradient Descent(964/999): loss=0.11623703101637317\n",
      "Gradient Descent(965/999): loss=0.11623199647076962\n",
      "Gradient Descent(966/999): loss=0.11622696695510043\n",
      "Gradient Descent(967/999): loss=0.11622194246247151\n",
      "Gradient Descent(968/999): loss=0.116216922986\n",
      "Gradient Descent(969/999): loss=0.11621190851881397\n",
      "Gradient Descent(970/999): loss=0.1162068990540526\n",
      "Gradient Descent(971/999): loss=0.11620189458486609\n",
      "Gradient Descent(972/999): loss=0.11619689510441558\n",
      "Gradient Descent(973/999): loss=0.11619190060587326\n",
      "Gradient Descent(974/999): loss=0.11618691108242224\n",
      "Gradient Descent(975/999): loss=0.11618192652725663\n",
      "Gradient Descent(976/999): loss=0.11617694693358145\n",
      "Gradient Descent(977/999): loss=0.11617197229461255\n",
      "Gradient Descent(978/999): loss=0.11616700260357683\n",
      "Gradient Descent(979/999): loss=0.11616203785371187\n",
      "Gradient Descent(980/999): loss=0.11615707803826623\n",
      "Gradient Descent(981/999): loss=0.11615212315049925\n",
      "Gradient Descent(982/999): loss=0.1161471731836811\n",
      "Gradient Descent(983/999): loss=0.11614222813109275\n",
      "Gradient Descent(984/999): loss=0.11613728798602593\n",
      "Gradient Descent(985/999): loss=0.11613235274178314\n",
      "Gradient Descent(986/999): loss=0.11612742239167759\n",
      "Gradient Descent(987/999): loss=0.1161224969290333\n",
      "Gradient Descent(988/999): loss=0.11611757634718481\n",
      "Gradient Descent(989/999): loss=0.11611266063947753\n",
      "Gradient Descent(990/999): loss=0.11610774979926744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(991/999): loss=0.11610284381992117\n",
      "Gradient Descent(992/999): loss=0.11609794269481599\n",
      "Gradient Descent(993/999): loss=0.1160930464173398\n",
      "Gradient Descent(994/999): loss=0.116088154980891\n",
      "Gradient Descent(995/999): loss=0.11608326837887868\n",
      "Gradient Descent(996/999): loss=0.11607838660472243\n",
      "Gradient Descent(997/999): loss=0.11607350965185233\n",
      "Gradient Descent(998/999): loss=0.11606863751370906\n",
      "Gradient Descent(999/999): loss=0.11606377018374371\n",
      "Gradient Descent(0/999): loss=0.23372288779265524\n",
      "Gradient Descent(1/999): loss=0.22237253440338314\n",
      "Gradient Descent(2/999): loss=0.2122075944543592\n",
      "Gradient Descent(3/999): loss=0.20310406813183957\n",
      "Gradient Descent(4/999): loss=0.19495092737822503\n",
      "Gradient Descent(5/999): loss=0.18764875889105354\n",
      "Gradient Descent(6/999): loss=0.18110854908076265\n",
      "Gradient Descent(7/999): loss=0.17525059613660818\n",
      "Gradient Descent(8/999): loss=0.17000353590367917\n",
      "Gradient Descent(9/999): loss=0.16530346966498505\n",
      "Gradient Descent(10/999): loss=0.16109318316810528\n",
      "Gradient Descent(11/999): loss=0.15732144735111034\n",
      "Gradient Descent(12/999): loss=0.15394239222101697\n",
      "Gradient Descent(13/999): loss=0.1509149462321336\n",
      "Gradient Descent(14/999): loss=0.14820233431221078\n",
      "Gradient Descent(15/999): loss=0.1457716284011233\n",
      "Gradient Descent(16/999): loss=0.14359334500863546\n",
      "Gradient Descent(17/999): loss=0.1416410848724815\n",
      "Gradient Descent(18/999): loss=0.1398912103125566\n",
      "Gradient Descent(19/999): loss=0.1383225563377477\n",
      "Gradient Descent(20/999): loss=0.13691617197446798\n",
      "Gradient Descent(21/999): loss=0.13565508865533712\n",
      "Gradient Descent(22/999): loss=0.13452411283718843\n",
      "Gradient Descent(23/999): loss=0.13350964031372048\n",
      "Gradient Descent(24/999): loss=0.13259948995327017\n",
      "Gradient Descent(25/999): loss=0.1317827548296041\n",
      "Gradient Descent(26/999): loss=0.1310496689262072\n",
      "Gradient Descent(27/999): loss=0.13039148778489215\n",
      "Gradient Descent(28/999): loss=0.1298003816399846\n",
      "Gradient Descent(29/999): loss=0.12926933973194144\n",
      "Gradient Descent(30/999): loss=0.12879208463089808\n",
      "Gradient Descent(31/999): loss=0.12836299552298525\n",
      "Gradient Descent(32/999): loss=0.12797703952180053\n",
      "Gradient Descent(33/999): loss=0.12762971016550717\n",
      "Gradient Descent(34/999): loss=0.1273169723478556\n",
      "Gradient Descent(35/999): loss=0.1270352130100629\n",
      "Gradient Descent(36/999): loss=0.12678119699089382\n",
      "Gradient Descent(37/999): loss=0.12655202749533376\n",
      "Gradient Descent(38/999): loss=0.1263451106986933\n",
      "Gradient Descent(39/999): loss=0.12615812405352833\n",
      "Gradient Descent(40/999): loss=0.1259889879120161\n",
      "Gradient Descent(41/999): loss=0.12583584011695204\n",
      "Gradient Descent(42/999): loss=0.12569701325081234\n",
      "Gradient Descent(43/999): loss=0.1255710142648179\n",
      "Gradient Descent(44/999): loss=0.12545650623902266\n",
      "Gradient Descent(45/999): loss=0.12535229205049545\n",
      "Gradient Descent(46/999): loss=0.12525729974998634\n",
      "Gradient Descent(47/999): loss=0.12517056946834956\n",
      "Gradient Descent(48/999): loss=0.1250912416926921\n",
      "Gradient Descent(49/999): loss=0.12501854676895877\n",
      "Gradient Descent(50/999): loss=0.12495179550265376\n",
      "Gradient Descent(51/999): loss=0.12489037074282028\n",
      "Gradient Descent(52/999): loss=0.12483371984641894\n",
      "Gradient Descent(53/999): loss=0.1247813479310038\n",
      "Gradient Descent(54/999): loss=0.12473281183323132\n",
      "Gradient Descent(55/999): loss=0.12468771469936434\n",
      "Gradient Descent(56/999): loss=0.12464570114165603\n",
      "Gradient Descent(57/999): loss=0.12460645290141749\n",
      "Gradient Descent(58/999): loss=0.12456968496576311\n",
      "Gradient Descent(59/999): loss=0.12453514209057409\n",
      "Gradient Descent(60/999): loss=0.12450259568718558\n",
      "Gradient Descent(61/999): loss=0.12447184103474711\n",
      "Gradient Descent(62/999): loss=0.12444269478418767\n",
      "Gradient Descent(63/999): loss=0.12441499272328048\n",
      "Gradient Descent(64/999): loss=0.12438858777549305\n",
      "Gradient Descent(65/999): loss=0.12436334820816677\n",
      "Gradient Descent(66/999): loss=0.12433915602812719\n",
      "Gradient Descent(67/999): loss=0.12431590554511818\n",
      "Gradient Descent(68/999): loss=0.12429350208550394\n",
      "Gradient Descent(69/999): loss=0.1242718608405189\n",
      "Gradient Descent(70/999): loss=0.12425090583499068\n",
      "Gradient Descent(71/999): loss=0.12423056900393356\n",
      "Gradient Descent(72/999): loss=0.12421078936572787\n",
      "Gradient Descent(73/999): loss=0.12419151228178156\n",
      "Gradient Descent(74/999): loss=0.12417268879362757\n",
      "Gradient Descent(75/999): loss=0.12415427502935578\n",
      "Gradient Descent(76/999): loss=0.12413623167212706\n",
      "Gradient Descent(77/999): loss=0.12411852348427503\n",
      "Gradient Descent(78/999): loss=0.12410111888118072\n",
      "Gradient Descent(79/999): loss=0.12408398954971292\n",
      "Gradient Descent(80/999): loss=0.12406711010657323\n",
      "Gradient Descent(81/999): loss=0.12405045779237098\n",
      "Gradient Descent(82/999): loss=0.12403401219769042\n",
      "Gradient Descent(83/999): loss=0.12401775501780386\n",
      "Gradient Descent(84/999): loss=0.12400166983303411\n",
      "Gradient Descent(85/999): loss=0.12398574191208328\n",
      "Gradient Descent(86/999): loss=0.1239699580359255\n",
      "Gradient Descent(87/999): loss=0.12395430634011277\n",
      "Gradient Descent(88/999): loss=0.12393877617356709\n",
      "Gradient Descent(89/999): loss=0.12392335797213602\n",
      "Gradient Descent(90/999): loss=0.12390804314536531\n",
      "Gradient Descent(91/999): loss=0.12389282397510797\n",
      "Gradient Descent(92/999): loss=0.12387769352473084\n",
      "Gradient Descent(93/999): loss=0.12386264555780994\n",
      "Gradient Descent(94/999): loss=0.12384767446532309\n",
      "Gradient Descent(95/999): loss=0.12383277520045008\n",
      "Gradient Descent(96/999): loss=0.12381794322018538\n",
      "Gradient Descent(97/999): loss=0.1238031744330506\n",
      "Gradient Descent(98/999): loss=0.1237884651522685\n",
      "Gradient Descent(99/999): loss=0.1237738120538282\n",
      "Gradient Descent(100/999): loss=0.12375921213892892\n",
      "Gradient Descent(101/999): loss=0.1237446627003454\n",
      "Gradient Descent(102/999): loss=0.12373016129230423\n",
      "Gradient Descent(103/999): loss=0.12371570570350414\n",
      "Gradient Descent(104/999): loss=0.12370129393295169\n",
      "Gradient Descent(105/999): loss=0.12368692416831763\n",
      "Gradient Descent(106/999): loss=0.12367259476655067\n",
      "Gradient Descent(107/999): loss=0.12365830423651239\n",
      "Gradient Descent(108/999): loss=0.12364405122342234\n",
      "Gradient Descent(109/999): loss=0.12362983449492365\n",
      "Gradient Descent(110/999): loss=0.12361565292860013\n",
      "Gradient Descent(111/999): loss=0.12360150550079281\n",
      "Gradient Descent(112/999): loss=0.12358739127658047\n",
      "Gradient Descent(113/999): loss=0.1235733094008023\n",
      "Gradient Descent(114/999): loss=0.12355925909001354\n",
      "Gradient Descent(115/999): loss=0.12354523962527737\n",
      "Gradient Descent(116/999): loss=0.12353125034570468\n",
      "Gradient Descent(117/999): loss=0.12351729064266445\n",
      "Gradient Descent(118/999): loss=0.1235033599545943\n",
      "Gradient Descent(119/999): loss=0.12348945776234839\n",
      "Gradient Descent(120/999): loss=0.12347558358502699\n",
      "Gradient Descent(121/999): loss=0.12346173697623716\n",
      "Gradient Descent(122/999): loss=0.12344791752073939\n",
      "Gradient Descent(123/999): loss=0.12343412483144028\n",
      "Gradient Descent(124/999): loss=0.1234203585466951\n",
      "Gradient Descent(125/999): loss=0.12340661832788737\n",
      "Gradient Descent(126/999): loss=0.1233929038572576\n",
      "Gradient Descent(127/999): loss=0.12337921483595396\n",
      "Gradient Descent(128/999): loss=0.12336555098228297\n",
      "Gradient Descent(129/999): loss=0.12335191203013854\n",
      "Gradient Descent(130/999): loss=0.12333829772759097\n",
      "Gradient Descent(131/999): loss=0.12332470783561958\n",
      "Gradient Descent(132/999): loss=0.12331114212697367\n",
      "Gradient Descent(133/999): loss=0.12329760038514848\n",
      "Gradient Descent(134/999): loss=0.12328408240346464\n",
      "Gradient Descent(135/999): loss=0.12327058798423962\n",
      "Gradient Descent(136/999): loss=0.12325711693804228\n",
      "Gradient Descent(137/999): loss=0.1232436690830218\n",
      "Gradient Descent(138/999): loss=0.12323024424430261\n",
      "Gradient Descent(139/999): loss=0.12321684225343973\n",
      "Gradient Descent(140/999): loss=0.12320346294792701\n",
      "Gradient Descent(141/999): loss=0.12319010617075357\n",
      "Gradient Descent(142/999): loss=0.12317677177000333\n",
      "Gradient Descent(143/999): loss=0.12316345959849288\n",
      "Gradient Descent(144/999): loss=0.1231501695134442\n",
      "Gradient Descent(145/999): loss=0.12313690137618832\n",
      "Gradient Descent(146/999): loss=0.12312365505189689\n",
      "Gradient Descent(147/999): loss=0.12311043040933864\n",
      "Gradient Descent(148/999): loss=0.12309722732065843\n",
      "Gradient Descent(149/999): loss=0.12308404566117649\n",
      "Gradient Descent(150/999): loss=0.12307088530920565\n",
      "Gradient Descent(151/999): loss=0.12305774614588495\n",
      "Gradient Descent(152/999): loss=0.1230446280550277\n",
      "Gradient Descent(153/999): loss=0.12303153092298298\n",
      "Gradient Descent(154/999): loss=0.12301845463850873\n",
      "Gradient Descent(155/999): loss=0.1230053990926557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(156/999): loss=0.1229923641786606\n",
      "Gradient Descent(157/999): loss=0.12297934979184828\n",
      "Gradient Descent(158/999): loss=0.1229663558295416\n",
      "Gradient Descent(159/999): loss=0.12295338219097812\n",
      "Gradient Descent(160/999): loss=0.12294042877723328\n",
      "Gradient Descent(161/999): loss=0.12292749549114934\n",
      "Gradient Descent(162/999): loss=0.12291458223726917\n",
      "Gradient Descent(163/999): loss=0.12290168892177515\n",
      "Gradient Descent(164/999): loss=0.122888815452432\n",
      "Gradient Descent(165/999): loss=0.12287596173853334\n",
      "Gradient Descent(166/999): loss=0.12286312769085209\n",
      "Gradient Descent(167/999): loss=0.12285031322159358\n",
      "Gradient Descent(168/999): loss=0.12283751824435185\n",
      "Gradient Descent(169/999): loss=0.1228247426740684\n",
      "Gradient Descent(170/999): loss=0.12281198642699317\n",
      "Gradient Descent(171/999): loss=0.12279924942064803\n",
      "Gradient Descent(172/999): loss=0.12278653157379191\n",
      "Gradient Descent(173/999): loss=0.1227738328063877\n",
      "Gradient Descent(174/999): loss=0.12276115303957127\n",
      "Gradient Descent(175/999): loss=0.1227484921956213\n",
      "Gradient Descent(176/999): loss=0.12273585019793107\n",
      "Gradient Descent(177/999): loss=0.12272322697098124\n",
      "Gradient Descent(178/999): loss=0.12271062244031385\n",
      "Gradient Descent(179/999): loss=0.12269803653250745\n",
      "Gradient Descent(180/999): loss=0.12268546917515316\n",
      "Gradient Descent(181/999): loss=0.12267292029683179\n",
      "Gradient Descent(182/999): loss=0.12266038982709153\n",
      "Gradient Descent(183/999): loss=0.1226478776964269\n",
      "Gradient Descent(184/999): loss=0.12263538383625806\n",
      "Gradient Descent(185/999): loss=0.12262290817891087\n",
      "Gradient Descent(186/999): loss=0.1226104506575978\n",
      "Gradient Descent(187/999): loss=0.1225980112063993\n",
      "Gradient Descent(188/999): loss=0.12258558976024574\n",
      "Gradient Descent(189/999): loss=0.12257318625489996\n",
      "Gradient Descent(190/999): loss=0.12256080062694027\n",
      "Gradient Descent(191/999): loss=0.12254843281374408\n",
      "Gradient Descent(192/999): loss=0.12253608275347164\n",
      "Gradient Descent(193/999): loss=0.12252375038505069\n",
      "Gradient Descent(194/999): loss=0.12251143564816112\n",
      "Gradient Descent(195/999): loss=0.12249913848322025\n",
      "Gradient Descent(196/999): loss=0.12248685883136841\n",
      "Gradient Descent(197/999): loss=0.1224745966344549\n",
      "Gradient Descent(198/999): loss=0.12246235183502424\n",
      "Gradient Descent(199/999): loss=0.12245012437630284\n",
      "Gradient Descent(200/999): loss=0.12243791420218594\n",
      "Gradient Descent(201/999): loss=0.12242572125722483\n",
      "Gradient Descent(202/999): loss=0.12241354548661436\n",
      "Gradient Descent(203/999): loss=0.12240138683618082\n",
      "Gradient Descent(204/999): loss=0.12238924525237005\n",
      "Gradient Descent(205/999): loss=0.12237712068223573\n",
      "Gradient Descent(206/999): loss=0.12236501307342802\n",
      "Gradient Descent(207/999): loss=0.12235292237418247\n",
      "Gradient Descent(208/999): loss=0.12234084853330907\n",
      "Gradient Descent(209/999): loss=0.12232879150018151\n",
      "Gradient Descent(210/999): loss=0.122316751224727\n",
      "Gradient Descent(211/999): loss=0.12230472765741573\n",
      "Gradient Descent(212/999): loss=0.12229272074925111\n",
      "Gradient Descent(213/999): loss=0.12228073045175984\n",
      "Gradient Descent(214/999): loss=0.12226875671698233\n",
      "Gradient Descent(215/999): loss=0.12225679949746336\n",
      "Gradient Descent(216/999): loss=0.12224485874624284\n",
      "Gradient Descent(217/999): loss=0.12223293441684677\n",
      "Gradient Descent(218/999): loss=0.12222102646327845\n",
      "Gradient Descent(219/999): loss=0.12220913484000981\n",
      "Gradient Descent(220/999): loss=0.1221972595019729\n",
      "Gradient Descent(221/999): loss=0.12218540040455153\n",
      "Gradient Descent(222/999): loss=0.12217355750357335\n",
      "Gradient Descent(223/999): loss=0.12216173075530155\n",
      "Gradient Descent(224/999): loss=0.1221499201164273\n",
      "Gradient Descent(225/999): loss=0.12213812554406187\n",
      "Gradient Descent(226/999): loss=0.1221263469957293\n",
      "Gradient Descent(227/999): loss=0.12211458442935895\n",
      "Gradient Descent(228/999): loss=0.12210283780327821\n",
      "Gradient Descent(229/999): loss=0.12209110707620557\n",
      "Gradient Descent(230/999): loss=0.12207939220724351\n",
      "Gradient Descent(231/999): loss=0.12206769315587185\n",
      "Gradient Descent(232/999): loss=0.12205600988194096\n",
      "Gradient Descent(233/999): loss=0.12204434234566523\n",
      "Gradient Descent(234/999): loss=0.12203269050761685\n",
      "Gradient Descent(235/999): loss=0.1220210543287192\n",
      "Gradient Descent(236/999): loss=0.12200943377024093\n",
      "Gradient Descent(237/999): loss=0.12199782879378983\n",
      "Gradient Descent(238/999): loss=0.1219862393613069\n",
      "Gradient Descent(239/999): loss=0.1219746654350606\n",
      "Gradient Descent(240/999): loss=0.121963106977641\n",
      "Gradient Descent(241/999): loss=0.12195156395195443\n",
      "Gradient Descent(242/999): loss=0.12194003632121776\n",
      "Gradient Descent(243/999): loss=0.1219285240489532\n",
      "Gradient Descent(244/999): loss=0.12191702709898301\n",
      "Gradient Descent(245/999): loss=0.12190554543542417\n",
      "Gradient Descent(246/999): loss=0.12189407902268352\n",
      "Gradient Descent(247/999): loss=0.12188262782545274\n",
      "Gradient Descent(248/999): loss=0.12187119180870337\n",
      "Gradient Descent(249/999): loss=0.12185977093768219\n",
      "Gradient Descent(250/999): loss=0.12184836517790645\n",
      "Gradient Descent(251/999): loss=0.12183697449515926\n",
      "Gradient Descent(252/999): loss=0.12182559885548512\n",
      "Gradient Descent(253/999): loss=0.12181423822518553\n",
      "Gradient Descent(254/999): loss=0.12180289257081464\n",
      "Gradient Descent(255/999): loss=0.12179156185917486\n",
      "Gradient Descent(256/999): loss=0.12178024605731298\n",
      "Gradient Descent(257/999): loss=0.12176894513251585\n",
      "Gradient Descent(258/999): loss=0.12175765905230641\n",
      "Gradient Descent(259/999): loss=0.12174638778443994\n",
      "Gradient Descent(260/999): loss=0.12173513129689988\n",
      "Gradient Descent(261/999): loss=0.12172388955789434\n",
      "Gradient Descent(262/999): loss=0.12171266253585229\n",
      "Gradient Descent(263/999): loss=0.1217014501994199\n",
      "Gradient Descent(264/999): loss=0.12169025251745699\n",
      "Gradient Descent(265/999): loss=0.12167906945903352\n",
      "Gradient Descent(266/999): loss=0.12166790099342624\n",
      "Gradient Descent(267/999): loss=0.12165674709011522\n",
      "Gradient Descent(268/999): loss=0.12164560771878054\n",
      "Gradient Descent(269/999): loss=0.12163448284929927\n",
      "Gradient Descent(270/999): loss=0.12162337245174194\n",
      "Gradient Descent(271/999): loss=0.12161227649636978\n",
      "Gradient Descent(272/999): loss=0.12160119495363149\n",
      "Gradient Descent(273/999): loss=0.12159012779416013\n",
      "Gradient Descent(274/999): loss=0.12157907498877049\n",
      "Gradient Descent(275/999): loss=0.1215680365084559\n",
      "Gradient Descent(276/999): loss=0.12155701232438565\n",
      "Gradient Descent(277/999): loss=0.12154600240790207\n",
      "Gradient Descent(278/999): loss=0.1215350067305178\n",
      "Gradient Descent(279/999): loss=0.12152402526391323\n",
      "Gradient Descent(280/999): loss=0.12151305797993386\n",
      "Gradient Descent(281/999): loss=0.12150210485058766\n",
      "Gradient Descent(282/999): loss=0.12149116584804255\n",
      "Gradient Descent(283/999): loss=0.12148024094462406\n",
      "Gradient Descent(284/999): loss=0.12146933011281283\n",
      "Gradient Descent(285/999): loss=0.12145843332524218\n",
      "Gradient Descent(286/999): loss=0.1214475505546959\n",
      "Gradient Descent(287/999): loss=0.12143668177410578\n",
      "Gradient Descent(288/999): loss=0.12142582695654966\n",
      "Gradient Descent(289/999): loss=0.12141498607524889\n",
      "Gradient Descent(290/999): loss=0.12140415910356642\n",
      "Gradient Descent(291/999): loss=0.12139334601500465\n",
      "Gradient Descent(292/999): loss=0.1213825467832032\n",
      "Gradient Descent(293/999): loss=0.12137176138193709\n",
      "Gradient Descent(294/999): loss=0.12136098978511453\n",
      "Gradient Descent(295/999): loss=0.12135023196677519\n",
      "Gradient Descent(296/999): loss=0.12133948790108802\n",
      "Gradient Descent(297/999): loss=0.12132875756234962\n",
      "Gradient Descent(298/999): loss=0.12131804092498213\n",
      "Gradient Descent(299/999): loss=0.12130733796353171\n",
      "Gradient Descent(300/999): loss=0.12129664865266646\n",
      "Gradient Descent(301/999): loss=0.1212859729671749\n",
      "Gradient Descent(302/999): loss=0.12127531088196411\n",
      "Gradient Descent(303/999): loss=0.12126466237205819\n",
      "Gradient Descent(304/999): loss=0.12125402741259637\n",
      "Gradient Descent(305/999): loss=0.12124340597883175\n",
      "Gradient Descent(306/999): loss=0.12123279804612941\n",
      "Gradient Descent(307/999): loss=0.12122220358996491\n",
      "Gradient Descent(308/999): loss=0.12121162258592282\n",
      "Gradient Descent(309/999): loss=0.12120105500969529\n",
      "Gradient Descent(310/999): loss=0.12119050083708043\n",
      "Gradient Descent(311/999): loss=0.12117996004398088\n",
      "Gradient Descent(312/999): loss=0.12116943260640252\n",
      "Gradient Descent(313/999): loss=0.12115891850045295\n",
      "Gradient Descent(314/999): loss=0.12114841770234021\n",
      "Gradient Descent(315/999): loss=0.12113793018837136\n",
      "Gradient Descent(316/999): loss=0.12112745593495125\n",
      "Gradient Descent(317/999): loss=0.12111699491858122\n",
      "Gradient Descent(318/999): loss=0.1211065471158577\n",
      "Gradient Descent(319/999): loss=0.1210961125034712\n",
      "Gradient Descent(320/999): loss=0.12108569105820488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(321/999): loss=0.12107528275693352\n",
      "Gradient Descent(322/999): loss=0.1210648875766222\n",
      "Gradient Descent(323/999): loss=0.12105450549432523\n",
      "Gradient Descent(324/999): loss=0.12104413648718505\n",
      "Gradient Descent(325/999): loss=0.12103378053243108\n",
      "Gradient Descent(326/999): loss=0.12102343760737856\n",
      "Gradient Descent(327/999): loss=0.12101310768942762\n",
      "Gradient Descent(328/999): loss=0.12100279075606218\n",
      "Gradient Descent(329/999): loss=0.12099248678484884\n",
      "Gradient Descent(330/999): loss=0.12098219575343595\n",
      "Gradient Descent(331/999): loss=0.1209719176395527\n",
      "Gradient Descent(332/999): loss=0.12096165242100793\n",
      "Gradient Descent(333/999): loss=0.12095140007568934\n",
      "Gradient Descent(334/999): loss=0.12094116058156255\n",
      "Gradient Descent(335/999): loss=0.12093093391667012\n",
      "Gradient Descent(336/999): loss=0.12092072005913063\n",
      "Gradient Descent(337/999): loss=0.1209105189871379\n",
      "Gradient Descent(338/999): loss=0.12090033067895996\n",
      "Gradient Descent(339/999): loss=0.12089015511293841\n",
      "Gradient Descent(340/999): loss=0.12087999226748734\n",
      "Gradient Descent(341/999): loss=0.1208698421210927\n",
      "Gradient Descent(342/999): loss=0.1208597046523114\n",
      "Gradient Descent(343/999): loss=0.12084957983977054\n",
      "Gradient Descent(344/999): loss=0.12083946766216658\n",
      "Gradient Descent(345/999): loss=0.12082936809826468\n",
      "Gradient Descent(346/999): loss=0.1208192811268979\n",
      "Gradient Descent(347/999): loss=0.12080920672696634\n",
      "Gradient Descent(348/999): loss=0.12079914487743666\n",
      "Gradient Descent(349/999): loss=0.12078909555734123\n",
      "Gradient Descent(350/999): loss=0.12077905874577734\n",
      "Gradient Descent(351/999): loss=0.12076903442190665\n",
      "Gradient Descent(352/999): loss=0.12075902256495455\n",
      "Gradient Descent(353/999): loss=0.1207490231542094\n",
      "Gradient Descent(354/999): loss=0.12073903616902189\n",
      "Gradient Descent(355/999): loss=0.12072906158880442\n",
      "Gradient Descent(356/999): loss=0.1207190993930305\n",
      "Gradient Descent(357/999): loss=0.12070914956123414\n",
      "Gradient Descent(358/999): loss=0.12069921207300914\n",
      "Gradient Descent(359/999): loss=0.12068928690800872\n",
      "Gradient Descent(360/999): loss=0.12067937404594463\n",
      "Gradient Descent(361/999): loss=0.12066947346658685\n",
      "Gradient Descent(362/999): loss=0.12065958514976292\n",
      "Gradient Descent(363/999): loss=0.12064970907535733\n",
      "Gradient Descent(364/999): loss=0.12063984522331113\n",
      "Gradient Descent(365/999): loss=0.12062999357362124\n",
      "Gradient Descent(366/999): loss=0.12062015410633999\n",
      "Gradient Descent(367/999): loss=0.12061032680157467\n",
      "Gradient Descent(368/999): loss=0.12060051163948694\n",
      "Gradient Descent(369/999): loss=0.12059070860029229\n",
      "Gradient Descent(370/999): loss=0.12058091766425974\n",
      "Gradient Descent(371/999): loss=0.12057113881171119\n",
      "Gradient Descent(372/999): loss=0.12056137202302095\n",
      "Gradient Descent(373/999): loss=0.12055161727861546\n",
      "Gradient Descent(374/999): loss=0.12054187455897258\n",
      "Gradient Descent(375/999): loss=0.12053214384462134\n",
      "Gradient Descent(376/999): loss=0.12052242511614145\n",
      "Gradient Descent(377/999): loss=0.12051271835416279\n",
      "Gradient Descent(378/999): loss=0.12050302353936508\n",
      "Gradient Descent(379/999): loss=0.12049334065247748\n",
      "Gradient Descent(380/999): loss=0.12048366967427804\n",
      "Gradient Descent(381/999): loss=0.12047401058559347\n",
      "Gradient Descent(382/999): loss=0.12046436336729865\n",
      "Gradient Descent(383/999): loss=0.12045472800031627\n",
      "Gradient Descent(384/999): loss=0.12044510446561636\n",
      "Gradient Descent(385/999): loss=0.1204354927442161\n",
      "Gradient Descent(386/999): loss=0.12042589281717929\n",
      "Gradient Descent(387/999): loss=0.12041630466561606\n",
      "Gradient Descent(388/999): loss=0.12040672827068244\n",
      "Gradient Descent(389/999): loss=0.12039716361358013\n",
      "Gradient Descent(390/999): loss=0.12038761067555608\n",
      "Gradient Descent(391/999): loss=0.12037806943790211\n",
      "Gradient Descent(392/999): loss=0.12036853988195467\n",
      "Gradient Descent(393/999): loss=0.12035902198909453\n",
      "Gradient Descent(394/999): loss=0.12034951574074625\n",
      "Gradient Descent(395/999): loss=0.12034002111837815\n",
      "Gradient Descent(396/999): loss=0.12033053810350171\n",
      "Gradient Descent(397/999): loss=0.12032106667767162\n",
      "Gradient Descent(398/999): loss=0.12031160682248507\n",
      "Gradient Descent(399/999): loss=0.1203021585195818\n",
      "Gradient Descent(400/999): loss=0.12029272175064357\n",
      "Gradient Descent(401/999): loss=0.12028329649739401\n",
      "Gradient Descent(402/999): loss=0.12027388274159825\n",
      "Gradient Descent(403/999): loss=0.12026448046506276\n",
      "Gradient Descent(404/999): loss=0.12025508964963497\n",
      "Gradient Descent(405/999): loss=0.12024571027720297\n",
      "Gradient Descent(406/999): loss=0.12023634232969545\n",
      "Gradient Descent(407/999): loss=0.12022698578908113\n",
      "Gradient Descent(408/999): loss=0.12021764063736882\n",
      "Gradient Descent(409/999): loss=0.12020830685660697\n",
      "Gradient Descent(410/999): loss=0.12019898442888342\n",
      "Gradient Descent(411/999): loss=0.12018967333632524\n",
      "Gradient Descent(412/999): loss=0.12018037356109855\n",
      "Gradient Descent(413/999): loss=0.12017108508540805\n",
      "Gradient Descent(414/999): loss=0.12016180789149707\n",
      "Gradient Descent(415/999): loss=0.12015254196164701\n",
      "Gradient Descent(416/999): loss=0.12014328727817752\n",
      "Gradient Descent(417/999): loss=0.12013404382344593\n",
      "Gradient Descent(418/999): loss=0.12012481157984718\n",
      "Gradient Descent(419/999): loss=0.12011559052981366\n",
      "Gradient Descent(420/999): loss=0.1201063806558148\n",
      "Gradient Descent(421/999): loss=0.12009718194035714\n",
      "Gradient Descent(422/999): loss=0.12008799436598384\n",
      "Gradient Descent(423/999): loss=0.12007881791527465\n",
      "Gradient Descent(424/999): loss=0.12006965257084573\n",
      "Gradient Descent(425/999): loss=0.12006049831534933\n",
      "Gradient Descent(426/999): loss=0.12005135513147368\n",
      "Gradient Descent(427/999): loss=0.12004222300194275\n",
      "Gradient Descent(428/999): loss=0.12003310190951613\n",
      "Gradient Descent(429/999): loss=0.12002399183698881\n",
      "Gradient Descent(430/999): loss=0.12001489276719093\n",
      "Gradient Descent(431/999): loss=0.12000580468298773\n",
      "Gradient Descent(432/999): loss=0.11999672756727932\n",
      "Gradient Descent(433/999): loss=0.1199876614030004\n",
      "Gradient Descent(434/999): loss=0.1199786061731203\n",
      "Gradient Descent(435/999): loss=0.11996956186064256\n",
      "Gradient Descent(436/999): loss=0.119960528448605\n",
      "Gradient Descent(437/999): loss=0.11995150592007943\n",
      "Gradient Descent(438/999): loss=0.11994249425817141\n",
      "Gradient Descent(439/999): loss=0.11993349344602033\n",
      "Gradient Descent(440/999): loss=0.119924503466799\n",
      "Gradient Descent(441/999): loss=0.1199155243037137\n",
      "Gradient Descent(442/999): loss=0.11990655594000382\n",
      "Gradient Descent(443/999): loss=0.11989759835894184\n",
      "Gradient Descent(444/999): loss=0.11988865154383326\n",
      "Gradient Descent(445/999): loss=0.11987971547801621\n",
      "Gradient Descent(446/999): loss=0.11987079014486152\n",
      "Gradient Descent(447/999): loss=0.1198618755277725\n",
      "Gradient Descent(448/999): loss=0.11985297161018475\n",
      "Gradient Descent(449/999): loss=0.11984407837556624\n",
      "Gradient Descent(450/999): loss=0.11983519580741672\n",
      "Gradient Descent(451/999): loss=0.11982632388926809\n",
      "Gradient Descent(452/999): loss=0.119817462604684\n",
      "Gradient Descent(453/999): loss=0.11980861193725967\n",
      "Gradient Descent(454/999): loss=0.11979977187062198\n",
      "Gradient Descent(455/999): loss=0.1197909423884291\n",
      "Gradient Descent(456/999): loss=0.11978212347437053\n",
      "Gradient Descent(457/999): loss=0.11977331511216692\n",
      "Gradient Descent(458/999): loss=0.11976451728556989\n",
      "Gradient Descent(459/999): loss=0.11975572997836209\n",
      "Gradient Descent(460/999): loss=0.11974695317435678\n",
      "Gradient Descent(461/999): loss=0.11973818685739798\n",
      "Gradient Descent(462/999): loss=0.11972943101136028\n",
      "Gradient Descent(463/999): loss=0.11972068562014862\n",
      "Gradient Descent(464/999): loss=0.11971195066769831\n",
      "Gradient Descent(465/999): loss=0.11970322613797482\n",
      "Gradient Descent(466/999): loss=0.11969451201497372\n",
      "Gradient Descent(467/999): loss=0.11968580828272055\n",
      "Gradient Descent(468/999): loss=0.11967711492527072\n",
      "Gradient Descent(469/999): loss=0.11966843192670941\n",
      "Gradient Descent(470/999): loss=0.11965975927115145\n",
      "Gradient Descent(471/999): loss=0.11965109694274123\n",
      "Gradient Descent(472/999): loss=0.11964244492565244\n",
      "Gradient Descent(473/999): loss=0.11963380320408831\n",
      "Gradient Descent(474/999): loss=0.11962517176228123\n",
      "Gradient Descent(475/999): loss=0.11961655058449262\n",
      "Gradient Descent(476/999): loss=0.1196079396550131\n",
      "Gradient Descent(477/999): loss=0.11959933895816209\n",
      "Gradient Descent(478/999): loss=0.11959074847828793\n",
      "Gradient Descent(479/999): loss=0.11958216819976766\n",
      "Gradient Descent(480/999): loss=0.11957359810700693\n",
      "Gradient Descent(481/999): loss=0.11956503818444009\n",
      "Gradient Descent(482/999): loss=0.11955648841652976\n",
      "Gradient Descent(483/999): loss=0.11954794878776702\n",
      "Gradient Descent(484/999): loss=0.11953941928267124\n",
      "Gradient Descent(485/999): loss=0.1195308998857899\n",
      "Gradient Descent(486/999): loss=0.11952239058169864\n",
      "Gradient Descent(487/999): loss=0.11951389135500108\n",
      "Gradient Descent(488/999): loss=0.11950540219032875\n",
      "Gradient Descent(489/999): loss=0.11949692307234103\n",
      "Gradient Descent(490/999): loss=0.119488453985725\n",
      "Gradient Descent(491/999): loss=0.11947999491519544\n",
      "Gradient Descent(492/999): loss=0.1194715458454947\n",
      "Gradient Descent(493/999): loss=0.11946310676139266\n",
      "Gradient Descent(494/999): loss=0.1194546776476865\n",
      "Gradient Descent(495/999): loss=0.11944625848920083\n",
      "Gradient Descent(496/999): loss=0.11943784927078745\n",
      "Gradient Descent(497/999): loss=0.11942944997732535\n",
      "Gradient Descent(498/999): loss=0.1194210605937206\n",
      "Gradient Descent(499/999): loss=0.11941268110490631\n",
      "Gradient Descent(500/999): loss=0.11940431149584249\n",
      "Gradient Descent(501/999): loss=0.11939595175151596\n",
      "Gradient Descent(502/999): loss=0.11938760185694039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(503/999): loss=0.1193792617971561\n",
      "Gradient Descent(504/999): loss=0.11937093155723014\n",
      "Gradient Descent(505/999): loss=0.11936261112225595\n",
      "Gradient Descent(506/999): loss=0.11935430047735353\n",
      "Gradient Descent(507/999): loss=0.11934599960766937\n",
      "Gradient Descent(508/999): loss=0.11933770849837612\n",
      "Gradient Descent(509/999): loss=0.1193294271346728\n",
      "Gradient Descent(510/999): loss=0.11932115550178463\n",
      "Gradient Descent(511/999): loss=0.11931289358496291\n",
      "Gradient Descent(512/999): loss=0.11930464136948497\n",
      "Gradient Descent(513/999): loss=0.1192963988406542\n",
      "Gradient Descent(514/999): loss=0.11928816598379978\n",
      "Gradient Descent(515/999): loss=0.11927994278427687\n",
      "Gradient Descent(516/999): loss=0.11927172922746629\n",
      "Gradient Descent(517/999): loss=0.11926352529877465\n",
      "Gradient Descent(518/999): loss=0.11925533098363417\n",
      "Gradient Descent(519/999): loss=0.1192471462675026\n",
      "Gradient Descent(520/999): loss=0.1192389711358633\n",
      "Gradient Descent(521/999): loss=0.11923080557422497\n",
      "Gradient Descent(522/999): loss=0.11922264956812183\n",
      "Gradient Descent(523/999): loss=0.11921450310311321\n",
      "Gradient Descent(524/999): loss=0.11920636616478389\n",
      "Gradient Descent(525/999): loss=0.11919823873874372\n",
      "Gradient Descent(526/999): loss=0.11919012081062777\n",
      "Gradient Descent(527/999): loss=0.11918201236609607\n",
      "Gradient Descent(528/999): loss=0.11917391339083376\n",
      "Gradient Descent(529/999): loss=0.11916582387055084\n",
      "Gradient Descent(530/999): loss=0.11915774379098222\n",
      "Gradient Descent(531/999): loss=0.11914967313788766\n",
      "Gradient Descent(532/999): loss=0.11914161189705164\n",
      "Gradient Descent(533/999): loss=0.11913356005428336\n",
      "Gradient Descent(534/999): loss=0.11912551759541666\n",
      "Gradient Descent(535/999): loss=0.11911748450631\n",
      "Gradient Descent(536/999): loss=0.1191094607728463\n",
      "Gradient Descent(537/999): loss=0.11910144638093302\n",
      "Gradient Descent(538/999): loss=0.11909344131650196\n",
      "Gradient Descent(539/999): loss=0.11908544556550932\n",
      "Gradient Descent(540/999): loss=0.1190774591139356\n",
      "Gradient Descent(541/999): loss=0.11906948194778552\n",
      "Gradient Descent(542/999): loss=0.11906151405308799\n",
      "Gradient Descent(543/999): loss=0.11905355541589605\n",
      "Gradient Descent(544/999): loss=0.11904560602228681\n",
      "Gradient Descent(545/999): loss=0.11903766585836144\n",
      "Gradient Descent(546/999): loss=0.11902973491024499\n",
      "Gradient Descent(547/999): loss=0.11902181316408653\n",
      "Gradient Descent(548/999): loss=0.11901390060605885\n",
      "Gradient Descent(549/999): loss=0.11900599722235869\n",
      "Gradient Descent(550/999): loss=0.11899810299920645\n",
      "Gradient Descent(551/999): loss=0.11899021792284624\n",
      "Gradient Descent(552/999): loss=0.11898234197954582\n",
      "Gradient Descent(553/999): loss=0.11897447515559655\n",
      "Gradient Descent(554/999): loss=0.11896661743731327\n",
      "Gradient Descent(555/999): loss=0.11895876881103447\n",
      "Gradient Descent(556/999): loss=0.11895092926312185\n",
      "Gradient Descent(557/999): loss=0.11894309877996066\n",
      "Gradient Descent(558/999): loss=0.11893527734795944\n",
      "Gradient Descent(559/999): loss=0.11892746495355004\n",
      "Gradient Descent(560/999): loss=0.11891966158318744\n",
      "Gradient Descent(561/999): loss=0.11891186722334991\n",
      "Gradient Descent(562/999): loss=0.11890408186053884\n",
      "Gradient Descent(563/999): loss=0.11889630548127865\n",
      "Gradient Descent(564/999): loss=0.11888853807211683\n",
      "Gradient Descent(565/999): loss=0.11888077961962387\n",
      "Gradient Descent(566/999): loss=0.11887303011039316\n",
      "Gradient Descent(567/999): loss=0.11886528953104102\n",
      "Gradient Descent(568/999): loss=0.11885755786820658\n",
      "Gradient Descent(569/999): loss=0.11884983510855175\n",
      "Gradient Descent(570/999): loss=0.11884212123876121\n",
      "Gradient Descent(571/999): loss=0.11883441624554233\n",
      "Gradient Descent(572/999): loss=0.11882672011562515\n",
      "Gradient Descent(573/999): loss=0.11881903283576226\n",
      "Gradient Descent(574/999): loss=0.11881135439272886\n",
      "Gradient Descent(575/999): loss=0.11880368477332261\n",
      "Gradient Descent(576/999): loss=0.11879602396436367\n",
      "Gradient Descent(577/999): loss=0.1187883719526946\n",
      "Gradient Descent(578/999): loss=0.11878072872518033\n",
      "Gradient Descent(579/999): loss=0.11877309426870813\n",
      "Gradient Descent(580/999): loss=0.11876546857018752\n",
      "Gradient Descent(581/999): loss=0.11875785161655023\n",
      "Gradient Descent(582/999): loss=0.11875024339475025\n",
      "Gradient Descent(583/999): loss=0.11874264389176371\n",
      "Gradient Descent(584/999): loss=0.11873505309458873\n",
      "Gradient Descent(585/999): loss=0.1187274709902456\n",
      "Gradient Descent(586/999): loss=0.11871989756577657\n",
      "Gradient Descent(587/999): loss=0.11871233280824586\n",
      "Gradient Descent(588/999): loss=0.1187047767047396\n",
      "Gradient Descent(589/999): loss=0.1186972292423658\n",
      "Gradient Descent(590/999): loss=0.1186896904082543\n",
      "Gradient Descent(591/999): loss=0.11868216018955678\n",
      "Gradient Descent(592/999): loss=0.11867463857344658\n",
      "Gradient Descent(593/999): loss=0.11866712554711882\n",
      "Gradient Descent(594/999): loss=0.11865962109779023\n",
      "Gradient Descent(595/999): loss=0.11865212521269913\n",
      "Gradient Descent(596/999): loss=0.11864463787910555\n",
      "Gradient Descent(597/999): loss=0.11863715908429089\n",
      "Gradient Descent(598/999): loss=0.11862968881555808\n",
      "Gradient Descent(599/999): loss=0.11862222706023158\n",
      "Gradient Descent(600/999): loss=0.11861477380565717\n",
      "Gradient Descent(601/999): loss=0.11860732903920203\n",
      "Gradient Descent(602/999): loss=0.11859989274825466\n",
      "Gradient Descent(603/999): loss=0.11859246492022484\n",
      "Gradient Descent(604/999): loss=0.11858504554254351\n",
      "Gradient Descent(605/999): loss=0.11857763460266295\n",
      "Gradient Descent(606/999): loss=0.1185702320880565\n",
      "Gradient Descent(607/999): loss=0.11856283798621868\n",
      "Gradient Descent(608/999): loss=0.11855545228466495\n",
      "Gradient Descent(609/999): loss=0.11854807497093199\n",
      "Gradient Descent(610/999): loss=0.11854070603257733\n",
      "Gradient Descent(611/999): loss=0.1185333454571795\n",
      "Gradient Descent(612/999): loss=0.11852599323233795\n",
      "Gradient Descent(613/999): loss=0.11851864934567302\n",
      "Gradient Descent(614/999): loss=0.1185113137848258\n",
      "Gradient Descent(615/999): loss=0.1185039865374583\n",
      "Gradient Descent(616/999): loss=0.11849666759125316\n",
      "Gradient Descent(617/999): loss=0.1184893569339138\n",
      "Gradient Descent(618/999): loss=0.11848205455316434\n",
      "Gradient Descent(619/999): loss=0.1184747604367494\n",
      "Gradient Descent(620/999): loss=0.11846747457243437\n",
      "Gradient Descent(621/999): loss=0.1184601969480051\n",
      "Gradient Descent(622/999): loss=0.11845292755126795\n",
      "Gradient Descent(623/999): loss=0.1184456663700498\n",
      "Gradient Descent(624/999): loss=0.11843841339219792\n",
      "Gradient Descent(625/999): loss=0.11843116860558003\n",
      "Gradient Descent(626/999): loss=0.11842393199808421\n",
      "Gradient Descent(627/999): loss=0.11841670355761882\n",
      "Gradient Descent(628/999): loss=0.11840948327211252\n",
      "Gradient Descent(629/999): loss=0.11840227112951424\n",
      "Gradient Descent(630/999): loss=0.11839506711779314\n",
      "Gradient Descent(631/999): loss=0.11838787122493846\n",
      "Gradient Descent(632/999): loss=0.1183806834389597\n",
      "Gradient Descent(633/999): loss=0.11837350374788634\n",
      "Gradient Descent(634/999): loss=0.11836633213976802\n",
      "Gradient Descent(635/999): loss=0.11835916860267429\n",
      "Gradient Descent(636/999): loss=0.11835201312469475\n",
      "Gradient Descent(637/999): loss=0.11834486569393898\n",
      "Gradient Descent(638/999): loss=0.11833772629853644\n",
      "Gradient Descent(639/999): loss=0.11833059492663639\n",
      "Gradient Descent(640/999): loss=0.11832347156640806\n",
      "Gradient Descent(641/999): loss=0.11831635620604034\n",
      "Gradient Descent(642/999): loss=0.118309248833742\n",
      "Gradient Descent(643/999): loss=0.11830214943774146\n",
      "Gradient Descent(644/999): loss=0.11829505800628687\n",
      "Gradient Descent(645/999): loss=0.118287974527646\n",
      "Gradient Descent(646/999): loss=0.11828089899010623\n",
      "Gradient Descent(647/999): loss=0.11827383138197461\n",
      "Gradient Descent(648/999): loss=0.11826677169157758\n",
      "Gradient Descent(649/999): loss=0.11825971990726124\n",
      "Gradient Descent(650/999): loss=0.11825267601739109\n",
      "Gradient Descent(651/999): loss=0.11824564001035207\n",
      "Gradient Descent(652/999): loss=0.11823861187454847\n",
      "Gradient Descent(653/999): loss=0.11823159159840407\n",
      "Gradient Descent(654/999): loss=0.11822457917036185\n",
      "Gradient Descent(655/999): loss=0.1182175745788842\n",
      "Gradient Descent(656/999): loss=0.11821057781245264\n",
      "Gradient Descent(657/999): loss=0.11820358885956803\n",
      "Gradient Descent(658/999): loss=0.11819660770875036\n",
      "Gradient Descent(659/999): loss=0.11818963434853876\n",
      "Gradient Descent(660/999): loss=0.1181826687674915\n",
      "Gradient Descent(661/999): loss=0.118175710954186\n",
      "Gradient Descent(662/999): loss=0.11816876089721856\n",
      "Gradient Descent(663/999): loss=0.11816181858520468\n",
      "Gradient Descent(664/999): loss=0.1181548840067787\n",
      "Gradient Descent(665/999): loss=0.11814795715059398\n",
      "Gradient Descent(666/999): loss=0.11814103800532276\n",
      "Gradient Descent(667/999): loss=0.11813412655965617\n",
      "Gradient Descent(668/999): loss=0.1181272228023042\n",
      "Gradient Descent(669/999): loss=0.11812032672199553\n",
      "Gradient Descent(670/999): loss=0.1181134383074778\n",
      "Gradient Descent(671/999): loss=0.11810655754751723\n",
      "Gradient Descent(672/999): loss=0.11809968443089883\n",
      "Gradient Descent(673/999): loss=0.11809281894642625\n",
      "Gradient Descent(674/999): loss=0.11808596108292177\n",
      "Gradient Descent(675/999): loss=0.1180791108292263\n",
      "Gradient Descent(676/999): loss=0.11807226817419932\n",
      "Gradient Descent(677/999): loss=0.11806543310671878\n",
      "Gradient Descent(678/999): loss=0.11805860561568125\n",
      "Gradient Descent(679/999): loss=0.11805178569000167\n",
      "Gradient Descent(680/999): loss=0.11804497331861341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(681/999): loss=0.11803816849046832\n",
      "Gradient Descent(682/999): loss=0.11803137119453658\n",
      "Gradient Descent(683/999): loss=0.11802458141980667\n",
      "Gradient Descent(684/999): loss=0.11801779915528547\n",
      "Gradient Descent(685/999): loss=0.11801102438999798\n",
      "Gradient Descent(686/999): loss=0.11800425711298758\n",
      "Gradient Descent(687/999): loss=0.11799749731331582\n",
      "Gradient Descent(688/999): loss=0.11799074498006235\n",
      "Gradient Descent(689/999): loss=0.11798400010232497\n",
      "Gradient Descent(690/999): loss=0.11797726266921969\n",
      "Gradient Descent(691/999): loss=0.11797053266988054\n",
      "Gradient Descent(692/999): loss=0.11796381009345952\n",
      "Gradient Descent(693/999): loss=0.11795709492912673\n",
      "Gradient Descent(694/999): loss=0.11795038716607023\n",
      "Gradient Descent(695/999): loss=0.11794368679349594\n",
      "Gradient Descent(696/999): loss=0.1179369938006278\n",
      "Gradient Descent(697/999): loss=0.11793030817670762\n",
      "Gradient Descent(698/999): loss=0.11792362991099495\n",
      "Gradient Descent(699/999): loss=0.11791695899276732\n",
      "Gradient Descent(700/999): loss=0.1179102954113199\n",
      "Gradient Descent(701/999): loss=0.11790363915596566\n",
      "Gradient Descent(702/999): loss=0.11789699021603534\n",
      "Gradient Descent(703/999): loss=0.11789034858087725\n",
      "Gradient Descent(704/999): loss=0.11788371423985752\n",
      "Gradient Descent(705/999): loss=0.11787708718235977\n",
      "Gradient Descent(706/999): loss=0.1178704673977853\n",
      "Gradient Descent(707/999): loss=0.1178638548755529\n",
      "Gradient Descent(708/999): loss=0.11785724960509893\n",
      "Gradient Descent(709/999): loss=0.11785065157587725\n",
      "Gradient Descent(710/999): loss=0.11784406077735922\n",
      "Gradient Descent(711/999): loss=0.11783747719903356\n",
      "Gradient Descent(712/999): loss=0.11783090083040644\n",
      "Gradient Descent(713/999): loss=0.11782433166100141\n",
      "Gradient Descent(714/999): loss=0.11781776968035934\n",
      "Gradient Descent(715/999): loss=0.1178112148780385\n",
      "Gradient Descent(716/999): loss=0.11780466724361434\n",
      "Gradient Descent(717/999): loss=0.11779812676667954\n",
      "Gradient Descent(718/999): loss=0.11779159343684414\n",
      "Gradient Descent(719/999): loss=0.11778506724373526\n",
      "Gradient Descent(720/999): loss=0.11777854817699715\n",
      "Gradient Descent(721/999): loss=0.11777203622629133\n",
      "Gradient Descent(722/999): loss=0.11776553138129632\n",
      "Gradient Descent(723/999): loss=0.11775903363170769\n",
      "Gradient Descent(724/999): loss=0.1177525429672381\n",
      "Gradient Descent(725/999): loss=0.1177460593776172\n",
      "Gradient Descent(726/999): loss=0.11773958285259166\n",
      "Gradient Descent(727/999): loss=0.11773311338192499\n",
      "Gradient Descent(728/999): loss=0.11772665095539772\n",
      "Gradient Descent(729/999): loss=0.11772019556280723\n",
      "Gradient Descent(730/999): loss=0.11771374719396778\n",
      "Gradient Descent(731/999): loss=0.11770730583871039\n",
      "Gradient Descent(732/999): loss=0.11770087148688298\n",
      "Gradient Descent(733/999): loss=0.11769444412835012\n",
      "Gradient Descent(734/999): loss=0.11768802375299321\n",
      "Gradient Descent(735/999): loss=0.11768161035071034\n",
      "Gradient Descent(736/999): loss=0.11767520391141627\n",
      "Gradient Descent(737/999): loss=0.1176688044250424\n",
      "Gradient Descent(738/999): loss=0.11766241188153678\n",
      "Gradient Descent(739/999): loss=0.11765602627086401\n",
      "Gradient Descent(740/999): loss=0.11764964758300525\n",
      "Gradient Descent(741/999): loss=0.11764327580795823\n",
      "Gradient Descent(742/999): loss=0.11763691093573718\n",
      "Gradient Descent(743/999): loss=0.11763055295637277\n",
      "Gradient Descent(744/999): loss=0.11762420185991213\n",
      "Gradient Descent(745/999): loss=0.11761785763641884\n",
      "Gradient Descent(746/999): loss=0.11761152027597278\n",
      "Gradient Descent(747/999): loss=0.11760518976867028\n",
      "Gradient Descent(748/999): loss=0.11759886610462396\n",
      "Gradient Descent(749/999): loss=0.1175925492739627\n",
      "Gradient Descent(750/999): loss=0.11758623926683173\n",
      "Gradient Descent(751/999): loss=0.11757993607339248\n",
      "Gradient Descent(752/999): loss=0.11757363968382256\n",
      "Gradient Descent(753/999): loss=0.11756735008831584\n",
      "Gradient Descent(754/999): loss=0.11756106727708229\n",
      "Gradient Descent(755/999): loss=0.11755479124034802\n",
      "Gradient Descent(756/999): loss=0.11754852196835525\n",
      "Gradient Descent(757/999): loss=0.11754225945136222\n",
      "Gradient Descent(758/999): loss=0.1175360036796433\n",
      "Gradient Descent(759/999): loss=0.11752975464348883\n",
      "Gradient Descent(760/999): loss=0.11752351233320511\n",
      "Gradient Descent(761/999): loss=0.11751727673911441\n",
      "Gradient Descent(762/999): loss=0.11751104785155499\n",
      "Gradient Descent(763/999): loss=0.11750482566088091\n",
      "Gradient Descent(764/999): loss=0.11749861015746219\n",
      "Gradient Descent(765/999): loss=0.11749240133168466\n",
      "Gradient Descent(766/999): loss=0.11748619917394991\n",
      "Gradient Descent(767/999): loss=0.11748000367467548\n",
      "Gradient Descent(768/999): loss=0.1174738148242945\n",
      "Gradient Descent(769/999): loss=0.11746763261325589\n",
      "Gradient Descent(770/999): loss=0.11746145703202435\n",
      "Gradient Descent(771/999): loss=0.11745528807108015\n",
      "Gradient Descent(772/999): loss=0.11744912572091927\n",
      "Gradient Descent(773/999): loss=0.1174429699720533\n",
      "Gradient Descent(774/999): loss=0.11743682081500942\n",
      "Gradient Descent(775/999): loss=0.11743067824033038\n",
      "Gradient Descent(776/999): loss=0.11742454223857453\n",
      "Gradient Descent(777/999): loss=0.11741841280031563\n",
      "Gradient Descent(778/999): loss=0.117412289916143\n",
      "Gradient Descent(779/999): loss=0.11740617357666139\n",
      "Gradient Descent(780/999): loss=0.11740006377249095\n",
      "Gradient Descent(781/999): loss=0.11739396049426737\n",
      "Gradient Descent(782/999): loss=0.11738786373264154\n",
      "Gradient Descent(783/999): loss=0.11738177347827977\n",
      "Gradient Descent(784/999): loss=0.11737568972186375\n",
      "Gradient Descent(785/999): loss=0.11736961245409043\n",
      "Gradient Descent(786/999): loss=0.11736354166567199\n",
      "Gradient Descent(787/999): loss=0.11735747734733588\n",
      "Gradient Descent(788/999): loss=0.11735141948982479\n",
      "Gradient Descent(789/999): loss=0.1173453680838966\n",
      "Gradient Descent(790/999): loss=0.11733932312032432\n",
      "Gradient Descent(791/999): loss=0.11733328458989606\n",
      "Gradient Descent(792/999): loss=0.11732725248341516\n",
      "Gradient Descent(793/999): loss=0.11732122679169994\n",
      "Gradient Descent(794/999): loss=0.11731520750558383\n",
      "Gradient Descent(795/999): loss=0.11730919461591523\n",
      "Gradient Descent(796/999): loss=0.11730318811355762\n",
      "Gradient Descent(797/999): loss=0.11729718798938939\n",
      "Gradient Descent(798/999): loss=0.11729119423430392\n",
      "Gradient Descent(799/999): loss=0.11728520683920957\n",
      "Gradient Descent(800/999): loss=0.11727922579502942\n",
      "Gradient Descent(801/999): loss=0.11727325109270162\n",
      "Gradient Descent(802/999): loss=0.11726728272317906\n",
      "Gradient Descent(803/999): loss=0.1172613206774295\n",
      "Gradient Descent(804/999): loss=0.11725536494643544\n",
      "Gradient Descent(805/999): loss=0.11724941552119418\n",
      "Gradient Descent(806/999): loss=0.11724347239271779\n",
      "Gradient Descent(807/999): loss=0.11723753555203299\n",
      "Gradient Descent(808/999): loss=0.11723160499018127\n",
      "Gradient Descent(809/999): loss=0.1172256806982187\n",
      "Gradient Descent(810/999): loss=0.11721976266721605\n",
      "Gradient Descent(811/999): loss=0.11721385088825871\n",
      "Gradient Descent(812/999): loss=0.11720794535244661\n",
      "Gradient Descent(813/999): loss=0.11720204605089428\n",
      "Gradient Descent(814/999): loss=0.11719615297473077\n",
      "Gradient Descent(815/999): loss=0.11719026611509965\n",
      "Gradient Descent(816/999): loss=0.11718438546315897\n",
      "Gradient Descent(817/999): loss=0.11717851101008128\n",
      "Gradient Descent(818/999): loss=0.11717264274705348\n",
      "Gradient Descent(819/999): loss=0.11716678066527694\n",
      "Gradient Descent(820/999): loss=0.11716092475596743\n",
      "Gradient Descent(821/999): loss=0.11715507501035506\n",
      "Gradient Descent(822/999): loss=0.1171492314196842\n",
      "Gradient Descent(823/999): loss=0.11714339397521369\n",
      "Gradient Descent(824/999): loss=0.11713756266821651\n",
      "Gradient Descent(825/999): loss=0.11713173748998\n",
      "Gradient Descent(826/999): loss=0.11712591843180564\n",
      "Gradient Descent(827/999): loss=0.11712010548500922\n",
      "Gradient Descent(828/999): loss=0.11711429864092063\n",
      "Gradient Descent(829/999): loss=0.11710849789088393\n",
      "Gradient Descent(830/999): loss=0.11710270322625738\n",
      "Gradient Descent(831/999): loss=0.11709691463841329\n",
      "Gradient Descent(832/999): loss=0.11709113211873809\n",
      "Gradient Descent(833/999): loss=0.11708535565863226\n",
      "Gradient Descent(834/999): loss=0.1170795852495103\n",
      "Gradient Descent(835/999): loss=0.11707382088280073\n",
      "Gradient Descent(836/999): loss=0.11706806254994606\n",
      "Gradient Descent(837/999): loss=0.11706231024240281\n",
      "Gradient Descent(838/999): loss=0.11705656395164134\n",
      "Gradient Descent(839/999): loss=0.11705082366914603\n",
      "Gradient Descent(840/999): loss=0.11704508938641504\n",
      "Gradient Descent(841/999): loss=0.11703936109496049\n",
      "Gradient Descent(842/999): loss=0.11703363878630833\n",
      "Gradient Descent(843/999): loss=0.11702792245199825\n",
      "Gradient Descent(844/999): loss=0.11702221208358382\n",
      "Gradient Descent(845/999): loss=0.11701650767263234\n",
      "Gradient Descent(846/999): loss=0.11701080921072489\n",
      "Gradient Descent(847/999): loss=0.11700511668945619\n",
      "Gradient Descent(848/999): loss=0.11699943010043475\n",
      "Gradient Descent(849/999): loss=0.11699374943528268\n",
      "Gradient Descent(850/999): loss=0.11698807468563578\n",
      "Gradient Descent(851/999): loss=0.11698240584314348\n",
      "Gradient Descent(852/999): loss=0.1169767428994688\n",
      "Gradient Descent(853/999): loss=0.11697108584628833\n",
      "Gradient Descent(854/999): loss=0.1169654346752922\n",
      "Gradient Descent(855/999): loss=0.11695978937818413\n",
      "Gradient Descent(856/999): loss=0.11695414994668128\n",
      "Gradient Descent(857/999): loss=0.11694851637251434\n",
      "Gradient Descent(858/999): loss=0.11694288864742744\n",
      "Gradient Descent(859/999): loss=0.11693726676317809\n",
      "Gradient Descent(860/999): loss=0.11693165071153735\n",
      "Gradient Descent(861/999): loss=0.11692604048428956\n",
      "Gradient Descent(862/999): loss=0.11692043607323244\n",
      "Gradient Descent(863/999): loss=0.11691483747017709\n",
      "Gradient Descent(864/999): loss=0.11690924466694787\n",
      "Gradient Descent(865/999): loss=0.11690365765538252\n",
      "Gradient Descent(866/999): loss=0.11689807642733198\n",
      "Gradient Descent(867/999): loss=0.11689250097466043\n",
      "Gradient Descent(868/999): loss=0.11688693128924539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(869/999): loss=0.11688136736297747\n",
      "Gradient Descent(870/999): loss=0.11687580918776047\n",
      "Gradient Descent(871/999): loss=0.11687025675551144\n",
      "Gradient Descent(872/999): loss=0.11686471005816045\n",
      "Gradient Descent(873/999): loss=0.11685916908765072\n",
      "Gradient Descent(874/999): loss=0.11685363383593858\n",
      "Gradient Descent(875/999): loss=0.11684810429499344\n",
      "Gradient Descent(876/999): loss=0.11684258045679773\n",
      "Gradient Descent(877/999): loss=0.11683706231334685\n",
      "Gradient Descent(878/999): loss=0.11683154985664929\n",
      "Gradient Descent(879/999): loss=0.11682604307872646\n",
      "Gradient Descent(880/999): loss=0.11682054197161275\n",
      "Gradient Descent(881/999): loss=0.11681504652735543\n",
      "Gradient Descent(882/999): loss=0.11680955673801477\n",
      "Gradient Descent(883/999): loss=0.11680407259566379\n",
      "Gradient Descent(884/999): loss=0.11679859409238852\n",
      "Gradient Descent(885/999): loss=0.1167931212202877\n",
      "Gradient Descent(886/999): loss=0.11678765397147303\n",
      "Gradient Descent(887/999): loss=0.1167821923380688\n",
      "Gradient Descent(888/999): loss=0.11677673631221235\n",
      "Gradient Descent(889/999): loss=0.1167712858860535\n",
      "Gradient Descent(890/999): loss=0.11676584105175493\n",
      "Gradient Descent(891/999): loss=0.11676040180149205\n",
      "Gradient Descent(892/999): loss=0.11675496812745292\n",
      "Gradient Descent(893/999): loss=0.11674954002183822\n",
      "Gradient Descent(894/999): loss=0.11674411747686134\n",
      "Gradient Descent(895/999): loss=0.11673870048474826\n",
      "Gradient Descent(896/999): loss=0.1167332890377375\n",
      "Gradient Descent(897/999): loss=0.11672788312808027\n",
      "Gradient Descent(898/999): loss=0.11672248274804023\n",
      "Gradient Descent(899/999): loss=0.11671708788989368\n",
      "Gradient Descent(900/999): loss=0.11671169854592925\n",
      "Gradient Descent(901/999): loss=0.11670631470844826\n",
      "Gradient Descent(902/999): loss=0.11670093636976435\n",
      "Gradient Descent(903/999): loss=0.11669556352220369\n",
      "Gradient Descent(904/999): loss=0.1166901961581048\n",
      "Gradient Descent(905/999): loss=0.1166848342698187\n",
      "Gradient Descent(906/999): loss=0.11667947784970864\n",
      "Gradient Descent(907/999): loss=0.11667412689015037\n",
      "Gradient Descent(908/999): loss=0.1166687813835319\n",
      "Gradient Descent(909/999): loss=0.1166634413222536\n",
      "Gradient Descent(910/999): loss=0.11665810669872809\n",
      "Gradient Descent(911/999): loss=0.11665277750538028\n",
      "Gradient Descent(912/999): loss=0.11664745373464731\n",
      "Gradient Descent(913/999): loss=0.11664213537897856\n",
      "Gradient Descent(914/999): loss=0.11663682243083569\n",
      "Gradient Descent(915/999): loss=0.11663151488269241\n",
      "Gradient Descent(916/999): loss=0.1166262127270347\n",
      "Gradient Descent(917/999): loss=0.11662091595636065\n",
      "Gradient Descent(918/999): loss=0.11661562456318045\n",
      "Gradient Descent(919/999): loss=0.11661033854001646\n",
      "Gradient Descent(920/999): loss=0.11660505787940302\n",
      "Gradient Descent(921/999): loss=0.11659978257388662\n",
      "Gradient Descent(922/999): loss=0.11659451261602573\n",
      "Gradient Descent(923/999): loss=0.11658924799839086\n",
      "Gradient Descent(924/999): loss=0.1165839887135646\n",
      "Gradient Descent(925/999): loss=0.11657873475414131\n",
      "Gradient Descent(926/999): loss=0.11657348611272751\n",
      "Gradient Descent(927/999): loss=0.11656824278194156\n",
      "Gradient Descent(928/999): loss=0.11656300475441372\n",
      "Gradient Descent(929/999): loss=0.11655777202278622\n",
      "Gradient Descent(930/999): loss=0.11655254457971305\n",
      "Gradient Descent(931/999): loss=0.1165473224178602\n",
      "Gradient Descent(932/999): loss=0.11654210552990532\n",
      "Gradient Descent(933/999): loss=0.11653689390853801\n",
      "Gradient Descent(934/999): loss=0.11653168754645961\n",
      "Gradient Descent(935/999): loss=0.11652648643638322\n",
      "Gradient Descent(936/999): loss=0.11652129057103365\n",
      "Gradient Descent(937/999): loss=0.11651609994314757\n",
      "Gradient Descent(938/999): loss=0.1165109145454732\n",
      "Gradient Descent(939/999): loss=0.1165057343707706\n",
      "Gradient Descent(940/999): loss=0.11650055941181132\n",
      "Gradient Descent(941/999): loss=0.11649538966137875\n",
      "Gradient Descent(942/999): loss=0.11649022511226777\n",
      "Gradient Descent(943/999): loss=0.11648506575728493\n",
      "Gradient Descent(944/999): loss=0.11647991158924834\n",
      "Gradient Descent(945/999): loss=0.11647476260098767\n",
      "Gradient Descent(946/999): loss=0.11646961878534419\n",
      "Gradient Descent(947/999): loss=0.11646448013517062\n",
      "Gradient Descent(948/999): loss=0.11645934664333125\n",
      "Gradient Descent(949/999): loss=0.11645421830270188\n",
      "Gradient Descent(950/999): loss=0.11644909510616966\n",
      "Gradient Descent(951/999): loss=0.11644397704663331\n",
      "Gradient Descent(952/999): loss=0.11643886411700285\n",
      "Gradient Descent(953/999): loss=0.11643375631019988\n",
      "Gradient Descent(954/999): loss=0.1164286536191572\n",
      "Gradient Descent(955/999): loss=0.11642355603681917\n",
      "Gradient Descent(956/999): loss=0.11641846355614131\n",
      "Gradient Descent(957/999): loss=0.11641337617009058\n",
      "Gradient Descent(958/999): loss=0.11640829387164522\n",
      "Gradient Descent(959/999): loss=0.11640321665379483\n",
      "Gradient Descent(960/999): loss=0.11639814450954013\n",
      "Gradient Descent(961/999): loss=0.1163930774318932\n",
      "Gradient Descent(962/999): loss=0.11638801541387732\n",
      "Gradient Descent(963/999): loss=0.11638295844852699\n",
      "Gradient Descent(964/999): loss=0.11637790652888791\n",
      "Gradient Descent(965/999): loss=0.11637285964801693\n",
      "Gradient Descent(966/999): loss=0.11636781779898205\n",
      "Gradient Descent(967/999): loss=0.11636278097486244\n",
      "Gradient Descent(968/999): loss=0.11635774916874832\n",
      "Gradient Descent(969/999): loss=0.11635272237374107\n",
      "Gradient Descent(970/999): loss=0.11634770058295316\n",
      "Gradient Descent(971/999): loss=0.116342683789508\n",
      "Gradient Descent(972/999): loss=0.11633767198654016\n",
      "Gradient Descent(973/999): loss=0.11633266516719519\n",
      "Gradient Descent(974/999): loss=0.11632766332462964\n",
      "Gradient Descent(975/999): loss=0.11632266645201098\n",
      "Gradient Descent(976/999): loss=0.11631767454251778\n",
      "Gradient Descent(977/999): loss=0.11631268758933935\n",
      "Gradient Descent(978/999): loss=0.11630770558567616\n",
      "Gradient Descent(979/999): loss=0.11630272852473938\n",
      "Gradient Descent(980/999): loss=0.1162977563997512\n",
      "Gradient Descent(981/999): loss=0.1162927892039446\n",
      "Gradient Descent(982/999): loss=0.11628782693056344\n",
      "Gradient Descent(983/999): loss=0.1162828695728624\n",
      "Gradient Descent(984/999): loss=0.11627791712410702\n",
      "Gradient Descent(985/999): loss=0.1162729695775735\n",
      "Gradient Descent(986/999): loss=0.11626802692654897\n",
      "Gradient Descent(987/999): loss=0.11626308916433119\n",
      "Gradient Descent(988/999): loss=0.11625815628422875\n",
      "Gradient Descent(989/999): loss=0.11625322827956092\n",
      "Gradient Descent(990/999): loss=0.11624830514365762\n",
      "Gradient Descent(991/999): loss=0.11624338686985951\n",
      "Gradient Descent(992/999): loss=0.11623847345151796\n",
      "Gradient Descent(993/999): loss=0.1162335648819948\n",
      "Gradient Descent(994/999): loss=0.1162286611546627\n",
      "Gradient Descent(995/999): loss=0.11622376226290479\n",
      "Gradient Descent(996/999): loss=0.11621886820011486\n",
      "Gradient Descent(997/999): loss=0.11621397895969723\n",
      "Gradient Descent(998/999): loss=0.11620909453506685\n",
      "Gradient Descent(999/999): loss=0.11620421491964905\n",
      "Gradient Descent(0/999): loss=0.14802841371423545\n",
      "Gradient Descent(1/999): loss=0.14733369692909587\n",
      "Gradient Descent(2/999): loss=0.14665012610258044\n",
      "Gradient Descent(3/999): loss=0.14597752155645172\n",
      "Gradient Descent(4/999): loss=0.1453157065092214\n",
      "Gradient Descent(5/999): loss=0.14466450702944947\n",
      "Gradient Descent(6/999): loss=0.14402375198979586\n",
      "Gradient Descent(7/999): loss=0.143393273021813\n",
      "Gradient Descent(8/999): loss=0.14277290447146757\n",
      "Gradient Descent(9/999): loss=0.14216248335537834\n",
      "Gradient Descent(10/999): loss=0.14156184931776092\n",
      "Gradient Descent(11/999): loss=0.14097084458806525\n",
      "Gradient Descent(12/999): loss=0.1403893139392972\n",
      "Gradient Descent(13/999): loss=0.13981710464701136\n",
      "Gradient Descent(14/999): loss=0.13925406644896535\n",
      "Gradient Descent(15/999): loss=0.13870005150542414\n",
      "Gradient Descent(16/999): loss=0.13815491436010494\n",
      "Gradient Descent(17/999): loss=0.13761851190175148\n",
      "Gradient Descent(18/999): loss=0.13709070332632728\n",
      "Gradient Descent(19/999): loss=0.13657135009981938\n",
      "Gradient Descent(20/999): loss=0.13606031592164092\n",
      "Gradient Descent(21/999): loss=0.13555746668862417\n",
      "Gradient Descent(22/999): loss=0.13506267045959405\n",
      "Gradient Descent(23/999): loss=0.13457579742051257\n",
      "Gradient Descent(24/999): loss=0.1340967198501853\n",
      "Gradient Descent(25/999): loss=0.13362531208652104\n",
      "Gradient Descent(26/999): loss=0.13316145049333483\n",
      "Gradient Descent(27/999): loss=0.13270501342768673\n",
      "Gradient Descent(28/999): loss=0.13225588120774662\n",
      "Gradient Descent(29/999): loss=0.13181393608117786\n",
      "Gradient Descent(30/999): loss=0.13137906219402973\n",
      "Gradient Descent(31/999): loss=0.13095114556013213\n",
      "Gradient Descent(32/999): loss=0.13053007403098313\n",
      "Gradient Descent(33/999): loss=0.13011573726612277\n",
      "Gradient Descent(34/999): loss=0.12970802670398332\n",
      "Gradient Descent(35/999): loss=0.1293068355332108\n",
      "Gradient Descent(36/999): loss=0.1289120586644477\n",
      "Gradient Descent(37/999): loss=0.1285235927025714\n",
      "Gradient Descent(38/999): loss=0.12814133591937985\n",
      "Gradient Descent(39/999): loss=0.1277651882267176\n",
      "Gradient Descent(40/999): loss=0.12739505115003516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(41/999): loss=0.12703082780237465\n",
      "Gradient Descent(42/999): loss=0.12667242285877464\n",
      "Gradient Descent(43/999): loss=0.12631974253108802\n",
      "Gradient Descent(44/999): loss=0.12597269454320545\n",
      "Gradient Descent(45/999): loss=0.1256311881066786\n",
      "Gradient Descent(46/999): loss=0.12529513389673608\n",
      "Gradient Descent(47/999): loss=0.12496444402868627\n",
      "Gradient Descent(48/999): loss=0.1246390320347005\n",
      "Gradient Descent(49/999): loss=0.12431881284097054\n",
      "Gradient Descent(50/999): loss=0.12400370274523434\n",
      "Gradient Descent(51/999): loss=0.12369361939466393\n",
      "Gradient Descent(52/999): loss=0.12338848176411023\n",
      "Gradient Descent(53/999): loss=0.12308821013469828\n",
      "Gradient Descent(54/999): loss=0.12279272607276746\n",
      "Gradient Descent(55/999): loss=0.12250195240915168\n",
      "Gradient Descent(56/999): loss=0.12221581321879299\n",
      "Gradient Descent(57/999): loss=0.12193423380068454\n",
      "Gradient Descent(58/999): loss=0.1216571406581362\n",
      "Gradient Descent(59/999): loss=0.12138446147935897\n",
      "Gradient Descent(60/999): loss=0.12111612511836202\n",
      "Gradient Descent(61/999): loss=0.12085206157615778\n",
      "Gradient Descent(62/999): loss=0.12059220198227012\n",
      "Gradient Descent(63/999): loss=0.12033647857654055\n",
      "Gradient Descent(64/999): loss=0.12008482469122779\n",
      "Gradient Descent(65/999): loss=0.1198371747333961\n",
      "Gradient Descent(66/999): loss=0.11959346416758711\n",
      "Gradient Descent(67/999): loss=0.1193536294987714\n",
      "Gradient Descent(68/999): loss=0.11911760825557491\n",
      "Gradient Descent(69/999): loss=0.1188853389737755\n",
      "Gradient Descent(70/999): loss=0.11865676118006582\n",
      "Gradient Descent(71/999): loss=0.11843181537607791\n",
      "Gradient Descent(72/999): loss=0.11821044302266531\n",
      "Gradient Descent(73/999): loss=0.11799258652443853\n",
      "Gradient Descent(74/999): loss=0.11777818921455006\n",
      "Gradient Descent(75/999): loss=0.11756719533972425\n",
      "Gradient Descent(76/999): loss=0.11735955004552903\n",
      "Gradient Descent(77/999): loss=0.1171551993618847\n",
      "Gradient Descent(78/999): loss=0.11695409018880669\n",
      "Gradient Descent(79/999): loss=0.11675617028237761\n",
      "Gradient Descent(80/999): loss=0.11656138824094624\n",
      "Gradient Descent(81/999): loss=0.11636969349154837\n",
      "Gradient Descent(82/999): loss=0.11618103627654668\n",
      "Gradient Descent(83/999): loss=0.11599536764048646\n",
      "Gradient Descent(84/999): loss=0.11581263941716265\n",
      "Gradient Descent(85/999): loss=0.11563280421689566\n",
      "Gradient Descent(86/999): loss=0.1154558154140124\n",
      "Gradient Descent(87/999): loss=0.11528162713452896\n",
      "Gradient Descent(88/999): loss=0.11511019424403203\n",
      "Gradient Descent(89/999): loss=0.11494147233575554\n",
      "Gradient Descent(90/999): loss=0.11477541771884961\n",
      "Gradient Descent(91/999): loss=0.11461198740683873\n",
      "Gradient Descent(92/999): loss=0.11445113910626582\n",
      "Gradient Descent(93/999): loss=0.11429283120551938\n",
      "Gradient Descent(94/999): loss=0.11413702276384105\n",
      "Gradient Descent(95/999): loss=0.11398367350050999\n",
      "Gradient Descent(96/999): loss=0.11383274378420195\n",
      "Gradient Descent(97/999): loss=0.11368419462251972\n",
      "Gradient Descent(98/999): loss=0.1135379876516923\n",
      "Gradient Descent(99/999): loss=0.11339408512644057\n",
      "Gradient Descent(100/999): loss=0.1132524499100056\n",
      "Gradient Descent(101/999): loss=0.11311304546433855\n",
      "Gradient Descent(102/999): loss=0.11297583584044801\n",
      "Gradient Descent(103/999): loss=0.1128407856689035\n",
      "Gradient Descent(104/999): loss=0.11270786015049178\n",
      "Gradient Descent(105/999): loss=0.11257702504702372\n",
      "Gradient Descent(106/999): loss=0.11244824667228981\n",
      "Gradient Descent(107/999): loss=0.11232149188316098\n",
      "Gradient Descent(108/999): loss=0.11219672807083315\n",
      "Gradient Descent(109/999): loss=0.11207392315221301\n",
      "Gradient Descent(110/999): loss=0.1119530455614424\n",
      "Gradient Descent(111/999): loss=0.11183406424155963\n",
      "Gradient Descent(112/999): loss=0.11171694863629503\n",
      "Gradient Descent(113/999): loss=0.11160166868199896\n",
      "Gradient Descent(114/999): loss=0.11148819479969956\n",
      "Gradient Descent(115/999): loss=0.11137649788728923\n",
      "Gradient Descent(116/999): loss=0.11126654931183631\n",
      "Gradient Descent(117/999): loss=0.11115832090202132\n",
      "Gradient Descent(118/999): loss=0.11105178494069476\n",
      "Gradient Descent(119/999): loss=0.11094691415755505\n",
      "Gradient Descent(120/999): loss=0.11084368172194456\n",
      "Gradient Descent(121/999): loss=0.11074206123576162\n",
      "Gradient Descent(122/999): loss=0.11064202672648706\n",
      "Gradient Descent(123/999): loss=0.11054355264032267\n",
      "Gradient Descent(124/999): loss=0.11044661383544115\n",
      "Gradient Descent(125/999): loss=0.11035118557534385\n",
      "Gradient Descent(126/999): loss=0.1102572435223264\n",
      "Gradient Descent(127/999): loss=0.11016476373104929\n",
      "Gradient Descent(128/999): loss=0.11007372264221223\n",
      "Gradient Descent(129/999): loss=0.10998409707633047\n",
      "Gradient Descent(130/999): loss=0.10989586422761141\n",
      "Gradient Descent(131/999): loss=0.10980900165793002\n",
      "Gradient Descent(132/999): loss=0.10972348729090138\n",
      "Gradient Descent(133/999): loss=0.1096392994060486\n",
      "Gradient Descent(134/999): loss=0.10955641663306509\n",
      "Gradient Descent(135/999): loss=0.10947481794616898\n",
      "Gradient Descent(136/999): loss=0.1093944826585489\n",
      "Gradient Descent(137/999): loss=0.10931539041689894\n",
      "Gradient Descent(138/999): loss=0.10923752119604212\n",
      "Gradient Descent(139/999): loss=0.1091608552936402\n",
      "Gradient Descent(140/999): loss=0.10908537332498888\n",
      "Gradient Descent(141/999): loss=0.109011056217897\n",
      "Gradient Descent(142/999): loss=0.10893788520764826\n",
      "Gradient Descent(143/999): loss=0.10886584183204415\n",
      "Gradient Descent(144/999): loss=0.10879490792652667\n",
      "Gradient Descent(145/999): loss=0.10872506561937989\n",
      "Gradient Descent(146/999): loss=0.10865629732700886\n",
      "Gradient Descent(147/999): loss=0.10858858574929438\n",
      "Gradient Descent(148/999): loss=0.10852191386502294\n",
      "Gradient Descent(149/999): loss=0.10845626492739027\n",
      "Gradient Descent(150/999): loss=0.10839162245957722\n",
      "Gradient Descent(151/999): loss=0.10832797025039718\n",
      "Gradient Descent(152/999): loss=0.10826529235001356\n",
      "Gradient Descent(153/999): loss=0.10820357306572635\n",
      "Gradient Descent(154/999): loss=0.10814279695782658\n",
      "Gradient Descent(155/999): loss=0.10808294883551751\n",
      "Gradient Descent(156/999): loss=0.108024013752902\n",
      "Gradient Descent(157/999): loss=0.10796597700503387\n",
      "Gradient Descent(158/999): loss=0.10790882412403359\n",
      "Gradient Descent(159/999): loss=0.10785254087526633\n",
      "Gradient Descent(160/999): loss=0.10779711325358136\n",
      "Gradient Descent(161/999): loss=0.10774252747961258\n",
      "Gradient Descent(162/999): loss=0.1076887699961384\n",
      "Gradient Descent(163/999): loss=0.10763582746450027\n",
      "Gradient Descent(164/999): loss=0.10758368676107899\n",
      "Gradient Descent(165/999): loss=0.1075323349738281\n",
      "Gradient Descent(166/999): loss=0.10748175939886263\n",
      "Gradient Descent(167/999): loss=0.10743194753710344\n",
      "Gradient Descent(168/999): loss=0.10738288709097506\n",
      "Gradient Descent(169/999): loss=0.1073345659611574\n",
      "Gradient Descent(170/999): loss=0.10728697224338912\n",
      "Gradient Descent(171/999): loss=0.1072400942253233\n",
      "Gradient Descent(172/999): loss=0.10719392038343309\n",
      "Gradient Descent(173/999): loss=0.10714843937996779\n",
      "Gradient Descent(174/999): loss=0.10710364005995773\n",
      "Gradient Descent(175/999): loss=0.10705951144826742\n",
      "Gradient Descent(176/999): loss=0.10701604274669636\n",
      "Gradient Descent(177/999): loss=0.10697322333112659\n",
      "Gradient Descent(178/999): loss=0.10693104274871587\n",
      "Gradient Descent(179/999): loss=0.10688949071513676\n",
      "Gradient Descent(180/999): loss=0.10684855711185953\n",
      "Gradient Descent(181/999): loss=0.10680823198347938\n",
      "Gradient Descent(182/999): loss=0.10676850553508654\n",
      "Gradient Descent(183/999): loss=0.10672936812967866\n",
      "Gradient Descent(184/999): loss=0.10669081028561513\n",
      "Gradient Descent(185/999): loss=0.10665282267411244\n",
      "Gradient Descent(186/999): loss=0.10661539611677967\n",
      "Gradient Descent(187/999): loss=0.106578521583194\n",
      "Gradient Descent(188/999): loss=0.10654219018851507\n",
      "Gradient Descent(189/999): loss=0.10650639319113817\n",
      "Gradient Descent(190/999): loss=0.10647112199038457\n",
      "Gradient Descent(191/999): loss=0.10643636812422995\n",
      "Gradient Descent(192/999): loss=0.1064021232670689\n",
      "Gradient Descent(193/999): loss=0.10636837922751546\n",
      "Gradient Descent(194/999): loss=0.10633512794623938\n",
      "Gradient Descent(195/999): loss=0.10630236149383708\n",
      "Gradient Descent(196/999): loss=0.10627007206873701\n",
      "Gradient Descent(197/999): loss=0.10623825199513884\n",
      "Gradient Descent(198/999): loss=0.10620689372098552\n",
      "Gradient Descent(199/999): loss=0.10617598981596874\n",
      "Gradient Descent(200/999): loss=0.10614553296956558\n",
      "Gradient Descent(201/999): loss=0.1061155159891078\n",
      "Gradient Descent(202/999): loss=0.10608593179788163\n",
      "Gradient Descent(203/999): loss=0.10605677343325831\n",
      "Gradient Descent(204/999): loss=0.10602803404485485\n",
      "Gradient Descent(205/999): loss=0.10599970689272446\n",
      "Gradient Descent(206/999): loss=0.10597178534557614\n",
      "Gradient Descent(207/999): loss=0.10594426287902287\n",
      "Gradient Descent(208/999): loss=0.10591713307385806\n",
      "Gradient Descent(209/999): loss=0.10589038961436008\n",
      "Gradient Descent(210/999): loss=0.1058640262866236\n",
      "Gradient Descent(211/999): loss=0.10583803697691833\n",
      "Gradient Descent(212/999): loss=0.10581241567007395\n",
      "Gradient Descent(213/999): loss=0.10578715644789101\n",
      "Gradient Descent(214/999): loss=0.10576225348757784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(215/999): loss=0.10573770106021212\n",
      "Gradient Descent(216/999): loss=0.10571349352922774\n",
      "Gradient Descent(217/999): loss=0.10568962534892572\n",
      "Gradient Descent(218/999): loss=0.1056660910630093\n",
      "Gradient Descent(219/999): loss=0.10564288530314263\n",
      "Gradient Descent(220/999): loss=0.10562000278753261\n",
      "Gradient Descent(221/999): loss=0.10559743831953382\n",
      "Gradient Descent(222/999): loss=0.10557518678627552\n",
      "Gradient Descent(223/999): loss=0.10555324315731128\n",
      "Gradient Descent(224/999): loss=0.10553160248329004\n",
      "Gradient Descent(225/999): loss=0.10551025989464867\n",
      "Gradient Descent(226/999): loss=0.10548921060032576\n",
      "Gradient Descent(227/999): loss=0.10546844988649581\n",
      "Gradient Descent(228/999): loss=0.1054479731153244\n",
      "Gradient Descent(229/999): loss=0.10542777572374264\n",
      "Gradient Descent(230/999): loss=0.10540785322224212\n",
      "Gradient Descent(231/999): loss=0.10538820119368891\n",
      "Gradient Descent(232/999): loss=0.10536881529215666\n",
      "Gradient Descent(233/999): loss=0.10534969124177866\n",
      "Gradient Descent(234/999): loss=0.10533082483561836\n",
      "Gradient Descent(235/999): loss=0.10531221193455807\n",
      "Gradient Descent(236/999): loss=0.10529384846620558\n",
      "Gradient Descent(237/999): loss=0.10527573042381849\n",
      "Gradient Descent(238/999): loss=0.10525785386524565\n",
      "Gradient Descent(239/999): loss=0.1052402149118861\n",
      "Gradient Descent(240/999): loss=0.10522280974766421\n",
      "Gradient Descent(241/999): loss=0.10520563461802186\n",
      "Gradient Descent(242/999): loss=0.10518868582892654\n",
      "Gradient Descent(243/999): loss=0.1051719597458958\n",
      "Gradient Descent(244/999): loss=0.10515545279303683\n",
      "Gradient Descent(245/999): loss=0.10513916145210224\n",
      "Gradient Descent(246/999): loss=0.10512308226156038\n",
      "Gradient Descent(247/999): loss=0.10510721181568128\n",
      "Gradient Descent(248/999): loss=0.10509154676363681\n",
      "Gradient Descent(249/999): loss=0.1050760838086157\n",
      "Gradient Descent(250/999): loss=0.10506081970695252\n",
      "Gradient Descent(251/999): loss=0.10504575126727107\n",
      "Gradient Descent(252/999): loss=0.1050308753496413\n",
      "Gradient Descent(253/999): loss=0.10501618886474977\n",
      "Gradient Descent(254/999): loss=0.10500168877308376\n",
      "Gradient Descent(255/999): loss=0.10498737208412848\n",
      "Gradient Descent(256/999): loss=0.10497323585557687\n",
      "Gradient Descent(257/999): loss=0.10495927719255263\n",
      "Gradient Descent(258/999): loss=0.10494549324684543\n",
      "Gradient Descent(259/999): loss=0.10493188121615861\n",
      "Gradient Descent(260/999): loss=0.10491843834336896\n",
      "Gradient Descent(261/999): loss=0.10490516191579848\n",
      "Gradient Descent(262/999): loss=0.10489204926449766\n",
      "Gradient Descent(263/999): loss=0.10487909776354079\n",
      "Gradient Descent(264/999): loss=0.1048663048293321\n",
      "Gradient Descent(265/999): loss=0.10485366791992341\n",
      "Gradient Descent(266/999): loss=0.10484118453434273\n",
      "Gradient Descent(267/999): loss=0.10482885221193354\n",
      "Gradient Descent(268/999): loss=0.10481666853170493\n",
      "Gradient Descent(269/999): loss=0.10480463111169211\n",
      "Gradient Descent(270/999): loss=0.10479273760832711\n",
      "Gradient Descent(271/999): loss=0.10478098571582002\n",
      "Gradient Descent(272/999): loss=0.10476937316554957\n",
      "Gradient Descent(273/999): loss=0.10475789772546426\n",
      "Gradient Descent(274/999): loss=0.10474655719949257\n",
      "Gradient Descent(275/999): loss=0.1047353494269629\n",
      "Gradient Descent(276/999): loss=0.10472427228203303\n",
      "Gradient Descent(277/999): loss=0.10471332367312838\n",
      "Gradient Descent(278/999): loss=0.10470250154238978\n",
      "Gradient Descent(279/999): loss=0.10469180386512968\n",
      "Gradient Descent(280/999): loss=0.10468122864929755\n",
      "Gradient Descent(281/999): loss=0.10467077393495357\n",
      "Gradient Descent(282/999): loss=0.10466043779375112\n",
      "Gradient Descent(283/999): loss=0.10465021832842732\n",
      "Gradient Descent(284/999): loss=0.10464011367230179\n",
      "Gradient Descent(285/999): loss=0.10463012198878391\n",
      "Gradient Descent(286/999): loss=0.10462024147088746\n",
      "Gradient Descent(287/999): loss=0.10461047034075338\n",
      "Gradient Descent(288/999): loss=0.10460080684918033\n",
      "Gradient Descent(289/999): loss=0.10459124927516252\n",
      "Gradient Descent(290/999): loss=0.10458179592543518\n",
      "Gradient Descent(291/999): loss=0.10457244513402741\n",
      "Gradient Descent(292/999): loss=0.10456319526182209\n",
      "Gradient Descent(293/999): loss=0.10455404469612287\n",
      "Gradient Descent(294/999): loss=0.10454499185022834\n",
      "Gradient Descent(295/999): loss=0.10453603516301292\n",
      "Gradient Descent(296/999): loss=0.10452717309851446\n",
      "Gradient Descent(297/999): loss=0.10451840414552874\n",
      "Gradient Descent(298/999): loss=0.10450972681721003\n",
      "Gradient Descent(299/999): loss=0.10450113965067863\n",
      "Gradient Descent(300/999): loss=0.10449264120663446\n",
      "Gradient Descent(301/999): loss=0.10448423006897682\n",
      "Gradient Descent(302/999): loss=0.10447590484443044\n",
      "Gradient Descent(303/999): loss=0.10446766416217748\n",
      "Gradient Descent(304/999): loss=0.10445950667349549\n",
      "Gradient Descent(305/999): loss=0.10445143105140116\n",
      "Gradient Descent(306/999): loss=0.10444343599029982\n",
      "Gradient Descent(307/999): loss=0.10443552020564081\n",
      "Gradient Descent(308/999): loss=0.10442768243357786\n",
      "Gradient Descent(309/999): loss=0.10441992143063565\n",
      "Gradient Descent(310/999): loss=0.10441223597338121\n",
      "Gradient Descent(311/999): loss=0.10440462485810077\n",
      "Gradient Descent(312/999): loss=0.10439708690048194\n",
      "Gradient Descent(313/999): loss=0.10438962093530105\n",
      "Gradient Descent(314/999): loss=0.10438222581611503\n",
      "Gradient Descent(315/999): loss=0.10437490041495895\n",
      "Gradient Descent(316/999): loss=0.10436764362204808\n",
      "Gradient Descent(317/999): loss=0.10436045434548463\n",
      "Gradient Descent(318/999): loss=0.10435333151096969\n",
      "Gradient Descent(319/999): loss=0.10434627406151906\n",
      "Gradient Descent(320/999): loss=0.10433928095718449\n",
      "Gradient Descent(321/999): loss=0.10433235117477888\n",
      "Gradient Descent(322/999): loss=0.10432548370760607\n",
      "Gradient Descent(323/999): loss=0.10431867756519493\n",
      "Gradient Descent(324/999): loss=0.10431193177303792\n",
      "Gradient Descent(325/999): loss=0.10430524537233356\n",
      "Gradient Descent(326/999): loss=0.10429861741973333\n",
      "Gradient Descent(327/999): loss=0.10429204698709253\n",
      "Gradient Descent(328/999): loss=0.10428553316122512\n",
      "Gradient Descent(329/999): loss=0.10427907504366259\n",
      "Gradient Descent(330/999): loss=0.10427267175041675\n",
      "Gradient Descent(331/999): loss=0.10426632241174616\n",
      "Gradient Descent(332/999): loss=0.10426002617192645\n",
      "Gradient Descent(333/999): loss=0.10425378218902448\n",
      "Gradient Descent(334/999): loss=0.10424758963467595\n",
      "Gradient Descent(335/999): loss=0.10424144769386653\n",
      "Gradient Descent(336/999): loss=0.10423535556471675\n",
      "Gradient Descent(337/999): loss=0.10422931245827016\n",
      "Gradient Descent(338/999): loss=0.1042233175982852\n",
      "Gradient Descent(339/999): loss=0.10421737022102984\n",
      "Gradient Descent(340/999): loss=0.10421146957508023\n",
      "Gradient Descent(341/999): loss=0.10420561492112222\n",
      "Gradient Descent(342/999): loss=0.10419980553175594\n",
      "Gradient Descent(343/999): loss=0.10419404069130389\n",
      "Gradient Descent(344/999): loss=0.10418831969562199\n",
      "Gradient Descent(345/999): loss=0.10418264185191348\n",
      "Gradient Descent(346/999): loss=0.10417700647854608\n",
      "Gradient Descent(347/999): loss=0.10417141290487204\n",
      "Gradient Descent(348/999): loss=0.10416586047105093\n",
      "Gradient Descent(349/999): loss=0.10416034852787552\n",
      "Gradient Descent(350/999): loss=0.10415487643660029\n",
      "Gradient Descent(351/999): loss=0.10414944356877283\n",
      "Gradient Descent(352/999): loss=0.10414404930606776\n",
      "Gradient Descent(353/999): loss=0.10413869304012367\n",
      "Gradient Descent(354/999): loss=0.1041333741723823\n",
      "Gradient Descent(355/999): loss=0.10412809211393065\n",
      "Gradient Descent(356/999): loss=0.10412284628534527\n",
      "Gradient Descent(357/999): loss=0.10411763611653953\n",
      "Gradient Descent(358/999): loss=0.10411246104661291\n",
      "Gradient Descent(359/999): loss=0.10410732052370303\n",
      "Gradient Descent(360/999): loss=0.1041022140048398\n",
      "Gradient Descent(361/999): loss=0.10409714095580214\n",
      "Gradient Descent(362/999): loss=0.10409210085097698\n",
      "Gradient Descent(363/999): loss=0.10408709317322039\n",
      "Gradient Descent(364/999): loss=0.10408211741372106\n",
      "Gradient Descent(365/999): loss=0.10407717307186612\n",
      "Gradient Descent(366/999): loss=0.10407225965510862\n",
      "Gradient Descent(367/999): loss=0.10406737667883793\n",
      "Gradient Descent(368/999): loss=0.10406252366625139\n",
      "Gradient Descent(369/999): loss=0.1040577001482288\n",
      "Gradient Descent(370/999): loss=0.10405290566320823\n",
      "Gradient Descent(371/999): loss=0.10404813975706434\n",
      "Gradient Descent(372/999): loss=0.10404340198298852\n",
      "Gradient Descent(373/999): loss=0.10403869190137094\n",
      "Gradient Descent(374/999): loss=0.10403400907968426\n",
      "Gradient Descent(375/999): loss=0.10402935309236981\n",
      "Gradient Descent(376/999): loss=0.10402472352072491\n",
      "Gradient Descent(377/999): loss=0.10402011995279259\n",
      "Gradient Descent(378/999): loss=0.10401554198325275\n",
      "Gradient Descent(379/999): loss=0.10401098921331499\n",
      "Gradient Descent(380/999): loss=0.10400646125061369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(381/999): loss=0.10400195770910418\n",
      "Gradient Descent(382/999): loss=0.10399747820896088\n",
      "Gradient Descent(383/999): loss=0.103993022376477\n",
      "Gradient Descent(384/999): loss=0.10398858984396611\n",
      "Gradient Descent(385/999): loss=0.10398418024966476\n",
      "Gradient Descent(386/999): loss=0.10397979323763729\n",
      "Gradient Descent(387/999): loss=0.10397542845768164\n",
      "Gradient Descent(388/999): loss=0.10397108556523707\n",
      "Gradient Descent(389/999): loss=0.1039667642212932\n",
      "Gradient Descent(390/999): loss=0.10396246409230038\n",
      "Gradient Descent(391/999): loss=0.10395818485008188\n",
      "Gradient Descent(392/999): loss=0.10395392617174712\n",
      "Gradient Descent(393/999): loss=0.10394968773960643\n",
      "Gradient Descent(394/999): loss=0.10394546924108736\n",
      "Gradient Descent(395/999): loss=0.10394127036865197\n",
      "Gradient Descent(396/999): loss=0.10393709081971592\n",
      "Gradient Descent(397/999): loss=0.10393293029656832\n",
      "Gradient Descent(398/999): loss=0.10392878850629338\n",
      "Gradient Descent(399/999): loss=0.10392466516069306\n",
      "Gradient Descent(400/999): loss=0.10392055997621094\n",
      "Gradient Descent(401/999): loss=0.10391647267385751\n",
      "Gradient Descent(402/999): loss=0.10391240297913633\n",
      "Gradient Descent(403/999): loss=0.1039083506219719\n",
      "Gradient Descent(404/999): loss=0.10390431533663815\n",
      "Gradient Descent(405/999): loss=0.10390029686168839\n",
      "Gradient Descent(406/999): loss=0.10389629493988638\n",
      "Gradient Descent(407/999): loss=0.1038923093181383\n",
      "Gradient Descent(408/999): loss=0.10388833974742621\n",
      "Gradient Descent(409/999): loss=0.10388438598274208\n",
      "Gradient Descent(410/999): loss=0.10388044778302329\n",
      "Gradient Descent(411/999): loss=0.10387652491108902\n",
      "Gradient Descent(412/999): loss=0.10387261713357768\n",
      "Gradient Descent(413/999): loss=0.10386872422088521\n",
      "Gradient Descent(414/999): loss=0.10386484594710467\n",
      "Gradient Descent(415/999): loss=0.10386098208996654\n",
      "Gradient Descent(416/999): loss=0.10385713243078008\n",
      "Gradient Descent(417/999): loss=0.10385329675437575\n",
      "Gradient Descent(418/999): loss=0.10384947484904827\n",
      "Gradient Descent(419/999): loss=0.1038456665065009\n",
      "Gradient Descent(420/999): loss=0.10384187152179039\n",
      "Gradient Descent(421/999): loss=0.10383808969327309\n",
      "Gradient Descent(422/999): loss=0.10383432082255142\n",
      "Gradient Descent(423/999): loss=0.10383056471442202\n",
      "Gradient Descent(424/999): loss=0.10382682117682383\n",
      "Gradient Descent(425/999): loss=0.10382309002078766\n",
      "Gradient Descent(426/999): loss=0.10381937106038615\n",
      "Gradient Descent(427/999): loss=0.1038156641126851\n",
      "Gradient Descent(428/999): loss=0.10381196899769472\n",
      "Gradient Descent(429/999): loss=0.10380828553832261\n",
      "Gradient Descent(430/999): loss=0.1038046135603268\n",
      "Gradient Descent(431/999): loss=0.10380095289226983\n",
      "Gradient Descent(432/999): loss=0.10379730336547359\n",
      "Gradient Descent(433/999): loss=0.10379366481397476\n",
      "Gradient Descent(434/999): loss=0.10379003707448121\n",
      "Gradient Descent(435/999): loss=0.10378641998632868\n",
      "Gradient Descent(436/999): loss=0.10378281339143873\n",
      "Gradient Descent(437/999): loss=0.1037792171342767\n",
      "Gradient Descent(438/999): loss=0.10377563106181101\n",
      "Gradient Descent(439/999): loss=0.10377205502347257\n",
      "Gradient Descent(440/999): loss=0.10376848887111535\n",
      "Gradient Descent(441/999): loss=0.10376493245897704\n",
      "Gradient Descent(442/999): loss=0.10376138564364078\n",
      "Gradient Descent(443/999): loss=0.10375784828399723\n",
      "Gradient Descent(444/999): loss=0.1037543202412076\n",
      "Gradient Descent(445/999): loss=0.10375080137866684\n",
      "Gradient Descent(446/999): loss=0.10374729156196755\n",
      "Gradient Descent(447/999): loss=0.10374379065886492\n",
      "Gradient Descent(448/999): loss=0.10374029853924142\n",
      "Gradient Descent(449/999): loss=0.10373681507507289\n",
      "Gradient Descent(450/999): loss=0.10373334014039451\n",
      "Gradient Descent(451/999): loss=0.1037298736112678\n",
      "Gradient Descent(452/999): loss=0.10372641536574777\n",
      "Gradient Descent(453/999): loss=0.10372296528385096\n",
      "Gradient Descent(454/999): loss=0.10371952324752366\n",
      "Gradient Descent(455/999): loss=0.10371608914061105\n",
      "Gradient Descent(456/999): loss=0.10371266284882617\n",
      "Gradient Descent(457/999): loss=0.1037092442597203\n",
      "Gradient Descent(458/999): loss=0.10370583326265288\n",
      "Gradient Descent(459/999): loss=0.10370242974876265\n",
      "Gradient Descent(460/999): loss=0.10369903361093893\n",
      "Gradient Descent(461/999): loss=0.1036956447437933\n",
      "Gradient Descent(462/999): loss=0.10369226304363187\n",
      "Gradient Descent(463/999): loss=0.10368888840842794\n",
      "Gradient Descent(464/999): loss=0.10368552073779527\n",
      "Gradient Descent(465/999): loss=0.10368215993296137\n",
      "Gradient Descent(466/999): loss=0.10367880589674186\n",
      "Gradient Descent(467/999): loss=0.10367545853351443\n",
      "Gradient Descent(468/999): loss=0.10367211774919398\n",
      "Gradient Descent(469/999): loss=0.10366878345120774\n",
      "Gradient Descent(470/999): loss=0.10366545554847084\n",
      "Gradient Descent(471/999): loss=0.1036621339513624\n",
      "Gradient Descent(472/999): loss=0.1036588185717019\n",
      "Gradient Descent(473/999): loss=0.10365550932272592\n",
      "Gradient Descent(474/999): loss=0.10365220611906542\n",
      "Gradient Descent(475/999): loss=0.10364890887672308\n",
      "Gradient Descent(476/999): loss=0.10364561751305136\n",
      "Gradient Descent(477/999): loss=0.1036423319467306\n",
      "Gradient Descent(478/999): loss=0.10363905209774771\n",
      "Gradient Descent(479/999): loss=0.10363577788737512\n",
      "Gradient Descent(480/999): loss=0.1036325092381499\n",
      "Gradient Descent(481/999): loss=0.10362924607385357\n",
      "Gradient Descent(482/999): loss=0.10362598831949198\n",
      "Gradient Descent(483/999): loss=0.10362273590127542\n",
      "Gradient Descent(484/999): loss=0.10361948874659953\n",
      "Gradient Descent(485/999): loss=0.10361624678402574\n",
      "Gradient Descent(486/999): loss=0.10361300994326297\n",
      "Gradient Descent(487/999): loss=0.10360977815514877\n",
      "Gradient Descent(488/999): loss=0.10360655135163124\n",
      "Gradient Descent(489/999): loss=0.10360332946575117\n",
      "Gradient Descent(490/999): loss=0.1036001124316244\n",
      "Gradient Descent(491/999): loss=0.10359690018442445\n",
      "Gradient Descent(492/999): loss=0.10359369266036553\n",
      "Gradient Descent(493/999): loss=0.10359048979668575\n",
      "Gradient Descent(494/999): loss=0.10358729153163049\n",
      "Gradient Descent(495/999): loss=0.10358409780443645\n",
      "Gradient Descent(496/999): loss=0.10358090855531531\n",
      "Gradient Descent(497/999): loss=0.10357772372543837\n",
      "Gradient Descent(498/999): loss=0.10357454325692075\n",
      "Gradient Descent(499/999): loss=0.10357136709280648\n",
      "Gradient Descent(500/999): loss=0.1035681951770532\n",
      "Gradient Descent(501/999): loss=0.10356502745451775\n",
      "Gradient Descent(502/999): loss=0.10356186387094149\n",
      "Gradient Descent(503/999): loss=0.10355870437293614\n",
      "Gradient Descent(504/999): loss=0.10355554890796953\n",
      "Gradient Descent(505/999): loss=0.1035523974243522\n",
      "Gradient Descent(506/999): loss=0.10354924987122344\n",
      "Gradient Descent(507/999): loss=0.10354610619853803\n",
      "Gradient Descent(508/999): loss=0.1035429663570533\n",
      "Gradient Descent(509/999): loss=0.1035398302983159\n",
      "Gradient Descent(510/999): loss=0.10353669797464922\n",
      "Gradient Descent(511/999): loss=0.1035335693391409\n",
      "Gradient Descent(512/999): loss=0.10353044434563045\n",
      "Gradient Descent(513/999): loss=0.10352732294869714\n",
      "Gradient Descent(514/999): loss=0.10352420510364806\n",
      "Gradient Descent(515/999): loss=0.10352109076650647\n",
      "Gradient Descent(516/999): loss=0.10351797989400013\n",
      "Gradient Descent(517/999): loss=0.10351487244355008\n",
      "Gradient Descent(518/999): loss=0.1035117683732593\n",
      "Gradient Descent(519/999): loss=0.10350866764190192\n",
      "Gradient Descent(520/999): loss=0.1035055702089122\n",
      "Gradient Descent(521/999): loss=0.10350247603437406\n",
      "Gradient Descent(522/999): loss=0.10349938507901044\n",
      "Gradient Descent(523/999): loss=0.10349629730417319\n",
      "Gradient Descent(524/999): loss=0.10349321267183278\n",
      "Gradient Descent(525/999): loss=0.10349013114456837\n",
      "Gradient Descent(526/999): loss=0.10348705268555804\n",
      "Gradient Descent(527/999): loss=0.1034839772585691\n",
      "Gradient Descent(528/999): loss=0.10348090482794856\n",
      "Gradient Descent(529/999): loss=0.10347783535861385\n",
      "Gradient Descent(530/999): loss=0.10347476881604364\n",
      "Gradient Descent(531/999): loss=0.10347170516626872\n",
      "Gradient Descent(532/999): loss=0.10346864437586312\n",
      "Gradient Descent(533/999): loss=0.10346558641193532\n",
      "Gradient Descent(534/999): loss=0.10346253124211977\n",
      "Gradient Descent(535/999): loss=0.1034594788345683\n",
      "Gradient Descent(536/999): loss=0.10345642915794183\n",
      "Gradient Descent(537/999): loss=0.10345338218140203\n",
      "Gradient Descent(538/999): loss=0.10345033787460352\n",
      "Gradient Descent(539/999): loss=0.1034472962076856\n",
      "Gradient Descent(540/999): loss=0.10344425715126465\n",
      "Gradient Descent(541/999): loss=0.10344122067642651\n",
      "Gradient Descent(542/999): loss=0.10343818675471868\n",
      "Gradient Descent(543/999): loss=0.10343515535814295\n",
      "Gradient Descent(544/999): loss=0.10343212645914822\n",
      "Gradient Descent(545/999): loss=0.10342910003062313\n",
      "Gradient Descent(546/999): loss=0.10342607604588906\n",
      "Gradient Descent(547/999): loss=0.10342305447869317\n",
      "Gradient Descent(548/999): loss=0.10342003530320139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(549/999): loss=0.10341701849399187\n",
      "Gradient Descent(550/999): loss=0.10341400402604826\n",
      "Gradient Descent(551/999): loss=0.1034109918747531\n",
      "Gradient Descent(552/999): loss=0.1034079820158814\n",
      "Gradient Descent(553/999): loss=0.1034049744255945\n",
      "Gradient Descent(554/999): loss=0.10340196908043361\n",
      "Gradient Descent(555/999): loss=0.10339896595731385\n",
      "Gradient Descent(556/999): loss=0.10339596503351808\n",
      "Gradient Descent(557/999): loss=0.10339296628669108\n",
      "Gradient Descent(558/999): loss=0.10338996969483373\n",
      "Gradient Descent(559/999): loss=0.10338697523629718\n",
      "Gradient Descent(560/999): loss=0.10338398288977727\n",
      "Gradient Descent(561/999): loss=0.10338099263430897\n",
      "Gradient Descent(562/999): loss=0.10337800444926086\n",
      "Gradient Descent(563/999): loss=0.10337501831432988\n",
      "Gradient Descent(564/999): loss=0.10337203420953583\n",
      "Gradient Descent(565/999): loss=0.10336905211521641\n",
      "Gradient Descent(566/999): loss=0.10336607201202194\n",
      "Gradient Descent(567/999): loss=0.10336309388091025\n",
      "Gradient Descent(568/999): loss=0.10336011770314205\n",
      "Gradient Descent(569/999): loss=0.1033571434602756\n",
      "Gradient Descent(570/999): loss=0.10335417113416233\n",
      "Gradient Descent(571/999): loss=0.10335120070694177\n",
      "Gradient Descent(572/999): loss=0.10334823216103724\n",
      "Gradient Descent(573/999): loss=0.10334526547915093\n",
      "Gradient Descent(574/999): loss=0.10334230064425975\n",
      "Gradient Descent(575/999): loss=0.10333933763961059\n",
      "Gradient Descent(576/999): loss=0.10333637644871613\n",
      "Gradient Descent(577/999): loss=0.10333341705535067\n",
      "Gradient Descent(578/999): loss=0.10333045944354566\n",
      "Gradient Descent(579/999): loss=0.10332750359758565\n",
      "Gradient Descent(580/999): loss=0.10332454950200433\n",
      "Gradient Descent(581/999): loss=0.10332159714158042\n",
      "Gradient Descent(582/999): loss=0.10331864650133366\n",
      "Gradient Descent(583/999): loss=0.10331569756652102\n",
      "Gradient Descent(584/999): loss=0.1033127503226329\n",
      "Gradient Descent(585/999): loss=0.10330980475538923\n",
      "Gradient Descent(586/999): loss=0.10330686085073591\n",
      "Gradient Descent(587/999): loss=0.10330391859484112\n",
      "Gradient Descent(588/999): loss=0.10330097797409174\n",
      "Gradient Descent(589/999): loss=0.10329803897508973\n",
      "Gradient Descent(590/999): loss=0.1032951015846489\n",
      "Gradient Descent(591/999): loss=0.10329216578979125\n",
      "Gradient Descent(592/999): loss=0.10328923157774375\n",
      "Gradient Descent(593/999): loss=0.10328629893593504\n",
      "Gradient Descent(594/999): loss=0.10328336785199205\n",
      "Gradient Descent(595/999): loss=0.10328043831373694\n",
      "Gradient Descent(596/999): loss=0.10327751030918399\n",
      "Gradient Descent(597/999): loss=0.10327458382653629\n",
      "Gradient Descent(598/999): loss=0.10327165885418295\n",
      "Gradient Descent(599/999): loss=0.10326873538069599\n",
      "Gradient Descent(600/999): loss=0.10326581339482725\n",
      "Gradient Descent(601/999): loss=0.10326289288550575\n",
      "Gradient Descent(602/999): loss=0.10325997384183475\n",
      "Gradient Descent(603/999): loss=0.10325705625308883\n",
      "Gradient Descent(604/999): loss=0.10325414010871121\n",
      "Gradient Descent(605/999): loss=0.10325122539831112\n",
      "Gradient Descent(606/999): loss=0.10324831211166094\n",
      "Gradient Descent(607/999): loss=0.10324540023869372\n",
      "Gradient Descent(608/999): loss=0.1032424897695005\n",
      "Gradient Descent(609/999): loss=0.10323958069432787\n",
      "Gradient Descent(610/999): loss=0.10323667300357539\n",
      "Gradient Descent(611/999): loss=0.10323376668779305\n",
      "Gradient Descent(612/999): loss=0.10323086173767904\n",
      "Gradient Descent(613/999): loss=0.1032279581440772\n",
      "Gradient Descent(614/999): loss=0.10322505589797472\n",
      "Gradient Descent(615/999): loss=0.10322215499049986\n",
      "Gradient Descent(616/999): loss=0.10321925541291967\n",
      "Gradient Descent(617/999): loss=0.10321635715663766\n",
      "Gradient Descent(618/999): loss=0.1032134602131918\n",
      "Gradient Descent(619/999): loss=0.10321056457425218\n",
      "Gradient Descent(620/999): loss=0.10320767023161885\n",
      "Gradient Descent(621/999): loss=0.10320477717721999\n",
      "Gradient Descent(622/999): loss=0.10320188540310948\n",
      "Gradient Descent(623/999): loss=0.10319899490146525\n",
      "Gradient Descent(624/999): loss=0.10319610566458697\n",
      "Gradient Descent(625/999): loss=0.10319321768489419\n",
      "Gradient Descent(626/999): loss=0.10319033095492457\n",
      "Gradient Descent(627/999): loss=0.10318744546733176\n",
      "Gradient Descent(628/999): loss=0.10318456121488352\n",
      "Gradient Descent(629/999): loss=0.10318167819046017\n",
      "Gradient Descent(630/999): loss=0.10317879638705245\n",
      "Gradient Descent(631/999): loss=0.10317591579775989\n",
      "Gradient Descent(632/999): loss=0.10317303641578908\n",
      "Gradient Descent(633/999): loss=0.10317015823445189\n",
      "Gradient Descent(634/999): loss=0.10316728124716375\n",
      "Gradient Descent(635/999): loss=0.10316440544744208\n",
      "Gradient Descent(636/999): loss=0.10316153082890456\n",
      "Gradient Descent(637/999): loss=0.10315865738526754\n",
      "Gradient Descent(638/999): loss=0.1031557851103445\n",
      "Gradient Descent(639/999): loss=0.10315291399804441\n",
      "Gradient Descent(640/999): loss=0.10315004404237019\n",
      "Gradient Descent(641/999): loss=0.10314717523741727\n",
      "Gradient Descent(642/999): loss=0.10314430757737211\n",
      "Gradient Descent(643/999): loss=0.10314144105651059\n",
      "Gradient Descent(644/999): loss=0.10313857566919668\n",
      "Gradient Descent(645/999): loss=0.10313571140988112\n",
      "Gradient Descent(646/999): loss=0.10313284827309976\n",
      "Gradient Descent(647/999): loss=0.10312998625347243\n",
      "Gradient Descent(648/999): loss=0.10312712534570144\n",
      "Gradient Descent(649/999): loss=0.1031242655445704\n",
      "Gradient Descent(650/999): loss=0.10312140684494271\n",
      "Gradient Descent(651/999): loss=0.10311854924176043\n",
      "Gradient Descent(652/999): loss=0.10311569273004302\n",
      "Gradient Descent(653/999): loss=0.10311283730488596\n",
      "Gradient Descent(654/999): loss=0.10310998296145964\n",
      "Gradient Descent(655/999): loss=0.10310712969500814\n",
      "Gradient Descent(656/999): loss=0.10310427750084801\n",
      "Gradient Descent(657/999): loss=0.1031014263743671\n",
      "Gradient Descent(658/999): loss=0.10309857631102347\n",
      "Gradient Descent(659/999): loss=0.10309572730634423\n",
      "Gradient Descent(660/999): loss=0.10309287935592432\n",
      "Gradient Descent(661/999): loss=0.10309003245542565\n",
      "Gradient Descent(662/999): loss=0.1030871866005758\n",
      "Gradient Descent(663/999): loss=0.10308434178716708\n",
      "Gradient Descent(664/999): loss=0.10308149801105543\n",
      "Gradient Descent(665/999): loss=0.10307865526815949\n",
      "Gradient Descent(666/999): loss=0.10307581355445951\n",
      "Gradient Descent(667/999): loss=0.10307297286599633\n",
      "Gradient Descent(668/999): loss=0.10307013319887044\n",
      "Gradient Descent(669/999): loss=0.1030672945492411\n",
      "Gradient Descent(670/999): loss=0.10306445691332533\n",
      "Gradient Descent(671/999): loss=0.10306162028739684\n",
      "Gradient Descent(672/999): loss=0.1030587846677854\n",
      "Gradient Descent(673/999): loss=0.10305595005087574\n",
      "Gradient Descent(674/999): loss=0.10305311643310665\n",
      "Gradient Descent(675/999): loss=0.10305028381097033\n",
      "Gradient Descent(676/999): loss=0.10304745218101116\n",
      "Gradient Descent(677/999): loss=0.10304462153982531\n",
      "Gradient Descent(678/999): loss=0.1030417918840595\n",
      "Gradient Descent(679/999): loss=0.10303896321041042\n",
      "Gradient Descent(680/999): loss=0.10303613551562382\n",
      "Gradient Descent(681/999): loss=0.10303330879649392\n",
      "Gradient Descent(682/999): loss=0.10303048304986222\n",
      "Gradient Descent(683/999): loss=0.10302765827261724\n",
      "Gradient Descent(684/999): loss=0.1030248344616934\n",
      "Gradient Descent(685/999): loss=0.1030220116140704\n",
      "Gradient Descent(686/999): loss=0.1030191897267725\n",
      "Gradient Descent(687/999): loss=0.10301636879686785\n",
      "Gradient Descent(688/999): loss=0.10301354882146763\n",
      "Gradient Descent(689/999): loss=0.10301072979772553\n",
      "Gradient Descent(690/999): loss=0.10300791172283695\n",
      "Gradient Descent(691/999): loss=0.10300509459403848\n",
      "Gradient Descent(692/999): loss=0.103002278408607\n",
      "Gradient Descent(693/999): loss=0.10299946316385918\n",
      "Gradient Descent(694/999): loss=0.10299664885715085\n",
      "Gradient Descent(695/999): loss=0.10299383548587637\n",
      "Gradient Descent(696/999): loss=0.10299102304746796\n",
      "Gradient Descent(697/999): loss=0.1029882115393951\n",
      "Gradient Descent(698/999): loss=0.10298540095916395\n",
      "Gradient Descent(699/999): loss=0.10298259130431675\n",
      "Gradient Descent(700/999): loss=0.10297978257243129\n",
      "Gradient Descent(701/999): loss=0.10297697476112022\n",
      "Gradient Descent(702/999): loss=0.10297416786803062\n",
      "Gradient Descent(703/999): loss=0.10297136189084333\n",
      "Gradient Descent(704/999): loss=0.1029685568272726\n",
      "Gradient Descent(705/999): loss=0.10296575267506523\n",
      "Gradient Descent(706/999): loss=0.10296294943200039\n",
      "Gradient Descent(707/999): loss=0.10296014709588885\n",
      "Gradient Descent(708/999): loss=0.10295734566457261\n",
      "Gradient Descent(709/999): loss=0.10295454513592438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(710/999): loss=0.10295174550784704\n",
      "Gradient Descent(711/999): loss=0.10294894677827311\n",
      "Gradient Descent(712/999): loss=0.10294614894516449\n",
      "Gradient Descent(713/999): loss=0.10294335200651172\n",
      "Gradient Descent(714/999): loss=0.10294055596033365\n",
      "Gradient Descent(715/999): loss=0.10293776080467698\n",
      "Gradient Descent(716/999): loss=0.10293496653761593\n",
      "Gradient Descent(717/999): loss=0.10293217315725148\n",
      "Gradient Descent(718/999): loss=0.10292938066171123\n",
      "Gradient Descent(719/999): loss=0.10292658904914892\n",
      "Gradient Descent(720/999): loss=0.10292379831774387\n",
      "Gradient Descent(721/999): loss=0.10292100846570074\n",
      "Gradient Descent(722/999): loss=0.10291821949124902\n",
      "Gradient Descent(723/999): loss=0.1029154313926427\n",
      "Gradient Descent(724/999): loss=0.10291264416815973\n",
      "Gradient Descent(725/999): loss=0.10290985781610192\n",
      "Gradient Descent(726/999): loss=0.10290707233479424\n",
      "Gradient Descent(727/999): loss=0.10290428772258457\n",
      "Gradient Descent(728/999): loss=0.10290150397784345\n",
      "Gradient Descent(729/999): loss=0.10289872109896361\n",
      "Gradient Descent(730/999): loss=0.1028959390843595\n",
      "Gradient Descent(731/999): loss=0.10289315793246721\n",
      "Gradient Descent(732/999): loss=0.10289037764174389\n",
      "Gradient Descent(733/999): loss=0.10288759821066752\n",
      "Gradient Descent(734/999): loss=0.10288481963773656\n",
      "Gradient Descent(735/999): loss=0.1028820419214695\n",
      "Gradient Descent(736/999): loss=0.10287926506040491\n",
      "Gradient Descent(737/999): loss=0.1028764890531006\n",
      "Gradient Descent(738/999): loss=0.10287371389813371\n",
      "Gradient Descent(739/999): loss=0.1028709395941002\n",
      "Gradient Descent(740/999): loss=0.10286816613961466\n",
      "Gradient Descent(741/999): loss=0.10286539353330985\n",
      "Gradient Descent(742/999): loss=0.10286262177383676\n",
      "Gradient Descent(743/999): loss=0.10285985085986375\n",
      "Gradient Descent(744/999): loss=0.1028570807900769\n",
      "Gradient Descent(745/999): loss=0.10285431156317919\n",
      "Gradient Descent(746/999): loss=0.10285154317789062\n",
      "Gradient Descent(747/999): loss=0.10284877563294771\n",
      "Gradient Descent(748/999): loss=0.10284600892710334\n",
      "Gradient Descent(749/999): loss=0.10284324305912643\n",
      "Gradient Descent(750/999): loss=0.10284047802780165\n",
      "Gradient Descent(751/999): loss=0.10283771383192933\n",
      "Gradient Descent(752/999): loss=0.10283495047032506\n",
      "Gradient Descent(753/999): loss=0.10283218794181945\n",
      "Gradient Descent(754/999): loss=0.10282942624525798\n",
      "Gradient Descent(755/999): loss=0.10282666537950072\n",
      "Gradient Descent(756/999): loss=0.10282390534342199\n",
      "Gradient Descent(757/999): loss=0.10282114613591034\n",
      "Gradient Descent(758/999): loss=0.10281838775586813\n",
      "Gradient Descent(759/999): loss=0.10281563020221149\n",
      "Gradient Descent(760/999): loss=0.10281287347386985\n",
      "Gradient Descent(761/999): loss=0.102810117569786\n",
      "Gradient Descent(762/999): loss=0.10280736248891566\n",
      "Gradient Descent(763/999): loss=0.10280460823022744\n",
      "Gradient Descent(764/999): loss=0.10280185479270255\n",
      "Gradient Descent(765/999): loss=0.10279910217533453\n",
      "Gradient Descent(766/999): loss=0.10279635037712923\n",
      "Gradient Descent(767/999): loss=0.10279359939710446\n",
      "Gradient Descent(768/999): loss=0.10279084923428981\n",
      "Gradient Descent(769/999): loss=0.1027880998877266\n",
      "Gradient Descent(770/999): loss=0.10278535135646752\n",
      "Gradient Descent(771/999): loss=0.10278260363957654\n",
      "Gradient Descent(772/999): loss=0.10277985673612876\n",
      "Gradient Descent(773/999): loss=0.10277711064521008\n",
      "Gradient Descent(774/999): loss=0.10277436536591728\n",
      "Gradient Descent(775/999): loss=0.10277162089735752\n",
      "Gradient Descent(776/999): loss=0.1027688772386485\n",
      "Gradient Descent(777/999): loss=0.1027661343889181\n",
      "Gradient Descent(778/999): loss=0.10276339234730424\n",
      "Gradient Descent(779/999): loss=0.10276065111295472\n",
      "Gradient Descent(780/999): loss=0.10275791068502718\n",
      "Gradient Descent(781/999): loss=0.10275517106268872\n",
      "Gradient Descent(782/999): loss=0.10275243224511602\n",
      "Gradient Descent(783/999): loss=0.10274969423149485\n",
      "Gradient Descent(784/999): loss=0.10274695702102032\n",
      "Gradient Descent(785/999): loss=0.10274422061289641\n",
      "Gradient Descent(786/999): loss=0.10274148500633595\n",
      "Gradient Descent(787/999): loss=0.1027387502005605\n",
      "Gradient Descent(788/999): loss=0.10273601619480026\n",
      "Gradient Descent(789/999): loss=0.10273328298829368\n",
      "Gradient Descent(790/999): loss=0.10273055058028772\n",
      "Gradient Descent(791/999): loss=0.10272781897003722\n",
      "Gradient Descent(792/999): loss=0.10272508815680537\n",
      "Gradient Descent(793/999): loss=0.1027223581398631\n",
      "Gradient Descent(794/999): loss=0.10271962891848913\n",
      "Gradient Descent(795/999): loss=0.10271690049196983\n",
      "Gradient Descent(796/999): loss=0.10271417285959908\n",
      "Gradient Descent(797/999): loss=0.10271144602067829\n",
      "Gradient Descent(798/999): loss=0.10270871997451603\n",
      "Gradient Descent(799/999): loss=0.10270599472042813\n",
      "Gradient Descent(800/999): loss=0.10270327025773747\n",
      "Gradient Descent(801/999): loss=0.10270054658577381\n",
      "Gradient Descent(802/999): loss=0.10269782370387388\n",
      "Gradient Descent(803/999): loss=0.10269510161138105\n",
      "Gradient Descent(804/999): loss=0.10269238030764535\n",
      "Gradient Descent(805/999): loss=0.10268965979202332\n",
      "Gradient Descent(806/999): loss=0.10268694006387791\n",
      "Gradient Descent(807/999): loss=0.10268422112257843\n",
      "Gradient Descent(808/999): loss=0.10268150296750028\n",
      "Gradient Descent(809/999): loss=0.10267878559802518\n",
      "Gradient Descent(810/999): loss=0.10267606901354069\n",
      "Gradient Descent(811/999): loss=0.10267335321344037\n",
      "Gradient Descent(812/999): loss=0.10267063819712362\n",
      "Gradient Descent(813/999): loss=0.10266792396399552\n",
      "Gradient Descent(814/999): loss=0.10266521051346682\n",
      "Gradient Descent(815/999): loss=0.1026624978449539\n",
      "Gradient Descent(816/999): loss=0.10265978595787846\n",
      "Gradient Descent(817/999): loss=0.10265707485166774\n",
      "Gradient Descent(818/999): loss=0.10265436452575412\n",
      "Gradient Descent(819/999): loss=0.10265165497957536\n",
      "Gradient Descent(820/999): loss=0.1026489462125742\n",
      "Gradient Descent(821/999): loss=0.10264623822419855\n",
      "Gradient Descent(822/999): loss=0.10264353101390117\n",
      "Gradient Descent(823/999): loss=0.10264082458113984\n",
      "Gradient Descent(824/999): loss=0.10263811892537703\n",
      "Gradient Descent(825/999): loss=0.10263541404607997\n",
      "Gradient Descent(826/999): loss=0.10263270994272064\n",
      "Gradient Descent(827/999): loss=0.10263000661477553\n",
      "Gradient Descent(828/999): loss=0.1026273040617257\n",
      "Gradient Descent(829/999): loss=0.10262460228305655\n",
      "Gradient Descent(830/999): loss=0.10262190127825797\n",
      "Gradient Descent(831/999): loss=0.10261920104682411\n",
      "Gradient Descent(832/999): loss=0.10261650158825332\n",
      "Gradient Descent(833/999): loss=0.10261380290204819\n",
      "Gradient Descent(834/999): loss=0.10261110498771535\n",
      "Gradient Descent(835/999): loss=0.10260840784476553\n",
      "Gradient Descent(836/999): loss=0.10260571147271337\n",
      "Gradient Descent(837/999): loss=0.10260301587107748\n",
      "Gradient Descent(838/999): loss=0.10260032103938029\n",
      "Gradient Descent(839/999): loss=0.10259762697714804\n",
      "Gradient Descent(840/999): loss=0.10259493368391069\n",
      "Gradient Descent(841/999): loss=0.10259224115920185\n",
      "Gradient Descent(842/999): loss=0.10258954940255882\n",
      "Gradient Descent(843/999): loss=0.10258685841352233\n",
      "Gradient Descent(844/999): loss=0.1025841681916368\n",
      "Gradient Descent(845/999): loss=0.1025814787364499\n",
      "Gradient Descent(846/999): loss=0.10257879004751279\n",
      "Gradient Descent(847/999): loss=0.10257610212438002\n",
      "Gradient Descent(848/999): loss=0.10257341496660931\n",
      "Gradient Descent(849/999): loss=0.10257072857376162\n",
      "Gradient Descent(850/999): loss=0.1025680429454013\n",
      "Gradient Descent(851/999): loss=0.10256535808109558\n",
      "Gradient Descent(852/999): loss=0.10256267398041488\n",
      "Gradient Descent(853/999): loss=0.1025599906429327\n",
      "Gradient Descent(854/999): loss=0.10255730806822545\n",
      "Gradient Descent(855/999): loss=0.1025546262558726\n",
      "Gradient Descent(856/999): loss=0.10255194520545632\n",
      "Gradient Descent(857/999): loss=0.10254926491656184\n",
      "Gradient Descent(858/999): loss=0.10254658538877708\n",
      "Gradient Descent(859/999): loss=0.10254390662169273\n",
      "Gradient Descent(860/999): loss=0.1025412286149023\n",
      "Gradient Descent(861/999): loss=0.10253855136800183\n",
      "Gradient Descent(862/999): loss=0.10253587488059014\n",
      "Gradient Descent(863/999): loss=0.10253319915226848\n",
      "Gradient Descent(864/999): loss=0.10253052418264082\n",
      "Gradient Descent(865/999): loss=0.1025278499713136\n",
      "Gradient Descent(866/999): loss=0.10252517651789564\n",
      "Gradient Descent(867/999): loss=0.10252250382199835\n",
      "Gradient Descent(868/999): loss=0.1025198318832354\n",
      "Gradient Descent(869/999): loss=0.1025171607012229\n",
      "Gradient Descent(870/999): loss=0.10251449027557928\n",
      "Gradient Descent(871/999): loss=0.10251182060592524\n",
      "Gradient Descent(872/999): loss=0.10250915169188374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(873/999): loss=0.10250648353308002\n",
      "Gradient Descent(874/999): loss=0.10250381612914136\n",
      "Gradient Descent(875/999): loss=0.10250114947969738\n",
      "Gradient Descent(876/999): loss=0.10249848358437962\n",
      "Gradient Descent(877/999): loss=0.10249581844282189\n",
      "Gradient Descent(878/999): loss=0.10249315405465992\n",
      "Gradient Descent(879/999): loss=0.10249049041953155\n",
      "Gradient Descent(880/999): loss=0.10248782753707651\n",
      "Gradient Descent(881/999): loss=0.10248516540693664\n",
      "Gradient Descent(882/999): loss=0.10248250402875553\n",
      "Gradient Descent(883/999): loss=0.1024798434021788\n",
      "Gradient Descent(884/999): loss=0.10247718352685391\n",
      "Gradient Descent(885/999): loss=0.1024745244024302\n",
      "Gradient Descent(886/999): loss=0.10247186602855868\n",
      "Gradient Descent(887/999): loss=0.10246920840489228\n",
      "Gradient Descent(888/999): loss=0.10246655153108575\n",
      "Gradient Descent(889/999): loss=0.1024638954067953\n",
      "Gradient Descent(890/999): loss=0.10246124003167918\n",
      "Gradient Descent(891/999): loss=0.10245858540539703\n",
      "Gradient Descent(892/999): loss=0.10245593152761036\n",
      "Gradient Descent(893/999): loss=0.10245327839798221\n",
      "Gradient Descent(894/999): loss=0.10245062601617722\n",
      "Gradient Descent(895/999): loss=0.10244797438186161\n",
      "Gradient Descent(896/999): loss=0.10244532349470319\n",
      "Gradient Descent(897/999): loss=0.10244267335437127\n",
      "Gradient Descent(898/999): loss=0.10244002396053668\n",
      "Gradient Descent(899/999): loss=0.10243737531287175\n",
      "Gradient Descent(900/999): loss=0.10243472741105024\n",
      "Gradient Descent(901/999): loss=0.10243208025474736\n",
      "Gradient Descent(902/999): loss=0.10242943384363978\n",
      "Gradient Descent(903/999): loss=0.10242678817740551\n",
      "Gradient Descent(904/999): loss=0.10242414325572394\n",
      "Gradient Descent(905/999): loss=0.10242149907827586\n",
      "Gradient Descent(906/999): loss=0.10241885564474336\n",
      "Gradient Descent(907/999): loss=0.10241621295480986\n",
      "Gradient Descent(908/999): loss=0.10241357100816008\n",
      "Gradient Descent(909/999): loss=0.10241092980447998\n",
      "Gradient Descent(910/999): loss=0.10240828934345682\n",
      "Gradient Descent(911/999): loss=0.10240564962477908\n",
      "Gradient Descent(912/999): loss=0.10240301064813642\n",
      "Gradient Descent(913/999): loss=0.10240037241321984\n",
      "Gradient Descent(914/999): loss=0.10239773491972129\n",
      "Gradient Descent(915/999): loss=0.1023950981673341\n",
      "Gradient Descent(916/999): loss=0.10239246215575265\n",
      "Gradient Descent(917/999): loss=0.10238982688467244\n",
      "Gradient Descent(918/999): loss=0.10238719235379014\n",
      "Gradient Descent(919/999): loss=0.10238455856280351\n",
      "Gradient Descent(920/999): loss=0.10238192551141129\n",
      "Gradient Descent(921/999): loss=0.10237929319931345\n",
      "Gradient Descent(922/999): loss=0.10237666162621084\n",
      "Gradient Descent(923/999): loss=0.10237403079180551\n",
      "Gradient Descent(924/999): loss=0.10237140069580039\n",
      "Gradient Descent(925/999): loss=0.10236877133789948\n",
      "Gradient Descent(926/999): loss=0.10236614271780772\n",
      "Gradient Descent(927/999): loss=0.10236351483523116\n",
      "Gradient Descent(928/999): loss=0.10236088768987663\n",
      "Gradient Descent(929/999): loss=0.10235826128145203\n",
      "Gradient Descent(930/999): loss=0.10235563560966608\n",
      "Gradient Descent(931/999): loss=0.10235301067422858\n",
      "Gradient Descent(932/999): loss=0.10235038647485004\n",
      "Gradient Descent(933/999): loss=0.10234776301124207\n",
      "Gradient Descent(934/999): loss=0.10234514028311699\n",
      "Gradient Descent(935/999): loss=0.10234251829018803\n",
      "Gradient Descent(936/999): loss=0.10233989703216935\n",
      "Gradient Descent(937/999): loss=0.10233727650877587\n",
      "Gradient Descent(938/999): loss=0.10233465671972335\n",
      "Gradient Descent(939/999): loss=0.10233203766472838\n",
      "Gradient Descent(940/999): loss=0.10232941934350837\n",
      "Gradient Descent(941/999): loss=0.10232680175578149\n",
      "Gradient Descent(942/999): loss=0.10232418490126673\n",
      "Gradient Descent(943/999): loss=0.10232156877968379\n",
      "Gradient Descent(944/999): loss=0.10231895339075321\n",
      "Gradient Descent(945/999): loss=0.10231633873419624\n",
      "Gradient Descent(946/999): loss=0.10231372480973483\n",
      "Gradient Descent(947/999): loss=0.10231111161709172\n",
      "Gradient Descent(948/999): loss=0.10230849915599034\n",
      "Gradient Descent(949/999): loss=0.10230588742615486\n",
      "Gradient Descent(950/999): loss=0.10230327642731005\n",
      "Gradient Descent(951/999): loss=0.10230066615918153\n",
      "Gradient Descent(952/999): loss=0.10229805662149538\n",
      "Gradient Descent(953/999): loss=0.10229544781397859\n",
      "Gradient Descent(954/999): loss=0.1022928397363586\n",
      "Gradient Descent(955/999): loss=0.10229023238836361\n",
      "Gradient Descent(956/999): loss=0.1022876257697225\n",
      "Gradient Descent(957/999): loss=0.10228501988016464\n",
      "Gradient Descent(958/999): loss=0.10228241471942018\n",
      "Gradient Descent(959/999): loss=0.10227981028721973\n",
      "Gradient Descent(960/999): loss=0.10227720658329464\n",
      "Gradient Descent(961/999): loss=0.10227460360737675\n",
      "Gradient Descent(962/999): loss=0.10227200135919859\n",
      "Gradient Descent(963/999): loss=0.1022693998384932\n",
      "Gradient Descent(964/999): loss=0.10226679904499421\n",
      "Gradient Descent(965/999): loss=0.1022641989784358\n",
      "Gradient Descent(966/999): loss=0.10226159963855277\n",
      "Gradient Descent(967/999): loss=0.10225900102508036\n",
      "Gradient Descent(968/999): loss=0.10225640313775447\n",
      "Gradient Descent(969/999): loss=0.10225380597631148\n",
      "Gradient Descent(970/999): loss=0.10225120954048826\n",
      "Gradient Descent(971/999): loss=0.10224861383002229\n",
      "Gradient Descent(972/999): loss=0.10224601884465145\n",
      "Gradient Descent(973/999): loss=0.10224342458411423\n",
      "Gradient Descent(974/999): loss=0.10224083104814957\n",
      "Gradient Descent(975/999): loss=0.10223823823649693\n",
      "Gradient Descent(976/999): loss=0.10223564614889621\n",
      "Gradient Descent(977/999): loss=0.10223305478508782\n",
      "Gradient Descent(978/999): loss=0.10223046414481265\n",
      "Gradient Descent(979/999): loss=0.10222787422781206\n",
      "Gradient Descent(980/999): loss=0.10222528503382783\n",
      "Gradient Descent(981/999): loss=0.10222269656260222\n",
      "Gradient Descent(982/999): loss=0.10222010881387798\n",
      "Gradient Descent(983/999): loss=0.10221752178739824\n",
      "Gradient Descent(984/999): loss=0.10221493548290658\n",
      "Gradient Descent(985/999): loss=0.10221234990014708\n",
      "Gradient Descent(986/999): loss=0.10220976503886414\n",
      "Gradient Descent(987/999): loss=0.10220718089880268\n",
      "Gradient Descent(988/999): loss=0.10220459747970792\n",
      "Gradient Descent(989/999): loss=0.1022020147813256\n",
      "Gradient Descent(990/999): loss=0.10219943280340181\n",
      "Gradient Descent(991/999): loss=0.10219685154568306\n",
      "Gradient Descent(992/999): loss=0.10219427100791627\n",
      "Gradient Descent(993/999): loss=0.10219169118984865\n",
      "Gradient Descent(994/999): loss=0.10218911209122795\n",
      "Gradient Descent(995/999): loss=0.10218653371180215\n",
      "Gradient Descent(996/999): loss=0.10218395605131973\n",
      "Gradient Descent(997/999): loss=0.10218137910952946\n",
      "Gradient Descent(998/999): loss=0.10217880288618048\n",
      "Gradient Descent(999/999): loss=0.10217622738102235\n",
      "Gradient Descent(0/999): loss=0.1474419593440508\n",
      "Gradient Descent(1/999): loss=0.14675243087194023\n",
      "Gradient Descent(2/999): loss=0.14607397366219915\n",
      "Gradient Descent(3/999): loss=0.1454064090911196\n",
      "Gradient Descent(4/999): loss=0.1447495614171568\n",
      "Gradient Descent(5/999): loss=0.1441032577344238\n",
      "Gradient Descent(6/999): loss=0.14346732792693725\n",
      "Gradient Descent(7/999): loss=0.14284160462360107\n",
      "Gradient Descent(8/999): loss=0.14222592315391644\n",
      "Gradient Descent(9/999): loss=0.14162012150440667\n",
      "Gradient Descent(10/999): loss=0.14102404027574508\n",
      "Gradient Descent(11/999): loss=0.14043752264057438\n",
      "Gradient Descent(12/999): loss=0.1398604143020069\n",
      "Gradient Descent(13/999): loss=0.13929256345279475\n",
      "Gradient Descent(14/999): loss=0.13873382073515814\n",
      "Gradient Descent(15/999): loss=0.138184039201262\n",
      "Gradient Descent(16/999): loss=0.137643074274331\n",
      "Gradient Descent(17/999): loss=0.1371107837103913\n",
      "Gradient Descent(18/999): loss=0.13658702756062954\n",
      "Gradient Descent(19/999): loss=0.13607166813436\n",
      "Gradient Descent(20/999): loss=0.13556456996258825\n",
      "Gradient Descent(21/999): loss=0.13506559976216373\n",
      "Gradient Descent(22/999): loss=0.1345746264005097\n",
      "Gradient Descent(23/999): loss=0.1340915208609234\n",
      "Gradient Descent(24/999): loss=0.13361615620843506\n",
      "Gradient Descent(25/999): loss=0.13314840755621848\n",
      "Gradient Descent(26/999): loss=0.13268815203254353\n",
      "Gradient Descent(27/999): loss=0.13223526874826144\n",
      "Gradient Descent(28/999): loss=0.1317896387648152\n",
      "Gradient Descent(29/999): loss=0.13135114506276588\n",
      "Gradient Descent(30/999): loss=0.13091967251082687\n",
      "Gradient Descent(31/999): loss=0.13049510783539783\n",
      "Gradient Descent(32/999): loss=0.13007733959059004\n",
      "Gradient Descent(33/999): loss=0.1296662581287355\n",
      "Gradient Descent(34/999): loss=0.12926175557137212\n",
      "Gradient Descent(35/999): loss=0.12886372578069646\n",
      "Gradient Descent(36/999): loss=0.1284720643314778\n",
      "Gradient Descent(37/999): loss=0.12808666848342493\n",
      "Gradient Descent(38/999): loss=0.12770743715399893\n",
      "Gradient Descent(39/999): loss=0.12733427089166469\n",
      "Gradient Descent(40/999): loss=0.1269670718495741\n",
      "Gradient Descent(41/999): loss=0.12660574375967332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(42/999): loss=0.12625019190722897\n",
      "Gradient Descent(43/999): loss=0.12590032310576402\n",
      "Gradient Descent(44/999): loss=0.12555604567239909\n",
      "Gradient Descent(45/999): loss=0.12521726940359104\n",
      "Gradient Descent(46/999): loss=0.12488390555126362\n",
      "Gradient Descent(47/999): loss=0.12455586679932253\n",
      "Gradient Descent(48/999): loss=0.1242330672405499\n",
      "Gradient Descent(49/999): loss=0.12391542235387172\n",
      "Gradient Descent(50/999): loss=0.12360284898199142\n",
      "Gradient Descent(51/999): loss=0.12329526530938512\n",
      "Gradient Descent(52/999): loss=0.12299259084065135\n",
      "Gradient Descent(53/999): loss=0.12269474637921045\n",
      "Gradient Descent(54/999): loss=0.12240165400634741\n",
      "Gradient Descent(55/999): loss=0.12211323706059309\n",
      "Gradient Descent(56/999): loss=0.121829420117438\n",
      "Gradient Descent(57/999): loss=0.12155012896937352\n",
      "Gradient Descent(58/999): loss=0.12127529060625492\n",
      "Gradient Descent(59/999): loss=0.12100483319598149\n",
      "Gradient Descent(60/999): loss=0.12073868606548842\n",
      "Gradient Descent(61/999): loss=0.120476779682045\n",
      "Gradient Descent(62/999): loss=0.12021904563485504\n",
      "Gradient Descent(63/999): loss=0.11996541661695413\n",
      "Gradient Descent(64/999): loss=0.11971582640739849\n",
      "Gradient Descent(65/999): loss=0.11947020985374185\n",
      "Gradient Descent(66/999): loss=0.1192285028547947\n",
      "Gradient Descent(67/999): loss=0.11899064234366187\n",
      "Gradient Descent(68/999): loss=0.11875656627105372\n",
      "Gradient Descent(69/999): loss=0.11852621358886663\n",
      "Gradient Descent(70/999): loss=0.11829952423402858\n",
      "Gradient Descent(71/999): loss=0.11807643911260499\n",
      "Gradient Descent(72/999): loss=0.11785690008416161\n",
      "Gradient Descent(73/999): loss=0.11764084994637916\n",
      "Gradient Descent(74/999): loss=0.11742823241991657\n",
      "Gradient Descent(75/999): loss=0.1172189921335182\n",
      "Gradient Descent(76/999): loss=0.11701307460936171\n",
      "Gradient Descent(77/999): loss=0.11681042624864203\n",
      "Gradient Descent(78/999): loss=0.11661099431738803\n",
      "Gradient Descent(79/999): loss=0.11641472693250805\n",
      "Gradient Descent(80/999): loss=0.11622157304806077\n",
      "Gradient Descent(81/999): loss=0.11603148244174719\n",
      "Gradient Descent(82/999): loss=0.115844405701621\n",
      "Gradient Descent(83/999): loss=0.11566029421301317\n",
      "Gradient Descent(84/999): loss=0.11547910014566755\n",
      "Gradient Descent(85/999): loss=0.11530077644108394\n",
      "Gradient Descent(86/999): loss=0.11512527680006561\n",
      "Gradient Descent(87/999): loss=0.11495255567046765\n",
      "Gradient Descent(88/999): loss=0.11478256823514274\n",
      "Gradient Descent(89/999): loss=0.11461527040008207\n",
      "Gradient Descent(90/999): loss=0.11445061878274673\n",
      "Gradient Descent(91/999): loss=0.11428857070058823\n",
      "Gradient Descent(92/999): loss=0.11412908415975366\n",
      "Gradient Descent(93/999): loss=0.11397211784397325\n",
      "Gradient Descent(94/999): loss=0.11381763110362754\n",
      "Gradient Descent(95/999): loss=0.11366558394499053\n",
      "Gradient Descent(96/999): loss=0.11351593701964649\n",
      "Gradient Descent(97/999): loss=0.1133686516140777\n",
      "Gradient Descent(98/999): loss=0.11322368963942026\n",
      "Gradient Descent(99/999): loss=0.11308101362138473\n",
      "Gradient Descent(100/999): loss=0.11294058669033993\n",
      "Gradient Descent(101/999): loss=0.11280237257155656\n",
      "Gradient Descent(102/999): loss=0.11266633557560796\n",
      "Gradient Descent(103/999): loss=0.11253244058892631\n",
      "Gradient Descent(104/999): loss=0.11240065306451079\n",
      "Gradient Descent(105/999): loss=0.11227093901278562\n",
      "Gradient Descent(106/999): loss=0.11214326499260605\n",
      "Gradient Descent(107/999): loss=0.11201759810240906\n",
      "Gradient Descent(108/999): loss=0.11189390597150702\n",
      "Gradient Descent(109/999): loss=0.11177215675152166\n",
      "Gradient Descent(110/999): loss=0.11165231910795663\n",
      "Gradient Descent(111/999): loss=0.1115343622119057\n",
      "Gradient Descent(112/999): loss=0.11141825573189465\n",
      "Gradient Descent(113/999): loss=0.11130396982585541\n",
      "Gradient Descent(114/999): loss=0.11119147513322897\n",
      "Gradient Descent(115/999): loss=0.111080742767196\n",
      "Gradient Descent(116/999): loss=0.11097174430703319\n",
      "Gradient Descent(117/999): loss=0.11086445179059227\n",
      "Gradient Descent(118/999): loss=0.11075883770690087\n",
      "Gradient Descent(119/999): loss=0.11065487498888264\n",
      "Gradient Descent(120/999): loss=0.11055253700619459\n",
      "Gradient Descent(121/999): loss=0.1104517975581805\n",
      "Gradient Descent(122/999): loss=0.11035263086693758\n",
      "Gradient Descent(123/999): loss=0.11025501157049518\n",
      "Gradient Descent(124/999): loss=0.11015891471610359\n",
      "Gradient Descent(125/999): loss=0.11006431575363108\n",
      "Gradient Descent(126/999): loss=0.10997119052906747\n",
      "Gradient Descent(127/999): loss=0.10987951527813264\n",
      "Gradient Descent(128/999): loss=0.10978926661998785\n",
      "Gradient Descent(129/999): loss=0.10970042155104907\n",
      "Gradient Descent(130/999): loss=0.10961295743889955\n",
      "Gradient Descent(131/999): loss=0.109526852016301\n",
      "Gradient Descent(132/999): loss=0.10944208337530129\n",
      "Gradient Descent(133/999): loss=0.10935862996143722\n",
      "Gradient Descent(134/999): loss=0.10927647056803076\n",
      "Gradient Descent(135/999): loss=0.10919558433057745\n",
      "Gradient Descent(136/999): loss=0.10911595072122533\n",
      "Gradient Descent(137/999): loss=0.10903754954334281\n",
      "Gradient Descent(138/999): loss=0.10896036092617441\n",
      "Gradient Descent(139/999): loss=0.10888436531958243\n",
      "Gradient Descent(140/999): loss=0.108809543488874\n",
      "Gradient Descent(141/999): loss=0.10873587650971071\n",
      "Gradient Descent(142/999): loss=0.10866334576310153\n",
      "Gradient Descent(143/999): loss=0.10859193293047537\n",
      "Gradient Descent(144/999): loss=0.10852161998883403\n",
      "Gradient Descent(145/999): loss=0.10845238920598287\n",
      "Gradient Descent(146/999): loss=0.10838422313583855\n",
      "Gradient Descent(147/999): loss=0.10831710461381254\n",
      "Gradient Descent(148/999): loss=0.10825101675226917\n",
      "Gradient Descent(149/999): loss=0.10818594293605677\n",
      "Gradient Descent(150/999): loss=0.10812186681811102\n",
      "Gradient Descent(151/999): loss=0.10805877231512945\n",
      "Gradient Descent(152/999): loss=0.10799664360331512\n",
      "Gradient Descent(153/999): loss=0.1079354651141899\n",
      "Gradient Descent(154/999): loss=0.1078752215304744\n",
      "Gradient Descent(155/999): loss=0.10781589778203499\n",
      "Gradient Descent(156/999): loss=0.10775747904189587\n",
      "Gradient Descent(157/999): loss=0.1076999507223158\n",
      "Gradient Descent(158/999): loss=0.1076432984709277\n",
      "Gradient Descent(159/999): loss=0.10758750816694103\n",
      "Gradient Descent(160/999): loss=0.10753256591740508\n",
      "Gradient Descent(161/999): loss=0.10747845805353265\n",
      "Gradient Descent(162/999): loss=0.1074251711270832\n",
      "Gradient Descent(163/999): loss=0.10737269190680415\n",
      "Gradient Descent(164/999): loss=0.10732100737492972\n",
      "Gradient Descent(165/999): loss=0.10727010472373626\n",
      "Gradient Descent(166/999): loss=0.10721997135215303\n",
      "Gradient Descent(167/999): loss=0.10717059486242793\n",
      "Gradient Descent(168/999): loss=0.10712196305684668\n",
      "Gradient Descent(169/999): loss=0.10707406393450537\n",
      "Gradient Descent(170/999): loss=0.10702688568813462\n",
      "Gradient Descent(171/999): loss=0.10698041670097529\n",
      "Gradient Descent(172/999): loss=0.10693464554370452\n",
      "Gradient Descent(173/999): loss=0.10688956097141146\n",
      "Gradient Descent(174/999): loss=0.10684515192062155\n",
      "Gradient Descent(175/999): loss=0.106801407506369\n",
      "Gradient Descent(176/999): loss=0.10675831701931664\n",
      "Gradient Descent(177/999): loss=0.1067158699229219\n",
      "Gradient Descent(178/999): loss=0.1066740558506487\n",
      "Gradient Descent(179/999): loss=0.10663286460322448\n",
      "Gradient Descent(180/999): loss=0.1065922861459412\n",
      "Gradient Descent(181/999): loss=0.10655231060600012\n",
      "Gradient Descent(182/999): loss=0.10651292826989919\n",
      "Gradient Descent(183/999): loss=0.10647412958086286\n",
      "Gradient Descent(184/999): loss=0.1064359051363132\n",
      "Gradient Descent(185/999): loss=0.10639824568538184\n",
      "Gradient Descent(186/999): loss=0.1063611421264621\n",
      "Gradient Descent(187/999): loss=0.10632458550480056\n",
      "Gradient Descent(188/999): loss=0.10628856701012752\n",
      "Gradient Descent(189/999): loss=0.10625307797432577\n",
      "Gradient Descent(190/999): loss=0.10621810986913684\n",
      "Gradient Descent(191/999): loss=0.10618365430390432\n",
      "Gradient Descent(192/999): loss=0.10614970302335358\n",
      "Gradient Descent(193/999): loss=0.10611624790540741\n",
      "Gradient Descent(194/999): loss=0.10608328095903674\n",
      "Gradient Descent(195/999): loss=0.1060507943221462\n",
      "Gradient Descent(196/999): loss=0.10601878025949361\n",
      "Gradient Descent(197/999): loss=0.10598723116064328\n",
      "Gradient Descent(198/999): loss=0.10595613953795213\n",
      "Gradient Descent(199/999): loss=0.10592549802458846\n",
      "Gradient Descent(200/999): loss=0.1058952993725825\n",
      "Gradient Descent(201/999): loss=0.10586553645090856\n",
      "Gradient Descent(202/999): loss=0.10583620224359817\n",
      "Gradient Descent(203/999): loss=0.10580728984788344\n",
      "Gradient Descent(204/999): loss=0.1057787924723707\n",
      "Gradient Descent(205/999): loss=0.10575070343524333\n",
      "Gradient Descent(206/999): loss=0.10572301616249365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(207/999): loss=0.1056957241861834\n",
      "Gradient Descent(208/999): loss=0.10566882114273246\n",
      "Gradient Descent(209/999): loss=0.10564230077123464\n",
      "Gradient Descent(210/999): loss=0.1056161569118013\n",
      "Gradient Descent(211/999): loss=0.10559038350393125\n",
      "Gradient Descent(212/999): loss=0.1055649745849071\n",
      "Gradient Descent(213/999): loss=0.10553992428821744\n",
      "Gradient Descent(214/999): loss=0.10551522684200476\n",
      "Gradient Descent(215/999): loss=0.10549087656753778\n",
      "Gradient Descent(216/999): loss=0.10546686787770916\n",
      "Gradient Descent(217/999): loss=0.10544319527555697\n",
      "Gradient Descent(218/999): loss=0.10541985335281008\n",
      "Gradient Descent(219/999): loss=0.10539683678845728\n",
      "Gradient Descent(220/999): loss=0.10537414034733919\n",
      "Gradient Descent(221/999): loss=0.105351758878763\n",
      "Gradient Descent(222/999): loss=0.10532968731513966\n",
      "Gradient Descent(223/999): loss=0.10530792067064283\n",
      "Gradient Descent(224/999): loss=0.10528645403988972\n",
      "Gradient Descent(225/999): loss=0.10526528259664313\n",
      "Gradient Descent(226/999): loss=0.10524440159253423\n",
      "Gradient Descent(227/999): loss=0.1052238063558063\n",
      "Gradient Descent(228/999): loss=0.10520349229007854\n",
      "Gradient Descent(229/999): loss=0.10518345487312968\n",
      "Gradient Descent(230/999): loss=0.10516368965570158\n",
      "Gradient Descent(231/999): loss=0.10514419226032191\n",
      "Gradient Descent(232/999): loss=0.10512495838014577\n",
      "Gradient Descent(233/999): loss=0.10510598377781624\n",
      "Gradient Descent(234/999): loss=0.1050872642843429\n",
      "Gradient Descent(235/999): loss=0.10506879579799905\n",
      "Gradient Descent(236/999): loss=0.10505057428323603\n",
      "Gradient Descent(237/999): loss=0.10503259576961578\n",
      "Gradient Descent(238/999): loss=0.10501485635075995\n",
      "Gradient Descent(239/999): loss=0.10499735218331642\n",
      "Gradient Descent(240/999): loss=0.10498007948594229\n",
      "Gradient Descent(241/999): loss=0.10496303453830343\n",
      "Gradient Descent(242/999): loss=0.10494621368008991\n",
      "Gradient Descent(243/999): loss=0.10492961331004773\n",
      "Gradient Descent(244/999): loss=0.10491322988502566\n",
      "Gradient Descent(245/999): loss=0.10489705991903789\n",
      "Gradient Descent(246/999): loss=0.10488109998234166\n",
      "Gradient Descent(247/999): loss=0.10486534670052969\n",
      "Gradient Descent(248/999): loss=0.1048497967536373\n",
      "Gradient Descent(249/999): loss=0.10483444687526408\n",
      "Gradient Descent(250/999): loss=0.1048192938517094\n",
      "Gradient Descent(251/999): loss=0.10480433452112237\n",
      "Gradient Descent(252/999): loss=0.10478956577266492\n",
      "Gradient Descent(253/999): loss=0.10477498454568891\n",
      "Gradient Descent(254/999): loss=0.10476058782892621\n",
      "Gradient Descent(255/999): loss=0.1047463726596919\n",
      "Gradient Descent(256/999): loss=0.10473233612310046\n",
      "Gradient Descent(257/999): loss=0.10471847535129436\n",
      "Gradient Descent(258/999): loss=0.10470478752268547\n",
      "Gradient Descent(259/999): loss=0.10469126986120826\n",
      "Gradient Descent(260/999): loss=0.1046779196355854\n",
      "Gradient Descent(261/999): loss=0.10466473415860503\n",
      "Gradient Descent(262/999): loss=0.10465171078640984\n",
      "Gradient Descent(263/999): loss=0.10463884691779735\n",
      "Gradient Descent(264/999): loss=0.10462613999353186\n",
      "Gradient Descent(265/999): loss=0.10461358749566718\n",
      "Gradient Descent(266/999): loss=0.10460118694688046\n",
      "Gradient Descent(267/999): loss=0.10458893590981663\n",
      "Gradient Descent(268/999): loss=0.10457683198644363\n",
      "Gradient Descent(269/999): loss=0.10456487281741782\n",
      "Gradient Descent(270/999): loss=0.1045530560814598\n",
      "Gradient Descent(271/999): loss=0.10454137949474022\n",
      "Gradient Descent(272/999): loss=0.1045298408102754\n",
      "Gradient Descent(273/999): loss=0.10451843781733298\n",
      "Gradient Descent(274/999): loss=0.10450716834084685\n",
      "Gradient Descent(275/999): loss=0.10449603024084174\n",
      "Gradient Descent(276/999): loss=0.10448502141186701\n",
      "Gradient Descent(277/999): loss=0.10447413978243962\n",
      "Gradient Descent(278/999): loss=0.10446338331449588\n",
      "Gradient Descent(279/999): loss=0.10445275000285242\n",
      "Gradient Descent(280/999): loss=0.10444223787467556\n",
      "Gradient Descent(281/999): loss=0.10443184498895933\n",
      "Gradient Descent(282/999): loss=0.10442156943601197\n",
      "Gradient Descent(283/999): loss=0.1044114093369506\n",
      "Gradient Descent(284/999): loss=0.10440136284320413\n",
      "Gradient Descent(285/999): loss=0.1043914281360242\n",
      "Gradient Descent(286/999): loss=0.10438160342600399\n",
      "Gradient Descent(287/999): loss=0.10437188695260467\n",
      "Gradient Descent(288/999): loss=0.10436227698368974\n",
      "Gradient Descent(289/999): loss=0.10435277181506666\n",
      "Gradient Descent(290/999): loss=0.10434336977003605\n",
      "Gradient Descent(291/999): loss=0.10433406919894798\n",
      "Gradient Descent(292/999): loss=0.10432486847876557\n",
      "Gradient Descent(293/999): loss=0.10431576601263559\n",
      "Gradient Descent(294/999): loss=0.10430676022946593\n",
      "Gradient Descent(295/999): loss=0.10429784958351\n",
      "Gradient Descent(296/999): loss=0.10428903255395774\n",
      "Gradient Descent(297/999): loss=0.10428030764453322\n",
      "Gradient Descent(298/999): loss=0.10427167338309883\n",
      "Gradient Descent(299/999): loss=0.10426312832126582\n",
      "Gradient Descent(300/999): loss=0.104254671034011\n",
      "Gradient Descent(301/999): loss=0.10424630011929983\n",
      "Gradient Descent(302/999): loss=0.10423801419771546\n",
      "Gradient Descent(303/999): loss=0.10422981191209378\n",
      "Gradient Descent(304/999): loss=0.10422169192716432\n",
      "Gradient Descent(305/999): loss=0.10421365292919711\n",
      "Gradient Descent(306/999): loss=0.104205693625655\n",
      "Gradient Descent(307/999): loss=0.10419781274485175\n",
      "Gradient Descent(308/999): loss=0.10419000903561558\n",
      "Gradient Descent(309/999): loss=0.10418228126695818\n",
      "Gradient Descent(310/999): loss=0.10417462822774905\n",
      "Gradient Descent(311/999): loss=0.10416704872639508\n",
      "Gradient Descent(312/999): loss=0.1041595415905253\n",
      "Gradient Descent(313/999): loss=0.1041521056666807\n",
      "Gradient Descent(314/999): loss=0.10414473982000913\n",
      "Gradient Descent(315/999): loss=0.1041374429339651\n",
      "Gradient Descent(316/999): loss=0.10413021391001423\n",
      "Gradient Descent(317/999): loss=0.10412305166734281\n",
      "Gradient Descent(318/999): loss=0.1041159551425719\n",
      "Gradient Descent(319/999): loss=0.10410892328947574\n",
      "Gradient Descent(320/999): loss=0.10410195507870534\n",
      "Gradient Descent(321/999): loss=0.10409504949751591\n",
      "Gradient Descent(322/999): loss=0.10408820554949902\n",
      "Gradient Descent(323/999): loss=0.10408142225431907\n",
      "Gradient Descent(324/999): loss=0.10407469864745385\n",
      "Gradient Descent(325/999): loss=0.10406803377993946\n",
      "Gradient Descent(326/999): loss=0.10406142671811924\n",
      "Gradient Descent(327/999): loss=0.10405487654339678\n",
      "Gradient Descent(328/999): loss=0.10404838235199292\n",
      "Gradient Descent(329/999): loss=0.10404194325470671\n",
      "Gradient Descent(330/999): loss=0.10403555837668013\n",
      "Gradient Descent(331/999): loss=0.10402922685716666\n",
      "Gradient Descent(332/999): loss=0.10402294784930367\n",
      "Gradient Descent(333/999): loss=0.10401672051988832\n",
      "Gradient Descent(334/999): loss=0.10401054404915715\n",
      "Gradient Descent(335/999): loss=0.10400441763056929\n",
      "Gradient Descent(336/999): loss=0.10399834047059307\n",
      "Gradient Descent(337/999): loss=0.10399231178849609\n",
      "Gradient Descent(338/999): loss=0.10398633081613883\n",
      "Gradient Descent(339/999): loss=0.10398039679777123\n",
      "Gradient Descent(340/999): loss=0.103974508989833\n",
      "Gradient Descent(341/999): loss=0.10396866666075685\n",
      "Gradient Descent(342/999): loss=0.10396286909077493\n",
      "Gradient Descent(343/999): loss=0.10395711557172856\n",
      "Gradient Descent(344/999): loss=0.10395140540688079\n",
      "Gradient Descent(345/999): loss=0.10394573791073217\n",
      "Gradient Descent(346/999): loss=0.10394011240883948\n",
      "Gradient Descent(347/999): loss=0.10393452823763719\n",
      "Gradient Descent(348/999): loss=0.10392898474426211\n",
      "Gradient Descent(349/999): loss=0.10392348128638057\n",
      "Gradient Descent(350/999): loss=0.10391801723201867\n",
      "Gradient Descent(351/999): loss=0.10391259195939492\n",
      "Gradient Descent(352/999): loss=0.103907204856756\n",
      "Gradient Descent(353/999): loss=0.10390185532221477\n",
      "Gradient Descent(354/999): loss=0.10389654276359124\n",
      "Gradient Descent(355/999): loss=0.1038912665982558\n",
      "Gradient Descent(356/999): loss=0.10388602625297519\n",
      "Gradient Descent(357/999): loss=0.10388082116376092\n",
      "Gradient Descent(358/999): loss=0.10387565077572002\n",
      "Gradient Descent(359/999): loss=0.1038705145429084\n",
      "Gradient Descent(360/999): loss=0.10386541192818628\n",
      "Gradient Descent(361/999): loss=0.10386034240307626\n",
      "Gradient Descent(362/999): loss=0.10385530544762366\n",
      "Gradient Descent(363/999): loss=0.10385030055025868\n",
      "Gradient Descent(364/999): loss=0.1038453272076614\n",
      "Gradient Descent(365/999): loss=0.10384038492462858\n",
      "Gradient Descent(366/999): loss=0.10383547321394267\n",
      "Gradient Descent(367/999): loss=0.10383059159624289\n",
      "Gradient Descent(368/999): loss=0.10382573959989866\n",
      "Gradient Descent(369/999): loss=0.10382091676088467\n",
      "Gradient Descent(370/999): loss=0.10381612262265826\n",
      "Gradient Descent(371/999): loss=0.10381135673603872\n",
      "Gradient Descent(372/999): loss=0.10380661865908844\n",
      "Gradient Descent(373/999): loss=0.10380190795699602\n",
      "Gradient Descent(374/999): loss=0.1037972242019614\n",
      "Gradient Descent(375/999): loss=0.10379256697308255\n",
      "Gradient Descent(376/999): loss=0.10378793585624443\n",
      "Gradient Descent(377/999): loss=0.10378333044400924\n",
      "Gradient Descent(378/999): loss=0.1037787503355087\n",
      "Gradient Descent(379/999): loss=0.10377419513633829\n",
      "Gradient Descent(380/999): loss=0.10376966445845258\n",
      "Gradient Descent(381/999): loss=0.10376515792006308\n",
      "Gradient Descent(382/999): loss=0.10376067514553686\n",
      "Gradient Descent(383/999): loss=0.10375621576529745\n",
      "Gradient Descent(384/999): loss=0.10375177941572704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(385/999): loss=0.10374736573907042\n",
      "Gradient Descent(386/999): loss=0.10374297438334035\n",
      "Gradient Descent(387/999): loss=0.10373860500222447\n",
      "Gradient Descent(388/999): loss=0.1037342572549936\n",
      "Gradient Descent(389/999): loss=0.10372993080641206\n",
      "Gradient Descent(390/999): loss=0.10372562532664857\n",
      "Gradient Descent(391/999): loss=0.10372134049118933\n",
      "Gradient Descent(392/999): loss=0.10371707598075211\n",
      "Gradient Descent(393/999): loss=0.1037128314812019\n",
      "Gradient Descent(394/999): loss=0.10370860668346786\n",
      "Gradient Descent(395/999): loss=0.1037044012834615\n",
      "Gradient Descent(396/999): loss=0.10370021498199637\n",
      "Gradient Descent(397/999): loss=0.1036960474847091\n",
      "Gradient Descent(398/999): loss=0.10369189850198127\n",
      "Gradient Descent(399/999): loss=0.10368776774886315\n",
      "Gradient Descent(400/999): loss=0.10368365494499816\n",
      "Gradient Descent(401/999): loss=0.10367955981454888\n",
      "Gradient Descent(402/999): loss=0.10367548208612412\n",
      "Gradient Descent(403/999): loss=0.10367142149270711\n",
      "Gradient Descent(404/999): loss=0.10366737777158497\n",
      "Gradient Descent(405/999): loss=0.1036633506642794\n",
      "Gradient Descent(406/999): loss=0.10365933991647797\n",
      "Gradient Descent(407/999): loss=0.10365534527796733\n",
      "Gradient Descent(408/999): loss=0.10365136650256677\n",
      "Gradient Descent(409/999): loss=0.10364740334806351\n",
      "Gradient Descent(410/999): loss=0.10364345557614814\n",
      "Gradient Descent(411/999): loss=0.10363952295235217\n",
      "Gradient Descent(412/999): loss=0.10363560524598568\n",
      "Gradient Descent(413/999): loss=0.1036317022300766\n",
      "Gradient Descent(414/999): loss=0.10362781368131062\n",
      "Gradient Descent(415/999): loss=0.1036239393799721\n",
      "Gradient Descent(416/999): loss=0.1036200791098862\n",
      "Gradient Descent(417/999): loss=0.1036162326583615\n",
      "Gradient Descent(418/999): loss=0.10361239981613406\n",
      "Gradient Descent(419/999): loss=0.10360858037731188\n",
      "Gradient Descent(420/999): loss=0.10360477413932068\n",
      "Gradient Descent(421/999): loss=0.10360098090285022\n",
      "Gradient Descent(422/999): loss=0.10359720047180174\n",
      "Gradient Descent(423/999): loss=0.10359343265323619\n",
      "Gradient Descent(424/999): loss=0.10358967725732292\n",
      "Gradient Descent(425/999): loss=0.10358593409729006\n",
      "Gradient Descent(426/999): loss=0.10358220298937472\n",
      "Gradient Descent(427/999): loss=0.10357848375277463\n",
      "Gradient Descent(428/999): loss=0.1035747762096004\n",
      "Gradient Descent(429/999): loss=0.10357108018482833\n",
      "Gradient Descent(430/999): loss=0.1035673955062544\n",
      "Gradient Descent(431/999): loss=0.10356372200444872\n",
      "Gradient Descent(432/999): loss=0.10356005951271062\n",
      "Gradient Descent(433/999): loss=0.10355640786702477\n",
      "Gradient Descent(434/999): loss=0.10355276690601786\n",
      "Gradient Descent(435/999): loss=0.10354913647091589\n",
      "Gradient Descent(436/999): loss=0.10354551640550222\n",
      "Gradient Descent(437/999): loss=0.10354190655607631\n",
      "Gradient Descent(438/999): loss=0.10353830677141335\n",
      "Gradient Descent(439/999): loss=0.10353471690272392\n",
      "Gradient Descent(440/999): loss=0.10353113680361496\n",
      "Gradient Descent(441/999): loss=0.10352756633005114\n",
      "Gradient Descent(442/999): loss=0.10352400534031665\n",
      "Gradient Descent(443/999): loss=0.10352045369497781\n",
      "Gradient Descent(444/999): loss=0.1035169112568464\n",
      "Gradient Descent(445/999): loss=0.10351337789094317\n",
      "Gradient Descent(446/999): loss=0.1035098534644625\n",
      "Gradient Descent(447/999): loss=0.1035063378467371\n",
      "Gradient Descent(448/999): loss=0.1035028309092036\n",
      "Gradient Descent(449/999): loss=0.10349933252536861\n",
      "Gradient Descent(450/999): loss=0.10349584257077524\n",
      "Gradient Descent(451/999): loss=0.10349236092297039\n",
      "Gradient Descent(452/999): loss=0.10348888746147232\n",
      "Gradient Descent(453/999): loss=0.10348542206773881\n",
      "Gradient Descent(454/999): loss=0.10348196462513592\n",
      "Gradient Descent(455/999): loss=0.10347851501890724\n",
      "Gradient Descent(456/999): loss=0.10347507313614353\n",
      "Gradient Descent(457/999): loss=0.10347163886575293\n",
      "Gradient Descent(458/999): loss=0.10346821209843167\n",
      "Gradient Descent(459/999): loss=0.10346479272663515\n",
      "Gradient Descent(460/999): loss=0.10346138064454966\n",
      "Gradient Descent(461/999): loss=0.10345797574806438\n",
      "Gradient Descent(462/999): loss=0.1034545779347439\n",
      "Gradient Descent(463/999): loss=0.10345118710380126\n",
      "Gradient Descent(464/999): loss=0.1034478031560712\n",
      "Gradient Descent(465/999): loss=0.10344442599398423\n",
      "Gradient Descent(466/999): loss=0.10344105552154066\n",
      "Gradient Descent(467/999): loss=0.10343769164428543\n",
      "Gradient Descent(468/999): loss=0.10343433426928302\n",
      "Gradient Descent(469/999): loss=0.10343098330509318\n",
      "Gradient Descent(470/999): loss=0.10342763866174656\n",
      "Gradient Descent(471/999): loss=0.10342430025072102\n",
      "Gradient Descent(472/999): loss=0.10342096798491854\n",
      "Gradient Descent(473/999): loss=0.10341764177864188\n",
      "Gradient Descent(474/999): loss=0.10341432154757219\n",
      "Gradient Descent(475/999): loss=0.10341100720874666\n",
      "Gradient Descent(476/999): loss=0.10340769868053683\n",
      "Gradient Descent(477/999): loss=0.10340439588262684\n",
      "Gradient Descent(478/999): loss=0.10340109873599239\n",
      "Gradient Descent(479/999): loss=0.10339780716287987\n",
      "Gradient Descent(480/999): loss=0.10339452108678576\n",
      "Gradient Descent(481/999): loss=0.10339124043243665\n",
      "Gradient Descent(482/999): loss=0.1033879651257693\n",
      "Gradient Descent(483/999): loss=0.10338469509391097\n",
      "Gradient Descent(484/999): loss=0.10338143026516051\n",
      "Gradient Descent(485/999): loss=0.10337817056896921\n",
      "Gradient Descent(486/999): loss=0.10337491593592232\n",
      "Gradient Descent(487/999): loss=0.10337166629772074\n",
      "Gradient Descent(488/999): loss=0.10336842158716299\n",
      "Gradient Descent(489/999): loss=0.10336518173812752\n",
      "Gradient Descent(490/999): loss=0.10336194668555525\n",
      "Gradient Descent(491/999): loss=0.10335871636543253\n",
      "Gradient Descent(492/999): loss=0.10335549071477426\n",
      "Gradient Descent(493/999): loss=0.10335226967160711\n",
      "Gradient Descent(494/999): loss=0.10334905317495342\n",
      "Gradient Descent(495/999): loss=0.10334584116481496\n",
      "Gradient Descent(496/999): loss=0.10334263358215734\n",
      "Gradient Descent(497/999): loss=0.10333943036889424\n",
      "Gradient Descent(498/999): loss=0.10333623146787227\n",
      "Gradient Descent(499/999): loss=0.10333303682285579\n",
      "Gradient Descent(500/999): loss=0.1033298463785122\n",
      "Gradient Descent(501/999): loss=0.10332666008039738\n",
      "Gradient Descent(502/999): loss=0.10332347787494127\n",
      "Gradient Descent(503/999): loss=0.10332029970943384\n",
      "Gradient Descent(504/999): loss=0.10331712553201115\n",
      "Gradient Descent(505/999): loss=0.10331395529164183\n",
      "Gradient Descent(506/999): loss=0.10331078893811348\n",
      "Gradient Descent(507/999): loss=0.1033076264220195\n",
      "Gradient Descent(508/999): loss=0.10330446769474629\n",
      "Gradient Descent(509/999): loss=0.10330131270846007\n",
      "Gradient Descent(510/999): loss=0.1032981614160947\n",
      "Gradient Descent(511/999): loss=0.103295013771339\n",
      "Gradient Descent(512/999): loss=0.10329186972862478\n",
      "Gradient Descent(513/999): loss=0.1032887292431147\n",
      "Gradient Descent(514/999): loss=0.10328559227069058\n",
      "Gradient Descent(515/999): loss=0.10328245876794163\n",
      "Gradient Descent(516/999): loss=0.10327932869215335\n",
      "Gradient Descent(517/999): loss=0.10327620200129599\n",
      "Gradient Descent(518/999): loss=0.10327307865401365\n",
      "Gradient Descent(519/999): loss=0.10326995860961341\n",
      "Gradient Descent(520/999): loss=0.10326684182805457\n",
      "Gradient Descent(521/999): loss=0.10326372826993817\n",
      "Gradient Descent(522/999): loss=0.10326061789649664\n",
      "Gradient Descent(523/999): loss=0.1032575106695836\n",
      "Gradient Descent(524/999): loss=0.10325440655166386\n",
      "Gradient Descent(525/999): loss=0.10325130550580351\n",
      "Gradient Descent(526/999): loss=0.10324820749566027\n",
      "Gradient Descent(527/999): loss=0.10324511248547395\n",
      "Gradient Descent(528/999): loss=0.10324202044005702\n",
      "Gradient Descent(529/999): loss=0.10323893132478537\n",
      "Gradient Descent(530/999): loss=0.1032358451055893\n",
      "Gradient Descent(531/999): loss=0.10323276174894438\n",
      "Gradient Descent(532/999): loss=0.103229681221863\n",
      "Gradient Descent(533/999): loss=0.1032266034918853\n",
      "Gradient Descent(534/999): loss=0.10322352852707098\n",
      "Gradient Descent(535/999): loss=0.10322045629599078\n",
      "Gradient Descent(536/999): loss=0.10321738676771813\n",
      "Gradient Descent(537/999): loss=0.10321431991182137\n",
      "Gradient Descent(538/999): loss=0.10321125569835535\n",
      "Gradient Descent(539/999): loss=0.10320819409785392\n",
      "Gradient Descent(540/999): loss=0.10320513508132204\n",
      "Gradient Descent(541/999): loss=0.10320207862022818\n",
      "Gradient Descent(542/999): loss=0.10319902468649692\n",
      "Gradient Descent(543/999): loss=0.10319597325250145\n",
      "Gradient Descent(544/999): loss=0.10319292429105656\n",
      "Gradient Descent(545/999): loss=0.10318987777541136\n",
      "Gradient Descent(546/999): loss=0.10318683367924215\n",
      "Gradient Descent(547/999): loss=0.10318379197664593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(548/999): loss=0.1031807526421331\n",
      "Gradient Descent(549/999): loss=0.10317771565062128\n",
      "Gradient Descent(550/999): loss=0.10317468097742832\n",
      "Gradient Descent(551/999): loss=0.10317164859826619\n",
      "Gradient Descent(552/999): loss=0.10316861848923436\n",
      "Gradient Descent(553/999): loss=0.10316559062681371\n",
      "Gradient Descent(554/999): loss=0.10316256498786038\n",
      "Gradient Descent(555/999): loss=0.10315954154959954\n",
      "Gradient Descent(556/999): loss=0.10315652028961966\n",
      "Gradient Descent(557/999): loss=0.10315350118586646\n",
      "Gradient Descent(558/999): loss=0.10315048421663735\n",
      "Gradient Descent(559/999): loss=0.10314746936057556\n",
      "Gradient Descent(560/999): loss=0.10314445659666464\n",
      "Gradient Descent(561/999): loss=0.10314144590422307\n",
      "Gradient Descent(562/999): loss=0.10313843726289869\n",
      "Gradient Descent(563/999): loss=0.10313543065266342\n",
      "Gradient Descent(564/999): loss=0.10313242605380814\n",
      "Gradient Descent(565/999): loss=0.10312942344693747\n",
      "Gradient Descent(566/999): loss=0.10312642281296473\n",
      "Gradient Descent(567/999): loss=0.10312342413310692\n",
      "Gradient Descent(568/999): loss=0.10312042738887983\n",
      "Gradient Descent(569/999): loss=0.10311743256209328\n",
      "Gradient Descent(570/999): loss=0.10311443963484634\n",
      "Gradient Descent(571/999): loss=0.10311144858952263\n",
      "Gradient Descent(572/999): loss=0.10310845940878578\n",
      "Gradient Descent(573/999): loss=0.10310547207557491\n",
      "Gradient Descent(574/999): loss=0.10310248657310017\n",
      "Gradient Descent(575/999): loss=0.10309950288483836\n",
      "Gradient Descent(576/999): loss=0.10309652099452857\n",
      "Gradient Descent(577/999): loss=0.10309354088616814\n",
      "Gradient Descent(578/999): loss=0.10309056254400836\n",
      "Gradient Descent(579/999): loss=0.10308758595255028\n",
      "Gradient Descent(580/999): loss=0.10308461109654092\n",
      "Gradient Descent(581/999): loss=0.1030816379609691\n",
      "Gradient Descent(582/999): loss=0.10307866653106158\n",
      "Gradient Descent(583/999): loss=0.10307569679227936\n",
      "Gradient Descent(584/999): loss=0.1030727287303137\n",
      "Gradient Descent(585/999): loss=0.10306976233108259\n",
      "Gradient Descent(586/999): loss=0.10306679758072691\n",
      "Gradient Descent(587/999): loss=0.10306383446560702\n",
      "Gradient Descent(588/999): loss=0.10306087297229902\n",
      "Gradient Descent(589/999): loss=0.10305791308759153\n",
      "Gradient Descent(590/999): loss=0.10305495479848202\n",
      "Gradient Descent(591/999): loss=0.10305199809217354\n",
      "Gradient Descent(592/999): loss=0.10304904295607144\n",
      "Gradient Descent(593/999): loss=0.10304608937778002\n",
      "Gradient Descent(594/999): loss=0.10304313734509943\n",
      "Gradient Descent(595/999): loss=0.1030401868460224\n",
      "Gradient Descent(596/999): loss=0.1030372378687312\n",
      "Gradient Descent(597/999): loss=0.10303429040159462\n",
      "Gradient Descent(598/999): loss=0.10303134443316475\n",
      "Gradient Descent(599/999): loss=0.10302839995217443\n",
      "Gradient Descent(600/999): loss=0.10302545694753389\n",
      "Gradient Descent(601/999): loss=0.10302251540832821\n",
      "Gradient Descent(602/999): loss=0.10301957532381426\n",
      "Gradient Descent(603/999): loss=0.10301663668341816\n",
      "Gradient Descent(604/999): loss=0.10301369947673243\n",
      "Gradient Descent(605/999): loss=0.10301076369351324\n",
      "Gradient Descent(606/999): loss=0.1030078293236779\n",
      "Gradient Descent(607/999): loss=0.10300489635730221\n",
      "Gradient Descent(608/999): loss=0.10300196478461794\n",
      "Gradient Descent(609/999): loss=0.10299903459601015\n",
      "Gradient Descent(610/999): loss=0.10299610578201505\n",
      "Gradient Descent(611/999): loss=0.10299317833331717\n",
      "Gradient Descent(612/999): loss=0.10299025224074722\n",
      "Gradient Descent(613/999): loss=0.10298732749527972\n",
      "Gradient Descent(614/999): loss=0.1029844040880306\n",
      "Gradient Descent(615/999): loss=0.10298148201025492\n",
      "Gradient Descent(616/999): loss=0.10297856125334474\n",
      "Gradient Descent(617/999): loss=0.1029756418088267\n",
      "Gradient Descent(618/999): loss=0.1029727236683601\n",
      "Gradient Descent(619/999): loss=0.10296980682373462\n",
      "Gradient Descent(620/999): loss=0.10296689126686821\n",
      "Gradient Descent(621/999): loss=0.10296397698980499\n",
      "Gradient Descent(622/999): loss=0.10296106398471343\n",
      "Gradient Descent(623/999): loss=0.10295815224388406\n",
      "Gradient Descent(624/999): loss=0.10295524175972769\n",
      "Gradient Descent(625/999): loss=0.10295233252477341\n",
      "Gradient Descent(626/999): loss=0.10294942453166667\n",
      "Gradient Descent(627/999): loss=0.10294651777316745\n",
      "Gradient Descent(628/999): loss=0.10294361224214836\n",
      "Gradient Descent(629/999): loss=0.10294070793159284\n",
      "Gradient Descent(630/999): loss=0.10293780483459346\n",
      "Gradient Descent(631/999): loss=0.10293490294434991\n",
      "Gradient Descent(632/999): loss=0.10293200225416768\n",
      "Gradient Descent(633/999): loss=0.10292910275745595\n",
      "Gradient Descent(634/999): loss=0.10292620444772618\n",
      "Gradient Descent(635/999): loss=0.10292330731859028\n",
      "Gradient Descent(636/999): loss=0.10292041136375916\n",
      "Gradient Descent(637/999): loss=0.10291751657704103\n",
      "Gradient Descent(638/999): loss=0.10291462295233983\n",
      "Gradient Descent(639/999): loss=0.10291173048365379\n",
      "Gradient Descent(640/999): loss=0.10290883916507378\n",
      "Gradient Descent(641/999): loss=0.10290594899078183\n",
      "Gradient Descent(642/999): loss=0.10290305995504981\n",
      "Gradient Descent(643/999): loss=0.10290017205223777\n",
      "Gradient Descent(644/999): loss=0.10289728527679269\n",
      "Gradient Descent(645/999): loss=0.102894399623247\n",
      "Gradient Descent(646/999): loss=0.10289151508621715\n",
      "Gradient Descent(647/999): loss=0.10288863166040245\n",
      "Gradient Descent(648/999): loss=0.10288574934058348\n",
      "Gradient Descent(649/999): loss=0.10288286812162095\n",
      "Gradient Descent(650/999): loss=0.10287998799845435\n",
      "Gradient Descent(651/999): loss=0.10287710896610076\n",
      "Gradient Descent(652/999): loss=0.10287423101965348\n",
      "Gradient Descent(653/999): loss=0.1028713541542808\n",
      "Gradient Descent(654/999): loss=0.10286847836522497\n",
      "Gradient Descent(655/999): loss=0.10286560364780081\n",
      "Gradient Descent(656/999): loss=0.10286272999739457\n",
      "Gradient Descent(657/999): loss=0.10285985740946293\n",
      "Gradient Descent(658/999): loss=0.10285698587953176\n",
      "Gradient Descent(659/999): loss=0.10285411540319489\n",
      "Gradient Descent(660/999): loss=0.10285124597611317\n",
      "Gradient Descent(661/999): loss=0.10284837759401345\n",
      "Gradient Descent(662/999): loss=0.10284551025268733\n",
      "Gradient Descent(663/999): loss=0.1028426439479902\n",
      "Gradient Descent(664/999): loss=0.10283977867584021\n",
      "Gradient Descent(665/999): loss=0.10283691443221733\n",
      "Gradient Descent(666/999): loss=0.10283405121316222\n",
      "Gradient Descent(667/999): loss=0.1028311890147754\n",
      "Gradient Descent(668/999): loss=0.10282832783321612\n",
      "Gradient Descent(669/999): loss=0.10282546766470149\n",
      "Gradient Descent(670/999): loss=0.10282260850550565\n",
      "Gradient Descent(671/999): loss=0.10281975035195874\n",
      "Gradient Descent(672/999): loss=0.10281689320044596\n",
      "Gradient Descent(673/999): loss=0.10281403704740673\n",
      "Gradient Descent(674/999): loss=0.10281118188933398\n",
      "Gradient Descent(675/999): loss=0.10280832772277294\n",
      "Gradient Descent(676/999): loss=0.10280547454432064\n",
      "Gradient Descent(677/999): loss=0.10280262235062476\n",
      "Gradient Descent(678/999): loss=0.10279977113838323\n",
      "Gradient Descent(679/999): loss=0.10279692090434298\n",
      "Gradient Descent(680/999): loss=0.10279407164529937\n",
      "Gradient Descent(681/999): loss=0.10279122335809546\n",
      "Gradient Descent(682/999): loss=0.10278837603962111\n",
      "Gradient Descent(683/999): loss=0.10278552968681232\n",
      "Gradient Descent(684/999): loss=0.1027826842966504\n",
      "Gradient Descent(685/999): loss=0.10277983986616131\n",
      "Gradient Descent(686/999): loss=0.102776996392415\n",
      "Gradient Descent(687/999): loss=0.10277415387252449\n",
      "Gradient Descent(688/999): loss=0.10277131230364539\n",
      "Gradient Descent(689/999): loss=0.10276847168297512\n",
      "Gradient Descent(690/999): loss=0.10276563200775232\n",
      "Gradient Descent(691/999): loss=0.10276279327525596\n",
      "Gradient Descent(692/999): loss=0.10275995548280496\n",
      "Gradient Descent(693/999): loss=0.10275711862775745\n",
      "Gradient Descent(694/999): loss=0.10275428270751008\n",
      "Gradient Descent(695/999): loss=0.10275144771949743\n",
      "Gradient Descent(696/999): loss=0.10274861366119145\n",
      "Gradient Descent(697/999): loss=0.10274578053010086\n",
      "Gradient Descent(698/999): loss=0.1027429483237704\n",
      "Gradient Descent(699/999): loss=0.10274011703978046\n",
      "Gradient Descent(700/999): loss=0.10273728667574648\n",
      "Gradient Descent(701/999): loss=0.10273445722931814\n",
      "Gradient Descent(702/999): loss=0.10273162869817914\n",
      "Gradient Descent(703/999): loss=0.1027288010800465\n",
      "Gradient Descent(704/999): loss=0.10272597437266992\n",
      "Gradient Descent(705/999): loss=0.1027231485738315\n",
      "Gradient Descent(706/999): loss=0.10272032368134495\n",
      "Gradient Descent(707/999): loss=0.10271749969305534\n",
      "Gradient Descent(708/999): loss=0.10271467660683842\n",
      "Gradient Descent(709/999): loss=0.1027118544206001\n",
      "Gradient Descent(710/999): loss=0.10270903313227614\n",
      "Gradient Descent(711/999): loss=0.10270621273983155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(712/999): loss=0.10270339324126013\n",
      "Gradient Descent(713/999): loss=0.10270057463458394\n",
      "Gradient Descent(714/999): loss=0.10269775691785307\n",
      "Gradient Descent(715/999): loss=0.1026949400891449\n",
      "Gradient Descent(716/999): loss=0.10269212414656399\n",
      "Gradient Descent(717/999): loss=0.1026893090882412\n",
      "Gradient Descent(718/999): loss=0.10268649491233373\n",
      "Gradient Descent(719/999): loss=0.10268368161702439\n",
      "Gradient Descent(720/999): loss=0.1026808692005213\n",
      "Gradient Descent(721/999): loss=0.10267805766105752\n",
      "Gradient Descent(722/999): loss=0.10267524699689054\n",
      "Gradient Descent(723/999): loss=0.10267243720630195\n",
      "Gradient Descent(724/999): loss=0.10266962828759708\n",
      "Gradient Descent(725/999): loss=0.10266682023910467\n",
      "Gradient Descent(726/999): loss=0.10266401305917619\n",
      "Gradient Descent(727/999): loss=0.10266120674618588\n",
      "Gradient Descent(728/999): loss=0.10265840129853013\n",
      "Gradient Descent(729/999): loss=0.10265559671462723\n",
      "Gradient Descent(730/999): loss=0.10265279299291698\n",
      "Gradient Descent(731/999): loss=0.10264999013186028\n",
      "Gradient Descent(732/999): loss=0.10264718812993899\n",
      "Gradient Descent(733/999): loss=0.10264438698565528\n",
      "Gradient Descent(734/999): loss=0.10264158669753169\n",
      "Gradient Descent(735/999): loss=0.10263878726411044\n",
      "Gradient Descent(736/999): loss=0.10263598868395331\n",
      "Gradient Descent(737/999): loss=0.10263319095564136\n",
      "Gradient Descent(738/999): loss=0.10263039407777441\n",
      "Gradient Descent(739/999): loss=0.10262759804897108\n",
      "Gradient Descent(740/999): loss=0.10262480286786807\n",
      "Gradient Descent(741/999): loss=0.10262200853312022\n",
      "Gradient Descent(742/999): loss=0.10261921504340005\n",
      "Gradient Descent(743/999): loss=0.10261642239739753\n",
      "Gradient Descent(744/999): loss=0.10261363059381971\n",
      "Gradient Descent(745/999): loss=0.10261083963139055\n",
      "Gradient Descent(746/999): loss=0.10260804950885068\n",
      "Gradient Descent(747/999): loss=0.10260526022495699\n",
      "Gradient Descent(748/999): loss=0.10260247177848247\n",
      "Gradient Descent(749/999): loss=0.10259968416821591\n",
      "Gradient Descent(750/999): loss=0.10259689739296177\n",
      "Gradient Descent(751/999): loss=0.10259411145153964\n",
      "Gradient Descent(752/999): loss=0.10259132634278427\n",
      "Gradient Descent(753/999): loss=0.10258854206554532\n",
      "Gradient Descent(754/999): loss=0.1025857586186869\n",
      "Gradient Descent(755/999): loss=0.1025829760010876\n",
      "Gradient Descent(756/999): loss=0.10258019421164001\n",
      "Gradient Descent(757/999): loss=0.10257741324925075\n",
      "Gradient Descent(758/999): loss=0.10257463311283997\n",
      "Gradient Descent(759/999): loss=0.1025718538013414\n",
      "Gradient Descent(760/999): loss=0.102569075313702\n",
      "Gradient Descent(761/999): loss=0.10256629764888166\n",
      "Gradient Descent(762/999): loss=0.10256352080585322\n",
      "Gradient Descent(763/999): loss=0.102560744783602\n",
      "Gradient Descent(764/999): loss=0.10255796958112588\n",
      "Gradient Descent(765/999): loss=0.10255519519743486\n",
      "Gradient Descent(766/999): loss=0.10255242163155094\n",
      "Gradient Descent(767/999): loss=0.102549648882508\n",
      "Gradient Descent(768/999): loss=0.10254687694935159\n",
      "Gradient Descent(769/999): loss=0.10254410583113859\n",
      "Gradient Descent(770/999): loss=0.10254133552693726\n",
      "Gradient Descent(771/999): loss=0.1025385660358269\n",
      "Gradient Descent(772/999): loss=0.10253579735689772\n",
      "Gradient Descent(773/999): loss=0.10253302948925064\n",
      "Gradient Descent(774/999): loss=0.1025302624319972\n",
      "Gradient Descent(775/999): loss=0.10252749618425935\n",
      "Gradient Descent(776/999): loss=0.10252473074516916\n",
      "Gradient Descent(777/999): loss=0.10252196611386886\n",
      "Gradient Descent(778/999): loss=0.1025192022895105\n",
      "Gradient Descent(779/999): loss=0.10251643927125599\n",
      "Gradient Descent(780/999): loss=0.10251367705827674\n",
      "Gradient Descent(781/999): loss=0.10251091564975365\n",
      "Gradient Descent(782/999): loss=0.10250815504487683\n",
      "Gradient Descent(783/999): loss=0.10250539524284563\n",
      "Gradient Descent(784/999): loss=0.10250263624286825\n",
      "Gradient Descent(785/999): loss=0.10249987804416191\n",
      "Gradient Descent(786/999): loss=0.10249712064595236\n",
      "Gradient Descent(787/999): loss=0.10249436404747413\n",
      "Gradient Descent(788/999): loss=0.10249160824796992\n",
      "Gradient Descent(789/999): loss=0.10248885324669095\n",
      "Gradient Descent(790/999): loss=0.10248609904289645\n",
      "Gradient Descent(791/999): loss=0.10248334563585382\n",
      "Gradient Descent(792/999): loss=0.10248059302483822\n",
      "Gradient Descent(793/999): loss=0.10247784120913271\n",
      "Gradient Descent(794/999): loss=0.10247509018802792\n",
      "Gradient Descent(795/999): loss=0.10247233996082207\n",
      "Gradient Descent(796/999): loss=0.10246959052682077\n",
      "Gradient Descent(797/999): loss=0.10246684188533693\n",
      "Gradient Descent(798/999): loss=0.10246409403569054\n",
      "Gradient Descent(799/999): loss=0.10246134697720889\n",
      "Gradient Descent(800/999): loss=0.10245860070922597\n",
      "Gradient Descent(801/999): loss=0.10245585523108278\n",
      "Gradient Descent(802/999): loss=0.10245311054212691\n",
      "Gradient Descent(803/999): loss=0.10245036664171277\n",
      "Gradient Descent(804/999): loss=0.10244762352920106\n",
      "Gradient Descent(805/999): loss=0.10244488120395907\n",
      "Gradient Descent(806/999): loss=0.10244213966536031\n",
      "Gradient Descent(807/999): loss=0.10243939891278453\n",
      "Gradient Descent(808/999): loss=0.10243665894561761\n",
      "Gradient Descent(809/999): loss=0.10243391976325142\n",
      "Gradient Descent(810/999): loss=0.10243118136508379\n",
      "Gradient Descent(811/999): loss=0.10242844375051832\n",
      "Gradient Descent(812/999): loss=0.10242570691896438\n",
      "Gradient Descent(813/999): loss=0.10242297086983701\n",
      "Gradient Descent(814/999): loss=0.10242023560255671\n",
      "Gradient Descent(815/999): loss=0.10241750111654956\n",
      "Gradient Descent(816/999): loss=0.10241476741124693\n",
      "Gradient Descent(817/999): loss=0.10241203448608552\n",
      "Gradient Descent(818/999): loss=0.10240930234050726\n",
      "Gradient Descent(819/999): loss=0.10240657097395921\n",
      "Gradient Descent(820/999): loss=0.10240384038589335\n",
      "Gradient Descent(821/999): loss=0.10240111057576683\n",
      "Gradient Descent(822/999): loss=0.10239838154304151\n",
      "Gradient Descent(823/999): loss=0.1023956532871842\n",
      "Gradient Descent(824/999): loss=0.1023929258076663\n",
      "Gradient Descent(825/999): loss=0.10239019910396399\n",
      "Gradient Descent(826/999): loss=0.10238747317555799\n",
      "Gradient Descent(827/999): loss=0.10238474802193347\n",
      "Gradient Descent(828/999): loss=0.1023820236425802\n",
      "Gradient Descent(829/999): loss=0.10237930003699212\n",
      "Gradient Descent(830/999): loss=0.1023765772046676\n",
      "Gradient Descent(831/999): loss=0.10237385514510924\n",
      "Gradient Descent(832/999): loss=0.10237113385782376\n",
      "Gradient Descent(833/999): loss=0.10236841334232202\n",
      "Gradient Descent(834/999): loss=0.10236569359811885\n",
      "Gradient Descent(835/999): loss=0.10236297462473316\n",
      "Gradient Descent(836/999): loss=0.10236025642168765\n",
      "Gradient Descent(837/999): loss=0.10235753898850898\n",
      "Gradient Descent(838/999): loss=0.10235482232472752\n",
      "Gradient Descent(839/999): loss=0.1023521064298774\n",
      "Gradient Descent(840/999): loss=0.10234939130349634\n",
      "Gradient Descent(841/999): loss=0.10234667694512582\n",
      "Gradient Descent(842/999): loss=0.10234396335431076\n",
      "Gradient Descent(843/999): loss=0.10234125053059966\n",
      "Gradient Descent(844/999): loss=0.10233853847354434\n",
      "Gradient Descent(845/999): loss=0.10233582718270012\n",
      "Gradient Descent(846/999): loss=0.10233311665762562\n",
      "Gradient Descent(847/999): loss=0.10233040689788275\n",
      "Gradient Descent(848/999): loss=0.10232769790303664\n",
      "Gradient Descent(849/999): loss=0.1023249896726556\n",
      "Gradient Descent(850/999): loss=0.10232228220631108\n",
      "Gradient Descent(851/999): loss=0.1023195755035776\n",
      "Gradient Descent(852/999): loss=0.1023168695640327\n",
      "Gradient Descent(853/999): loss=0.10231416438725703\n",
      "Gradient Descent(854/999): loss=0.10231145997283395\n",
      "Gradient Descent(855/999): loss=0.10230875632034991\n",
      "Gradient Descent(856/999): loss=0.10230605342939413\n",
      "Gradient Descent(857/999): loss=0.1023033512995586\n",
      "Gradient Descent(858/999): loss=0.10230064993043816\n",
      "Gradient Descent(859/999): loss=0.10229794932163036\n",
      "Gradient Descent(860/999): loss=0.10229524947273529\n",
      "Gradient Descent(861/999): loss=0.10229255038335577\n",
      "Gradient Descent(862/999): loss=0.10228985205309728\n",
      "Gradient Descent(863/999): loss=0.10228715448156771\n",
      "Gradient Descent(864/999): loss=0.10228445766837758\n",
      "Gradient Descent(865/999): loss=0.10228176161313977\n",
      "Gradient Descent(866/999): loss=0.10227906631546972\n",
      "Gradient Descent(867/999): loss=0.10227637177498515\n",
      "Gradient Descent(868/999): loss=0.10227367799130621\n",
      "Gradient Descent(869/999): loss=0.10227098496405533\n",
      "Gradient Descent(870/999): loss=0.10226829269285727\n",
      "Gradient Descent(871/999): loss=0.10226560117733896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(872/999): loss=0.10226291041712966\n",
      "Gradient Descent(873/999): loss=0.10226022041186074\n",
      "Gradient Descent(874/999): loss=0.10225753116116572\n",
      "Gradient Descent(875/999): loss=0.10225484266468024\n",
      "Gradient Descent(876/999): loss=0.10225215492204197\n",
      "Gradient Descent(877/999): loss=0.1022494679328908\n",
      "Gradient Descent(878/999): loss=0.1022467816968684\n",
      "Gradient Descent(879/999): loss=0.10224409621361864\n",
      "Gradient Descent(880/999): loss=0.1022414114827872\n",
      "Gradient Descent(881/999): loss=0.10223872750402176\n",
      "Gradient Descent(882/999): loss=0.10223604427697183\n",
      "Gradient Descent(883/999): loss=0.10223336180128884\n",
      "Gradient Descent(884/999): loss=0.10223068007662608\n",
      "Gradient Descent(885/999): loss=0.10222799910263856\n",
      "Gradient Descent(886/999): loss=0.10222531887898319\n",
      "Gradient Descent(887/999): loss=0.10222263940531853\n",
      "Gradient Descent(888/999): loss=0.1022199606813049\n",
      "Gradient Descent(889/999): loss=0.10221728270660436\n",
      "Gradient Descent(890/999): loss=0.10221460548088059\n",
      "Gradient Descent(891/999): loss=0.10221192900379894\n",
      "Gradient Descent(892/999): loss=0.10220925327502642\n",
      "Gradient Descent(893/999): loss=0.10220657829423158\n",
      "Gradient Descent(894/999): loss=0.10220390406108455\n",
      "Gradient Descent(895/999): loss=0.10220123057525708\n",
      "Gradient Descent(896/999): loss=0.10219855783642234\n",
      "Gradient Descent(897/999): loss=0.10219588584425511\n",
      "Gradient Descent(898/999): loss=0.10219321459843157\n",
      "Gradient Descent(899/999): loss=0.10219054409862943\n",
      "Gradient Descent(900/999): loss=0.10218787434452775\n",
      "Gradient Descent(901/999): loss=0.10218520533580706\n",
      "Gradient Descent(902/999): loss=0.1021825370721493\n",
      "Gradient Descent(903/999): loss=0.10217986955323774\n",
      "Gradient Descent(904/999): loss=0.10217720277875703\n",
      "Gradient Descent(905/999): loss=0.10217453674839308\n",
      "Gradient Descent(906/999): loss=0.10217187146183324\n",
      "Gradient Descent(907/999): loss=0.10216920691876605\n",
      "Gradient Descent(908/999): loss=0.10216654311888133\n",
      "Gradient Descent(909/999): loss=0.10216388006187023\n",
      "Gradient Descent(910/999): loss=0.10216121774742501\n",
      "Gradient Descent(911/999): loss=0.10215855617523925\n",
      "Gradient Descent(912/999): loss=0.10215589534500763\n",
      "Gradient Descent(913/999): loss=0.10215323525642617\n",
      "Gradient Descent(914/999): loss=0.10215057590919183\n",
      "Gradient Descent(915/999): loss=0.1021479173030029\n",
      "Gradient Descent(916/999): loss=0.1021452594375587\n",
      "Gradient Descent(917/999): loss=0.10214260231255971\n",
      "Gradient Descent(918/999): loss=0.1021399459277075\n",
      "Gradient Descent(919/999): loss=0.10213729028270456\n",
      "Gradient Descent(920/999): loss=0.10213463537725477\n",
      "Gradient Descent(921/999): loss=0.10213198121106272\n",
      "Gradient Descent(922/999): loss=0.10212932778383424\n",
      "Gradient Descent(923/999): loss=0.10212667509527605\n",
      "Gradient Descent(924/999): loss=0.10212402314509599\n",
      "Gradient Descent(925/999): loss=0.10212137193300277\n",
      "Gradient Descent(926/999): loss=0.10211872145870614\n",
      "Gradient Descent(927/999): loss=0.10211607172191677\n",
      "Gradient Descent(928/999): loss=0.10211342272234633\n",
      "Gradient Descent(929/999): loss=0.10211077445970734\n",
      "Gradient Descent(930/999): loss=0.10210812693371321\n",
      "Gradient Descent(931/999): loss=0.10210548014407843\n",
      "Gradient Descent(932/999): loss=0.1021028340905182\n",
      "Gradient Descent(933/999): loss=0.1021001887727486\n",
      "Gradient Descent(934/999): loss=0.10209754419048672\n",
      "Gradient Descent(935/999): loss=0.10209490034345034\n",
      "Gradient Descent(936/999): loss=0.10209225723135816\n",
      "Gradient Descent(937/999): loss=0.1020896148539297\n",
      "Gradient Descent(938/999): loss=0.10208697321088525\n",
      "Gradient Descent(939/999): loss=0.10208433230194595\n",
      "Gradient Descent(940/999): loss=0.10208169212683366\n",
      "Gradient Descent(941/999): loss=0.10207905268527115\n",
      "Gradient Descent(942/999): loss=0.1020764139769818\n",
      "Gradient Descent(943/999): loss=0.10207377600168983\n",
      "Gradient Descent(944/999): loss=0.10207113875912026\n",
      "Gradient Descent(945/999): loss=0.10206850224899869\n",
      "Gradient Descent(946/999): loss=0.10206586647105162\n",
      "Gradient Descent(947/999): loss=0.10206323142500608\n",
      "Gradient Descent(948/999): loss=0.10206059711058996\n",
      "Gradient Descent(949/999): loss=0.10205796352753177\n",
      "Gradient Descent(950/999): loss=0.10205533067556075\n",
      "Gradient Descent(951/999): loss=0.10205269855440673\n",
      "Gradient Descent(952/999): loss=0.10205006716380026\n",
      "Gradient Descent(953/999): loss=0.10204743650347259\n",
      "Gradient Descent(954/999): loss=0.10204480657315551\n",
      "Gradient Descent(955/999): loss=0.1020421773725815\n",
      "Gradient Descent(956/999): loss=0.10203954890148371\n",
      "Gradient Descent(957/999): loss=0.10203692115959584\n",
      "Gradient Descent(958/999): loss=0.10203429414665226\n",
      "Gradient Descent(959/999): loss=0.10203166786238788\n",
      "Gradient Descent(960/999): loss=0.10202904230653823\n",
      "Gradient Descent(961/999): loss=0.10202641747883948\n",
      "Gradient Descent(962/999): loss=0.10202379337902824\n",
      "Gradient Descent(963/999): loss=0.10202117000684184\n",
      "Gradient Descent(964/999): loss=0.10201854736201811\n",
      "Gradient Descent(965/999): loss=0.1020159254442954\n",
      "Gradient Descent(966/999): loss=0.1020133042534126\n",
      "Gradient Descent(967/999): loss=0.10201068378910928\n",
      "Gradient Descent(968/999): loss=0.10200806405112534\n",
      "Gradient Descent(969/999): loss=0.10200544503920136\n",
      "Gradient Descent(970/999): loss=0.10200282675307831\n",
      "Gradient Descent(971/999): loss=0.10200020919249779\n",
      "Gradient Descent(972/999): loss=0.10199759235720182\n",
      "Gradient Descent(973/999): loss=0.10199497624693298\n",
      "Gradient Descent(974/999): loss=0.10199236086143423\n",
      "Gradient Descent(975/999): loss=0.10198974620044916\n",
      "Gradient Descent(976/999): loss=0.10198713226372176\n",
      "Gradient Descent(977/999): loss=0.10198451905099643\n",
      "Gradient Descent(978/999): loss=0.10198190656201815\n",
      "Gradient Descent(979/999): loss=0.10197929479653227\n",
      "Gradient Descent(980/999): loss=0.10197668375428466\n",
      "Gradient Descent(981/999): loss=0.10197407343502157\n",
      "Gradient Descent(982/999): loss=0.10197146383848973\n",
      "Gradient Descent(983/999): loss=0.10196885496443628\n",
      "Gradient Descent(984/999): loss=0.10196624681260884\n",
      "Gradient Descent(985/999): loss=0.10196363938275539\n",
      "Gradient Descent(986/999): loss=0.10196103267462432\n",
      "Gradient Descent(987/999): loss=0.10195842668796454\n",
      "Gradient Descent(988/999): loss=0.10195582142252521\n",
      "Gradient Descent(989/999): loss=0.10195321687805602\n",
      "Gradient Descent(990/999): loss=0.10195061305430698\n",
      "Gradient Descent(991/999): loss=0.10194800995102854\n",
      "Gradient Descent(992/999): loss=0.10194540756797149\n",
      "Gradient Descent(993/999): loss=0.10194280590488704\n",
      "Gradient Descent(994/999): loss=0.10194020496152673\n",
      "Gradient Descent(995/999): loss=0.10193760473764256\n",
      "Gradient Descent(996/999): loss=0.10193500523298678\n",
      "Gradient Descent(997/999): loss=0.10193240644731212\n",
      "Gradient Descent(998/999): loss=0.10192980838037155\n",
      "Gradient Descent(999/999): loss=0.10192721103191846\n",
      "Gradient Descent(0/999): loss=0.14738697924684596\n",
      "Gradient Descent(1/999): loss=0.14669780540718066\n",
      "Gradient Descent(2/999): loss=0.1460196931226375\n",
      "Gradient Descent(3/999): loss=0.1453524639961078\n",
      "Gradient Descent(4/999): loss=0.1446959425078572\n",
      "Gradient Descent(5/999): loss=0.14404995596911616\n",
      "Gradient Descent(6/999): loss=0.14341433447641927\n",
      "Gradient Descent(7/999): loss=0.14278891086668083\n",
      "Gradient Descent(8/999): loss=0.142173520672995\n",
      "Gradient Descent(9/999): loss=0.14156800208114903\n",
      "Gradient Descent(10/999): loss=0.14097219588683788\n",
      "Gradient Descent(11/999): loss=0.14038594545356856\n",
      "Gradient Descent(12/999): loss=0.13980909667124386\n",
      "Gradient Descent(13/999): loss=0.1392414979154141\n",
      "Gradient Descent(14/999): loss=0.13868300000718564\n",
      "Gradient Descent(15/999): loss=0.13813345617377662\n",
      "Gradient Descent(16/999): loss=0.13759272200970835\n",
      "Gradient Descent(17/999): loss=0.1370606554386232\n",
      "Gradient Descent(18/999): loss=0.1365371166757178\n",
      "Gradient Descent(19/999): loss=0.13602196819078255\n",
      "Gradient Descent(20/999): loss=0.13551507467183693\n",
      "Gradient Descent(21/999): loss=0.13501630298935202\n",
      "Gradient Descent(22/999): loss=0.13452552216104877\n",
      "Gradient Descent(23/999): loss=0.1340426033172657\n",
      "Gradient Descent(24/999): loss=0.13356741966688376\n",
      "Gradient Descent(25/999): loss=0.1330998464638017\n",
      "Gradient Descent(26/999): loss=0.13263976097395172\n",
      "Gradient Descent(27/999): loss=0.13218704244284718\n",
      "Gradient Descent(28/999): loss=0.1317415720636539\n",
      "Gradient Descent(29/999): loss=0.13130323294577625\n",
      "Gradient Descent(30/999): loss=0.13087191008395027\n",
      "Gradient Descent(31/999): loss=0.13044749032783454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(32/999): loss=0.13002986235209238\n",
      "Gradient Descent(33/999): loss=0.12961891662695624\n",
      "Gradient Descent(34/999): loss=0.1292145453892666\n",
      "Gradient Descent(35/999): loss=0.12881664261397835\n",
      "Gradient Descent(36/999): loss=0.12842510398612678\n",
      "Gradient Descent(37/999): loss=0.12803982687324525\n",
      "Gradient Descent(38/999): loss=0.12766071029822834\n",
      "Gradient Descent(39/999): loss=0.12728765491263214\n",
      "Gradient Descent(40/999): loss=0.12692056297040594\n",
      "Gradient Descent(41/999): loss=0.12655933830204652\n",
      "Gradient Descent(42/999): loss=0.12620388628917031\n",
      "Gradient Descent(43/999): loss=0.12585411383949513\n",
      "Gradient Descent(44/999): loss=0.12550992936222574\n",
      "Gradient Descent(45/999): loss=0.12517124274383595\n",
      "Gradient Descent(46/999): loss=0.12483796532424195\n",
      "Gradient Descent(47/999): loss=0.12451000987335947\n",
      "Gradient Descent(48/999): loss=0.12418729056803936\n",
      "Gradient Descent(49/999): loss=0.12386972296937486\n",
      "Gradient Descent(50/999): loss=0.12355722400037514\n",
      "Gradient Descent(51/999): loss=0.12324971192399892\n",
      "Gradient Descent(52/999): loss=0.1229471063215423\n",
      "Gradient Descent(53/999): loss=0.12264932807137514\n",
      "Gradient Descent(54/999): loss=0.12235629932802056\n",
      "Gradient Descent(55/999): loss=0.12206794350157169\n",
      "Gradient Descent(56/999): loss=0.12178418523744057\n",
      "Gradient Descent(57/999): loss=0.12150495039643362\n",
      "Gradient Descent(58/999): loss=0.1212301660351483\n",
      "Gradient Descent(59/999): loss=0.12095976038668636\n",
      "Gradient Descent(60/999): loss=0.1206936628416777\n",
      "Gradient Descent(61/999): loss=0.12043180392961066\n",
      "Gradient Descent(62/999): loss=0.12017411530046326\n",
      "Gradient Descent(63/999): loss=0.11992052970663071\n",
      "Gradient Descent(64/999): loss=0.11967098098514449\n",
      "Gradient Descent(65/999): loss=0.11942540404017812\n",
      "Gradient Descent(66/999): loss=0.11918373482583525\n",
      "Gradient Descent(67/999): loss=0.11894591032921516\n",
      "Gradient Descent(68/999): loss=0.11871186855375158\n",
      "Gradient Descent(69/999): loss=0.11848154850282012\n",
      "Gradient Descent(70/999): loss=0.11825489016361015\n",
      "Gradient Descent(71/999): loss=0.11803183449125683\n",
      "Gradient Descent(72/999): loss=0.11781232339322895\n",
      "Gradient Descent(73/999): loss=0.1175962997139689\n",
      "Gradient Descent(74/999): loss=0.1173837072197798\n",
      "Gradient Descent(75/999): loss=0.11717449058395712\n",
      "Gradient Descent(76/999): loss=0.11696859537215946\n",
      "Gradient Descent(77/999): loss=0.11676596802801575\n",
      "Gradient Descent(78/999): loss=0.11656655585896432\n",
      "Gradient Descent(79/999): loss=0.11637030702232044\n",
      "Gradient Descent(80/999): loss=0.11617717051156863\n",
      "Gradient Descent(81/999): loss=0.11598709614287606\n",
      "Gradient Descent(82/999): loss=0.11580003454182339\n",
      "Gradient Descent(83/999): loss=0.11561593713034972\n",
      "Gradient Descent(84/999): loss=0.11543475611390805\n",
      "Gradient Descent(85/999): loss=0.11525644446882792\n",
      "Gradient Descent(86/999): loss=0.11508095592988206\n",
      "Gradient Descent(87/999): loss=0.11490824497805299\n",
      "Gradient Descent(88/999): loss=0.11473826682849789\n",
      "Gradient Descent(89/999): loss=0.11457097741870663\n",
      "Gradient Descent(90/999): loss=0.11440633339685136\n",
      "Gradient Descent(91/999): loss=0.114244292110324\n",
      "Gradient Descent(92/999): loss=0.11408481159445824\n",
      "Gradient Descent(93/999): loss=0.1139278505614338\n",
      "Gradient Descent(94/999): loss=0.11377336838935956\n",
      "Gradient Descent(95/999): loss=0.11362132511153274\n",
      "Gradient Descent(96/999): loss=0.11347168140587124\n",
      "Gradient Descent(97/999): loss=0.11332439858451664\n",
      "Gradient Descent(98/999): loss=0.11317943858360455\n",
      "Gradient Descent(99/999): loss=0.11303676395320046\n",
      "Gradient Descent(100/999): loss=0.1128963378473974\n",
      "Gradient Descent(101/999): loss=0.11275812401457364\n",
      "Gradient Descent(102/999): loss=0.11262208678780734\n",
      "Gradient Descent(103/999): loss=0.11248819107544598\n",
      "Gradient Descent(104/999): loss=0.11235640235182787\n",
      "Gradient Descent(105/999): loss=0.11222668664815288\n",
      "Gradient Descent(106/999): loss=0.1120990105435013\n",
      "Gradient Descent(107/999): loss=0.11197334115599665\n",
      "Gradient Descent(108/999): loss=0.11184964613411161\n",
      "Gradient Descent(109/999): loss=0.11172789364811386\n",
      "Gradient Descent(110/999): loss=0.11160805238165017\n",
      "Gradient Descent(111/999): loss=0.11149009152346588\n",
      "Gradient Descent(112/999): loss=0.1113739807592583\n",
      "Gradient Descent(113/999): loss=0.11125969026366127\n",
      "Gradient Descent(114/999): loss=0.11114719069235904\n",
      "Gradient Descent(115/999): loss=0.11103645317432743\n",
      "Gradient Descent(116/999): loss=0.11092744930420004\n",
      "Gradient Descent(117/999): loss=0.1108201511347577\n",
      "Gradient Descent(118/999): loss=0.11071453116953886\n",
      "Gradient Descent(119/999): loss=0.11061056235556936\n",
      "Gradient Descent(120/999): loss=0.11050821807620947\n",
      "Gradient Descent(121/999): loss=0.11040747214411606\n",
      "Gradient Descent(122/999): loss=0.11030829879431851\n",
      "Gradient Descent(123/999): loss=0.11021067267740643\n",
      "Gradient Descent(124/999): loss=0.11011456885282665\n",
      "Gradient Descent(125/999): loss=0.1100199627822891\n",
      "Gradient Descent(126/999): loss=0.10992683032327856\n",
      "Gradient Descent(127/999): loss=0.1098351477226712\n",
      "Gradient Descent(128/999): loss=0.10974489161045396\n",
      "Gradient Descent(129/999): loss=0.10965603899354544\n",
      "Gradient Descent(130/999): loss=0.10956856724971635\n",
      "Gradient Descent(131/999): loss=0.109482454121608\n",
      "Gradient Descent(132/999): loss=0.10939767771084709\n",
      "Gradient Descent(133/999): loss=0.10931421647225605\n",
      "Gradient Descent(134/999): loss=0.10923204920815573\n",
      "Gradient Descent(135/999): loss=0.10915115506276123\n",
      "Gradient Descent(136/999): loss=0.10907151351666683\n",
      "Gradient Descent(137/999): loss=0.10899310438142101\n",
      "Gradient Descent(138/999): loss=0.10891590779418825\n",
      "Gradient Descent(139/999): loss=0.10883990421249705\n",
      "Gradient Descent(140/999): loss=0.108765074409073\n",
      "Gradient Descent(141/999): loss=0.10869139946675477\n",
      "Gradient Descent(142/999): loss=0.10861886077349227\n",
      "Gradient Descent(143/999): loss=0.10854744001742568\n",
      "Gradient Descent(144/999): loss=0.10847711918204345\n",
      "Gradient Descent(145/999): loss=0.10840788054141869\n",
      "Gradient Descent(146/999): loss=0.10833970665552234\n",
      "Gradient Descent(147/999): loss=0.10827258036561187\n",
      "Gradient Descent(148/999): loss=0.10820648478969438\n",
      "Gradient Descent(149/999): loss=0.10814140331806295\n",
      "Gradient Descent(150/999): loss=0.1080773196089048\n",
      "Gradient Descent(151/999): loss=0.10801421758398064\n",
      "Gradient Descent(152/999): loss=0.10795208142437314\n",
      "Gradient Descent(153/999): loss=0.10789089556630463\n",
      "Gradient Descent(154/999): loss=0.10783064469702171\n",
      "Gradient Descent(155/999): loss=0.1077713137507467\n",
      "Gradient Descent(156/999): loss=0.10771288790469383\n",
      "Gradient Descent(157/999): loss=0.10765535257515027\n",
      "Gradient Descent(158/999): loss=0.10759869341362008\n",
      "Gradient Descent(159/999): loss=0.10754289630303025\n",
      "Gradient Descent(160/999): loss=0.10748794735399812\n",
      "Gradient Descent(161/999): loss=0.10743383290115913\n",
      "Gradient Descent(162/999): loss=0.10738053949955323\n",
      "Gradient Descent(163/999): loss=0.10732805392107034\n",
      "Gradient Descent(164/999): loss=0.10727636315095264\n",
      "Gradient Descent(165/999): loss=0.10722545438435324\n",
      "Gradient Descent(166/999): loss=0.10717531502295082\n",
      "Gradient Descent(167/999): loss=0.10712593267161821\n",
      "Gradient Descent(168/999): loss=0.10707729513514541\n",
      "Gradient Descent(169/999): loss=0.10702939041501482\n",
      "Gradient Descent(170/999): loss=0.10698220670622897\n",
      "Gradient Descent(171/999): loss=0.10693573239418913\n",
      "Gradient Descent(172/999): loss=0.10688995605162442\n",
      "Gradient Descent(173/999): loss=0.1068448664355703\n",
      "Gradient Descent(174/999): loss=0.10680045248439601\n",
      "Gradient Descent(175/999): loss=0.10675670331487967\n",
      "Gradient Descent(176/999): loss=0.10671360821933078\n",
      "Gradient Descent(177/999): loss=0.10667115666275918\n",
      "Gradient Descent(178/999): loss=0.10662933828008933\n",
      "Gradient Descent(179/999): loss=0.10658814287342003\n",
      "Gradient Descent(180/999): loss=0.10654756040932771\n",
      "Gradient Descent(181/999): loss=0.10650758101621385\n",
      "Gradient Descent(182/999): loss=0.10646819498169481\n",
      "Gradient Descent(183/999): loss=0.10642939275003371\n",
      "Gradient Descent(184/999): loss=0.10639116491961417\n",
      "Gradient Descent(185/999): loss=0.10635350224045409\n",
      "Gradient Descent(186/999): loss=0.10631639561176029\n",
      "Gradient Descent(187/999): loss=0.10627983607952204\n",
      "Gradient Descent(188/999): loss=0.10624381483414362\n",
      "Gradient Descent(189/999): loss=0.10620832320811507\n",
      "Gradient Descent(190/999): loss=0.10617335267372055\n",
      "Gradient Descent(191/999): loss=0.10613889484078344\n",
      "Gradient Descent(192/999): loss=0.10610494145444803\n",
      "Gradient Descent(193/999): loss=0.10607148439299703\n",
      "Gradient Descent(194/999): loss=0.10603851566570399\n",
      "Gradient Descent(195/999): loss=0.10600602741072065\n",
      "Gradient Descent(196/999): loss=0.10597401189299822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(197/999): loss=0.10594246150224226\n",
      "Gradient Descent(198/999): loss=0.10591136875090057\n",
      "Gradient Descent(199/999): loss=0.10588072627218323\n",
      "Gradient Descent(200/999): loss=0.10585052681811505\n",
      "Gradient Descent(201/999): loss=0.10582076325761919\n",
      "Gradient Descent(202/999): loss=0.10579142857463154\n",
      "Gradient Descent(203/999): loss=0.1057625158662459\n",
      "Gradient Descent(204/999): loss=0.10573401834088862\n",
      "Gradient Descent(205/999): loss=0.10570592931652305\n",
      "Gradient Descent(206/999): loss=0.10567824221888278\n",
      "Gradient Descent(207/999): loss=0.10565095057973346\n",
      "Gradient Descent(208/999): loss=0.10562404803516244\n",
      "Gradient Descent(209/999): loss=0.10559752832389632\n",
      "Gradient Descent(210/999): loss=0.10557138528564558\n",
      "Gradient Descent(211/999): loss=0.10554561285947539\n",
      "Gradient Descent(212/999): loss=0.1055202050822035\n",
      "Gradient Descent(213/999): loss=0.10549515608682346\n",
      "Gradient Descent(214/999): loss=0.10547046010095346\n",
      "Gradient Descent(215/999): loss=0.10544611144530995\n",
      "Gradient Descent(216/999): loss=0.10542210453220623\n",
      "Gradient Descent(217/999): loss=0.10539843386407495\n",
      "Gradient Descent(218/999): loss=0.10537509403201475\n",
      "Gradient Descent(219/999): loss=0.10535207971435988\n",
      "Gradient Descent(220/999): loss=0.10532938567527352\n",
      "Gradient Descent(221/999): loss=0.10530700676336309\n",
      "Gradient Descent(222/999): loss=0.10528493791031857\n",
      "Gradient Descent(223/999): loss=0.10526317412957216\n",
      "Gradient Descent(224/999): loss=0.10524171051498007\n",
      "Gradient Descent(225/999): loss=0.10522054223952537\n",
      "Gradient Descent(226/999): loss=0.10519966455404144\n",
      "Gradient Descent(227/999): loss=0.10517907278595677\n",
      "Gradient Descent(228/999): loss=0.10515876233805924\n",
      "Gradient Descent(229/999): loss=0.10513872868728068\n",
      "Gradient Descent(230/999): loss=0.10511896738350106\n",
      "Gradient Descent(231/999): loss=0.105099474048372\n",
      "Gradient Descent(232/999): loss=0.10508024437415898\n",
      "Gradient Descent(233/999): loss=0.10506127412260251\n",
      "Gradient Descent(234/999): loss=0.1050425591237975\n",
      "Gradient Descent(235/999): loss=0.10502409527509085\n",
      "Gradient Descent(236/999): loss=0.10500587853999668\n",
      "Gradient Descent(237/999): loss=0.10498790494712919\n",
      "Gradient Descent(238/999): loss=0.10497017058915248\n",
      "Gradient Descent(239/999): loss=0.10495267162174768\n",
      "Gradient Descent(240/999): loss=0.10493540426259655\n",
      "Gradient Descent(241/999): loss=0.10491836479038123\n",
      "Gradient Descent(242/999): loss=0.10490154954380067\n",
      "Gradient Descent(243/999): loss=0.10488495492060242\n",
      "Gradient Descent(244/999): loss=0.10486857737663029\n",
      "Gradient Descent(245/999): loss=0.10485241342488726\n",
      "Gradient Descent(246/999): loss=0.10483645963461367\n",
      "Gradient Descent(247/999): loss=0.10482071263038016\n",
      "Gradient Descent(248/999): loss=0.10480516909119499\n",
      "Gradient Descent(249/999): loss=0.1047898257496264\n",
      "Gradient Descent(250/999): loss=0.10477467939093853\n",
      "Gradient Descent(251/999): loss=0.10475972685224137\n",
      "Gradient Descent(252/999): loss=0.1047449650216548\n",
      "Gradient Descent(253/999): loss=0.1047303908374857\n",
      "Gradient Descent(254/999): loss=0.10471600128741851\n",
      "Gradient Descent(255/999): loss=0.10470179340771875\n",
      "Gradient Descent(256/999): loss=0.10468776428244962\n",
      "Gradient Descent(257/999): loss=0.10467391104270087\n",
      "Gradient Descent(258/999): loss=0.10466023086583047\n",
      "Gradient Descent(259/999): loss=0.10464672097471825\n",
      "Gradient Descent(260/999): loss=0.10463337863703169\n",
      "Gradient Descent(261/999): loss=0.10462020116450364\n",
      "Gradient Descent(262/999): loss=0.10460718591222143\n",
      "Gradient Descent(263/999): loss=0.10459433027792768\n",
      "Gradient Descent(264/999): loss=0.1045816317013324\n",
      "Gradient Descent(265/999): loss=0.10456908766343587\n",
      "Gradient Descent(266/999): loss=0.10455669568586291\n",
      "Gradient Descent(267/999): loss=0.10454445333020755\n",
      "Gradient Descent(268/999): loss=0.1045323581973883\n",
      "Gradient Descent(269/999): loss=0.10452040792701402\n",
      "Gradient Descent(270/999): loss=0.10450860019675964\n",
      "Gradient Descent(271/999): loss=0.1044969327217526\n",
      "Gradient Descent(272/999): loss=0.10448540325396838\n",
      "Gradient Descent(273/999): loss=0.10447400958163636\n",
      "Gradient Descent(274/999): loss=0.104462749528655\n",
      "Gradient Descent(275/999): loss=0.10445162095401668\n",
      "Gradient Descent(276/999): loss=0.10444062175124158\n",
      "Gradient Descent(277/999): loss=0.10442974984782068\n",
      "Gradient Descent(278/999): loss=0.10441900320466797\n",
      "Gradient Descent(279/999): loss=0.10440837981558145\n",
      "Gradient Descent(280/999): loss=0.10439787770671248\n",
      "Gradient Descent(281/999): loss=0.10438749493604418\n",
      "Gradient Descent(282/999): loss=0.10437722959287797\n",
      "Gradient Descent(283/999): loss=0.1043670797973283\n",
      "Gradient Descent(284/999): loss=0.10435704369982592\n",
      "Gradient Descent(285/999): loss=0.10434711948062861\n",
      "Gradient Descent(286/999): loss=0.1043373053493404\n",
      "Gradient Descent(287/999): loss=0.10432759954443797\n",
      "Gradient Descent(288/999): loss=0.10431800033280515\n",
      "Gradient Descent(289/999): loss=0.10430850600927469\n",
      "Gradient Descent(290/999): loss=0.10429911489617742\n",
      "Gradient Descent(291/999): loss=0.1042898253428988\n",
      "Gradient Descent(292/999): loss=0.10428063572544252\n",
      "Gradient Descent(293/999): loss=0.10427154444600117\n",
      "Gradient Descent(294/999): loss=0.10426254993253393\n",
      "Gradient Descent(295/999): loss=0.1042536506383508\n",
      "Gradient Descent(296/999): loss=0.1042448450417039\n",
      "Gradient Descent(297/999): loss=0.10423613164538502\n",
      "Gradient Descent(298/999): loss=0.10422750897632997\n",
      "Gradient Descent(299/999): loss=0.10421897558522912\n",
      "Gradient Descent(300/999): loss=0.10421053004614408\n",
      "Gradient Descent(301/999): loss=0.10420217095613113\n",
      "Gradient Descent(302/999): loss=0.10419389693486983\n",
      "Gradient Descent(303/999): loss=0.10418570662429867\n",
      "Gradient Descent(304/999): loss=0.10417759868825564\n",
      "Gradient Descent(305/999): loss=0.10416957181212522\n",
      "Gradient Descent(306/999): loss=0.10416162470249077\n",
      "Gradient Descent(307/999): loss=0.10415375608679271\n",
      "Gradient Descent(308/999): loss=0.10414596471299195\n",
      "Gradient Descent(309/999): loss=0.1041382493492391\n",
      "Gradient Descent(310/999): loss=0.1041306087835488\n",
      "Gradient Descent(311/999): loss=0.1041230418234791\n",
      "Gradient Descent(312/999): loss=0.10411554729581667\n",
      "Gradient Descent(313/999): loss=0.10410812404626621\n",
      "Gradient Descent(314/999): loss=0.10410077093914563\n",
      "Gradient Descent(315/999): loss=0.10409348685708573\n",
      "Gradient Descent(316/999): loss=0.10408627070073485\n",
      "Gradient Descent(317/999): loss=0.10407912138846825\n",
      "Gradient Descent(318/999): loss=0.10407203785610221\n",
      "Gradient Descent(319/999): loss=0.1040650190566127\n",
      "Gradient Descent(320/999): loss=0.1040580639598585\n",
      "Gradient Descent(321/999): loss=0.10405117155230938\n",
      "Gradient Descent(322/999): loss=0.10404434083677745\n",
      "Gradient Descent(323/999): loss=0.10403757083215413\n",
      "Gradient Descent(324/999): loss=0.10403086057315064\n",
      "Gradient Descent(325/999): loss=0.10402420911004273\n",
      "Gradient Descent(326/999): loss=0.10401761550841987\n",
      "Gradient Descent(327/999): loss=0.10401107884893805\n",
      "Gradient Descent(328/999): loss=0.10400459822707685\n",
      "Gradient Descent(329/999): loss=0.10399817275290038\n",
      "Gradient Descent(330/999): loss=0.10399180155082202\n",
      "Gradient Descent(331/999): loss=0.10398548375937294\n",
      "Gradient Descent(332/999): loss=0.1039792185309745\n",
      "Gradient Descent(333/999): loss=0.1039730050317141\n",
      "Gradient Descent(334/999): loss=0.10396684244112482\n",
      "Gradient Descent(335/999): loss=0.10396072995196859\n",
      "Gradient Descent(336/999): loss=0.10395466677002267\n",
      "Gradient Descent(337/999): loss=0.10394865211387004\n",
      "Gradient Descent(338/999): loss=0.10394268521469247\n",
      "Gradient Descent(339/999): loss=0.10393676531606755\n",
      "Gradient Descent(340/999): loss=0.10393089167376869\n",
      "Gradient Descent(341/999): loss=0.10392506355556855\n",
      "Gradient Descent(342/999): loss=0.10391928024104526\n",
      "Gradient Descent(343/999): loss=0.10391354102139215\n",
      "Gradient Descent(344/999): loss=0.1039078451992305\n",
      "Gradient Descent(345/999): loss=0.10390219208842497\n",
      "Gradient Descent(346/999): loss=0.10389658101390258\n",
      "Gradient Descent(347/999): loss=0.10389101131147394\n",
      "Gradient Descent(348/999): loss=0.1038854823276579\n",
      "Gradient Descent(349/999): loss=0.10387999341950899\n",
      "Gradient Descent(350/999): loss=0.10387454395444715\n",
      "Gradient Descent(351/999): loss=0.10386913331009075\n",
      "Gradient Descent(352/999): loss=0.10386376087409212\n",
      "Gradient Descent(353/999): loss=0.10385842604397556\n",
      "Gradient Descent(354/999): loss=0.10385312822697836\n",
      "Gradient Descent(355/999): loss=0.10384786683989383\n",
      "Gradient Descent(356/999): loss=0.10384264130891753\n",
      "Gradient Descent(357/999): loss=0.10383745106949521\n",
      "Gradient Descent(358/999): loss=0.10383229556617403\n",
      "Gradient Descent(359/999): loss=0.1038271742524555\n",
      "Gradient Descent(360/999): loss=0.10382208659065105\n",
      "Gradient Descent(361/999): loss=0.10381703205174012\n",
      "Gradient Descent(362/999): loss=0.10381201011523021\n",
      "Gradient Descent(363/999): loss=0.10380702026901932\n",
      "Gradient Descent(364/999): loss=0.10380206200926081\n",
      "Gradient Descent(365/999): loss=0.10379713484022994\n",
      "Gradient Descent(366/999): loss=0.10379223827419315\n",
      "Gradient Descent(367/999): loss=0.10378737183127905\n",
      "Gradient Descent(368/999): loss=0.10378253503935156\n",
      "Gradient Descent(369/999): loss=0.10377772743388529\n",
      "Gradient Descent(370/999): loss=0.10377294855784273\n",
      "Gradient Descent(371/999): loss=0.10376819796155337\n",
      "Gradient Descent(372/999): loss=0.10376347520259509\n",
      "Gradient Descent(373/999): loss=0.10375877984567705\n",
      "Gradient Descent(374/999): loss=0.10375411146252474\n",
      "Gradient Descent(375/999): loss=0.10374946963176691\n",
      "Gradient Descent(376/999): loss=0.10374485393882409\n",
      "Gradient Descent(377/999): loss=0.10374026397579912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(378/999): loss=0.10373569934136934\n",
      "Gradient Descent(379/999): loss=0.10373115964068061\n",
      "Gradient Descent(380/999): loss=0.10372664448524294\n",
      "Gradient Descent(381/999): loss=0.10372215349282776\n",
      "Gradient Descent(382/999): loss=0.1037176862873671\n",
      "Gradient Descent(383/999): loss=0.10371324249885414\n",
      "Gradient Descent(384/999): loss=0.10370882176324539\n",
      "Gradient Descent(385/999): loss=0.10370442372236459\n",
      "Gradient Descent(386/999): loss=0.10370004802380814\n",
      "Gradient Descent(387/999): loss=0.10369569432085175\n",
      "Gradient Descent(388/999): loss=0.1036913622723592\n",
      "Gradient Descent(389/999): loss=0.10368705154269187\n",
      "Gradient Descent(390/999): loss=0.10368276180162025\n",
      "Gradient Descent(391/999): loss=0.10367849272423671\n",
      "Gradient Descent(392/999): loss=0.10367424399086954\n",
      "Gradient Descent(393/999): loss=0.10367001528699862\n",
      "Gradient Descent(394/999): loss=0.10366580630317235\n",
      "Gradient Descent(395/999): loss=0.10366161673492567\n",
      "Gradient Descent(396/999): loss=0.10365744628270004\n",
      "Gradient Descent(397/999): loss=0.10365329465176391\n",
      "Gradient Descent(398/999): loss=0.10364916155213498\n",
      "Gradient Descent(399/999): loss=0.1036450466985038\n",
      "Gradient Descent(400/999): loss=0.10364094981015817\n",
      "Gradient Descent(401/999): loss=0.10363687061090907\n",
      "Gradient Descent(402/999): loss=0.10363280882901772\n",
      "Gradient Descent(403/999): loss=0.1036287641971238\n",
      "Gradient Descent(404/999): loss=0.1036247364521748\n",
      "Gradient Descent(405/999): loss=0.1036207253353566\n",
      "Gradient Descent(406/999): loss=0.10361673059202502\n",
      "Gradient Descent(407/999): loss=0.10361275197163865\n",
      "Gradient Descent(408/999): loss=0.10360878922769269\n",
      "Gradient Descent(409/999): loss=0.10360484211765372\n",
      "Gradient Descent(410/999): loss=0.10360091040289579\n",
      "Gradient Descent(411/999): loss=0.10359699384863731\n",
      "Gradient Descent(412/999): loss=0.10359309222387911\n",
      "Gradient Descent(413/999): loss=0.10358920530134333\n",
      "Gradient Descent(414/999): loss=0.10358533285741346\n",
      "Gradient Descent(415/999): loss=0.10358147467207544\n",
      "Gradient Descent(416/999): loss=0.10357763052885931\n",
      "Gradient Descent(417/999): loss=0.10357380021478206\n",
      "Gradient Descent(418/999): loss=0.10356998352029162\n",
      "Gradient Descent(419/999): loss=0.1035661802392114\n",
      "Gradient Descent(420/999): loss=0.10356239016868571\n",
      "Gradient Descent(421/999): loss=0.1035586131091264\n",
      "Gradient Descent(422/999): loss=0.10355484886416011\n",
      "Gradient Descent(423/999): loss=0.10355109724057629\n",
      "Gradient Descent(424/999): loss=0.1035473580482764\n",
      "Gradient Descent(425/999): loss=0.1035436311002236\n",
      "Gradient Descent(426/999): loss=0.10353991621239322\n",
      "Gradient Descent(427/999): loss=0.1035362132037246\n",
      "Gradient Descent(428/999): loss=0.10353252189607279\n",
      "Gradient Descent(429/999): loss=0.10352884211416186\n",
      "Gradient Descent(430/999): loss=0.10352517368553853\n",
      "Gradient Descent(431/999): loss=0.1035215164405266\n",
      "Gradient Descent(432/999): loss=0.10351787021218223\n",
      "Gradient Descent(433/999): loss=0.10351423483624977\n",
      "Gradient Descent(434/999): loss=0.10351061015111855\n",
      "Gradient Descent(435/999): loss=0.10350699599778011\n",
      "Gradient Descent(436/999): loss=0.1035033922197863\n",
      "Gradient Descent(437/999): loss=0.10349979866320787\n",
      "Gradient Descent(438/999): loss=0.103496215176594\n",
      "Gradient Descent(439/999): loss=0.10349264161093219\n",
      "Gradient Descent(440/999): loss=0.10348907781960907\n",
      "Gradient Descent(441/999): loss=0.10348552365837153\n",
      "Gradient Descent(442/999): loss=0.10348197898528887\n",
      "Gradient Descent(443/999): loss=0.10347844366071515\n",
      "Gradient Descent(444/999): loss=0.10347491754725235\n",
      "Gradient Descent(445/999): loss=0.1034714005097144\n",
      "Gradient Descent(446/999): loss=0.10346789241509105\n",
      "Gradient Descent(447/999): loss=0.10346439313251314\n",
      "Gradient Descent(448/999): loss=0.10346090253321796\n",
      "Gradient Descent(449/999): loss=0.1034574204905152\n",
      "Gradient Descent(450/999): loss=0.10345394687975351\n",
      "Gradient Descent(451/999): loss=0.10345048157828793\n",
      "Gradient Descent(452/999): loss=0.10344702446544704\n",
      "Gradient Descent(453/999): loss=0.10344357542250156\n",
      "Gradient Descent(454/999): loss=0.10344013433263266\n",
      "Gradient Descent(455/999): loss=0.10343670108090153\n",
      "Gradient Descent(456/999): loss=0.10343327555421881\n",
      "Gradient Descent(457/999): loss=0.10342985764131471\n",
      "Gradient Descent(458/999): loss=0.10342644723270987\n",
      "Gradient Descent(459/999): loss=0.10342304422068642\n",
      "Gradient Descent(460/999): loss=0.10341964849925939\n",
      "Gradient Descent(461/999): loss=0.10341625996414902\n",
      "Gradient Descent(462/999): loss=0.1034128785127531\n",
      "Gradient Descent(463/999): loss=0.10340950404412003\n",
      "Gradient Descent(464/999): loss=0.103406136458922\n",
      "Gradient Descent(465/999): loss=0.103402775659429\n",
      "Gradient Descent(466/999): loss=0.10339942154948302\n",
      "Gradient Descent(467/999): loss=0.10339607403447258\n",
      "Gradient Descent(468/999): loss=0.10339273302130789\n",
      "Gradient Descent(469/999): loss=0.10338939841839627\n",
      "Gradient Descent(470/999): loss=0.10338607013561804\n",
      "Gradient Descent(471/999): loss=0.10338274808430277\n",
      "Gradient Descent(472/999): loss=0.10337943217720583\n",
      "Gradient Descent(473/999): loss=0.10337612232848538\n",
      "Gradient Descent(474/999): loss=0.10337281845367993\n",
      "Gradient Descent(475/999): loss=0.1033695204696859\n",
      "Gradient Descent(476/999): loss=0.10336622829473573\n",
      "Gradient Descent(477/999): loss=0.10336294184837645\n",
      "Gradient Descent(478/999): loss=0.10335966105144838\n",
      "Gradient Descent(479/999): loss=0.10335638582606428\n",
      "Gradient Descent(480/999): loss=0.10335311609558889\n",
      "Gradient Descent(481/999): loss=0.10334985178461865\n",
      "Gradient Descent(482/999): loss=0.10334659281896189\n",
      "Gradient Descent(483/999): loss=0.10334333912561927\n",
      "Gradient Descent(484/999): loss=0.10334009063276459\n",
      "Gradient Descent(485/999): loss=0.10333684726972582\n",
      "Gradient Descent(486/999): loss=0.10333360896696647\n",
      "Gradient Descent(487/999): loss=0.10333037565606733\n",
      "Gradient Descent(488/999): loss=0.10332714726970846\n",
      "Gradient Descent(489/999): loss=0.1033239237416514\n",
      "Gradient Descent(490/999): loss=0.10332070500672172\n",
      "Gradient Descent(491/999): loss=0.10331749100079199\n",
      "Gradient Descent(492/999): loss=0.1033142816607647\n",
      "Gradient Descent(493/999): loss=0.10331107692455588\n",
      "Gradient Descent(494/999): loss=0.1033078767310785\n",
      "Gradient Descent(495/999): loss=0.10330468102022657\n",
      "Gradient Descent(496/999): loss=0.1033014897328593\n",
      "Gradient Descent(497/999): loss=0.10329830281078542\n",
      "Gradient Descent(498/999): loss=0.10329512019674811\n",
      "Gradient Descent(499/999): loss=0.10329194183440954\n",
      "Gradient Descent(500/999): loss=0.10328876766833646\n",
      "Gradient Descent(501/999): loss=0.1032855976439853\n",
      "Gradient Descent(502/999): loss=0.10328243170768812\n",
      "Gradient Descent(503/999): loss=0.10327926980663812\n",
      "Gradient Descent(504/999): loss=0.1032761118888761\n",
      "Gradient Descent(505/999): loss=0.10327295790327654\n",
      "Gradient Descent(506/999): loss=0.10326980779953436\n",
      "Gradient Descent(507/999): loss=0.10326666152815149\n",
      "Gradient Descent(508/999): loss=0.10326351904042401\n",
      "Gradient Descent(509/999): loss=0.10326038028842925\n",
      "Gradient Descent(510/999): loss=0.10325724522501327\n",
      "Gradient Descent(511/999): loss=0.1032541138037784\n",
      "Gradient Descent(512/999): loss=0.10325098597907097\n",
      "Gradient Descent(513/999): loss=0.10324786170596954\n",
      "Gradient Descent(514/999): loss=0.1032447409402729\n",
      "Gradient Descent(515/999): loss=0.10324162363848849\n",
      "Gradient Descent(516/999): loss=0.10323850975782103\n",
      "Gradient Descent(517/999): loss=0.10323539925616113\n",
      "Gradient Descent(518/999): loss=0.10323229209207453\n",
      "Gradient Descent(519/999): loss=0.10322918822479084\n",
      "Gradient Descent(520/999): loss=0.10322608761419311\n",
      "Gradient Descent(521/999): loss=0.10322299022080711\n",
      "Gradient Descent(522/999): loss=0.10321989600579111\n",
      "Gradient Descent(523/999): loss=0.10321680493092565\n",
      "Gradient Descent(524/999): loss=0.10321371695860339\n",
      "Gradient Descent(525/999): loss=0.10321063205181934\n",
      "Gradient Descent(526/999): loss=0.10320755017416124\n",
      "Gradient Descent(527/999): loss=0.10320447128979968\n",
      "Gradient Descent(528/999): loss=0.10320139536347911\n",
      "Gradient Descent(529/999): loss=0.10319832236050826\n",
      "Gradient Descent(530/999): loss=0.10319525224675127\n",
      "Gradient Descent(531/999): loss=0.10319218498861853\n",
      "Gradient Descent(532/999): loss=0.10318912055305803\n",
      "Gradient Descent(533/999): loss=0.10318605890754662\n",
      "Gradient Descent(534/999): loss=0.10318300002008148\n",
      "Gradient Descent(535/999): loss=0.10317994385917177\n",
      "Gradient Descent(536/999): loss=0.1031768903938303\n",
      "Gradient Descent(537/999): loss=0.10317383959356546\n",
      "Gradient Descent(538/999): loss=0.10317079142837333\n",
      "Gradient Descent(539/999): loss=0.10316774586872958\n",
      "Gradient Descent(540/999): loss=0.10316470288558188\n",
      "Gradient Descent(541/999): loss=0.10316166245034235\n",
      "Gradient Descent(542/999): loss=0.1031586245348799\n",
      "Gradient Descent(543/999): loss=0.10315558911151305\n",
      "Gradient Descent(544/999): loss=0.10315255615300253\n",
      "Gradient Descent(545/999): loss=0.10314952563254419\n",
      "Gradient Descent(546/999): loss=0.10314649752376204\n",
      "Gradient Descent(547/999): loss=0.10314347180070131\n",
      "Gradient Descent(548/999): loss=0.1031404484378216\n",
      "Gradient Descent(549/999): loss=0.10313742740999036\n",
      "Gradient Descent(550/999): loss=0.1031344086924761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(551/999): loss=0.10313139226094203\n",
      "Gradient Descent(552/999): loss=0.10312837809143978\n",
      "Gradient Descent(553/999): loss=0.10312536616040296\n",
      "Gradient Descent(554/999): loss=0.10312235644464114\n",
      "Gradient Descent(555/999): loss=0.10311934892133374\n",
      "Gradient Descent(556/999): loss=0.10311634356802404\n",
      "Gradient Descent(557/999): loss=0.10311334036261334\n",
      "Gradient Descent(558/999): loss=0.10311033928335522\n",
      "Gradient Descent(559/999): loss=0.10310734030884978\n",
      "Gradient Descent(560/999): loss=0.10310434341803812\n",
      "Gradient Descent(561/999): loss=0.10310134859019676\n",
      "Gradient Descent(562/999): loss=0.10309835580493246\n",
      "Gradient Descent(563/999): loss=0.10309536504217655\n",
      "Gradient Descent(564/999): loss=0.10309237628217989\n",
      "Gradient Descent(565/999): loss=0.10308938950550778\n",
      "Gradient Descent(566/999): loss=0.10308640469303476\n",
      "Gradient Descent(567/999): loss=0.1030834218259397\n",
      "Gradient Descent(568/999): loss=0.10308044088570083\n",
      "Gradient Descent(569/999): loss=0.10307746185409099\n",
      "Gradient Descent(570/999): loss=0.10307448471317276\n",
      "Gradient Descent(571/999): loss=0.10307150944529404\n",
      "Gradient Descent(572/999): loss=0.10306853603308304\n",
      "Gradient Descent(573/999): loss=0.10306556445944415\n",
      "Gradient Descent(574/999): loss=0.10306259470755331\n",
      "Gradient Descent(575/999): loss=0.10305962676085374\n",
      "Gradient Descent(576/999): loss=0.10305666060305135\n",
      "Gradient Descent(577/999): loss=0.10305369621811084\n",
      "Gradient Descent(578/999): loss=0.10305073359025135\n",
      "Gradient Descent(579/999): loss=0.10304777270394244\n",
      "Gradient Descent(580/999): loss=0.10304481354389991\n",
      "Gradient Descent(581/999): loss=0.10304185609508205\n",
      "Gradient Descent(582/999): loss=0.10303890034268556\n",
      "Gradient Descent(583/999): loss=0.10303594627214166\n",
      "Gradient Descent(584/999): loss=0.10303299386911265\n",
      "Gradient Descent(585/999): loss=0.10303004311948763\n",
      "Gradient Descent(586/999): loss=0.10302709400937939\n",
      "Gradient Descent(587/999): loss=0.10302414652512046\n",
      "Gradient Descent(588/999): loss=0.1030212006532597\n",
      "Gradient Descent(589/999): loss=0.10301825638055877\n",
      "Gradient Descent(590/999): loss=0.10301531369398877\n",
      "Gradient Descent(591/999): loss=0.1030123725807267\n",
      "Gradient Descent(592/999): loss=0.10300943302815235\n",
      "Gradient Descent(593/999): loss=0.10300649502384489\n",
      "Gradient Descent(594/999): loss=0.10300355855557973\n",
      "Gradient Descent(595/999): loss=0.10300062361132524\n",
      "Gradient Descent(596/999): loss=0.10299769017923982\n",
      "Gradient Descent(597/999): loss=0.1029947582476687\n",
      "Gradient Descent(598/999): loss=0.10299182780514089\n",
      "Gradient Descent(599/999): loss=0.10298889884036645\n",
      "Gradient Descent(600/999): loss=0.10298597134223328\n",
      "Gradient Descent(601/999): loss=0.10298304529980448\n",
      "Gradient Descent(602/999): loss=0.10298012070231531\n",
      "Gradient Descent(603/999): loss=0.10297719753917069\n",
      "Gradient Descent(604/999): loss=0.1029742757999422\n",
      "Gradient Descent(605/999): loss=0.10297135547436556\n",
      "Gradient Descent(606/999): loss=0.10296843655233795\n",
      "Gradient Descent(607/999): loss=0.10296551902391533\n",
      "Gradient Descent(608/999): loss=0.10296260287930999\n",
      "Gradient Descent(609/999): loss=0.10295968810888792\n",
      "Gradient Descent(610/999): loss=0.10295677470316648\n",
      "Gradient Descent(611/999): loss=0.10295386265281178\n",
      "Gradient Descent(612/999): loss=0.10295095194863643\n",
      "Gradient Descent(613/999): loss=0.10294804258159715\n",
      "Gradient Descent(614/999): loss=0.10294513454279228\n",
      "Gradient Descent(615/999): loss=0.1029422278234598\n",
      "Gradient Descent(616/999): loss=0.10293932241497487\n",
      "Gradient Descent(617/999): loss=0.10293641830884759\n",
      "Gradient Descent(618/999): loss=0.10293351549672101\n",
      "Gradient Descent(619/999): loss=0.1029306139703688\n",
      "Gradient Descent(620/999): loss=0.10292771372169325\n",
      "Gradient Descent(621/999): loss=0.10292481474272318\n",
      "Gradient Descent(622/999): loss=0.10292191702561186\n",
      "Gradient Descent(623/999): loss=0.10291902056263505\n",
      "Gradient Descent(624/999): loss=0.10291612534618902\n",
      "Gradient Descent(625/999): loss=0.10291323136878862\n",
      "Gradient Descent(626/999): loss=0.10291033862306528\n",
      "Gradient Descent(627/999): loss=0.10290744710176519\n",
      "Gradient Descent(628/999): loss=0.10290455679774764\n",
      "Gradient Descent(629/999): loss=0.10290166770398276\n",
      "Gradient Descent(630/999): loss=0.10289877981355017\n",
      "Gradient Descent(631/999): loss=0.10289589311963693\n",
      "Gradient Descent(632/999): loss=0.102893007615536\n",
      "Gradient Descent(633/999): loss=0.10289012329464442\n",
      "Gradient Descent(634/999): loss=0.10288724015046155\n",
      "Gradient Descent(635/999): loss=0.10288435817658777\n",
      "Gradient Descent(636/999): loss=0.10288147736672235\n",
      "Gradient Descent(637/999): loss=0.10287859771466233\n",
      "Gradient Descent(638/999): loss=0.10287571921430062\n",
      "Gradient Descent(639/999): loss=0.10287284185962457\n",
      "Gradient Descent(640/999): loss=0.10286996564471448\n",
      "Gradient Descent(641/999): loss=0.10286709056374212\n",
      "Gradient Descent(642/999): loss=0.1028642166109691\n",
      "Gradient Descent(643/999): loss=0.10286134378074561\n",
      "Gradient Descent(644/999): loss=0.10285847206750887\n",
      "Gradient Descent(645/999): loss=0.10285560146578178\n",
      "Gradient Descent(646/999): loss=0.10285273197017156\n",
      "Gradient Descent(647/999): loss=0.10284986357536828\n",
      "Gradient Descent(648/999): loss=0.10284699627614372\n",
      "Gradient Descent(649/999): loss=0.10284413006734985\n",
      "Gradient Descent(650/999): loss=0.10284126494391767\n",
      "Gradient Descent(651/999): loss=0.10283840090085587\n",
      "Gradient Descent(652/999): loss=0.10283553793324961\n",
      "Gradient Descent(653/999): loss=0.1028326760362593\n",
      "Gradient Descent(654/999): loss=0.10282981520511936\n",
      "Gradient Descent(655/999): loss=0.102826955435137\n",
      "Gradient Descent(656/999): loss=0.10282409672169117\n",
      "Gradient Descent(657/999): loss=0.10282123906023122\n",
      "Gradient Descent(658/999): loss=0.10281838244627595\n",
      "Gradient Descent(659/999): loss=0.10281552687541232\n",
      "Gradient Descent(660/999): loss=0.1028126723432945\n",
      "Gradient Descent(661/999): loss=0.1028098188456427\n",
      "Gradient Descent(662/999): loss=0.10280696637824215\n",
      "Gradient Descent(663/999): loss=0.10280411493694205\n",
      "Gradient Descent(664/999): loss=0.10280126451765448\n",
      "Gradient Descent(665/999): loss=0.10279841511635349\n",
      "Gradient Descent(666/999): loss=0.10279556672907389\n",
      "Gradient Descent(667/999): loss=0.10279271935191063\n",
      "Gradient Descent(668/999): loss=0.10278987298101752\n",
      "Gradient Descent(669/999): loss=0.10278702761260644\n",
      "Gradient Descent(670/999): loss=0.10278418324294625\n",
      "Gradient Descent(671/999): loss=0.1027813398683621\n",
      "Gradient Descent(672/999): loss=0.10277849748523431\n",
      "Gradient Descent(673/999): loss=0.10277565608999767\n",
      "Gradient Descent(674/999): loss=0.10277281567914029\n",
      "Gradient Descent(675/999): loss=0.1027699762492031\n",
      "Gradient Descent(676/999): loss=0.10276713779677873\n",
      "Gradient Descent(677/999): loss=0.10276430031851078\n",
      "Gradient Descent(678/999): loss=0.10276146381109295\n",
      "Gradient Descent(679/999): loss=0.10275862827126835\n",
      "Gradient Descent(680/999): loss=0.10275579369582852\n",
      "Gradient Descent(681/999): loss=0.10275296008161276\n",
      "Gradient Descent(682/999): loss=0.10275012742550742\n",
      "Gradient Descent(683/999): loss=0.10274729572444505\n",
      "Gradient Descent(684/999): loss=0.10274446497540367\n",
      "Gradient Descent(685/999): loss=0.10274163517540601\n",
      "Gradient Descent(686/999): loss=0.10273880632151891\n",
      "Gradient Descent(687/999): loss=0.10273597841085243\n",
      "Gradient Descent(688/999): loss=0.10273315144055938\n",
      "Gradient Descent(689/999): loss=0.10273032540783442\n",
      "Gradient Descent(690/999): loss=0.10272750030991344\n",
      "Gradient Descent(691/999): loss=0.10272467614407298\n",
      "Gradient Descent(692/999): loss=0.10272185290762958\n",
      "Gradient Descent(693/999): loss=0.10271903059793898\n",
      "Gradient Descent(694/999): loss=0.10271620921239562\n",
      "Gradient Descent(695/999): loss=0.10271338874843199\n",
      "Gradient Descent(696/999): loss=0.10271056920351807\n",
      "Gradient Descent(697/999): loss=0.10270775057516057\n",
      "Gradient Descent(698/999): loss=0.10270493286090253\n",
      "Gradient Descent(699/999): loss=0.10270211605832259\n",
      "Gradient Descent(700/999): loss=0.1026993001650345\n",
      "Gradient Descent(701/999): loss=0.10269648517868649\n",
      "Gradient Descent(702/999): loss=0.10269367109696087\n",
      "Gradient Descent(703/999): loss=0.10269085791757318\n",
      "Gradient Descent(704/999): loss=0.10268804563827197\n",
      "Gradient Descent(705/999): loss=0.10268523425683816\n",
      "Gradient Descent(706/999): loss=0.10268242377108441\n",
      "Gradient Descent(707/999): loss=0.10267961417885473\n",
      "Gradient Descent(708/999): loss=0.102676805478024\n",
      "Gradient Descent(709/999): loss=0.10267399766649729\n",
      "Gradient Descent(710/999): loss=0.10267119074220972\n",
      "Gradient Descent(711/999): loss=0.10266838470312549\n",
      "Gradient Descent(712/999): loss=0.1026655795472379\n",
      "Gradient Descent(713/999): loss=0.10266277527256858\n",
      "Gradient Descent(714/999): loss=0.10265997187716708\n",
      "Gradient Descent(715/999): loss=0.10265716935911051\n",
      "Gradient Descent(716/999): loss=0.10265436771650306\n",
      "Gradient Descent(717/999): loss=0.10265156694747554\n",
      "Gradient Descent(718/999): loss=0.10264876705018491\n",
      "Gradient Descent(719/999): loss=0.10264596802281398\n",
      "Gradient Descent(720/999): loss=0.1026431698635709\n",
      "Gradient Descent(721/999): loss=0.10264037257068884\n",
      "Gradient Descent(722/999): loss=0.10263757614242536\n",
      "Gradient Descent(723/999): loss=0.10263478057706239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(724/999): loss=0.10263198587290553\n",
      "Gradient Descent(725/999): loss=0.10262919202828374\n",
      "Gradient Descent(726/999): loss=0.10262639904154915\n",
      "Gradient Descent(727/999): loss=0.10262360691107629\n",
      "Gradient Descent(728/999): loss=0.10262081563526224\n",
      "Gradient Descent(729/999): loss=0.10261802521252582\n",
      "Gradient Descent(730/999): loss=0.10261523564130756\n",
      "Gradient Descent(731/999): loss=0.10261244692006911\n",
      "Gradient Descent(732/999): loss=0.10260965904729304\n",
      "Gradient Descent(733/999): loss=0.10260687202148247\n",
      "Gradient Descent(734/999): loss=0.10260408584116079\n",
      "Gradient Descent(735/999): loss=0.10260130050487122\n",
      "Gradient Descent(736/999): loss=0.10259851601117657\n",
      "Gradient Descent(737/999): loss=0.102595732358659\n",
      "Gradient Descent(738/999): loss=0.10259294954591949\n",
      "Gradient Descent(739/999): loss=0.1025901675715777\n",
      "Gradient Descent(740/999): loss=0.10258738643427176\n",
      "Gradient Descent(741/999): loss=0.10258460613265771\n",
      "Gradient Descent(742/999): loss=0.10258182666540944\n",
      "Gradient Descent(743/999): loss=0.10257904803121827\n",
      "Gradient Descent(744/999): loss=0.10257627022879273\n",
      "Gradient Descent(745/999): loss=0.10257349325685834\n",
      "Gradient Descent(746/999): loss=0.10257071711415718\n",
      "Gradient Descent(747/999): loss=0.10256794179944777\n",
      "Gradient Descent(748/999): loss=0.10256516731150475\n",
      "Gradient Descent(749/999): loss=0.10256239364911858\n",
      "Gradient Descent(750/999): loss=0.1025596208110954\n",
      "Gradient Descent(751/999): loss=0.10255684879625669\n",
      "Gradient Descent(752/999): loss=0.10255407760343895\n",
      "Gradient Descent(753/999): loss=0.10255130723149368\n",
      "Gradient Descent(754/999): loss=0.10254853767928691\n",
      "Gradient Descent(755/999): loss=0.10254576894569921\n",
      "Gradient Descent(756/999): loss=0.10254300102962513\n",
      "Gradient Descent(757/999): loss=0.10254023392997333\n",
      "Gradient Descent(758/999): loss=0.10253746764566603\n",
      "Gradient Descent(759/999): loss=0.10253470217563908\n",
      "Gradient Descent(760/999): loss=0.1025319375188415\n",
      "Gradient Descent(761/999): loss=0.10252917367423549\n",
      "Gradient Descent(762/999): loss=0.10252641064079604\n",
      "Gradient Descent(763/999): loss=0.10252364841751078\n",
      "Gradient Descent(764/999): loss=0.10252088700337988\n",
      "Gradient Descent(765/999): loss=0.10251812639741557\n",
      "Gradient Descent(766/999): loss=0.10251536659864237\n",
      "Gradient Descent(767/999): loss=0.10251260760609653\n",
      "Gradient Descent(768/999): loss=0.10250984941882593\n",
      "Gradient Descent(769/999): loss=0.10250709203589005\n",
      "Gradient Descent(770/999): loss=0.10250433545635956\n",
      "Gradient Descent(771/999): loss=0.10250157967931639\n",
      "Gradient Descent(772/999): loss=0.10249882470385326\n",
      "Gradient Descent(773/999): loss=0.10249607052907374\n",
      "Gradient Descent(774/999): loss=0.10249331715409193\n",
      "Gradient Descent(775/999): loss=0.10249056457803252\n",
      "Gradient Descent(776/999): loss=0.1024878128000302\n",
      "Gradient Descent(777/999): loss=0.10248506181923\n",
      "Gradient Descent(778/999): loss=0.10248231163478672\n",
      "Gradient Descent(779/999): loss=0.10247956224586503\n",
      "Gradient Descent(780/999): loss=0.10247681365163912\n",
      "Gradient Descent(781/999): loss=0.10247406585129275\n",
      "Gradient Descent(782/999): loss=0.10247131884401893\n",
      "Gradient Descent(783/999): loss=0.10246857262901986\n",
      "Gradient Descent(784/999): loss=0.10246582720550676\n",
      "Gradient Descent(785/999): loss=0.10246308257269966\n",
      "Gradient Descent(786/999): loss=0.10246033872982742\n",
      "Gradient Descent(787/999): loss=0.10245759567612742\n",
      "Gradient Descent(788/999): loss=0.10245485341084547\n",
      "Gradient Descent(789/999): loss=0.10245211193323588\n",
      "Gradient Descent(790/999): loss=0.1024493712425609\n",
      "Gradient Descent(791/999): loss=0.10244663133809102\n",
      "Gradient Descent(792/999): loss=0.10244389221910453\n",
      "Gradient Descent(793/999): loss=0.10244115388488768\n",
      "Gradient Descent(794/999): loss=0.10243841633473419\n",
      "Gradient Descent(795/999): loss=0.10243567956794551\n",
      "Gradient Descent(796/999): loss=0.1024329435838305\n",
      "Gradient Descent(797/999): loss=0.10243020838170516\n",
      "Gradient Descent(798/999): loss=0.10242747396089297\n",
      "Gradient Descent(799/999): loss=0.10242474032072427\n",
      "Gradient Descent(800/999): loss=0.10242200746053651\n",
      "Gradient Descent(801/999): loss=0.10241927537967384\n",
      "Gradient Descent(802/999): loss=0.1024165440774874\n",
      "Gradient Descent(803/999): loss=0.1024138135533347\n",
      "Gradient Descent(804/999): loss=0.10241108380658004\n",
      "Gradient Descent(805/999): loss=0.10240835483659401\n",
      "Gradient Descent(806/999): loss=0.1024056266427535\n",
      "Gradient Descent(807/999): loss=0.10240289922444178\n",
      "Gradient Descent(808/999): loss=0.10240017258104808\n",
      "Gradient Descent(809/999): loss=0.10239744671196785\n",
      "Gradient Descent(810/999): loss=0.1023947216166023\n",
      "Gradient Descent(811/999): loss=0.10239199729435858\n",
      "Gradient Descent(812/999): loss=0.10238927374464953\n",
      "Gradient Descent(813/999): loss=0.10238655096689381\n",
      "Gradient Descent(814/999): loss=0.10238382896051543\n",
      "Gradient Descent(815/999): loss=0.102381107724944\n",
      "Gradient Descent(816/999): loss=0.10237838725961451\n",
      "Gradient Descent(817/999): loss=0.10237566756396732\n",
      "Gradient Descent(818/999): loss=0.10237294863744788\n",
      "Gradient Descent(819/999): loss=0.10237023047950684\n",
      "Gradient Descent(820/999): loss=0.10236751308959997\n",
      "Gradient Descent(821/999): loss=0.10236479646718796\n",
      "Gradient Descent(822/999): loss=0.10236208061173642\n",
      "Gradient Descent(823/999): loss=0.10235936552271578\n",
      "Gradient Descent(824/999): loss=0.10235665119960118\n",
      "Gradient Descent(825/999): loss=0.10235393764187249\n",
      "Gradient Descent(826/999): loss=0.10235122484901416\n",
      "Gradient Descent(827/999): loss=0.1023485128205151\n",
      "Gradient Descent(828/999): loss=0.10234580155586884\n",
      "Gradient Descent(829/999): loss=0.10234309105457311\n",
      "Gradient Descent(830/999): loss=0.10234038131613003\n",
      "Gradient Descent(831/999): loss=0.10233767234004595\n",
      "Gradient Descent(832/999): loss=0.10233496412583148\n",
      "Gradient Descent(833/999): loss=0.10233225667300123\n",
      "Gradient Descent(834/999): loss=0.10232954998107387\n",
      "Gradient Descent(835/999): loss=0.10232684404957214\n",
      "Gradient Descent(836/999): loss=0.10232413887802258\n",
      "Gradient Descent(837/999): loss=0.10232143446595574\n",
      "Gradient Descent(838/999): loss=0.1023187308129058\n",
      "Gradient Descent(839/999): loss=0.10231602791841077\n",
      "Gradient Descent(840/999): loss=0.10231332578201237\n",
      "Gradient Descent(841/999): loss=0.10231062440325586\n",
      "Gradient Descent(842/999): loss=0.10230792378169014\n",
      "Gradient Descent(843/999): loss=0.10230522391686755\n",
      "Gradient Descent(844/999): loss=0.10230252480834391\n",
      "Gradient Descent(845/999): loss=0.10229982645567844\n",
      "Gradient Descent(846/999): loss=0.10229712885843376\n",
      "Gradient Descent(847/999): loss=0.10229443201617568\n",
      "Gradient Descent(848/999): loss=0.10229173592847327\n",
      "Gradient Descent(849/999): loss=0.10228904059489886\n",
      "Gradient Descent(850/999): loss=0.10228634601502787\n",
      "Gradient Descent(851/999): loss=0.10228365218843885\n",
      "Gradient Descent(852/999): loss=0.10228095911471327\n",
      "Gradient Descent(853/999): loss=0.10227826679343582\n",
      "Gradient Descent(854/999): loss=0.10227557522419385\n",
      "Gradient Descent(855/999): loss=0.10227288440657786\n",
      "Gradient Descent(856/999): loss=0.10227019434018109\n",
      "Gradient Descent(857/999): loss=0.1022675050245996\n",
      "Gradient Descent(858/999): loss=0.10226481645943221\n",
      "Gradient Descent(859/999): loss=0.10226212864428048\n",
      "Gradient Descent(860/999): loss=0.10225944157874872\n",
      "Gradient Descent(861/999): loss=0.10225675526244371\n",
      "Gradient Descent(862/999): loss=0.10225406969497497\n",
      "Gradient Descent(863/999): loss=0.10225138487595455\n",
      "Gradient Descent(864/999): loss=0.10224870080499696\n",
      "Gradient Descent(865/999): loss=0.10224601748171924\n",
      "Gradient Descent(866/999): loss=0.10224333490574093\n",
      "Gradient Descent(867/999): loss=0.1022406530766838\n",
      "Gradient Descent(868/999): loss=0.10223797199417214\n",
      "Gradient Descent(869/999): loss=0.10223529165783249\n",
      "Gradient Descent(870/999): loss=0.10223261206729371\n",
      "Gradient Descent(871/999): loss=0.10222993322218697\n",
      "Gradient Descent(872/999): loss=0.10222725512214549\n",
      "Gradient Descent(873/999): loss=0.10222457776680484\n",
      "Gradient Descent(874/999): loss=0.10222190115580271\n",
      "Gradient Descent(875/999): loss=0.10221922528877886\n",
      "Gradient Descent(876/999): loss=0.10221655016537518\n",
      "Gradient Descent(877/999): loss=0.10221387578523554\n",
      "Gradient Descent(878/999): loss=0.10221120214800598\n",
      "Gradient Descent(879/999): loss=0.10220852925333433\n",
      "Gradient Descent(880/999): loss=0.10220585710087063\n",
      "Gradient Descent(881/999): loss=0.1022031856902666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(882/999): loss=0.10220051502117598\n",
      "Gradient Descent(883/999): loss=0.10219784509325441\n",
      "Gradient Descent(884/999): loss=0.10219517590615929\n",
      "Gradient Descent(885/999): loss=0.1021925074595499\n",
      "Gradient Descent(886/999): loss=0.10218983975308728\n",
      "Gradient Descent(887/999): loss=0.1021871727864342\n",
      "Gradient Descent(888/999): loss=0.10218450655925525\n",
      "Gradient Descent(889/999): loss=0.10218184107121663\n",
      "Gradient Descent(890/999): loss=0.10217917632198618\n",
      "Gradient Descent(891/999): loss=0.10217651231123356\n",
      "Gradient Descent(892/999): loss=0.10217384903862994\n",
      "Gradient Descent(893/999): loss=0.10217118650384809\n",
      "Gradient Descent(894/999): loss=0.10216852470656239\n",
      "Gradient Descent(895/999): loss=0.10216586364644878\n",
      "Gradient Descent(896/999): loss=0.1021632033231847\n",
      "Gradient Descent(897/999): loss=0.10216054373644906\n",
      "Gradient Descent(898/999): loss=0.10215788488592242\n",
      "Gradient Descent(899/999): loss=0.1021552267712866\n",
      "Gradient Descent(900/999): loss=0.10215256939222496\n",
      "Gradient Descent(901/999): loss=0.10214991274842225\n",
      "Gradient Descent(902/999): loss=0.1021472568395647\n",
      "Gradient Descent(903/999): loss=0.10214460166533976\n",
      "Gradient Descent(904/999): loss=0.10214194722543633\n",
      "Gradient Descent(905/999): loss=0.10213929351954461\n",
      "Gradient Descent(906/999): loss=0.1021366405473562\n",
      "Gradient Descent(907/999): loss=0.10213398830856386\n",
      "Gradient Descent(908/999): loss=0.10213133680286172\n",
      "Gradient Descent(909/999): loss=0.1021286860299451\n",
      "Gradient Descent(910/999): loss=0.10212603598951057\n",
      "Gradient Descent(911/999): loss=0.102123386681256\n",
      "Gradient Descent(912/999): loss=0.10212073810488033\n",
      "Gradient Descent(913/999): loss=0.10211809026008377\n",
      "Gradient Descent(914/999): loss=0.10211544314656766\n",
      "Gradient Descent(915/999): loss=0.10211279676403451\n",
      "Gradient Descent(916/999): loss=0.10211015111218782\n",
      "Gradient Descent(917/999): loss=0.10210750619073249\n",
      "Gradient Descent(918/999): loss=0.1021048619993742\n",
      "Gradient Descent(919/999): loss=0.10210221853781994\n",
      "Gradient Descent(920/999): loss=0.10209957580577764\n",
      "Gradient Descent(921/999): loss=0.10209693380295627\n",
      "Gradient Descent(922/999): loss=0.10209429252906592\n",
      "Gradient Descent(923/999): loss=0.10209165198381762\n",
      "Gradient Descent(924/999): loss=0.10208901216692345\n",
      "Gradient Descent(925/999): loss=0.1020863730780964\n",
      "Gradient Descent(926/999): loss=0.10208373471705054\n",
      "Gradient Descent(927/999): loss=0.10208109708350081\n",
      "Gradient Descent(928/999): loss=0.10207846017716311\n",
      "Gradient Descent(929/999): loss=0.10207582399775429\n",
      "Gradient Descent(930/999): loss=0.10207318854499214\n",
      "Gradient Descent(931/999): loss=0.1020705538185952\n",
      "Gradient Descent(932/999): loss=0.10206791981828316\n",
      "Gradient Descent(933/999): loss=0.10206528654377635\n",
      "Gradient Descent(934/999): loss=0.10206265399479605\n",
      "Gradient Descent(935/999): loss=0.10206002217106441\n",
      "Gradient Descent(936/999): loss=0.10205739107230442\n",
      "Gradient Descent(937/999): loss=0.10205476069823986\n",
      "Gradient Descent(938/999): loss=0.10205213104859533\n",
      "Gradient Descent(939/999): loss=0.10204950212309621\n",
      "Gradient Descent(940/999): loss=0.10204687392146873\n",
      "Gradient Descent(941/999): loss=0.10204424644343989\n",
      "Gradient Descent(942/999): loss=0.10204161968873741\n",
      "Gradient Descent(943/999): loss=0.10203899365708974\n",
      "Gradient Descent(944/999): loss=0.10203636834822621\n",
      "Gradient Descent(945/999): loss=0.10203374376187671\n",
      "Gradient Descent(946/999): loss=0.10203111989777204\n",
      "Gradient Descent(947/999): loss=0.10202849675564354\n",
      "Gradient Descent(948/999): loss=0.10202587433522337\n",
      "Gradient Descent(949/999): loss=0.10202325263624432\n",
      "Gradient Descent(950/999): loss=0.1020206316584399\n",
      "Gradient Descent(951/999): loss=0.10201801140154425\n",
      "Gradient Descent(952/999): loss=0.10201539186529228\n",
      "Gradient Descent(953/999): loss=0.10201277304941939\n",
      "Gradient Descent(954/999): loss=0.10201015495366178\n",
      "Gradient Descent(955/999): loss=0.10200753757775617\n",
      "Gradient Descent(956/999): loss=0.10200492092143998\n",
      "Gradient Descent(957/999): loss=0.10200230498445124\n",
      "Gradient Descent(958/999): loss=0.10199968976652857\n",
      "Gradient Descent(959/999): loss=0.10199707526741117\n",
      "Gradient Descent(960/999): loss=0.10199446148683888\n",
      "Gradient Descent(961/999): loss=0.1019918484245521\n",
      "Gradient Descent(962/999): loss=0.10198923608029183\n",
      "Gradient Descent(963/999): loss=0.10198662445379961\n",
      "Gradient Descent(964/999): loss=0.10198401354481752\n",
      "Gradient Descent(965/999): loss=0.10198140335308827\n",
      "Gradient Descent(966/999): loss=0.10197879387835504\n",
      "Gradient Descent(967/999): loss=0.10197618512036156\n",
      "Gradient Descent(968/999): loss=0.10197357707885217\n",
      "Gradient Descent(969/999): loss=0.10197096975357157\n",
      "Gradient Descent(970/999): loss=0.10196836314426519\n",
      "Gradient Descent(971/999): loss=0.10196575725067879\n",
      "Gradient Descent(972/999): loss=0.10196315207255868\n",
      "Gradient Descent(973/999): loss=0.10196054760965172\n",
      "Gradient Descent(974/999): loss=0.1019579438617052\n",
      "Gradient Descent(975/999): loss=0.1019553408284669\n",
      "Gradient Descent(976/999): loss=0.10195273850968511\n",
      "Gradient Descent(977/999): loss=0.10195013690510857\n",
      "Gradient Descent(978/999): loss=0.10194753601448645\n",
      "Gradient Descent(979/999): loss=0.10194493583756842\n",
      "Gradient Descent(980/999): loss=0.1019423363741046\n",
      "Gradient Descent(981/999): loss=0.10193973762384552\n",
      "Gradient Descent(982/999): loss=0.10193713958654217\n",
      "Gradient Descent(983/999): loss=0.10193454226194594\n",
      "Gradient Descent(984/999): loss=0.10193194564980877\n",
      "Gradient Descent(985/999): loss=0.10192934974988285\n",
      "Gradient Descent(986/999): loss=0.10192675456192087\n",
      "Gradient Descent(987/999): loss=0.10192416008567595\n",
      "Gradient Descent(988/999): loss=0.1019215663209016\n",
      "Gradient Descent(989/999): loss=0.1019189732673517\n",
      "Gradient Descent(990/999): loss=0.10191638092478054\n",
      "Gradient Descent(991/999): loss=0.10191378929294284\n",
      "Gradient Descent(992/999): loss=0.1019111983715936\n",
      "Gradient Descent(993/999): loss=0.10190860816048836\n",
      "Gradient Descent(994/999): loss=0.10190601865938287\n",
      "Gradient Descent(995/999): loss=0.10190342986803334\n",
      "Gradient Descent(996/999): loss=0.10190084178619639\n",
      "Gradient Descent(997/999): loss=0.10189825441362883\n",
      "Gradient Descent(998/999): loss=0.10189566775008804\n",
      "Gradient Descent(999/999): loss=0.1018930817953316\n",
      "Gradient Descent(0/999): loss=0.1484316010937374\n",
      "Gradient Descent(1/999): loss=0.14773301101196282\n",
      "Gradient Descent(2/999): loss=0.14704562192738327\n",
      "Gradient Descent(3/999): loss=0.14636925338179332\n",
      "Gradient Descent(4/999): loss=0.14570372782458213\n",
      "Gradient Descent(5/999): loss=0.14504887056588542\n",
      "Gradient Descent(6/999): loss=0.14440450973049243\n",
      "Gradient Descent(7/999): loss=0.14377047621249503\n",
      "Gradient Descent(8/999): loss=0.14314660363066822\n",
      "Gradient Descent(9/999): loss=0.14253272828456867\n",
      "Gradient Descent(10/999): loss=0.1419286891113416\n",
      "Gradient Descent(11/999): loss=0.14133432764322276\n",
      "Gradient Descent(12/999): loss=0.14074948796572584\n",
      "Gradient Descent(13/999): loss=0.14017401667650284\n",
      "Gradient Descent(14/999): loss=0.13960776284486814\n",
      "Gradient Descent(15/999): loss=0.13905057797197384\n",
      "Gradient Descent(16/999): loss=0.1385023159516278\n",
      "Gradient Descent(17/999): loss=0.1379628330317417\n",
      "Gradient Descent(18/999): loss=0.13743198777640153\n",
      "Gradient Descent(19/999): loss=0.13690964102854858\n",
      "Gradient Descent(20/999): loss=0.1363956558732615\n",
      "Gradient Descent(21/999): loss=0.13588989760163048\n",
      "Gradient Descent(22/999): loss=0.13539223367521344\n",
      "Gradient Descent(23/999): loss=0.13490253369106445\n",
      "Gradient Descent(24/999): loss=0.1344206693473262\n",
      "Gradient Descent(25/999): loss=0.1339465144093764\n",
      "Gradient Descent(26/999): loss=0.13347994467652002\n",
      "Gradient Descent(27/999): loss=0.13302083794921774\n",
      "Gradient Descent(28/999): loss=0.13256907399684317\n",
      "Gradient Descent(29/999): loss=0.13212453452595851\n",
      "Gradient Descent(30/999): loss=0.13168710314910242\n",
      "Gradient Descent(31/999): loss=0.1312566653540801\n",
      "Gradient Descent(32/999): loss=0.1308331084737484\n",
      "Gradient Descent(33/999): loss=0.130416321656288\n",
      "Gradient Descent(34/999): loss=0.1300061958359539\n",
      "Gradient Descent(35/999): loss=0.12960262370429804\n",
      "Gradient Descent(36/999): loss=0.129205499681855\n",
      "Gradient Descent(37/999): loss=0.12881471989028412\n",
      "Gradient Descent(38/999): loss=0.1284301821249603\n",
      "Gradient Descent(39/999): loss=0.12805178582800686\n",
      "Gradient Descent(40/999): loss=0.1276794320617623\n",
      "Gradient Descent(41/999): loss=0.12731302348267504\n",
      "Gradient Descent(42/999): loss=0.12695246431561866\n",
      "Gradient Descent(43/999): loss=0.12659766032862074\n",
      "Gradient Descent(44/999): loss=0.1262485188079994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(45/999): loss=0.12590494853389975\n",
      "Gradient Descent(46/999): loss=0.1255668597562251\n",
      "Gradient Descent(47/999): loss=0.12523416417095573\n",
      "Gradient Descent(48/999): loss=0.12490677489684922\n",
      "Gradient Descent(49/999): loss=0.12458460645251616\n",
      "Gradient Descent(50/999): loss=0.12426757473386536\n",
      "Gradient Descent(51/999): loss=0.12395559699191233\n",
      "Gradient Descent(52/999): loss=0.12364859181094547\n",
      "Gradient Descent(53/999): loss=0.12334647908704376\n",
      "Gradient Descent(54/999): loss=0.1230491800069409\n",
      "Gradient Descent(55/999): loss=0.12275661702722998\n",
      "Gradient Descent(56/999): loss=0.12246871385390286\n",
      "Gradient Descent(57/999): loss=0.12218539542221969\n",
      "Gradient Descent(58/999): loss=0.1219065878769024\n",
      "Gradient Descent(59/999): loss=0.12163221855264748\n",
      "Gradient Descent(60/999): loss=0.12136221595495292\n",
      "Gradient Descent(61/999): loss=0.12109650974125358\n",
      "Gradient Descent(62/999): loss=0.12083503070236105\n",
      "Gradient Descent(63/999): loss=0.1205777107442023\n",
      "Gradient Descent(64/999): loss=0.12032448286985248\n",
      "Gradient Descent(65/999): loss=0.12007528116185745\n",
      "Gradient Descent(66/999): loss=0.11983004076484087\n",
      "Gradient Descent(67/999): loss=0.11958869786839181\n",
      "Gradient Descent(68/999): loss=0.11935118969022783\n",
      "Gradient Descent(69/999): loss=0.1191174544596295\n",
      "Gradient Descent(70/999): loss=0.11888743140114176\n",
      "Gradient Descent(71/999): loss=0.11866106071853795\n",
      "Gradient Descent(72/999): loss=0.11843828357904208\n",
      "Gradient Descent(73/999): loss=0.1182190420978055\n",
      "Gradient Descent(74/999): loss=0.1180032793226336\n",
      "Gradient Descent(75/999): loss=0.11779093921895843\n",
      "Gradient Descent(76/999): loss=0.11758196665505385\n",
      "Gradient Descent(77/999): loss=0.11737630738748848\n",
      "Gradient Descent(78/999): loss=0.11717390804681335\n",
      "Gradient Descent(79/999): loss=0.11697471612347983\n",
      "Gradient Descent(80/999): loss=0.11677867995398486\n",
      "Gradient Descent(81/999): loss=0.11658574870723905\n",
      "Gradient Descent(82/999): loss=0.11639587237115466\n",
      "Gradient Descent(83/999): loss=0.11620900173944947\n",
      "Gradient Descent(84/999): loss=0.11602508839866349\n",
      "Gradient Descent(85/999): loss=0.1158440847153848\n",
      "Gradient Descent(86/999): loss=0.11566594382368103\n",
      "Gradient Descent(87/999): loss=0.11549061961273352\n",
      "Gradient Descent(88/999): loss=0.1153180667146708\n",
      "Gradient Descent(89/999): loss=0.1151482404925979\n",
      "Gradient Descent(90/999): loss=0.11498109702881862\n",
      "Gradient Descent(91/999): loss=0.11481659311324756\n",
      "Gradient Descent(92/999): loss=0.11465468623200903\n",
      "Gradient Descent(93/999): loss=0.11449533455621932\n",
      "Gradient Descent(94/999): loss=0.11433849693094986\n",
      "Gradient Descent(95/999): loss=0.11418413286436847\n",
      "Gradient Descent(96/999): loss=0.11403220251705513\n",
      "Gradient Descent(97/999): loss=0.11388266669149018\n",
      "Gradient Descent(98/999): loss=0.11373548682171189\n",
      "Gradient Descent(99/999): loss=0.11359062496314044\n",
      "Gradient Descent(100/999): loss=0.11344804378256625\n",
      "Gradient Descent(101/999): loss=0.11330770654829916\n",
      "Gradient Descent(102/999): loss=0.11316957712047664\n",
      "Gradient Descent(103/999): loss=0.11303361994152811\n",
      "Gradient Descent(104/999): loss=0.11289980002679291\n",
      "Gradient Descent(105/999): loss=0.11276808295528928\n",
      "Gradient Descent(106/999): loss=0.11263843486063228\n",
      "Gradient Descent(107/999): loss=0.11251082242209785\n",
      "Gradient Descent(108/999): loss=0.11238521285583108\n",
      "Gradient Descent(109/999): loss=0.11226157390619594\n",
      "Gradient Descent(110/999): loss=0.11213987383726445\n",
      "Gradient Descent(111/999): loss=0.11202008142444295\n",
      "Gradient Descent(112/999): loss=0.11190216594623342\n",
      "Gradient Descent(113/999): loss=0.11178609717612728\n",
      "Gradient Descent(114/999): loss=0.1116718453746302\n",
      "Gradient Descent(115/999): loss=0.11155938128141493\n",
      "Gradient Descent(116/999): loss=0.11144867610760098\n",
      "Gradient Descent(117/999): loss=0.11133970152815842\n",
      "Gradient Descent(118/999): loss=0.11123242967443424\n",
      "Gradient Descent(119/999): loss=0.1111268331267991\n",
      "Gradient Descent(120/999): loss=0.11102288490741224\n",
      "Gradient Descent(121/999): loss=0.11092055847310377\n",
      "Gradient Descent(122/999): loss=0.11081982770837058\n",
      "Gradient Descent(123/999): loss=0.11072066691848596\n",
      "Gradient Descent(124/999): loss=0.11062305082271977\n",
      "Gradient Descent(125/999): loss=0.11052695454766813\n",
      "Gradient Descent(126/999): loss=0.11043235362069029\n",
      "Gradient Descent(127/999): loss=0.11033922396345154\n",
      "Gradient Descent(128/999): loss=0.11024754188557004\n",
      "Gradient Descent(129/999): loss=0.11015728407836581\n",
      "Gradient Descent(130/999): loss=0.1100684276087109\n",
      "Gradient Descent(131/999): loss=0.1099809499129781\n",
      "Gradient Descent(132/999): loss=0.10989482879108774\n",
      "Gradient Descent(133/999): loss=0.1098100424006498\n",
      "Gradient Descent(134/999): loss=0.10972656925120089\n",
      "Gradient Descent(135/999): loss=0.10964438819853373\n",
      "Gradient Descent(136/999): loss=0.1095634784391185\n",
      "Gradient Descent(137/999): loss=0.1094838195046133\n",
      "Gradient Descent(138/999): loss=0.10940539125646381\n",
      "Gradient Descent(139/999): loss=0.1093281738805895\n",
      "Gradient Descent(140/999): loss=0.10925214788215575\n",
      "Gradient Descent(141/999): loss=0.10917729408042981\n",
      "Gradient Descent(142/999): loss=0.10910359360372018\n",
      "Gradient Descent(143/999): loss=0.10903102788439706\n",
      "Gradient Descent(144/999): loss=0.10895957865399333\n",
      "Gradient Descent(145/999): loss=0.10888922793838415\n",
      "Gradient Descent(146/999): loss=0.10881995805304463\n",
      "Gradient Descent(147/999): loss=0.10875175159838373\n",
      "Gradient Descent(148/999): loss=0.10868459145515297\n",
      "Gradient Descent(149/999): loss=0.10861846077993001\n",
      "Gradient Descent(150/999): loss=0.10855334300067389\n",
      "Gradient Descent(151/999): loss=0.10848922181235289\n",
      "Gradient Descent(152/999): loss=0.10842608117264223\n",
      "Gradient Descent(153/999): loss=0.10836390529769103\n",
      "Gradient Descent(154/999): loss=0.10830267865795819\n",
      "Gradient Descent(155/999): loss=0.10824238597411441\n",
      "Gradient Descent(156/999): loss=0.10818301221301098\n",
      "Gradient Descent(157/999): loss=0.10812454258371301\n",
      "Gradient Descent(158/999): loss=0.1080669625335971\n",
      "Gradient Descent(159/999): loss=0.10801025774451134\n",
      "Gradient Descent(160/999): loss=0.10795441412899755\n",
      "Gradient Descent(161/999): loss=0.10789941782657429\n",
      "Gradient Descent(162/999): loss=0.1078452552000796\n",
      "Gradient Descent(163/999): loss=0.10779191283207298\n",
      "Gradient Descent(164/999): loss=0.10773937752129505\n",
      "Gradient Descent(165/999): loss=0.10768763627918446\n",
      "Gradient Descent(166/999): loss=0.10763667632645076\n",
      "Gradient Descent(167/999): loss=0.10758648508970259\n",
      "Gradient Descent(168/999): loss=0.10753705019813019\n",
      "Gradient Descent(169/999): loss=0.10748835948024134\n",
      "Gradient Descent(170/999): loss=0.10744040096064984\n",
      "Gradient Descent(171/999): loss=0.10739316285691587\n",
      "Gradient Descent(172/999): loss=0.10734663357643703\n",
      "Gradient Descent(173/999): loss=0.10730080171338999\n",
      "Gradient Descent(174/999): loss=0.10725565604572043\n",
      "Gradient Descent(175/999): loss=0.10721118553218278\n",
      "Gradient Descent(176/999): loss=0.1071673793094264\n",
      "Gradient Descent(177/999): loss=0.1071242266891297\n",
      "Gradient Descent(178/999): loss=0.10708171715517976\n",
      "Gradient Descent(179/999): loss=0.10703984036089777\n",
      "Gradient Descent(180/999): loss=0.10699858612630903\n",
      "Gradient Descent(181/999): loss=0.10695794443545689\n",
      "Gradient Descent(182/999): loss=0.10691790543376029\n",
      "Gradient Descent(183/999): loss=0.10687845942541316\n",
      "Gradient Descent(184/999): loss=0.10683959687082674\n",
      "Gradient Descent(185/999): loss=0.10680130838411216\n",
      "Gradient Descent(186/999): loss=0.10676358473060411\n",
      "Gradient Descent(187/999): loss=0.10672641682442435\n",
      "Gradient Descent(188/999): loss=0.10668979572608427\n",
      "Gradient Descent(189/999): loss=0.10665371264012631\n",
      "Gradient Descent(190/999): loss=0.10661815891280335\n",
      "Gradient Descent(191/999): loss=0.10658312602979514\n",
      "Gradient Descent(192/999): loss=0.10654860561396222\n",
      "Gradient Descent(193/999): loss=0.1065145894231353\n",
      "Gradient Descent(194/999): loss=0.10648106934794067\n",
      "Gradient Descent(195/999): loss=0.10644803740966076\n",
      "Gradient Descent(196/999): loss=0.10641548575812855\n",
      "Gradient Descent(197/999): loss=0.10638340666965669\n",
      "Gradient Descent(198/999): loss=0.10635179254499952\n",
      "Gradient Descent(199/999): loss=0.10632063590734794\n",
      "Gradient Descent(200/999): loss=0.10628992940035677\n",
      "Gradient Descent(201/999): loss=0.10625966578620377\n",
      "Gradient Descent(202/999): loss=0.10622983794367995\n",
      "Gradient Descent(203/999): loss=0.10620043886631066\n",
      "Gradient Descent(204/999): loss=0.10617146166050696\n",
      "Gradient Descent(205/999): loss=0.10614289954374681\n",
      "Gradient Descent(206/999): loss=0.10611474584278548\n",
      "Gradient Descent(207/999): loss=0.10608699399189492\n",
      "Gradient Descent(208/999): loss=0.10605963753113117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(209/999): loss=0.10603267010463031\n",
      "Gradient Descent(210/999): loss=0.10600608545893125\n",
      "Gradient Descent(211/999): loss=0.10597987744132578\n",
      "Gradient Descent(212/999): loss=0.10595403999823533\n",
      "Gradient Descent(213/999): loss=0.10592856717361364\n",
      "Gradient Descent(214/999): loss=0.10590345310737541\n",
      "Gradient Descent(215/999): loss=0.10587869203385003\n",
      "Gradient Descent(216/999): loss=0.10585427828026045\n",
      "Gradient Descent(217/999): loss=0.10583020626522638\n",
      "Gradient Descent(218/999): loss=0.10580647049729168\n",
      "Gradient Descent(219/999): loss=0.10578306557347547\n",
      "Gradient Descent(220/999): loss=0.10575998617784675\n",
      "Gradient Descent(221/999): loss=0.10573722708012154\n",
      "Gradient Descent(222/999): loss=0.10571478313428308\n",
      "Gradient Descent(223/999): loss=0.10569264927722416\n",
      "Gradient Descent(224/999): loss=0.10567082052741107\n",
      "Gradient Descent(225/999): loss=0.10564929198356948\n",
      "Gradient Descent(226/999): loss=0.10562805882339098\n",
      "Gradient Descent(227/999): loss=0.10560711630226113\n",
      "Gradient Descent(228/999): loss=0.10558645975200731\n",
      "Gradient Descent(229/999): loss=0.10556608457966725\n",
      "Gradient Descent(230/999): loss=0.10554598626627713\n",
      "Gradient Descent(231/999): loss=0.10552616036567938\n",
      "Gradient Descent(232/999): loss=0.10550660250334956\n",
      "Gradient Descent(233/999): loss=0.10548730837524205\n",
      "Gradient Descent(234/999): loss=0.10546827374665475\n",
      "Gradient Descent(235/999): loss=0.10544949445111163\n",
      "Gradient Descent(236/999): loss=0.1054309663892634\n",
      "Gradient Descent(237/999): loss=0.10541268552780605\n",
      "Gradient Descent(238/999): loss=0.10539464789841677\n",
      "Gradient Descent(239/999): loss=0.10537684959670668\n",
      "Gradient Descent(240/999): loss=0.10535928678119104\n",
      "Gradient Descent(241/999): loss=0.10534195567227547\n",
      "Gradient Descent(242/999): loss=0.10532485255125898\n",
      "Gradient Descent(243/999): loss=0.10530797375935257\n",
      "Gradient Descent(244/999): loss=0.10529131569671413\n",
      "Gradient Descent(245/999): loss=0.10527487482149854\n",
      "Gradient Descent(246/999): loss=0.10525864764892337\n",
      "Gradient Descent(247/999): loss=0.10524263075034919\n",
      "Gradient Descent(248/999): loss=0.1052268207523753\n",
      "Gradient Descent(249/999): loss=0.10521121433594947\n",
      "Gradient Descent(250/999): loss=0.10519580823549239\n",
      "Gradient Descent(251/999): loss=0.105180599238036\n",
      "Gradient Descent(252/999): loss=0.10516558418237587\n",
      "Gradient Descent(253/999): loss=0.10515075995823728\n",
      "Gradient Descent(254/999): loss=0.10513612350545429\n",
      "Gradient Descent(255/999): loss=0.10512167181316273\n",
      "Gradient Descent(256/999): loss=0.10510740191900568\n",
      "Gradient Descent(257/999): loss=0.10509331090835188\n",
      "Gradient Descent(258/999): loss=0.10507939591352677\n",
      "Gradient Descent(259/999): loss=0.10506565411305609\n",
      "Gradient Descent(260/999): loss=0.10505208273092115\n",
      "Gradient Descent(261/999): loss=0.10503867903582674\n",
      "Gradient Descent(262/999): loss=0.10502544034048042\n",
      "Gradient Descent(263/999): loss=0.10501236400088343\n",
      "Gradient Descent(264/999): loss=0.10499944741563338\n",
      "Gradient Descent(265/999): loss=0.10498668802523767\n",
      "Gradient Descent(266/999): loss=0.1049740833114384\n",
      "Gradient Descent(267/999): loss=0.10496163079654788\n",
      "Gradient Descent(268/999): loss=0.104949328042795\n",
      "Gradient Descent(269/999): loss=0.10493717265168209\n",
      "Gradient Descent(270/999): loss=0.10492516226335216\n",
      "Gradient Descent(271/999): loss=0.10491329455596611\n",
      "Gradient Descent(272/999): loss=0.10490156724509063\n",
      "Gradient Descent(273/999): loss=0.10488997808309491\n",
      "Gradient Descent(274/999): loss=0.10487852485855821\n",
      "Gradient Descent(275/999): loss=0.10486720539568603\n",
      "Gradient Descent(276/999): loss=0.10485601755373636\n",
      "Gradient Descent(277/999): loss=0.10484495922645475\n",
      "Gradient Descent(278/999): loss=0.10483402834151873\n",
      "Gradient Descent(279/999): loss=0.10482322285999102\n",
      "Gradient Descent(280/999): loss=0.10481254077578166\n",
      "Gradient Descent(281/999): loss=0.10480198011511874\n",
      "Gradient Descent(282/999): loss=0.10479153893602774\n",
      "Gradient Descent(283/999): loss=0.10478121532781909\n",
      "Gradient Descent(284/999): loss=0.10477100741058425\n",
      "Gradient Descent(285/999): loss=0.10476091333469954\n",
      "Gradient Descent(286/999): loss=0.10475093128033852\n",
      "Gradient Descent(287/999): loss=0.10474105945699148\n",
      "Gradient Descent(288/999): loss=0.10473129610299341\n",
      "Gradient Descent(289/999): loss=0.10472163948505912\n",
      "Gradient Descent(290/999): loss=0.10471208789782593\n",
      "Gradient Descent(291/999): loss=0.10470263966340387\n",
      "Gradient Descent(292/999): loss=0.10469329313093292\n",
      "Gradient Descent(293/999): loss=0.10468404667614774\n",
      "Gradient Descent(294/999): loss=0.1046748987009487\n",
      "Gradient Descent(295/999): loss=0.1046658476329809\n",
      "Gradient Descent(296/999): loss=0.10465689192521885\n",
      "Gradient Descent(297/999): loss=0.10464803005555864\n",
      "Gradient Descent(298/999): loss=0.10463926052641624\n",
      "Gradient Descent(299/999): loss=0.10463058186433266\n",
      "Gradient Descent(300/999): loss=0.10462199261958492\n",
      "Gradient Descent(301/999): loss=0.10461349136580388\n",
      "Gradient Descent(302/999): loss=0.10460507669959779\n",
      "Gradient Descent(303/999): loss=0.1045967472401821\n",
      "Gradient Descent(304/999): loss=0.10458850162901537\n",
      "Gradient Descent(305/999): loss=0.10458033852944072\n",
      "Gradient Descent(306/999): loss=0.10457225662633313\n",
      "Gradient Descent(307/999): loss=0.10456425462575283\n",
      "Gradient Descent(308/999): loss=0.10455633125460359\n",
      "Gradient Descent(309/999): loss=0.10454848526029721\n",
      "Gradient Descent(310/999): loss=0.10454071541042284\n",
      "Gradient Descent(311/999): loss=0.10453302049242205\n",
      "Gradient Descent(312/999): loss=0.10452539931326885\n",
      "Gradient Descent(313/999): loss=0.10451785069915501\n",
      "Gradient Descent(314/999): loss=0.10451037349518048\n",
      "Gradient Descent(315/999): loss=0.10450296656504868\n",
      "Gradient Descent(316/999): loss=0.10449562879076671\n",
      "Gradient Descent(317/999): loss=0.10448835907235053\n",
      "Gradient Descent(318/999): loss=0.10448115632753478\n",
      "Gradient Descent(319/999): loss=0.1044740194914873\n",
      "Gradient Descent(320/999): loss=0.10446694751652803\n",
      "Gradient Descent(321/999): loss=0.10445993937185312\n",
      "Gradient Descent(322/999): loss=0.10445299404326247\n",
      "Gradient Descent(323/999): loss=0.10444611053289263\n",
      "Gradient Descent(324/999): loss=0.10443928785895336\n",
      "Gradient Descent(325/999): loss=0.10443252505546875\n",
      "Gradient Descent(326/999): loss=0.10442582117202237\n",
      "Gradient Descent(327/999): loss=0.10441917527350648\n",
      "Gradient Descent(328/999): loss=0.10441258643987565\n",
      "Gradient Descent(329/999): loss=0.10440605376590378\n",
      "Gradient Descent(330/999): loss=0.1043995763609455\n",
      "Gradient Descent(331/999): loss=0.10439315334870117\n",
      "Gradient Descent(332/999): loss=0.10438678386698574\n",
      "Gradient Descent(333/999): loss=0.10438046706750136\n",
      "Gradient Descent(334/999): loss=0.10437420211561367\n",
      "Gradient Descent(335/999): loss=0.10436798819013154\n",
      "Gradient Descent(336/999): loss=0.10436182448309053\n",
      "Gradient Descent(337/999): loss=0.10435571019953979\n",
      "Gradient Descent(338/999): loss=0.10434964455733242\n",
      "Gradient Descent(339/999): loss=0.10434362678691902\n",
      "Gradient Descent(340/999): loss=0.10433765613114487\n",
      "Gradient Descent(341/999): loss=0.10433173184505024\n",
      "Gradient Descent(342/999): loss=0.10432585319567372\n",
      "Gradient Descent(343/999): loss=0.10432001946185902\n",
      "Gradient Descent(344/999): loss=0.10431422993406488\n",
      "Gradient Descent(345/999): loss=0.10430848391417767\n",
      "Gradient Descent(346/999): loss=0.10430278071532752\n",
      "Gradient Descent(347/999): loss=0.10429711966170692\n",
      "Gradient Descent(348/999): loss=0.10429150008839268\n",
      "Gradient Descent(349/999): loss=0.10428592134117048\n",
      "Gradient Descent(350/999): loss=0.10428038277636226\n",
      "Gradient Descent(351/999): loss=0.10427488376065662\n",
      "Gradient Descent(352/999): loss=0.10426942367094165\n",
      "Gradient Descent(353/999): loss=0.10426400189414063\n",
      "Gradient Descent(354/999): loss=0.10425861782705037\n",
      "Gradient Descent(355/999): loss=0.10425327087618212\n",
      "Gradient Descent(356/999): loss=0.10424796045760501\n",
      "Gradient Descent(357/999): loss=0.10424268599679211\n",
      "Gradient Descent(358/999): loss=0.10423744692846884\n",
      "Gradient Descent(359/999): loss=0.104232242696464\n",
      "Gradient Descent(360/999): loss=0.10422707275356301\n",
      "Gradient Descent(361/999): loss=0.10422193656136355\n",
      "Gradient Descent(362/999): loss=0.10421683359013376\n",
      "Gradient Descent(363/999): loss=0.10421176331867237\n",
      "Gradient Descent(364/999): loss=0.10420672523417131\n",
      "Gradient Descent(365/999): loss=0.1042017188320805\n",
      "Gradient Descent(366/999): loss=0.10419674361597478\n",
      "Gradient Descent(367/999): loss=0.10419179909742295\n",
      "Gradient Descent(368/999): loss=0.10418688479585908\n",
      "Gradient Descent(369/999): loss=0.10418200023845571\n",
      "Gradient Descent(370/999): loss=0.10417714495999912\n",
      "Gradient Descent(371/999): loss=0.10417231850276679\n",
      "Gradient Descent(372/999): loss=0.10416752041640658\n",
      "Gradient Descent(373/999): loss=0.10416275025781803\n",
      "Gradient Descent(374/999): loss=0.10415800759103554\n",
      "Gradient Descent(375/999): loss=0.10415329198711337\n",
      "Gradient Descent(376/999): loss=0.10414860302401252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(377/999): loss=0.10414394028648952\n",
      "Gradient Descent(378/999): loss=0.10413930336598692\n",
      "Gradient Descent(379/999): loss=0.10413469186052543\n",
      "Gradient Descent(380/999): loss=0.10413010537459821\n",
      "Gradient Descent(381/999): loss=0.10412554351906635\n",
      "Gradient Descent(382/999): loss=0.10412100591105627\n",
      "Gradient Descent(383/999): loss=0.10411649217385895\n",
      "Gradient Descent(384/999): loss=0.1041120019368304\n",
      "Gradient Descent(385/999): loss=0.10410753483529414\n",
      "Gradient Descent(386/999): loss=0.10410309051044493\n",
      "Gradient Descent(387/999): loss=0.1040986686092541\n",
      "Gradient Descent(388/999): loss=0.1040942687843767\n",
      "Gradient Descent(389/999): loss=0.10408989069405968\n",
      "Gradient Descent(390/999): loss=0.10408553400205203\n",
      "Gradient Descent(391/999): loss=0.10408119837751593\n",
      "Gradient Descent(392/999): loss=0.10407688349493957\n",
      "Gradient Descent(393/999): loss=0.1040725890340515\n",
      "Gradient Descent(394/999): loss=0.10406831467973597\n",
      "Gradient Descent(395/999): loss=0.10406406012194999\n",
      "Gradient Descent(396/999): loss=0.10405982505564162\n",
      "Gradient Descent(397/999): loss=0.1040556091806695\n",
      "Gradient Descent(398/999): loss=0.10405141220172368\n",
      "Gradient Descent(399/999): loss=0.10404723382824793\n",
      "Gradient Descent(400/999): loss=0.10404307377436295\n",
      "Gradient Descent(401/999): loss=0.10403893175879117\n",
      "Gradient Descent(402/999): loss=0.10403480750478258\n",
      "Gradient Descent(403/999): loss=0.1040307007400418\n",
      "Gradient Descent(404/999): loss=0.10402661119665611\n",
      "Gradient Descent(405/999): loss=0.10402253861102508\n",
      "Gradient Descent(406/999): loss=0.1040184827237911\n",
      "Gradient Descent(407/999): loss=0.10401444327977079\n",
      "Gradient Descent(408/999): loss=0.10401042002788795\n",
      "Gradient Descent(409/999): loss=0.10400641272110722\n",
      "Gradient Descent(410/999): loss=0.1040024211163692\n",
      "Gradient Descent(411/999): loss=0.10399844497452618\n",
      "Gradient Descent(412/999): loss=0.1039944840602792\n",
      "Gradient Descent(413/999): loss=0.10399053814211608\n",
      "Gradient Descent(414/999): loss=0.10398660699225024\n",
      "Gradient Descent(415/999): loss=0.10398269038656094\n",
      "Gradient Descent(416/999): loss=0.10397878810453401\n",
      "Gradient Descent(417/999): loss=0.10397489992920378\n",
      "Gradient Descent(418/999): loss=0.10397102564709593\n",
      "Gradient Descent(419/999): loss=0.1039671650481713\n",
      "Gradient Descent(420/999): loss=0.10396331792577036\n",
      "Gradient Descent(421/999): loss=0.10395948407655906\n",
      "Gradient Descent(422/999): loss=0.10395566330047483\n",
      "Gradient Descent(423/999): loss=0.10395185540067434\n",
      "Gradient Descent(424/999): loss=0.10394806018348125\n",
      "Gradient Descent(425/999): loss=0.10394427745833544\n",
      "Gradient Descent(426/999): loss=0.10394050703774255\n",
      "Gradient Descent(427/999): loss=0.10393674873722489\n",
      "Gradient Descent(428/999): loss=0.10393300237527257\n",
      "Gradient Descent(429/999): loss=0.10392926777329586\n",
      "Gradient Descent(430/999): loss=0.10392554475557807\n",
      "Gradient Descent(431/999): loss=0.10392183314922916\n",
      "Gradient Descent(432/999): loss=0.10391813278414046\n",
      "Gradient Descent(433/999): loss=0.10391444349293956\n",
      "Gradient Descent(434/999): loss=0.10391076511094643\n",
      "Gradient Descent(435/999): loss=0.10390709747613\n",
      "Gradient Descent(436/999): loss=0.10390344042906534\n",
      "Gradient Descent(437/999): loss=0.10389979381289188\n",
      "Gradient Descent(438/999): loss=0.10389615747327205\n",
      "Gradient Descent(439/999): loss=0.1038925312583504\n",
      "Gradient Descent(440/999): loss=0.10388891501871397\n",
      "Gradient Descent(441/999): loss=0.10388530860735264\n",
      "Gradient Descent(442/999): loss=0.10388171187962059\n",
      "Gradient Descent(443/999): loss=0.10387812469319807\n",
      "Gradient Descent(444/999): loss=0.1038745469080541\n",
      "Gradient Descent(445/999): loss=0.10387097838640941\n",
      "Gradient Descent(446/999): loss=0.10386741899270036\n",
      "Gradient Descent(447/999): loss=0.10386386859354296\n",
      "Gradient Descent(448/999): loss=0.1038603270576981\n",
      "Gradient Descent(449/999): loss=0.10385679425603672\n",
      "Gradient Descent(450/999): loss=0.10385327006150588\n",
      "Gradient Descent(451/999): loss=0.1038497543490954\n",
      "Gradient Descent(452/999): loss=0.10384624699580482\n",
      "Gradient Descent(453/999): loss=0.10384274788061122\n",
      "Gradient Descent(454/999): loss=0.10383925688443708\n",
      "Gradient Descent(455/999): loss=0.10383577389011915\n",
      "Gradient Descent(456/999): loss=0.10383229878237753\n",
      "Gradient Descent(457/999): loss=0.1038288314477853\n",
      "Gradient Descent(458/999): loss=0.10382537177473883\n",
      "Gradient Descent(459/999): loss=0.10382191965342807\n",
      "Gradient Descent(460/999): loss=0.10381847497580807\n",
      "Gradient Descent(461/999): loss=0.10381503763557015\n",
      "Gradient Descent(462/999): loss=0.10381160752811425\n",
      "Gradient Descent(463/999): loss=0.1038081845505212\n",
      "Gradient Descent(464/999): loss=0.10380476860152568\n",
      "Gradient Descent(465/999): loss=0.10380135958148963\n",
      "Gradient Descent(466/999): loss=0.10379795739237596\n",
      "Gradient Descent(467/999): loss=0.10379456193772281\n",
      "Gradient Descent(468/999): loss=0.10379117312261814\n",
      "Gradient Descent(469/999): loss=0.10378779085367479\n",
      "Gradient Descent(470/999): loss=0.10378441503900593\n",
      "Gradient Descent(471/999): loss=0.10378104558820073\n",
      "Gradient Descent(472/999): loss=0.10377768241230079\n",
      "Gradient Descent(473/999): loss=0.10377432542377668\n",
      "Gradient Descent(474/999): loss=0.10377097453650479\n",
      "Gradient Descent(475/999): loss=0.1037676296657449\n",
      "Gradient Descent(476/999): loss=0.1037642907281177\n",
      "Gradient Descent(477/999): loss=0.10376095764158291\n",
      "Gradient Descent(478/999): loss=0.10375763032541777\n",
      "Gradient Descent(479/999): loss=0.1037543087001958\n",
      "Gradient Descent(480/999): loss=0.10375099268776582\n",
      "Gradient Descent(481/999): loss=0.10374768221123155\n",
      "Gradient Descent(482/999): loss=0.10374437719493126\n",
      "Gradient Descent(483/999): loss=0.10374107756441804\n",
      "Gradient Descent(484/999): loss=0.10373778324643995\n",
      "Gradient Descent(485/999): loss=0.10373449416892105\n",
      "Gradient Descent(486/999): loss=0.10373121026094234\n",
      "Gradient Descent(487/999): loss=0.10372793145272312\n",
      "Gradient Descent(488/999): loss=0.10372465767560266\n",
      "Gradient Descent(489/999): loss=0.10372138886202216\n",
      "Gradient Descent(490/999): loss=0.10371812494550696\n",
      "Gradient Descent(491/999): loss=0.10371486586064915\n",
      "Gradient Descent(492/999): loss=0.10371161154309028\n",
      "Gradient Descent(493/999): loss=0.10370836192950456\n",
      "Gradient Descent(494/999): loss=0.10370511695758214\n",
      "Gradient Descent(495/999): loss=0.1037018765660128\n",
      "Gradient Descent(496/999): loss=0.10369864069446982\n",
      "Gradient Descent(497/999): loss=0.1036954092835941\n",
      "Gradient Descent(498/999): loss=0.10369218227497866\n",
      "Gradient Descent(499/999): loss=0.10368895961115315\n",
      "Gradient Descent(500/999): loss=0.10368574123556902\n",
      "Gradient Descent(501/999): loss=0.10368252709258434\n",
      "Gradient Descent(502/999): loss=0.1036793171274495\n",
      "Gradient Descent(503/999): loss=0.10367611128629256\n",
      "Gradient Descent(504/999): loss=0.10367290951610533\n",
      "Gradient Descent(505/999): loss=0.10366971176472937\n",
      "Gradient Descent(506/999): loss=0.10366651798084228\n",
      "Gradient Descent(507/999): loss=0.10366332811394426\n",
      "Gradient Descent(508/999): loss=0.10366014211434481\n",
      "Gradient Descent(509/999): loss=0.10365695993314976\n",
      "Gradient Descent(510/999): loss=0.10365378152224843\n",
      "Gradient Descent(511/999): loss=0.10365060683430094\n",
      "Gradient Descent(512/999): loss=0.10364743582272591\n",
      "Gradient Descent(513/999): loss=0.1036442684416881\n",
      "Gradient Descent(514/999): loss=0.10364110464608647\n",
      "Gradient Descent(515/999): loss=0.10363794439154239\n",
      "Gradient Descent(516/999): loss=0.10363478763438794\n",
      "Gradient Descent(517/999): loss=0.10363163433165443\n",
      "Gradient Descent(518/999): loss=0.10362848444106121\n",
      "Gradient Descent(519/999): loss=0.10362533792100452\n",
      "Gradient Descent(520/999): loss=0.10362219473054674\n",
      "Gradient Descent(521/999): loss=0.10361905482940538\n",
      "Gradient Descent(522/999): loss=0.1036159181779428\n",
      "Gradient Descent(523/999): loss=0.10361278473715563\n",
      "Gradient Descent(524/999): loss=0.10360965446866466\n",
      "Gradient Descent(525/999): loss=0.10360652733470478\n",
      "Gradient Descent(526/999): loss=0.103603403298115\n",
      "Gradient Descent(527/999): loss=0.10360028232232885\n",
      "Gradient Descent(528/999): loss=0.1035971643713647\n",
      "Gradient Descent(529/999): loss=0.10359404940981627\n",
      "Gradient Descent(530/999): loss=0.10359093740284363\n",
      "Gradient Descent(531/999): loss=0.10358782831616385\n",
      "Gradient Descent(532/999): loss=0.10358472211604199\n",
      "Gradient Descent(533/999): loss=0.10358161876928247\n",
      "Gradient Descent(534/999): loss=0.10357851824322019\n",
      "Gradient Descent(535/999): loss=0.10357542050571221\n",
      "Gradient Descent(536/999): loss=0.10357232552512903\n",
      "Gradient Descent(537/999): loss=0.10356923327034662\n",
      "Gradient Descent(538/999): loss=0.10356614371073804\n",
      "Gradient Descent(539/999): loss=0.10356305681616562\n",
      "Gradient Descent(540/999): loss=0.10355997255697294\n",
      "Gradient Descent(541/999): loss=0.10355689090397723\n",
      "Gradient Descent(542/999): loss=0.1035538118284615\n",
      "Gradient Descent(543/999): loss=0.10355073530216724\n",
      "Gradient Descent(544/999): loss=0.10354766129728701\n",
      "Gradient Descent(545/999): loss=0.10354458978645709\n",
      "Gradient Descent(546/999): loss=0.10354152074275043\n",
      "Gradient Descent(547/999): loss=0.10353845413966953\n",
      "Gradient Descent(548/999): loss=0.1035353899511395\n",
      "Gradient Descent(549/999): loss=0.10353232815150147\n",
      "Gradient Descent(550/999): loss=0.10352926871550554\n",
      "Gradient Descent(551/999): loss=0.10352621161830447\n",
      "Gradient Descent(552/999): loss=0.10352315683544715\n",
      "Gradient Descent(553/999): loss=0.10352010434287207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(554/999): loss=0.1035170541169012\n",
      "Gradient Descent(555/999): loss=0.10351400613423373\n",
      "Gradient Descent(556/999): loss=0.10351096037194009\n",
      "Gradient Descent(557/999): loss=0.10350791680745583\n",
      "Gradient Descent(558/999): loss=0.1035048754185758\n",
      "Gradient Descent(559/999): loss=0.10350183618344846\n",
      "Gradient Descent(560/999): loss=0.10349879908057001\n",
      "Gradient Descent(561/999): loss=0.10349576408877888\n",
      "Gradient Descent(562/999): loss=0.10349273118725028\n",
      "Gradient Descent(563/999): loss=0.1034897003554906\n",
      "Gradient Descent(564/999): loss=0.1034866715733324\n",
      "Gradient Descent(565/999): loss=0.10348364482092864\n",
      "Gradient Descent(566/999): loss=0.10348062007874805\n",
      "Gradient Descent(567/999): loss=0.10347759732756967\n",
      "Gradient Descent(568/999): loss=0.1034745765484781\n",
      "Gradient Descent(569/999): loss=0.10347155772285838\n",
      "Gradient Descent(570/999): loss=0.10346854083239133\n",
      "Gradient Descent(571/999): loss=0.1034655258590486\n",
      "Gradient Descent(572/999): loss=0.1034625127850882\n",
      "Gradient Descent(573/999): loss=0.1034595015930497\n",
      "Gradient Descent(574/999): loss=0.10345649226574971\n",
      "Gradient Descent(575/999): loss=0.10345348478627764\n",
      "Gradient Descent(576/999): loss=0.103450479137991\n",
      "Gradient Descent(577/999): loss=0.10344747530451123\n",
      "Gradient Descent(578/999): loss=0.10344447326971948\n",
      "Gradient Descent(579/999): loss=0.10344147301775243\n",
      "Gradient Descent(580/999): loss=0.10343847453299806\n",
      "Gradient Descent(581/999): loss=0.10343547780009163\n",
      "Gradient Descent(582/999): loss=0.10343248280391197\n",
      "Gradient Descent(583/999): loss=0.10342948952957698\n",
      "Gradient Descent(584/999): loss=0.10342649796244044\n",
      "Gradient Descent(585/999): loss=0.10342350808808776\n",
      "Gradient Descent(586/999): loss=0.10342051989233236\n",
      "Gradient Descent(587/999): loss=0.10341753336121211\n",
      "Gradient Descent(588/999): loss=0.10341454848098554\n",
      "Gradient Descent(589/999): loss=0.1034115652381285\n",
      "Gradient Descent(590/999): loss=0.10340858361933045\n",
      "Gradient Descent(591/999): loss=0.10340560361149106\n",
      "Gradient Descent(592/999): loss=0.103402625201717\n",
      "Gradient Descent(593/999): loss=0.10339964837731841\n",
      "Gradient Descent(594/999): loss=0.10339667312580567\n",
      "Gradient Descent(595/999): loss=0.1033936994348862\n",
      "Gradient Descent(596/999): loss=0.10339072729246139\n",
      "Gradient Descent(597/999): loss=0.10338775668662314\n",
      "Gradient Descent(598/999): loss=0.10338478760565135\n",
      "Gradient Descent(599/999): loss=0.10338182003801022\n",
      "Gradient Descent(600/999): loss=0.10337885397234582\n",
      "Gradient Descent(601/999): loss=0.10337588939748293\n",
      "Gradient Descent(602/999): loss=0.10337292630242226\n",
      "Gradient Descent(603/999): loss=0.1033699646763374\n",
      "Gradient Descent(604/999): loss=0.1033670045085723\n",
      "Gradient Descent(605/999): loss=0.10336404578863843\n",
      "Gradient Descent(606/999): loss=0.10336108850621203\n",
      "Gradient Descent(607/999): loss=0.1033581326511315\n",
      "Gradient Descent(608/999): loss=0.1033551782133948\n",
      "Gradient Descent(609/999): loss=0.10335222518315682\n",
      "Gradient Descent(610/999): loss=0.10334927355072702\n",
      "Gradient Descent(611/999): loss=0.10334632330656664\n",
      "Gradient Descent(612/999): loss=0.10334337444128658\n",
      "Gradient Descent(613/999): loss=0.10334042694564478\n",
      "Gradient Descent(614/999): loss=0.10333748081054395\n",
      "Gradient Descent(615/999): loss=0.1033345360270292\n",
      "Gradient Descent(616/999): loss=0.10333159258628571\n",
      "Gradient Descent(617/999): loss=0.10332865047963659\n",
      "Gradient Descent(618/999): loss=0.10332570969854055\n",
      "Gradient Descent(619/999): loss=0.10332277023458976\n",
      "Gradient Descent(620/999): loss=0.10331983207950765\n",
      "Gradient Descent(621/999): loss=0.10331689522514695\n",
      "Gradient Descent(622/999): loss=0.10331395966348737\n",
      "Gradient Descent(623/999): loss=0.1033110253866338\n",
      "Gradient Descent(624/999): loss=0.1033080923868141\n",
      "Gradient Descent(625/999): loss=0.10330516065637728\n",
      "Gradient Descent(626/999): loss=0.10330223018779136\n",
      "Gradient Descent(627/999): loss=0.10329930097364165\n",
      "Gradient Descent(628/999): loss=0.10329637300662878\n",
      "Gradient Descent(629/999): loss=0.10329344627956674\n",
      "Gradient Descent(630/999): loss=0.10329052078538126\n",
      "Gradient Descent(631/999): loss=0.10328759651710792\n",
      "Gradient Descent(632/999): loss=0.1032846734678902\n",
      "Gradient Descent(633/999): loss=0.10328175163097815\n",
      "Gradient Descent(634/999): loss=0.10327883099972622\n",
      "Gradient Descent(635/999): loss=0.1032759115675919\n",
      "Gradient Descent(636/999): loss=0.10327299332813392\n",
      "Gradient Descent(637/999): loss=0.10327007627501067\n",
      "Gradient Descent(638/999): loss=0.10326716040197863\n",
      "Gradient Descent(639/999): loss=0.10326424570289061\n",
      "Gradient Descent(640/999): loss=0.10326133217169448\n",
      "Gradient Descent(641/999): loss=0.10325841980243136\n",
      "Gradient Descent(642/999): loss=0.10325550858923435\n",
      "Gradient Descent(643/999): loss=0.10325259852632689\n",
      "Gradient Descent(644/999): loss=0.10324968960802135\n",
      "Gradient Descent(645/999): loss=0.10324678182871767\n",
      "Gradient Descent(646/999): loss=0.10324387518290186\n",
      "Gradient Descent(647/999): loss=0.10324096966514473\n",
      "Gradient Descent(648/999): loss=0.10323806527010027\n",
      "Gradient Descent(649/999): loss=0.10323516199250461\n",
      "Gradient Descent(650/999): loss=0.10323225982717456\n",
      "Gradient Descent(651/999): loss=0.10322935876900628\n",
      "Gradient Descent(652/999): loss=0.1032264588129741\n",
      "Gradient Descent(653/999): loss=0.10322355995412917\n",
      "Gradient Descent(654/999): loss=0.1032206621875983\n",
      "Gradient Descent(655/999): loss=0.10321776550858268\n",
      "Gradient Descent(656/999): loss=0.10321486991235669\n",
      "Gradient Descent(657/999): loss=0.10321197539426677\n",
      "Gradient Descent(658/999): loss=0.1032090819497302\n",
      "Gradient Descent(659/999): loss=0.10320618957423403\n",
      "Gradient Descent(660/999): loss=0.1032032982633339\n",
      "Gradient Descent(661/999): loss=0.10320040801265294\n",
      "Gradient Descent(662/999): loss=0.10319751881788071\n",
      "Gradient Descent(663/999): loss=0.10319463067477215\n",
      "Gradient Descent(664/999): loss=0.10319174357914641\n",
      "Gradient Descent(665/999): loss=0.103188857526886\n",
      "Gradient Descent(666/999): loss=0.1031859725139356\n",
      "Gradient Descent(667/999): loss=0.10318308853630122\n",
      "Gradient Descent(668/999): loss=0.10318020559004905\n",
      "Gradient Descent(669/999): loss=0.10317732367130458\n",
      "Gradient Descent(670/999): loss=0.10317444277625167\n",
      "Gradient Descent(671/999): loss=0.10317156290113152\n",
      "Gradient Descent(672/999): loss=0.10316868404224187\n",
      "Gradient Descent(673/999): loss=0.10316580619593599\n",
      "Gradient Descent(674/999): loss=0.10316292935862184\n",
      "Gradient Descent(675/999): loss=0.10316005352676114\n",
      "Gradient Descent(676/999): loss=0.10315717869686851\n",
      "Gradient Descent(677/999): loss=0.10315430486551075\n",
      "Gradient Descent(678/999): loss=0.10315143202930582\n",
      "Gradient Descent(679/999): loss=0.1031485601849221\n",
      "Gradient Descent(680/999): loss=0.10314568932907754\n",
      "Gradient Descent(681/999): loss=0.10314281945853901\n",
      "Gradient Descent(682/999): loss=0.10313995057012125\n",
      "Gradient Descent(683/999): loss=0.10313708266068634\n",
      "Gradient Descent(684/999): loss=0.10313421572714286\n",
      "Gradient Descent(685/999): loss=0.10313134976644507\n",
      "Gradient Descent(686/999): loss=0.10312848477559224\n",
      "Gradient Descent(687/999): loss=0.10312562075162794\n",
      "Gradient Descent(688/999): loss=0.10312275769163935\n",
      "Gradient Descent(689/999): loss=0.10311989559275649\n",
      "Gradient Descent(690/999): loss=0.10311703445215147\n",
      "Gradient Descent(691/999): loss=0.103114174267038\n",
      "Gradient Descent(692/999): loss=0.10311131503467062\n",
      "Gradient Descent(693/999): loss=0.10310845675234404\n",
      "Gradient Descent(694/999): loss=0.10310559941739245\n",
      "Gradient Descent(695/999): loss=0.103102743027189\n",
      "Gradient Descent(696/999): loss=0.103099887579145\n",
      "Gradient Descent(697/999): loss=0.10309703307070958\n",
      "Gradient Descent(698/999): loss=0.10309417949936883\n",
      "Gradient Descent(699/999): loss=0.10309132686264526\n",
      "Gradient Descent(700/999): loss=0.10308847515809727\n",
      "Gradient Descent(701/999): loss=0.10308562438331861\n",
      "Gradient Descent(702/999): loss=0.10308277453593774\n",
      "Gradient Descent(703/999): loss=0.10307992561361717\n",
      "Gradient Descent(704/999): loss=0.10307707761405316\n",
      "Gradient Descent(705/999): loss=0.10307423053497498\n",
      "Gradient Descent(706/999): loss=0.1030713843741445\n",
      "Gradient Descent(707/999): loss=0.10306853912935557\n",
      "Gradient Descent(708/999): loss=0.10306569479843353\n",
      "Gradient Descent(709/999): loss=0.10306285137923474\n",
      "Gradient Descent(710/999): loss=0.10306000886964611\n",
      "Gradient Descent(711/999): loss=0.10305716726758449\n",
      "Gradient Descent(712/999): loss=0.10305432657099631\n",
      "Gradient Descent(713/999): loss=0.10305148677785703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(714/999): loss=0.10304864788617071\n",
      "Gradient Descent(715/999): loss=0.10304580989396954\n",
      "Gradient Descent(716/999): loss=0.10304297279931339\n",
      "Gradient Descent(717/999): loss=0.10304013660028935\n",
      "Gradient Descent(718/999): loss=0.10303730129501132\n",
      "Gradient Descent(719/999): loss=0.10303446688161962\n",
      "Gradient Descent(720/999): loss=0.10303163335828042\n",
      "Gradient Descent(721/999): loss=0.10302880072318543\n",
      "Gradient Descent(722/999): loss=0.10302596897455152\n",
      "Gradient Descent(723/999): loss=0.10302313811062029\n",
      "Gradient Descent(724/999): loss=0.10302030812965755\n",
      "Gradient Descent(725/999): loss=0.10301747902995317\n",
      "Gradient Descent(726/999): loss=0.10301465080982049\n",
      "Gradient Descent(727/999): loss=0.10301182346759598\n",
      "Gradient Descent(728/999): loss=0.10300899700163896\n",
      "Gradient Descent(729/999): loss=0.10300617141033121\n",
      "Gradient Descent(730/999): loss=0.1030033466920765\n",
      "Gradient Descent(731/999): loss=0.1030005228453003\n",
      "Gradient Descent(732/999): loss=0.10299769986844952\n",
      "Gradient Descent(733/999): loss=0.10299487775999201\n",
      "Gradient Descent(734/999): loss=0.10299205651841645\n",
      "Gradient Descent(735/999): loss=0.10298923614223167\n",
      "Gradient Descent(736/999): loss=0.10298641662996666\n",
      "Gradient Descent(737/999): loss=0.10298359798017004\n",
      "Gradient Descent(738/999): loss=0.10298078019140992\n",
      "Gradient Descent(739/999): loss=0.10297796326227339\n",
      "Gradient Descent(740/999): loss=0.10297514719136636\n",
      "Gradient Descent(741/999): loss=0.10297233197731319\n",
      "Gradient Descent(742/999): loss=0.10296951761875635\n",
      "Gradient Descent(743/999): loss=0.10296670411435636\n",
      "Gradient Descent(744/999): loss=0.10296389146279118\n",
      "Gradient Descent(745/999): loss=0.10296107966275614\n",
      "Gradient Descent(746/999): loss=0.1029582687129636\n",
      "Gradient Descent(747/999): loss=0.1029554586121427\n",
      "Gradient Descent(748/999): loss=0.10295264935903908\n",
      "Gradient Descent(749/999): loss=0.10294984095241451\n",
      "Gradient Descent(750/999): loss=0.10294703339104687\n",
      "Gradient Descent(751/999): loss=0.10294422667372974\n",
      "Gradient Descent(752/999): loss=0.10294142079927197\n",
      "Gradient Descent(753/999): loss=0.1029386157664979\n",
      "Gradient Descent(754/999): loss=0.10293581157424664\n",
      "Gradient Descent(755/999): loss=0.10293300822137211\n",
      "Gradient Descent(756/999): loss=0.10293020570674272\n",
      "Gradient Descent(757/999): loss=0.10292740402924115\n",
      "Gradient Descent(758/999): loss=0.10292460318776403\n",
      "Gradient Descent(759/999): loss=0.10292180318122195\n",
      "Gradient Descent(760/999): loss=0.10291900400853898\n",
      "Gradient Descent(761/999): loss=0.10291620566865256\n",
      "Gradient Descent(762/999): loss=0.10291340816051331\n",
      "Gradient Descent(763/999): loss=0.10291061148308482\n",
      "Gradient Descent(764/999): loss=0.1029078156353434\n",
      "Gradient Descent(765/999): loss=0.10290502061627782\n",
      "Gradient Descent(766/999): loss=0.10290222642488939\n",
      "Gradient Descent(767/999): loss=0.10289943306019131\n",
      "Gradient Descent(768/999): loss=0.10289664052120884\n",
      "Gradient Descent(769/999): loss=0.102893848806979\n",
      "Gradient Descent(770/999): loss=0.10289105791655037\n",
      "Gradient Descent(771/999): loss=0.1028882678489829\n",
      "Gradient Descent(772/999): loss=0.10288547860334772\n",
      "Gradient Descent(773/999): loss=0.10288269017872696\n",
      "Gradient Descent(774/999): loss=0.10287990257421369\n",
      "Gradient Descent(775/999): loss=0.1028771157889115\n",
      "Gradient Descent(776/999): loss=0.10287432982193467\n",
      "Gradient Descent(777/999): loss=0.10287154467240756\n",
      "Gradient Descent(778/999): loss=0.10286876033946489\n",
      "Gradient Descent(779/999): loss=0.10286597682225135\n",
      "Gradient Descent(780/999): loss=0.10286319411992143\n",
      "Gradient Descent(781/999): loss=0.10286041223163928\n",
      "Gradient Descent(782/999): loss=0.10285763115657867\n",
      "Gradient Descent(783/999): loss=0.10285485089392268\n",
      "Gradient Descent(784/999): loss=0.10285207144286365\n",
      "Gradient Descent(785/999): loss=0.10284929280260295\n",
      "Gradient Descent(786/999): loss=0.10284651497235098\n",
      "Gradient Descent(787/999): loss=0.10284373795132686\n",
      "Gradient Descent(788/999): loss=0.10284096173875837\n",
      "Gradient Descent(789/999): loss=0.10283818633388181\n",
      "Gradient Descent(790/999): loss=0.10283541173594186\n",
      "Gradient Descent(791/999): loss=0.1028326379441915\n",
      "Gradient Descent(792/999): loss=0.10282986495789173\n",
      "Gradient Descent(793/999): loss=0.10282709277631161\n",
      "Gradient Descent(794/999): loss=0.10282432139872803\n",
      "Gradient Descent(795/999): loss=0.10282155082442555\n",
      "Gradient Descent(796/999): loss=0.1028187810526965\n",
      "Gradient Descent(797/999): loss=0.10281601208284051\n",
      "Gradient Descent(798/999): loss=0.10281324391416471\n",
      "Gradient Descent(799/999): loss=0.10281047654598345\n",
      "Gradient Descent(800/999): loss=0.10280770997761815\n",
      "Gradient Descent(801/999): loss=0.1028049442083974\n",
      "Gradient Descent(802/999): loss=0.1028021792376565\n",
      "Gradient Descent(803/999): loss=0.10279941506473769\n",
      "Gradient Descent(804/999): loss=0.10279665168898992\n",
      "Gradient Descent(805/999): loss=0.10279388910976857\n",
      "Gradient Descent(806/999): loss=0.10279112732643572\n",
      "Gradient Descent(807/999): loss=0.10278836633835964\n",
      "Gradient Descent(808/999): loss=0.10278560614491496\n",
      "Gradient Descent(809/999): loss=0.10278284674548249\n",
      "Gradient Descent(810/999): loss=0.10278008813944906\n",
      "Gradient Descent(811/999): loss=0.1027773303262076\n",
      "Gradient Descent(812/999): loss=0.1027745733051568\n",
      "Gradient Descent(813/999): loss=0.10277181707570118\n",
      "Gradient Descent(814/999): loss=0.10276906163725101\n",
      "Gradient Descent(815/999): loss=0.10276630698922211\n",
      "Gradient Descent(816/999): loss=0.10276355313103587\n",
      "Gradient Descent(817/999): loss=0.1027608000621191\n",
      "Gradient Descent(818/999): loss=0.10275804778190392\n",
      "Gradient Descent(819/999): loss=0.10275529628982775\n",
      "Gradient Descent(820/999): loss=0.10275254558533321\n",
      "Gradient Descent(821/999): loss=0.10274979566786797\n",
      "Gradient Descent(822/999): loss=0.10274704653688473\n",
      "Gradient Descent(823/999): loss=0.10274429819184114\n",
      "Gradient Descent(824/999): loss=0.10274155063219968\n",
      "Gradient Descent(825/999): loss=0.10273880385742767\n",
      "Gradient Descent(826/999): loss=0.10273605786699706\n",
      "Gradient Descent(827/999): loss=0.10273331266038455\n",
      "Gradient Descent(828/999): loss=0.1027305682370712\n",
      "Gradient Descent(829/999): loss=0.10272782459654278\n",
      "Gradient Descent(830/999): loss=0.10272508173828937\n",
      "Gradient Descent(831/999): loss=0.10272233966180531\n",
      "Gradient Descent(832/999): loss=0.10271959836658938\n",
      "Gradient Descent(833/999): loss=0.10271685785214447\n",
      "Gradient Descent(834/999): loss=0.10271411811797766\n",
      "Gradient Descent(835/999): loss=0.10271137916360003\n",
      "Gradient Descent(836/999): loss=0.10270864098852678\n",
      "Gradient Descent(837/999): loss=0.10270590359227698\n",
      "Gradient Descent(838/999): loss=0.1027031669743736\n",
      "Gradient Descent(839/999): loss=0.10270043113434349\n",
      "Gradient Descent(840/999): loss=0.1026976960717172\n",
      "Gradient Descent(841/999): loss=0.10269496178602902\n",
      "Gradient Descent(842/999): loss=0.10269222827681691\n",
      "Gradient Descent(843/999): loss=0.10268949554362233\n",
      "Gradient Descent(844/999): loss=0.10268676358599037\n",
      "Gradient Descent(845/999): loss=0.10268403240346959\n",
      "Gradient Descent(846/999): loss=0.10268130199561187\n",
      "Gradient Descent(847/999): loss=0.10267857236197261\n",
      "Gradient Descent(848/999): loss=0.10267584350211041\n",
      "Gradient Descent(849/999): loss=0.10267311541558719\n",
      "Gradient Descent(850/999): loss=0.1026703881019681\n",
      "Gradient Descent(851/999): loss=0.10266766156082142\n",
      "Gradient Descent(852/999): loss=0.10266493579171851\n",
      "Gradient Descent(853/999): loss=0.10266221079423385\n",
      "Gradient Descent(854/999): loss=0.10265948656794494\n",
      "Gradient Descent(855/999): loss=0.10265676311243219\n",
      "Gradient Descent(856/999): loss=0.102654040427279\n",
      "Gradient Descent(857/999): loss=0.10265131851207163\n",
      "Gradient Descent(858/999): loss=0.10264859736639914\n",
      "Gradient Descent(859/999): loss=0.10264587698985339\n",
      "Gradient Descent(860/999): loss=0.10264315738202905\n",
      "Gradient Descent(861/999): loss=0.10264043854252332\n",
      "Gradient Descent(862/999): loss=0.10263772047093626\n",
      "Gradient Descent(863/999): loss=0.10263500316687046\n",
      "Gradient Descent(864/999): loss=0.10263228662993104\n",
      "Gradient Descent(865/999): loss=0.10262957085972571\n",
      "Gradient Descent(866/999): loss=0.10262685585586472\n",
      "Gradient Descent(867/999): loss=0.10262414161796068\n",
      "Gradient Descent(868/999): loss=0.10262142814562865\n",
      "Gradient Descent(869/999): loss=0.10261871543848618\n",
      "Gradient Descent(870/999): loss=0.10261600349615295\n",
      "Gradient Descent(871/999): loss=0.10261329231825114\n",
      "Gradient Descent(872/999): loss=0.10261058190440511\n",
      "Gradient Descent(873/999): loss=0.10260787225424152\n",
      "Gradient Descent(874/999): loss=0.10260516336738915\n",
      "Gradient Descent(875/999): loss=0.10260245524347902\n",
      "Gradient Descent(876/999): loss=0.10259974788214424\n",
      "Gradient Descent(877/999): loss=0.10259704128302005\n",
      "Gradient Descent(878/999): loss=0.10259433544574369\n",
      "Gradient Descent(879/999): loss=0.10259163036995457\n",
      "Gradient Descent(880/999): loss=0.10258892605529399\n",
      "Gradient Descent(881/999): loss=0.10258622250140531\n",
      "Gradient Descent(882/999): loss=0.10258351970793371\n",
      "Gradient Descent(883/999): loss=0.10258081767452643\n",
      "Gradient Descent(884/999): loss=0.10257811640083249\n",
      "Gradient Descent(885/999): loss=0.10257541588650278\n",
      "Gradient Descent(886/999): loss=0.10257271613119008\n",
      "Gradient Descent(887/999): loss=0.10257001713454891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(888/999): loss=0.10256731889623558\n",
      "Gradient Descent(889/999): loss=0.10256462141590816\n",
      "Gradient Descent(890/999): loss=0.10256192469322635\n",
      "Gradient Descent(891/999): loss=0.10255922872785171\n",
      "Gradient Descent(892/999): loss=0.10255653351944724\n",
      "Gradient Descent(893/999): loss=0.10255383906767782\n",
      "Gradient Descent(894/999): loss=0.10255114537220976\n",
      "Gradient Descent(895/999): loss=0.10254845243271103\n",
      "Gradient Descent(896/999): loss=0.1025457602488512\n",
      "Gradient Descent(897/999): loss=0.10254306882030126\n",
      "Gradient Descent(898/999): loss=0.10254037814673388\n",
      "Gradient Descent(899/999): loss=0.1025376882278231\n",
      "Gradient Descent(900/999): loss=0.10253499906324451\n",
      "Gradient Descent(901/999): loss=0.10253231065267504\n",
      "Gradient Descent(902/999): loss=0.10252962299579323\n",
      "Gradient Descent(903/999): loss=0.10252693609227881\n",
      "Gradient Descent(904/999): loss=0.10252424994181304\n",
      "Gradient Descent(905/999): loss=0.10252156454407856\n",
      "Gradient Descent(906/999): loss=0.10251887989875921\n",
      "Gradient Descent(907/999): loss=0.10251619600554025\n",
      "Gradient Descent(908/999): loss=0.1025135128641083\n",
      "Gradient Descent(909/999): loss=0.10251083047415113\n",
      "Gradient Descent(910/999): loss=0.10250814883535783\n",
      "Gradient Descent(911/999): loss=0.10250546794741877\n",
      "Gradient Descent(912/999): loss=0.10250278781002549\n",
      "Gradient Descent(913/999): loss=0.1025001084228708\n",
      "Gradient Descent(914/999): loss=0.10249742978564862\n",
      "Gradient Descent(915/999): loss=0.10249475189805407\n",
      "Gradient Descent(916/999): loss=0.10249207475978347\n",
      "Gradient Descent(917/999): loss=0.10248939837053421\n",
      "Gradient Descent(918/999): loss=0.10248672273000485\n",
      "Gradient Descent(919/999): loss=0.10248404783789501\n",
      "Gradient Descent(920/999): loss=0.10248137369390543\n",
      "Gradient Descent(921/999): loss=0.10247870029773791\n",
      "Gradient Descent(922/999): loss=0.10247602764909529\n",
      "Gradient Descent(923/999): loss=0.10247335574768143\n",
      "Gradient Descent(924/999): loss=0.1024706845932013\n",
      "Gradient Descent(925/999): loss=0.10246801418536075\n",
      "Gradient Descent(926/999): loss=0.10246534452386669\n",
      "Gradient Descent(927/999): loss=0.10246267560842705\n",
      "Gradient Descent(928/999): loss=0.10246000743875068\n",
      "Gradient Descent(929/999): loss=0.10245734001454733\n",
      "Gradient Descent(930/999): loss=0.1024546733355278\n",
      "Gradient Descent(931/999): loss=0.10245200740140367\n",
      "Gradient Descent(932/999): loss=0.1024493422118875\n",
      "Gradient Descent(933/999): loss=0.10244667776669285\n",
      "Gradient Descent(934/999): loss=0.1024440140655339\n",
      "Gradient Descent(935/999): loss=0.10244135110812594\n",
      "Gradient Descent(936/999): loss=0.10243868889418506\n",
      "Gradient Descent(937/999): loss=0.10243602742342811\n",
      "Gradient Descent(938/999): loss=0.10243336669557282\n",
      "Gradient Descent(939/999): loss=0.10243070671033776\n",
      "Gradient Descent(940/999): loss=0.10242804746744227\n",
      "Gradient Descent(941/999): loss=0.10242538896660652\n",
      "Gradient Descent(942/999): loss=0.10242273120755144\n",
      "Gradient Descent(943/999): loss=0.10242007418999875\n",
      "Gradient Descent(944/999): loss=0.10241741791367089\n",
      "Gradient Descent(945/999): loss=0.1024147623782911\n",
      "Gradient Descent(946/999): loss=0.10241210758358331\n",
      "Gradient Descent(947/999): loss=0.10240945352927223\n",
      "Gradient Descent(948/999): loss=0.10240680021508326\n",
      "Gradient Descent(949/999): loss=0.10240414764074249\n",
      "Gradient Descent(950/999): loss=0.10240149580597674\n",
      "Gradient Descent(951/999): loss=0.10239884471051348\n",
      "Gradient Descent(952/999): loss=0.10239619435408095\n",
      "Gradient Descent(953/999): loss=0.10239354473640792\n",
      "Gradient Descent(954/999): loss=0.10239089585722393\n",
      "Gradient Descent(955/999): loss=0.10238824771625912\n",
      "Gradient Descent(956/999): loss=0.10238560031324428\n",
      "Gradient Descent(957/999): loss=0.10238295364791083\n",
      "Gradient Descent(958/999): loss=0.10238030771999077\n",
      "Gradient Descent(959/999): loss=0.10237766252921686\n",
      "Gradient Descent(960/999): loss=0.10237501807532223\n",
      "Gradient Descent(961/999): loss=0.1023723743580408\n",
      "Gradient Descent(962/999): loss=0.10236973137710705\n",
      "Gradient Descent(963/999): loss=0.10236708913225595\n",
      "Gradient Descent(964/999): loss=0.10236444762322307\n",
      "Gradient Descent(965/999): loss=0.10236180684974464\n",
      "Gradient Descent(966/999): loss=0.10235916681155734\n",
      "Gradient Descent(967/999): loss=0.10235652750839842\n",
      "Gradient Descent(968/999): loss=0.10235388894000569\n",
      "Gradient Descent(969/999): loss=0.10235125110611752\n",
      "Gradient Descent(970/999): loss=0.10234861400647272\n",
      "Gradient Descent(971/999): loss=0.10234597764081067\n",
      "Gradient Descent(972/999): loss=0.10234334200887132\n",
      "Gradient Descent(973/999): loss=0.10234070711039503\n",
      "Gradient Descent(974/999): loss=0.10233807294512269\n",
      "Gradient Descent(975/999): loss=0.1023354395127957\n",
      "Gradient Descent(976/999): loss=0.10233280681315587\n",
      "Gradient Descent(977/999): loss=0.10233017484594563\n",
      "Gradient Descent(978/999): loss=0.10232754361090779\n",
      "Gradient Descent(979/999): loss=0.10232491310778558\n",
      "Gradient Descent(980/999): loss=0.10232228333632275\n",
      "Gradient Descent(981/999): loss=0.10231965429626351\n",
      "Gradient Descent(982/999): loss=0.10231702598735253\n",
      "Gradient Descent(983/999): loss=0.10231439840933487\n",
      "Gradient Descent(984/999): loss=0.10231177156195598\n",
      "Gradient Descent(985/999): loss=0.1023091454449619\n",
      "Gradient Descent(986/999): loss=0.10230652005809895\n",
      "Gradient Descent(987/999): loss=0.10230389540111393\n",
      "Gradient Descent(988/999): loss=0.10230127147375404\n",
      "Gradient Descent(989/999): loss=0.10229864827576685\n",
      "Gradient Descent(990/999): loss=0.10229602580690039\n",
      "Gradient Descent(991/999): loss=0.10229340406690313\n",
      "Gradient Descent(992/999): loss=0.10229078305552372\n",
      "Gradient Descent(993/999): loss=0.10228816277251145\n",
      "Gradient Descent(994/999): loss=0.10228554321761582\n",
      "Gradient Descent(995/999): loss=0.10228292439058681\n",
      "Gradient Descent(996/999): loss=0.10228030629117475\n",
      "Gradient Descent(997/999): loss=0.10227768891913026\n",
      "Gradient Descent(998/999): loss=0.10227507227420438\n",
      "Gradient Descent(999/999): loss=0.10227245635614855\n"
     ]
    }
   ],
   "source": [
    "meanacc_dtest = []\n",
    "meanacc_dtrain = []\n",
    "\n",
    "# Execute for data originally with no NaNs\n",
    "dtmp_tr,dtmp_te=cross_validation(Y_no_nans,X_no_nans,k_fold=4,seed=1, function_name='gd', gamma=0.0000001, max_iters=1000)\n",
    "meanacc_dtest.append(dtmp_te)\n",
    "meanacc_dtrain.append(dtmp_tr)\n",
    "\n",
    "# Execute for data originally with NaNs\n",
    "dtmp_tr,dtmp_te=cross_validation(Y_cleaned,X_cleaned,k_fold=4,seed=1, function_name='gd', gamma=0.0000001, max_iters=1000)\n",
    "meanacc_dtest.append(dtmp_te)\n",
    "meanacc_dtrain.append(dtmp_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No NaNs error:\n",
      "Test: 0.6257487667371389 Data: 0.6257732362383526\n",
      "\n",
      "Cleaned NaNs error:\n",
      "Test: 0.6936179103164655 Data: 0.6937260378409683\n"
     ]
    }
   ],
   "source": [
    "print(\"No NaNs error:\")\n",
    "print(\"Test:\",meanacc_dtest[0],\"Data:\", meanacc_dtrain[0])\n",
    "print(\"\\nCleaned NaNs error:\", )\n",
    "print(\"Test:\",meanacc_dtest[1],\"Data:\", meanacc_dtrain[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Comment:</b> Apparently, GD is not converging fast enough. We get higher error than with Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descsent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(0/999): loss=0.2344471850285804\n",
      "Stochastic Gradient Descent(1/999): loss=0.22297924209834177\n",
      "Stochastic Gradient Descent(2/999): loss=0.2127135262756751\n",
      "Stochastic Gradient Descent(3/999): loss=0.2035237988318852\n",
      "Stochastic Gradient Descent(4/999): loss=0.19529707756213635\n",
      "Stochastic Gradient Descent(5/999): loss=0.18793224468320563\n",
      "Stochastic Gradient Descent(6/999): loss=0.181338800919803\n",
      "Stochastic Gradient Descent(7/999): loss=0.1754357504277822\n",
      "Stochastic Gradient Descent(8/999): loss=0.17015060281468786\n",
      "Stochastic Gradient Descent(9/999): loss=0.16541847996091408\n",
      "Stochastic Gradient Descent(10/999): loss=0.16118131663606572\n",
      "Stochastic Gradient Descent(11/999): loss=0.15738714506082038\n",
      "Stochastic Gradient Descent(12/999): loss=0.15398945459893632\n",
      "Stochastic Gradient Descent(13/999): loss=0.15094661868977616\n",
      "Stochastic Gradient Descent(14/999): loss=0.14822138196022944\n",
      "Stochastic Gradient Descent(15/999): loss=0.1457804011964236\n",
      "Stochastic Gradient Descent(16/999): loss=0.143593834519256\n",
      "Stochastic Gradient Descent(17/999): loss=0.14163497370172534\n",
      "Stochastic Gradient Descent(18/999): loss=0.13987991509762168\n",
      "Stochastic Gradient Descent(19/999): loss=0.13830726512688576\n",
      "Stochastic Gradient Descent(20/999): loss=0.13689787668874492\n",
      "Stochastic Gradient Descent(21/999): loss=0.13563461325481216\n",
      "Stochastic Gradient Descent(22/999): loss=0.13450213773539735\n",
      "Stochastic Gradient Descent(23/999): loss=0.13348672351752566\n",
      "Stochastic Gradient Descent(24/999): loss=0.13257608534635032\n",
      "Stochastic Gradient Descent(25/999): loss=0.13175922796614808\n",
      "Stochastic Gradient Descent(26/999): loss=0.1310263106559152\n",
      "Stochastic Gradient Descent(27/999): loss=0.13036852599042587\n",
      "Stochastic Gradient Descent(28/999): loss=0.12977799133289877\n",
      "Stochastic Gradient Descent(29/999): loss=0.12924765172228886\n",
      "Stochastic Gradient Descent(30/999): loss=0.12877119295862424\n",
      "Stochastic Gradient Descent(31/999): loss=0.12834296381546215\n",
      "Stochastic Gradient Descent(32/999): loss=0.12795790642100024\n",
      "Stochastic Gradient Descent(33/999): loss=0.12761149395003013\n",
      "Stochastic Gradient Descent(34/999): loss=0.1272996748590016\n",
      "Stochastic Gradient Descent(35/999): loss=0.12701882297708705\n",
      "Stochastic Gradient Descent(36/999): loss=0.12676569283829234\n",
      "Stochastic Gradient Descent(37/999): loss=0.12653737970423598\n",
      "Stochastic Gradient Descent(38/999): loss=0.12633128378501898\n",
      "Stochastic Gradient Descent(39/999): loss=0.1261450782173303\n",
      "Stochastic Gradient Descent(40/999): loss=0.1259766804052321\n",
      "Stochastic Gradient Descent(41/999): loss=0.12582422637050017\n",
      "Stochastic Gradient Descent(42/999): loss=0.125686047796478\n",
      "Stochastic Gradient Descent(43/999): loss=0.12556065148259174\n",
      "Stochastic Gradient Descent(44/999): loss=0.12544670095637656\n",
      "Stochastic Gradient Descent(45/999): loss=0.12534300001644794\n",
      "Stochastic Gradient Descent(46/999): loss=0.1252484780036445\n",
      "Stochastic Gradient Descent(47/999): loss=0.12516217661886347\n",
      "Stochastic Gradient Descent(48/999): loss=0.12508323812516564\n",
      "Stochastic Gradient Descent(49/999): loss=0.1250108947887846\n",
      "Stochastic Gradient Descent(50/999): loss=0.12494445942894067\n",
      "Stochastic Gradient Descent(51/999): loss=0.12488331696001989\n",
      "Stochastic Gradient Descent(52/999): loss=0.12482691682190862\n",
      "Stochastic Gradient Descent(53/999): loss=0.12477476620521594\n",
      "Stochastic Gradient Descent(54/999): loss=0.12472642398791096\n",
      "Stochastic Gradient Descent(55/999): loss=0.12468149530866793\n",
      "Stochastic Gradient Descent(56/999): loss=0.12463962671005697\n",
      "Stochastic Gradient Descent(57/999): loss=0.12460050179173969\n",
      "Stochastic Gradient Descent(58/999): loss=0.12456383732011332\n",
      "Stochastic Gradient Descent(59/999): loss=0.12452937974647056\n",
      "Stochastic Gradient Descent(60/999): loss=0.12449690209077621\n",
      "Stochastic Gradient Descent(61/999): loss=0.12446620115266675\n",
      "Stochastic Gradient Descent(62/999): loss=0.12443709501531028\n",
      "Stochastic Gradient Descent(63/999): loss=0.1244094208113736\n",
      "Stochastic Gradient Descent(64/999): loss=0.12438303272357158\n",
      "Stochastic Gradient Descent(65/999): loss=0.12435780019516562\n",
      "Stochastic Gradient Descent(66/999): loss=0.12433360632836406\n",
      "Stochastic Gradient Descent(67/999): loss=0.12431034645089235\n",
      "Stochastic Gradient Descent(68/999): loss=0.12428792683307405\n",
      "Stochastic Gradient Descent(69/999): loss=0.12426626353961702\n",
      "Stochastic Gradient Descent(70/999): loss=0.12424528140195933\n",
      "Stochastic Gradient Descent(71/999): loss=0.12422491309851513\n",
      "Stochastic Gradient Descent(72/999): loss=0.12420509833148984\n",
      "Stochastic Gradient Descent(73/999): loss=0.12418578309012383\n",
      "Stochastic Gradient Descent(74/999): loss=0.12416691899128908\n",
      "Stochastic Gradient Descent(75/999): loss=0.1241484626893157\n",
      "Stochastic Gradient Descent(76/999): loss=0.12413037534777911\n",
      "Stochastic Gradient Descent(77/999): loss=0.1241126221667409\n",
      "Stochastic Gradient Descent(78/999): loss=0.12409517195962083\n",
      "Stochastic Gradient Descent(79/999): loss=0.1240779967744881\n",
      "Stochastic Gradient Descent(80/999): loss=0.1240610715551076\n",
      "Stochastic Gradient Descent(81/999): loss=0.12404437383756652\n",
      "Stochastic Gradient Descent(82/999): loss=0.12402788347874584\n",
      "Stochastic Gradient Descent(83/999): loss=0.12401158241329158\n",
      "Stochastic Gradient Descent(84/999): loss=0.12399545443609464\n",
      "Stochastic Gradient Descent(85/999): loss=0.12397948500759982\n",
      "Stochastic Gradient Descent(86/999): loss=0.12396366107954726\n",
      "Stochastic Gradient Descent(87/999): loss=0.12394797093900063\n",
      "Stochastic Gradient Descent(88/999): loss=0.12393240406874265\n",
      "Stochastic Gradient Descent(89/999): loss=0.12391695102231824\n",
      "Stochastic Gradient Descent(90/999): loss=0.12390160331218861\n",
      "Stochastic Gradient Descent(91/999): loss=0.12388635330961881\n",
      "Stochastic Gradient Descent(92/999): loss=0.12387119415506695\n",
      "Stochastic Gradient Descent(93/999): loss=0.12385611967797303\n",
      "Stochastic Gradient Descent(94/999): loss=0.12384112432495975\n",
      "Stochastic Gradient Descent(95/999): loss=0.12382620309556273\n",
      "Stochastic Gradient Descent(96/999): loss=0.12381135148469938\n",
      "Stochastic Gradient Descent(97/999): loss=0.12379656543116908\n",
      "Stochastic Gradient Descent(98/999): loss=0.12378184127155134\n",
      "Stochastic Gradient Descent(99/999): loss=0.12376717569893565\n",
      "Stochastic Gradient Descent(100/999): loss=0.12375256572597532\n",
      "Stochastic Gradient Descent(101/999): loss=0.12373800865181181\n",
      "Stochastic Gradient Descent(102/999): loss=0.12372350203246325\n",
      "Stochastic Gradient Descent(103/999): loss=0.12370904365431319\n",
      "Stochastic Gradient Descent(104/999): loss=0.12369463151037467\n",
      "Stochastic Gradient Descent(105/999): loss=0.12368026377903808\n",
      "Stochastic Gradient Descent(106/999): loss=0.12366593880504208\n",
      "Stochastic Gradient Descent(107/999): loss=0.12365165508243459\n",
      "Stochastic Gradient Descent(108/999): loss=0.12363741123931461\n",
      "Stochastic Gradient Descent(109/999): loss=0.12362320602416856\n",
      "Stochastic Gradient Descent(110/999): loss=0.12360903829363314\n",
      "Stochastic Gradient Descent(111/999): loss=0.123594907001536\n",
      "Stochastic Gradient Descent(112/999): loss=0.1235808111890791\n",
      "Stochastic Gradient Descent(113/999): loss=0.1235667499760459\n",
      "Stochastic Gradient Descent(114/999): loss=0.1235527225529243\n",
      "Stochastic Gradient Descent(115/999): loss=0.12353872817384977\n",
      "Stochastic Gradient Descent(116/999): loss=0.12352476615028232\n",
      "Stochastic Gradient Descent(117/999): loss=0.1235108358453407\n",
      "Stochastic Gradient Descent(118/999): loss=0.12349693666872466\n",
      "Stochastic Gradient Descent(119/999): loss=0.12348306807216397\n",
      "Stochastic Gradient Descent(120/999): loss=0.12346922954533869\n",
      "Stochastic Gradient Descent(121/999): loss=0.12345542061222148\n",
      "Stochastic Gradient Descent(122/999): loss=0.12344164082779772\n",
      "Stochastic Gradient Descent(123/999): loss=0.12342788977512421\n",
      "Stochastic Gradient Descent(124/999): loss=0.12341416706269016\n",
      "Stochastic Gradient Descent(125/999): loss=0.12340047232204987\n",
      "Stochastic Gradient Descent(126/999): loss=0.1233868052056981\n",
      "Stochastic Gradient Descent(127/999): loss=0.1233731653851627\n",
      "Stochastic Gradient Descent(128/999): loss=0.12335955254929211\n",
      "Stochastic Gradient Descent(129/999): loss=0.12334596640271729\n",
      "Stochastic Gradient Descent(130/999): loss=0.12333240666446965\n",
      "Stochastic Gradient Descent(131/999): loss=0.1233188730667389\n",
      "Stochastic Gradient Descent(132/999): loss=0.12330536535375629\n",
      "Stochastic Gradient Descent(133/999): loss=0.1232918832807899\n",
      "Stochastic Gradient Descent(134/999): loss=0.12327842661324069\n",
      "Stochastic Gradient Descent(135/999): loss=0.12326499512582838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(136/999): loss=0.12325158860185835\n",
      "Stochastic Gradient Descent(137/999): loss=0.12323820683256036\n",
      "Stochastic Gradient Descent(138/999): loss=0.12322484961649294\n",
      "Stochastic Gradient Descent(139/999): loss=0.12321151675900481\n",
      "Stochastic Gradient Descent(140/999): loss=0.12319820807174936\n",
      "Stochastic Gradient Descent(141/999): loss=0.12318492337224528\n",
      "Stochastic Gradient Descent(142/999): loss=0.1231716624834792\n",
      "Stochastic Gradient Descent(143/999): loss=0.12315842523354598\n",
      "Stochastic Gradient Descent(144/999): loss=0.12314521145532269\n",
      "Stochastic Gradient Descent(145/999): loss=0.12313202098617264\n",
      "Stochastic Gradient Descent(146/999): loss=0.12311885366767693\n",
      "Stochastic Gradient Descent(147/999): loss=0.1231057093453899\n",
      "Stochastic Gradient Descent(148/999): loss=0.12309258786861695\n",
      "Stochastic Gradient Descent(149/999): loss=0.12307948909021169\n",
      "Stochastic Gradient Descent(150/999): loss=0.12306641286639078\n",
      "Stochastic Gradient Descent(151/999): loss=0.1230533590565649\n",
      "Stochastic Gradient Descent(152/999): loss=0.12304032752318364\n",
      "Stochastic Gradient Descent(153/999): loss=0.12302731813159365\n",
      "Stochastic Gradient Descent(154/999): loss=0.12301433074990792\n",
      "Stochastic Gradient Descent(155/999): loss=0.12300136524888587\n",
      "Stochastic Gradient Descent(156/999): loss=0.1229884215018226\n",
      "Stochastic Gradient Descent(157/999): loss=0.12297549938444653\n",
      "Stochastic Gradient Descent(158/999): loss=0.12296259877482513\n",
      "Stochastic Gradient Descent(159/999): loss=0.12294971955327687\n",
      "Stochastic Gradient Descent(160/999): loss=0.12293686160229002\n",
      "Stochastic Gradient Descent(161/999): loss=0.12292402480644662\n",
      "Stochastic Gradient Descent(162/999): loss=0.1229112090523519\n",
      "Stochastic Gradient Descent(163/999): loss=0.12289841422856801\n",
      "Stochastic Gradient Descent(164/999): loss=0.12288564022555232\n",
      "Stochastic Gradient Descent(165/999): loss=0.12287288693559899\n",
      "Stochastic Gradient Descent(166/999): loss=0.12286015425278453\n",
      "Stochastic Gradient Descent(167/999): loss=0.12284744207291617\n",
      "Stochastic Gradient Descent(168/999): loss=0.12283475029348327\n",
      "Stochastic Gradient Descent(169/999): loss=0.12282207881361115\n",
      "Stochastic Gradient Descent(170/999): loss=0.12280942753401744\n",
      "Stochastic Gradient Descent(171/999): loss=0.12279679635697077\n",
      "Stochastic Gradient Descent(172/999): loss=0.12278418518625102\n",
      "Stochastic Gradient Descent(173/999): loss=0.12277159392711184\n",
      "Stochastic Gradient Descent(174/999): loss=0.12275902248624464\n",
      "Stochastic Gradient Descent(175/999): loss=0.12274647077174416\n",
      "Stochastic Gradient Descent(176/999): loss=0.12273393869307563\n",
      "Stochastic Gradient Descent(177/999): loss=0.12272142616104283\n",
      "Stochastic Gradient Descent(178/999): loss=0.12270893308775797\n",
      "Stochastic Gradient Descent(179/999): loss=0.12269645938661228\n",
      "Stochastic Gradient Descent(180/999): loss=0.1226840049722478\n",
      "Stochastic Gradient Descent(181/999): loss=0.12267156976053006\n",
      "Stochastic Gradient Descent(182/999): loss=0.12265915366852195\n",
      "Stochastic Gradient Descent(183/999): loss=0.12264675661445812\n",
      "Stochastic Gradient Descent(184/999): loss=0.12263437851772037\n",
      "Stochastic Gradient Descent(185/999): loss=0.12262201929881383\n",
      "Stochastic Gradient Descent(186/999): loss=0.12260967887934371\n",
      "Stochastic Gradient Descent(187/999): loss=0.1225973571819929\n",
      "Stochastic Gradient Descent(188/999): loss=0.1225850541305\n",
      "Stochastic Gradient Descent(189/999): loss=0.12257276964963816\n",
      "Stochastic Gradient Descent(190/999): loss=0.12256050366519439\n",
      "Stochastic Gradient Descent(191/999): loss=0.12254825610394944\n",
      "Stochastic Gradient Descent(192/999): loss=0.12253602689365813\n",
      "Stochastic Gradient Descent(193/999): loss=0.12252381596303023\n",
      "Stochastic Gradient Descent(194/999): loss=0.12251162324171201\n",
      "Stochastic Gradient Descent(195/999): loss=0.12249944866026777\n",
      "Stochastic Gradient Descent(196/999): loss=0.12248729215016235\n",
      "Stochastic Gradient Descent(197/999): loss=0.12247515364374355\n",
      "Stochastic Gradient Descent(198/999): loss=0.12246303307422536\n",
      "Stochastic Gradient Descent(199/999): loss=0.12245093037567142\n",
      "Stochastic Gradient Descent(200/999): loss=0.12243884548297883\n",
      "Stochastic Gradient Descent(201/999): loss=0.12242677833186216\n",
      "Stochastic Gradient Descent(202/999): loss=0.12241472885883842\n",
      "Stochastic Gradient Descent(203/999): loss=0.12240269700121138\n",
      "Stochastic Gradient Descent(204/999): loss=0.12239068269705723\n",
      "Stochastic Gradient Descent(205/999): loss=0.12237868588520984\n",
      "Stochastic Gradient Descent(206/999): loss=0.12236670650524674\n",
      "Stochastic Gradient Descent(207/999): loss=0.12235474449747512\n",
      "Stochastic Gradient Descent(208/999): loss=0.12234279980291832\n",
      "Stochastic Gradient Descent(209/999): loss=0.12233087236330253\n",
      "Stochastic Gradient Descent(210/999): loss=0.1223189621210437\n",
      "Stochastic Gradient Descent(211/999): loss=0.12230706901923488\n",
      "Stochastic Gradient Descent(212/999): loss=0.12229519300163358\n",
      "Stochastic Gradient Descent(213/999): loss=0.1222833340126497\n",
      "Stochastic Gradient Descent(214/999): loss=0.12227149199733334\n",
      "Stochastic Gradient Descent(215/999): loss=0.12225966690136321\n",
      "Stochastic Gradient Descent(216/999): loss=0.12224785867103503\n",
      "Stochastic Gradient Descent(217/999): loss=0.12223606725325022\n",
      "Stochastic Gradient Descent(218/999): loss=0.1222242925955049\n",
      "Stochastic Gradient Descent(219/999): loss=0.12221253464587901\n",
      "Stochastic Gradient Descent(220/999): loss=0.1222007933530257\n",
      "Stochastic Gradient Descent(221/999): loss=0.12218906866616082\n",
      "Stochastic Gradient Descent(222/999): loss=0.12217736053505283\n",
      "Stochastic Gradient Descent(223/999): loss=0.12216566891001276\n",
      "Stochastic Gradient Descent(224/999): loss=0.12215399374188439\n",
      "Stochastic Gradient Descent(225/999): loss=0.12214233498203449\n",
      "Stochastic Gradient Descent(226/999): loss=0.12213069258234363\n",
      "Stochastic Gradient Descent(227/999): loss=0.12211906649519681\n",
      "Stochastic Gradient Descent(228/999): loss=0.12210745667347427\n",
      "Stochastic Gradient Descent(229/999): loss=0.12209586307054283\n",
      "Stochastic Gradient Descent(230/999): loss=0.12208428564024701\n",
      "Stochastic Gradient Descent(231/999): loss=0.12207272433690056\n",
      "Stochastic Gradient Descent(232/999): loss=0.12206117911527796\n",
      "Stochastic Gradient Descent(233/999): loss=0.1220496499306064\n",
      "Stochastic Gradient Descent(234/999): loss=0.12203813673855753\n",
      "Stochastic Gradient Descent(235/999): loss=0.12202663949523966\n",
      "Stochastic Gradient Descent(236/999): loss=0.12201515815719001\n",
      "Stochastic Gradient Descent(237/999): loss=0.12200369268136699\n",
      "Stochastic Gradient Descent(238/999): loss=0.12199224302514289\n",
      "Stochastic Gradient Descent(239/999): loss=0.12198080914629658\n",
      "Stochastic Gradient Descent(240/999): loss=0.12196939100300616\n",
      "Stochastic Gradient Descent(241/999): loss=0.12195798855384216\n",
      "Stochastic Gradient Descent(242/999): loss=0.12194660175776041\n",
      "Stochastic Gradient Descent(243/999): loss=0.12193523057409553\n",
      "Stochastic Gradient Descent(244/999): loss=0.12192387496255412\n",
      "Stochastic Gradient Descent(245/999): loss=0.12191253488320827\n",
      "Stochastic Gradient Descent(246/999): loss=0.12190121029648932\n",
      "Stochastic Gradient Descent(247/999): loss=0.12188990116318142\n",
      "Stochastic Gradient Descent(248/999): loss=0.1218786074444155\n",
      "Stochastic Gradient Descent(249/999): loss=0.12186732910166331\n",
      "Stochastic Gradient Descent(250/999): loss=0.12185606609673134\n",
      "Stochastic Gradient Descent(251/999): loss=0.12184481839175525\n",
      "Stochastic Gradient Descent(252/999): loss=0.121833585949194\n",
      "Stochastic Gradient Descent(253/999): loss=0.12182236873182442\n",
      "Stochastic Gradient Descent(254/999): loss=0.12181116670273573\n",
      "Stochastic Gradient Descent(255/999): loss=0.12179997982532402\n",
      "Stochastic Gradient Descent(256/999): loss=0.12178880806328729\n",
      "Stochastic Gradient Descent(257/999): loss=0.12177765138062008\n",
      "Stochastic Gradient Descent(258/999): loss=0.12176650974160844\n",
      "Stochastic Gradient Descent(259/999): loss=0.12175538311082512\n",
      "Stochastic Gradient Descent(260/999): loss=0.12174427145312455\n",
      "Stochastic Gradient Descent(261/999): loss=0.12173317473363805\n",
      "Stochastic Gradient Descent(262/999): loss=0.12172209291776934\n",
      "Stochastic Gradient Descent(263/999): loss=0.12171102597118987\n",
      "Stochastic Gradient Descent(264/999): loss=0.12169997385983417\n",
      "Stochastic Gradient Descent(265/999): loss=0.12168893654989572\n",
      "Stochastic Gradient Descent(266/999): loss=0.12167791400782234\n",
      "Stochastic Gradient Descent(267/999): loss=0.12166690620031215\n",
      "Stochastic Gradient Descent(268/999): loss=0.12165591309430933\n",
      "Stochastic Gradient Descent(269/999): loss=0.12164493465699996\n",
      "Stochastic Gradient Descent(270/999): loss=0.12163397085580825\n",
      "Stochastic Gradient Descent(271/999): loss=0.12162302165839228\n",
      "Stochastic Gradient Descent(272/999): loss=0.12161208703264044\n",
      "Stochastic Gradient Descent(273/999): loss=0.12160116694666746\n",
      "Stochastic Gradient Descent(274/999): loss=0.12159026136881082\n",
      "Stochastic Gradient Descent(275/999): loss=0.12157937026762705\n",
      "Stochastic Gradient Descent(276/999): loss=0.12156849361188812\n",
      "Stochastic Gradient Descent(277/999): loss=0.12155763137057808\n",
      "Stochastic Gradient Descent(278/999): loss=0.1215467835128895\n",
      "Stochastic Gradient Descent(279/999): loss=0.12153595000822015\n",
      "Stochastic Gradient Descent(280/999): loss=0.12152513082616967\n",
      "Stochastic Gradient Descent(281/999): loss=0.12151432593653638\n",
      "Stochastic Gradient Descent(282/999): loss=0.12150353530931404\n",
      "Stochastic Gradient Descent(283/999): loss=0.12149275891468884\n",
      "Stochastic Gradient Descent(284/999): loss=0.12148199672303614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(285/999): loss=0.12147124870491773\n",
      "Stochastic Gradient Descent(286/999): loss=0.12146051483107863\n",
      "Stochastic Gradient Descent(287/999): loss=0.12144979507244436\n",
      "Stochastic Gradient Descent(288/999): loss=0.1214390894001181\n",
      "Stochastic Gradient Descent(289/999): loss=0.12142839778537791\n",
      "Stochastic Gradient Descent(290/999): loss=0.12141772019967381\n",
      "Stochastic Gradient Descent(291/999): loss=0.1214070566146255\n",
      "Stochastic Gradient Descent(292/999): loss=0.12139640700201931\n",
      "Stochastic Gradient Descent(293/999): loss=0.12138577133380593\n",
      "Stochastic Gradient Descent(294/999): loss=0.12137514958209769\n",
      "Stochastic Gradient Descent(295/999): loss=0.12136454171916627\n",
      "Stochastic Gradient Descent(296/999): loss=0.12135394771744011\n",
      "Stochastic Gradient Descent(297/999): loss=0.12134336754950205\n",
      "Stochastic Gradient Descent(298/999): loss=0.12133280118808712\n",
      "Stochastic Gradient Descent(299/999): loss=0.12132224860608007\n",
      "Stochastic Gradient Descent(300/999): loss=0.12131170977651318\n",
      "Stochastic Gradient Descent(301/999): loss=0.12130118467256412\n",
      "Stochastic Gradient Descent(302/999): loss=0.12129067326755377\n",
      "Stochastic Gradient Descent(303/999): loss=0.12128017553494391\n",
      "Stochastic Gradient Descent(304/999): loss=0.12126969144833545\n",
      "Stochastic Gradient Descent(305/999): loss=0.12125922098146609\n",
      "Stochastic Gradient Descent(306/999): loss=0.12124876410820844\n",
      "Stochastic Gradient Descent(307/999): loss=0.12123832080256806\n",
      "Stochastic Gradient Descent(308/999): loss=0.12122789103868159\n",
      "Stochastic Gradient Descent(309/999): loss=0.12121747479081461\n",
      "Stochastic Gradient Descent(310/999): loss=0.1212070720333601\n",
      "Stochastic Gradient Descent(311/999): loss=0.12119668274083627\n",
      "Stochastic Gradient Descent(312/999): loss=0.12118630688788516\n",
      "Stochastic Gradient Descent(313/999): loss=0.12117594444927048\n",
      "Stochastic Gradient Descent(314/999): loss=0.12116559539987623\n",
      "Stochastic Gradient Descent(315/999): loss=0.12115525971470471\n",
      "Stochastic Gradient Descent(316/999): loss=0.12114493736887517\n",
      "Stochastic Gradient Descent(317/999): loss=0.1211346283376218\n",
      "Stochastic Gradient Descent(318/999): loss=0.12112433259629254\n",
      "Stochastic Gradient Descent(319/999): loss=0.1211140501203472\n",
      "Stochastic Gradient Descent(320/999): loss=0.12110378088535606\n",
      "Stochastic Gradient Descent(321/999): loss=0.1210935248669983\n",
      "Stochastic Gradient Descent(322/999): loss=0.12108328204106063\n",
      "Stochastic Gradient Descent(323/999): loss=0.12107305238343562\n",
      "Stochastic Gradient Descent(324/999): loss=0.12106283587012052\n",
      "Stochastic Gradient Descent(325/999): loss=0.12105263247721566\n",
      "Stochastic Gradient Descent(326/999): loss=0.1210424421809232\n",
      "Stochastic Gradient Descent(327/999): loss=0.1210322649575457\n",
      "Stochastic Gradient Descent(328/999): loss=0.12102210078348483\n",
      "Stochastic Gradient Descent(329/999): loss=0.1210119496352401\n",
      "Stochastic Gradient Descent(330/999): loss=0.12100181148940749\n",
      "Stochastic Gradient Descent(331/999): loss=0.12099168632267834\n",
      "Stochastic Gradient Descent(332/999): loss=0.12098157411183796\n",
      "Stochastic Gradient Descent(333/999): loss=0.12097147483376448\n",
      "Stochastic Gradient Descent(334/999): loss=0.12096138846542781\n",
      "Stochastic Gradient Descent(335/999): loss=0.12095131498388825\n",
      "Stochastic Gradient Descent(336/999): loss=0.12094125436629546\n",
      "Stochastic Gradient Descent(337/999): loss=0.12093120658988733\n",
      "Stochastic Gradient Descent(338/999): loss=0.12092117163198894\n",
      "Stochastic Gradient Descent(339/999): loss=0.12091114947001136\n",
      "Stochastic Gradient Descent(340/999): loss=0.12090114008145064\n",
      "Stochastic Gradient Descent(341/999): loss=0.12089114344388686\n",
      "Stochastic Gradient Descent(342/999): loss=0.12088115953498299\n",
      "Stochastic Gradient Descent(343/999): loss=0.12087118833248389\n",
      "Stochastic Gradient Descent(344/999): loss=0.12086122981421543\n",
      "Stochastic Gradient Descent(345/999): loss=0.1208512839580834\n",
      "Stochastic Gradient Descent(346/999): loss=0.1208413507420727\n",
      "Stochastic Gradient Descent(347/999): loss=0.12083143014424624\n",
      "Stochastic Gradient Descent(348/999): loss=0.12082152214274415\n",
      "Stochastic Gradient Descent(349/999): loss=0.12081162671578295\n",
      "Stochastic Gradient Descent(350/999): loss=0.1208017438416544\n",
      "Stochastic Gradient Descent(351/999): loss=0.1207918734987249\n",
      "Stochastic Gradient Descent(352/999): loss=0.12078201566543453\n",
      "Stochastic Gradient Descent(353/999): loss=0.12077217032029629\n",
      "Stochastic Gradient Descent(354/999): loss=0.12076233744189512\n",
      "Stochastic Gradient Descent(355/999): loss=0.12075251700888721\n",
      "Stochastic Gradient Descent(356/999): loss=0.1207427089999993\n",
      "Stochastic Gradient Descent(357/999): loss=0.1207329133940277\n",
      "Stochastic Gradient Descent(358/999): loss=0.12072313016983774\n",
      "Stochastic Gradient Descent(359/999): loss=0.1207133593063628\n",
      "Stochastic Gradient Descent(360/999): loss=0.12070360078260375\n",
      "Stochastic Gradient Descent(361/999): loss=0.12069385457762818\n",
      "Stochastic Gradient Descent(362/999): loss=0.1206841206705697\n",
      "Stochastic Gradient Descent(363/999): loss=0.1206743990406272\n",
      "Stochastic Gradient Descent(364/999): loss=0.12066468966706417\n",
      "Stochastic Gradient Descent(365/999): loss=0.12065499252920817\n",
      "Stochastic Gradient Descent(366/999): loss=0.12064530760644988\n",
      "Stochastic Gradient Descent(367/999): loss=0.12063563487824285\n",
      "Stochastic Gradient Descent(368/999): loss=0.12062597432410244\n",
      "Stochastic Gradient Descent(369/999): loss=0.1206163259236055\n",
      "Stochastic Gradient Descent(370/999): loss=0.12060668965638972\n",
      "Stochastic Gradient Descent(371/999): loss=0.12059706550215286\n",
      "Stochastic Gradient Descent(372/999): loss=0.12058745344065235\n",
      "Stochastic Gradient Descent(373/999): loss=0.12057785345170455\n",
      "Stochastic Gradient Descent(374/999): loss=0.1205682655151843\n",
      "Stochastic Gradient Descent(375/999): loss=0.12055868961102435\n",
      "Stochastic Gradient Descent(376/999): loss=0.12054912571921474\n",
      "Stochastic Gradient Descent(377/999): loss=0.1205395738198022\n",
      "Stochastic Gradient Descent(378/999): loss=0.1205300338928899\n",
      "Stochastic Gradient Descent(379/999): loss=0.12052050591863664\n",
      "Stochastic Gradient Descent(380/999): loss=0.1205109898772564\n",
      "Stochastic Gradient Descent(381/999): loss=0.12050148574901794\n",
      "Stochastic Gradient Descent(382/999): loss=0.1204919935142442\n",
      "Stochastic Gradient Descent(383/999): loss=0.12048251315331183\n",
      "Stochastic Gradient Descent(384/999): loss=0.12047304464665086\n",
      "Stochastic Gradient Descent(385/999): loss=0.1204635879747439\n",
      "Stochastic Gradient Descent(386/999): loss=0.12045414311812605\n",
      "Stochastic Gradient Descent(387/999): loss=0.12044471005738418\n",
      "Stochastic Gradient Descent(388/999): loss=0.12043528877315665\n",
      "Stochastic Gradient Descent(389/999): loss=0.12042587924613275\n",
      "Stochastic Gradient Descent(390/999): loss=0.1204164814570523\n",
      "Stochastic Gradient Descent(391/999): loss=0.12040709538670533\n",
      "Stochastic Gradient Descent(392/999): loss=0.1203977210159315\n",
      "Stochastic Gradient Descent(393/999): loss=0.1203883583256198\n",
      "Stochastic Gradient Descent(394/999): loss=0.12037900729670811\n",
      "Stochastic Gradient Descent(395/999): loss=0.12036966791018289\n",
      "Stochastic Gradient Descent(396/999): loss=0.12036034014707854\n",
      "Stochastic Gradient Descent(397/999): loss=0.1203510239884774\n",
      "Stochastic Gradient Descent(398/999): loss=0.12034171941550896\n",
      "Stochastic Gradient Descent(399/999): loss=0.12033242640934987\n",
      "Stochastic Gradient Descent(400/999): loss=0.12032314495122329\n",
      "Stochastic Gradient Descent(401/999): loss=0.12031387502239871\n",
      "Stochastic Gradient Descent(402/999): loss=0.12030461660419146\n",
      "Stochastic Gradient Descent(403/999): loss=0.12029536967796256\n",
      "Stochastic Gradient Descent(404/999): loss=0.1202861342251181\n",
      "Stochastic Gradient Descent(405/999): loss=0.1202769102271092\n",
      "Stochastic Gradient Descent(406/999): loss=0.12026769766543147\n",
      "Stochastic Gradient Descent(407/999): loss=0.12025849652162478\n",
      "Stochastic Gradient Descent(408/999): loss=0.12024930677727291\n",
      "Stochastic Gradient Descent(409/999): loss=0.1202401284140033\n",
      "Stochastic Gradient Descent(410/999): loss=0.1202309614134866\n",
      "Stochastic Gradient Descent(411/999): loss=0.1202218057574366\n",
      "Stochastic Gradient Descent(412/999): loss=0.12021266142760964\n",
      "Stochastic Gradient Descent(413/999): loss=0.12020352840580459\n",
      "Stochastic Gradient Descent(414/999): loss=0.1201944066738623\n",
      "Stochastic Gradient Descent(415/999): loss=0.12018529621366567\n",
      "Stochastic Gradient Descent(416/999): loss=0.12017619700713894\n",
      "Stochastic Gradient Descent(417/999): loss=0.12016710903624775\n",
      "Stochastic Gradient Descent(418/999): loss=0.12015803228299878\n",
      "Stochastic Gradient Descent(419/999): loss=0.12014896672943935\n",
      "Stochastic Gradient Descent(420/999): loss=0.12013991235765736\n",
      "Stochastic Gradient Descent(421/999): loss=0.12013086914978084\n",
      "Stochastic Gradient Descent(422/999): loss=0.12012183708797798\n",
      "Stochastic Gradient Descent(423/999): loss=0.12011281615445647\n",
      "Stochastic Gradient Descent(424/999): loss=0.12010380633146361\n",
      "Stochastic Gradient Descent(425/999): loss=0.12009480760128591\n",
      "Stochastic Gradient Descent(426/999): loss=0.12008581994624891\n",
      "Stochastic Gradient Descent(427/999): loss=0.12007684334871682\n",
      "Stochastic Gradient Descent(428/999): loss=0.12006787779109244\n",
      "Stochastic Gradient Descent(429/999): loss=0.120058923255817\n",
      "Stochastic Gradient Descent(430/999): loss=0.12004997972536958\n",
      "Stochastic Gradient Descent(431/999): loss=0.1200410471822673\n",
      "Stochastic Gradient Descent(432/999): loss=0.1200321256090648\n",
      "Stochastic Gradient Descent(433/999): loss=0.12002321498835423\n",
      "Stochastic Gradient Descent(434/999): loss=0.12001431530276506\n",
      "Stochastic Gradient Descent(435/999): loss=0.12000542653496349\n",
      "Stochastic Gradient Descent(436/999): loss=0.11999654866765282\n",
      "Stochastic Gradient Descent(437/999): loss=0.1199876816835728\n",
      "Stochastic Gradient Descent(438/999): loss=0.11997882556549962\n",
      "Stochastic Gradient Descent(439/999): loss=0.11996998029624568\n",
      "Stochastic Gradient Descent(440/999): loss=0.11996114585865945\n",
      "Stochastic Gradient Descent(441/999): loss=0.11995232223562517\n",
      "Stochastic Gradient Descent(442/999): loss=0.11994350941006275\n",
      "Stochastic Gradient Descent(443/999): loss=0.11993470736492759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(444/999): loss=0.11992591608321031\n",
      "Stochastic Gradient Descent(445/999): loss=0.11991713554793672\n",
      "Stochastic Gradient Descent(446/999): loss=0.11990836574216741\n",
      "Stochastic Gradient Descent(447/999): loss=0.11989960664899788\n",
      "Stochastic Gradient Descent(448/999): loss=0.1198908582515581\n",
      "Stochastic Gradient Descent(449/999): loss=0.11988212053301255\n",
      "Stochastic Gradient Descent(450/999): loss=0.1198733934765598\n",
      "Stochastic Gradient Descent(451/999): loss=0.11986467706543268\n",
      "Stochastic Gradient Descent(452/999): loss=0.11985597128289784\n",
      "Stochastic Gradient Descent(453/999): loss=0.11984727611225564\n",
      "Stochastic Gradient Descent(454/999): loss=0.11983859153684016\n",
      "Stochastic Gradient Descent(455/999): loss=0.11982991754001886\n",
      "Stochastic Gradient Descent(456/999): loss=0.11982125410519245\n",
      "Stochastic Gradient Descent(457/999): loss=0.11981260121579489\n",
      "Stochastic Gradient Descent(458/999): loss=0.11980395885529303\n",
      "Stochastic Gradient Descent(459/999): loss=0.11979532700718665\n",
      "Stochastic Gradient Descent(460/999): loss=0.11978670565500815\n",
      "Stochastic Gradient Descent(461/999): loss=0.11977809478232254\n",
      "Stochastic Gradient Descent(462/999): loss=0.11976949437272723\n",
      "Stochastic Gradient Descent(463/999): loss=0.11976090440985195\n",
      "Stochastic Gradient Descent(464/999): loss=0.1197523248773584\n",
      "Stochastic Gradient Descent(465/999): loss=0.11974375575894054\n",
      "Stochastic Gradient Descent(466/999): loss=0.11973519703832398\n",
      "Stochastic Gradient Descent(467/999): loss=0.1197266486992662\n",
      "Stochastic Gradient Descent(468/999): loss=0.11971811072555626\n",
      "Stochastic Gradient Descent(469/999): loss=0.11970958310101458\n",
      "Stochastic Gradient Descent(470/999): loss=0.11970106580949315\n",
      "Stochastic Gradient Descent(471/999): loss=0.11969255883487497\n",
      "Stochastic Gradient Descent(472/999): loss=0.11968406216107425\n",
      "Stochastic Gradient Descent(473/999): loss=0.1196755757720362\n",
      "Stochastic Gradient Descent(474/999): loss=0.11966709965173675\n",
      "Stochastic Gradient Descent(475/999): loss=0.11965863378418279\n",
      "Stochastic Gradient Descent(476/999): loss=0.11965017815341156\n",
      "Stochastic Gradient Descent(477/999): loss=0.11964173274349103\n",
      "Stochastic Gradient Descent(478/999): loss=0.11963329753851944\n",
      "Stochastic Gradient Descent(479/999): loss=0.11962487252262533\n",
      "Stochastic Gradient Descent(480/999): loss=0.11961645767996743\n",
      "Stochastic Gradient Descent(481/999): loss=0.11960805299473441\n",
      "Stochastic Gradient Descent(482/999): loss=0.11959965845114504\n",
      "Stochastic Gradient Descent(483/999): loss=0.11959127403344781\n",
      "Stochastic Gradient Descent(484/999): loss=0.11958289972592095\n",
      "Stochastic Gradient Descent(485/999): loss=0.11957453551287231\n",
      "Stochastic Gradient Descent(486/999): loss=0.1195661813786393\n",
      "Stochastic Gradient Descent(487/999): loss=0.11955783730758868\n",
      "Stochastic Gradient Descent(488/999): loss=0.11954950328411648\n",
      "Stochastic Gradient Descent(489/999): loss=0.11954117929264808\n",
      "Stochastic Gradient Descent(490/999): loss=0.11953286531763781\n",
      "Stochastic Gradient Descent(491/999): loss=0.11952456134356908\n",
      "Stochastic Gradient Descent(492/999): loss=0.11951626735495417\n",
      "Stochastic Gradient Descent(493/999): loss=0.11950798333633426\n",
      "Stochastic Gradient Descent(494/999): loss=0.11949970927227915\n",
      "Stochastic Gradient Descent(495/999): loss=0.11949144514738728\n",
      "Stochastic Gradient Descent(496/999): loss=0.11948319094628558\n",
      "Stochastic Gradient Descent(497/999): loss=0.11947494665362952\n",
      "Stochastic Gradient Descent(498/999): loss=0.11946671225410281\n",
      "Stochastic Gradient Descent(499/999): loss=0.11945848773241746\n",
      "Stochastic Gradient Descent(500/999): loss=0.11945027307331363\n",
      "Stochastic Gradient Descent(501/999): loss=0.11944206826155951\n",
      "Stochastic Gradient Descent(502/999): loss=0.11943387328195133\n",
      "Stochastic Gradient Descent(503/999): loss=0.11942568811931319\n",
      "Stochastic Gradient Descent(504/999): loss=0.11941751275849698\n",
      "Stochastic Gradient Descent(505/999): loss=0.11940934718438236\n",
      "Stochastic Gradient Descent(506/999): loss=0.11940119138187656\n",
      "Stochastic Gradient Descent(507/999): loss=0.11939304533591445\n",
      "Stochastic Gradient Descent(508/999): loss=0.11938490903145828\n",
      "Stochastic Gradient Descent(509/999): loss=0.11937678245349775\n",
      "Stochastic Gradient Descent(510/999): loss=0.11936866558704987\n",
      "Stochastic Gradient Descent(511/999): loss=0.11936055841715883\n",
      "Stochastic Gradient Descent(512/999): loss=0.11935246092889598\n",
      "Stochastic Gradient Descent(513/999): loss=0.11934437310735979\n",
      "Stochastic Gradient Descent(514/999): loss=0.11933629493767565\n",
      "Stochastic Gradient Descent(515/999): loss=0.11932822640499592\n",
      "Stochastic Gradient Descent(516/999): loss=0.11932016749449972\n",
      "Stochastic Gradient Descent(517/999): loss=0.11931211819139302\n",
      "Stochastic Gradient Descent(518/999): loss=0.11930407848090836\n",
      "Stochastic Gradient Descent(519/999): loss=0.11929604834830504\n",
      "Stochastic Gradient Descent(520/999): loss=0.11928802777886875\n",
      "Stochastic Gradient Descent(521/999): loss=0.1192800167579117\n",
      "Stochastic Gradient Descent(522/999): loss=0.11927201527077244\n",
      "Stochastic Gradient Descent(523/999): loss=0.11926402330281595\n",
      "Stochastic Gradient Descent(524/999): loss=0.11925604083943335\n",
      "Stochastic Gradient Descent(525/999): loss=0.11924806786604193\n",
      "Stochastic Gradient Descent(526/999): loss=0.1192401043680851\n",
      "Stochastic Gradient Descent(527/999): loss=0.11923215033103235\n",
      "Stochastic Gradient Descent(528/999): loss=0.11922420574037906\n",
      "Stochastic Gradient Descent(529/999): loss=0.11921627058164651\n",
      "Stochastic Gradient Descent(530/999): loss=0.11920834484038188\n",
      "Stochastic Gradient Descent(531/999): loss=0.11920042850215799\n",
      "Stochastic Gradient Descent(532/999): loss=0.11919252155257344\n",
      "Stochastic Gradient Descent(533/999): loss=0.11918462397725241\n",
      "Stochastic Gradient Descent(534/999): loss=0.11917673576184465\n",
      "Stochastic Gradient Descent(535/999): loss=0.11916885689202536\n",
      "Stochastic Gradient Descent(536/999): loss=0.11916098735349526\n",
      "Stochastic Gradient Descent(537/999): loss=0.11915312713198037\n",
      "Stochastic Gradient Descent(538/999): loss=0.11914527621323191\n",
      "Stochastic Gradient Descent(539/999): loss=0.11913743458302652\n",
      "Stochastic Gradient Descent(540/999): loss=0.11912960222716591\n",
      "Stochastic Gradient Descent(541/999): loss=0.11912177913147684\n",
      "Stochastic Gradient Descent(542/999): loss=0.11911396528181128\n",
      "Stochastic Gradient Descent(543/999): loss=0.11910616066404599\n",
      "Stochastic Gradient Descent(544/999): loss=0.11909836526408278\n",
      "Stochastic Gradient Descent(545/999): loss=0.1190905790678483\n",
      "Stochastic Gradient Descent(546/999): loss=0.11908280206129397\n",
      "Stochastic Gradient Descent(547/999): loss=0.11907503423039591\n",
      "Stochastic Gradient Descent(548/999): loss=0.11906727556115503\n",
      "Stochastic Gradient Descent(549/999): loss=0.1190595260395968\n",
      "Stochastic Gradient Descent(550/999): loss=0.11905178565177121\n",
      "Stochastic Gradient Descent(551/999): loss=0.11904405438375283\n",
      "Stochastic Gradient Descent(552/999): loss=0.11903633222164063\n",
      "Stochastic Gradient Descent(553/999): loss=0.11902861915155795\n",
      "Stochastic Gradient Descent(554/999): loss=0.1190209151596525\n",
      "Stochastic Gradient Descent(555/999): loss=0.11901322023209626\n",
      "Stochastic Gradient Descent(556/999): loss=0.11900553435508542\n",
      "Stochastic Gradient Descent(557/999): loss=0.1189978575148403\n",
      "Stochastic Gradient Descent(558/999): loss=0.11899018969760536\n",
      "Stochastic Gradient Descent(559/999): loss=0.11898253088964911\n",
      "Stochastic Gradient Descent(560/999): loss=0.118974881077264\n",
      "Stochastic Gradient Descent(561/999): loss=0.11896724024676647\n",
      "Stochastic Gradient Descent(562/999): loss=0.11895960838449685\n",
      "Stochastic Gradient Descent(563/999): loss=0.11895198547681925\n",
      "Stochastic Gradient Descent(564/999): loss=0.11894437151012158\n",
      "Stochastic Gradient Descent(565/999): loss=0.1189367664708155\n",
      "Stochastic Gradient Descent(566/999): loss=0.1189291703453363\n",
      "Stochastic Gradient Descent(567/999): loss=0.11892158312014287\n",
      "Stochastic Gradient Descent(568/999): loss=0.11891400478171772\n",
      "Stochastic Gradient Descent(569/999): loss=0.11890643531656685\n",
      "Stochastic Gradient Descent(570/999): loss=0.11889887471121967\n",
      "Stochastic Gradient Descent(571/999): loss=0.11889132295222911\n",
      "Stochastic Gradient Descent(572/999): loss=0.11888378002617127\n",
      "Stochastic Gradient Descent(573/999): loss=0.11887624591964573\n",
      "Stochastic Gradient Descent(574/999): loss=0.11886872061927523\n",
      "Stochastic Gradient Descent(575/999): loss=0.11886120411170575\n",
      "Stochastic Gradient Descent(576/999): loss=0.11885369638360636\n",
      "Stochastic Gradient Descent(577/999): loss=0.11884619742166935\n",
      "Stochastic Gradient Descent(578/999): loss=0.11883870721260996\n",
      "Stochastic Gradient Descent(579/999): loss=0.11883122574316644\n",
      "Stochastic Gradient Descent(580/999): loss=0.11882375300010002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(581/999): loss=0.11881628897019483\n",
      "Stochastic Gradient Descent(582/999): loss=0.11880883364025784\n",
      "Stochastic Gradient Descent(583/999): loss=0.11880138699711883\n",
      "Stochastic Gradient Descent(584/999): loss=0.11879394902763037\n",
      "Stochastic Gradient Descent(585/999): loss=0.11878651971866767\n",
      "Stochastic Gradient Descent(586/999): loss=0.11877909905712863\n",
      "Stochastic Gradient Descent(587/999): loss=0.11877168702993383\n",
      "Stochastic Gradient Descent(588/999): loss=0.11876428362402632\n",
      "Stochastic Gradient Descent(589/999): loss=0.11875688882637167\n",
      "Stochastic Gradient Descent(590/999): loss=0.11874950262395799\n",
      "Stochastic Gradient Descent(591/999): loss=0.11874212500379579\n",
      "Stochastic Gradient Descent(592/999): loss=0.11873475595291795\n",
      "Stochastic Gradient Descent(593/999): loss=0.11872739545837965\n",
      "Stochastic Gradient Descent(594/999): loss=0.11872004350725837\n",
      "Stochastic Gradient Descent(595/999): loss=0.1187127000866539\n",
      "Stochastic Gradient Descent(596/999): loss=0.1187053651836881\n",
      "Stochastic Gradient Descent(597/999): loss=0.11869803878550508\n",
      "Stochastic Gradient Descent(598/999): loss=0.118690720879271\n",
      "Stochastic Gradient Descent(599/999): loss=0.11868341145217408\n",
      "Stochastic Gradient Descent(600/999): loss=0.11867611049142457\n",
      "Stochastic Gradient Descent(601/999): loss=0.11866881798425469\n",
      "Stochastic Gradient Descent(602/999): loss=0.11866153391791853\n",
      "Stochastic Gradient Descent(603/999): loss=0.11865425827969216\n",
      "Stochastic Gradient Descent(604/999): loss=0.11864699105687335\n",
      "Stochastic Gradient Descent(605/999): loss=0.11863973223678177\n",
      "Stochastic Gradient Descent(606/999): loss=0.11863248180675878\n",
      "Stochastic Gradient Descent(607/999): loss=0.11862523975416746\n",
      "Stochastic Gradient Descent(608/999): loss=0.11861800606639253\n",
      "Stochastic Gradient Descent(609/999): loss=0.11861078073084036\n",
      "Stochastic Gradient Descent(610/999): loss=0.11860356373493887\n",
      "Stochastic Gradient Descent(611/999): loss=0.11859635506613748\n",
      "Stochastic Gradient Descent(612/999): loss=0.11858915471190716\n",
      "Stochastic Gradient Descent(613/999): loss=0.11858196265974023\n",
      "Stochastic Gradient Descent(614/999): loss=0.11857477889715053\n",
      "Stochastic Gradient Descent(615/999): loss=0.11856760341167313\n",
      "Stochastic Gradient Descent(616/999): loss=0.11856043619086451\n",
      "Stochastic Gradient Descent(617/999): loss=0.1185532772223024\n",
      "Stochastic Gradient Descent(618/999): loss=0.1185461264935857\n",
      "Stochastic Gradient Descent(619/999): loss=0.11853898399233463\n",
      "Stochastic Gradient Descent(620/999): loss=0.1185318497061904\n",
      "Stochastic Gradient Descent(621/999): loss=0.11852472362281545\n",
      "Stochastic Gradient Descent(622/999): loss=0.11851760572989323\n",
      "Stochastic Gradient Descent(623/999): loss=0.1185104960151282\n",
      "Stochastic Gradient Descent(624/999): loss=0.11850339446624585\n",
      "Stochastic Gradient Descent(625/999): loss=0.11849630107099256\n",
      "Stochastic Gradient Descent(626/999): loss=0.11848921581713565\n",
      "Stochastic Gradient Descent(627/999): loss=0.1184821386924633\n",
      "Stochastic Gradient Descent(628/999): loss=0.1184750696847845\n",
      "Stochastic Gradient Descent(629/999): loss=0.11846800878192897\n",
      "Stochastic Gradient Descent(630/999): loss=0.11846095597174722\n",
      "Stochastic Gradient Descent(631/999): loss=0.1184539112421105\n",
      "Stochastic Gradient Descent(632/999): loss=0.11844687458091063\n",
      "Stochastic Gradient Descent(633/999): loss=0.11843984597606012\n",
      "Stochastic Gradient Descent(634/999): loss=0.11843282541549202\n",
      "Stochastic Gradient Descent(635/999): loss=0.11842581288715986\n",
      "Stochastic Gradient Descent(636/999): loss=0.11841880837903786\n",
      "Stochastic Gradient Descent(637/999): loss=0.11841181187912046\n",
      "Stochastic Gradient Descent(638/999): loss=0.1184048233754228\n",
      "Stochastic Gradient Descent(639/999): loss=0.11839784285598004\n",
      "Stochastic Gradient Descent(640/999): loss=0.11839087030884803\n",
      "Stochastic Gradient Descent(641/999): loss=0.11838390572210278\n",
      "Stochastic Gradient Descent(642/999): loss=0.11837694908384044\n",
      "Stochastic Gradient Descent(643/999): loss=0.11837000038217767\n",
      "Stochastic Gradient Descent(644/999): loss=0.11836305960525106\n",
      "Stochastic Gradient Descent(645/999): loss=0.11835612674121751\n",
      "Stochastic Gradient Descent(646/999): loss=0.11834920177825393\n",
      "Stochastic Gradient Descent(647/999): loss=0.1183422847045574\n",
      "Stochastic Gradient Descent(648/999): loss=0.11833537550834498\n",
      "Stochastic Gradient Descent(649/999): loss=0.11832847417785368\n",
      "Stochastic Gradient Descent(650/999): loss=0.11832158070134058\n",
      "Stochastic Gradient Descent(651/999): loss=0.11831469506708259\n",
      "Stochastic Gradient Descent(652/999): loss=0.11830781726337655\n",
      "Stochastic Gradient Descent(653/999): loss=0.11830094727853915\n",
      "Stochastic Gradient Descent(654/999): loss=0.11829408510090689\n",
      "Stochastic Gradient Descent(655/999): loss=0.11828723071883601\n",
      "Stochastic Gradient Descent(656/999): loss=0.11828038412070248\n",
      "Stochastic Gradient Descent(657/999): loss=0.11827354529490204\n",
      "Stochastic Gradient Descent(658/999): loss=0.11826671422985002\n",
      "Stochastic Gradient Descent(659/999): loss=0.11825989091398142\n",
      "Stochastic Gradient Descent(660/999): loss=0.11825307533575082\n",
      "Stochastic Gradient Descent(661/999): loss=0.11824626748363229\n",
      "Stochastic Gradient Descent(662/999): loss=0.11823946734611952\n",
      "Stochastic Gradient Descent(663/999): loss=0.11823267491172558\n",
      "Stochastic Gradient Descent(664/999): loss=0.11822589016898304\n",
      "Stochastic Gradient Descent(665/999): loss=0.1182191131064439\n",
      "Stochastic Gradient Descent(666/999): loss=0.11821234371267939\n",
      "Stochastic Gradient Descent(667/999): loss=0.11820558197628024\n",
      "Stochastic Gradient Descent(668/999): loss=0.11819882788585644\n",
      "Stochastic Gradient Descent(669/999): loss=0.11819208143003716\n",
      "Stochastic Gradient Descent(670/999): loss=0.11818534259747082\n",
      "Stochastic Gradient Descent(671/999): loss=0.11817861137682512\n",
      "Stochastic Gradient Descent(672/999): loss=0.11817188775678682\n",
      "Stochastic Gradient Descent(673/999): loss=0.11816517172606181\n",
      "Stochastic Gradient Descent(674/999): loss=0.11815846327337512\n",
      "Stochastic Gradient Descent(675/999): loss=0.11815176238747079\n",
      "Stochastic Gradient Descent(676/999): loss=0.1181450690571118\n",
      "Stochastic Gradient Descent(677/999): loss=0.11813838327108026\n",
      "Stochastic Gradient Descent(678/999): loss=0.1181317050181771\n",
      "Stochastic Gradient Descent(679/999): loss=0.11812503428722225\n",
      "Stochastic Gradient Descent(680/999): loss=0.11811837106705438\n",
      "Stochastic Gradient Descent(681/999): loss=0.11811171534653113\n",
      "Stochastic Gradient Descent(682/999): loss=0.11810506711452894\n",
      "Stochastic Gradient Descent(683/999): loss=0.11809842635994282\n",
      "Stochastic Gradient Descent(684/999): loss=0.11809179307168682\n",
      "Stochastic Gradient Descent(685/999): loss=0.11808516723869346\n",
      "Stochastic Gradient Descent(686/999): loss=0.118078548849914\n",
      "Stochastic Gradient Descent(687/999): loss=0.11807193789431836\n",
      "Stochastic Gradient Descent(688/999): loss=0.11806533436089496\n",
      "Stochastic Gradient Descent(689/999): loss=0.11805873823865093\n",
      "Stochastic Gradient Descent(690/999): loss=0.11805214951661178\n",
      "Stochastic Gradient Descent(691/999): loss=0.1180455681838216\n",
      "Stochastic Gradient Descent(692/999): loss=0.11803899422934296\n",
      "Stochastic Gradient Descent(693/999): loss=0.11803242764225673\n",
      "Stochastic Gradient Descent(694/999): loss=0.11802586841166232\n",
      "Stochastic Gradient Descent(695/999): loss=0.11801931652667742\n",
      "Stochastic Gradient Descent(696/999): loss=0.11801277197643804\n",
      "Stochastic Gradient Descent(697/999): loss=0.11800623475009857\n",
      "Stochastic Gradient Descent(698/999): loss=0.1179997048368315\n",
      "Stochastic Gradient Descent(699/999): loss=0.11799318222582772\n",
      "Stochastic Gradient Descent(700/999): loss=0.1179866669062962\n",
      "Stochastic Gradient Descent(701/999): loss=0.11798015886746407\n",
      "Stochastic Gradient Descent(702/999): loss=0.11797365809857664\n",
      "Stochastic Gradient Descent(703/999): loss=0.11796716458889728\n",
      "Stochastic Gradient Descent(704/999): loss=0.11796067832770742\n",
      "Stochastic Gradient Descent(705/999): loss=0.11795419930430652\n",
      "Stochastic Gradient Descent(706/999): loss=0.11794772750801207\n",
      "Stochastic Gradient Descent(707/999): loss=0.11794126292815942\n",
      "Stochastic Gradient Descent(708/999): loss=0.11793480555410193\n",
      "Stochastic Gradient Descent(709/999): loss=0.11792835537521092\n",
      "Stochastic Gradient Descent(710/999): loss=0.11792191238087535\n",
      "Stochastic Gradient Descent(711/999): loss=0.11791547656050227\n",
      "Stochastic Gradient Descent(712/999): loss=0.11790904790351639\n",
      "Stochastic Gradient Descent(713/999): loss=0.11790262639936017\n",
      "Stochastic Gradient Descent(714/999): loss=0.11789621203749384\n",
      "Stochastic Gradient Descent(715/999): loss=0.11788980480739537\n",
      "Stochastic Gradient Descent(716/999): loss=0.11788340469856033\n",
      "Stochastic Gradient Descent(717/999): loss=0.11787701170050198\n",
      "Stochastic Gradient Descent(718/999): loss=0.11787062580275115\n",
      "Stochastic Gradient Descent(719/999): loss=0.11786424699485626\n",
      "Stochastic Gradient Descent(720/999): loss=0.11785787526638325\n",
      "Stochastic Gradient Descent(721/999): loss=0.1178515106069156\n",
      "Stochastic Gradient Descent(722/999): loss=0.11784515300605428\n",
      "Stochastic Gradient Descent(723/999): loss=0.11783880245341764\n",
      "Stochastic Gradient Descent(724/999): loss=0.11783245893864151\n",
      "Stochastic Gradient Descent(725/999): loss=0.11782612245137908\n",
      "Stochastic Gradient Descent(726/999): loss=0.1178197929813009\n",
      "Stochastic Gradient Descent(727/999): loss=0.11781347051809479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(728/999): loss=0.11780715505146597\n",
      "Stochastic Gradient Descent(729/999): loss=0.11780084657113676\n",
      "Stochastic Gradient Descent(730/999): loss=0.11779454506684689\n",
      "Stochastic Gradient Descent(731/999): loss=0.11778825052835315\n",
      "Stochastic Gradient Descent(732/999): loss=0.1177819629454295\n",
      "Stochastic Gradient Descent(733/999): loss=0.11777568230786718\n",
      "Stochastic Gradient Descent(734/999): loss=0.11776940860547432\n",
      "Stochastic Gradient Descent(735/999): loss=0.11776314182807622\n",
      "Stochastic Gradient Descent(736/999): loss=0.11775688196551533\n",
      "Stochastic Gradient Descent(737/999): loss=0.11775062900765089\n",
      "Stochastic Gradient Descent(738/999): loss=0.11774438294435931\n",
      "Stochastic Gradient Descent(739/999): loss=0.11773814376553386\n",
      "Stochastic Gradient Descent(740/999): loss=0.11773191146108472\n",
      "Stochastic Gradient Descent(741/999): loss=0.11772568602093905\n",
      "Stochastic Gradient Descent(742/999): loss=0.1177194674350407\n",
      "Stochastic Gradient Descent(743/999): loss=0.11771325569335053\n",
      "Stochastic Gradient Descent(744/999): loss=0.11770705078584606\n",
      "Stochastic Gradient Descent(745/999): loss=0.11770085270252172\n",
      "Stochastic Gradient Descent(746/999): loss=0.1176946614333885\n",
      "Stochastic Gradient Descent(747/999): loss=0.11768847696847423\n",
      "Stochastic Gradient Descent(748/999): loss=0.11768229929782335\n",
      "Stochastic Gradient Descent(749/999): loss=0.117676128411497\n",
      "Stochastic Gradient Descent(750/999): loss=0.11766996429957284\n",
      "Stochastic Gradient Descent(751/999): loss=0.11766380695214529\n",
      "Stochastic Gradient Descent(752/999): loss=0.11765765635932512\n",
      "Stochastic Gradient Descent(753/999): loss=0.1176515125112398\n",
      "Stochastic Gradient Descent(754/999): loss=0.1176453753980332\n",
      "Stochastic Gradient Descent(755/999): loss=0.11763924500986565\n",
      "Stochastic Gradient Descent(756/999): loss=0.11763312133691403\n",
      "Stochastic Gradient Descent(757/999): loss=0.1176270043693715\n",
      "Stochastic Gradient Descent(758/999): loss=0.11762089409744766\n",
      "Stochastic Gradient Descent(759/999): loss=0.11761479051136847\n",
      "Stochastic Gradient Descent(760/999): loss=0.11760869360137613\n",
      "Stochastic Gradient Descent(761/999): loss=0.1176026033577293\n",
      "Stochastic Gradient Descent(762/999): loss=0.11759651977070272\n",
      "Stochastic Gradient Descent(763/999): loss=0.11759044283058748\n",
      "Stochastic Gradient Descent(764/999): loss=0.11758437252769077\n",
      "Stochastic Gradient Descent(765/999): loss=0.11757830885233611\n",
      "Stochastic Gradient Descent(766/999): loss=0.11757225179486305\n",
      "Stochastic Gradient Descent(767/999): loss=0.11756620134562727\n",
      "Stochastic Gradient Descent(768/999): loss=0.11756015749500057\n",
      "Stochastic Gradient Descent(769/999): loss=0.11755412023337074\n",
      "Stochastic Gradient Descent(770/999): loss=0.11754808955114175\n",
      "Stochastic Gradient Descent(771/999): loss=0.11754206543873343\n",
      "Stochastic Gradient Descent(772/999): loss=0.11753604788658159\n",
      "Stochastic Gradient Descent(773/999): loss=0.1175300368851381\n",
      "Stochastic Gradient Descent(774/999): loss=0.11752403242487061\n",
      "Stochastic Gradient Descent(775/999): loss=0.11751803449626277\n",
      "Stochastic Gradient Descent(776/999): loss=0.11751204308981401\n",
      "Stochastic Gradient Descent(777/999): loss=0.11750605819603967\n",
      "Stochastic Gradient Descent(778/999): loss=0.11750007980547077\n",
      "Stochastic Gradient Descent(779/999): loss=0.11749410790865424\n",
      "Stochastic Gradient Descent(780/999): loss=0.11748814249615265\n",
      "Stochastic Gradient Descent(781/999): loss=0.11748218355854441\n",
      "Stochastic Gradient Descent(782/999): loss=0.11747623108642348\n",
      "Stochastic Gradient Descent(783/999): loss=0.11747028507039958\n",
      "Stochastic Gradient Descent(784/999): loss=0.11746434550109805\n",
      "Stochastic Gradient Descent(785/999): loss=0.11745841236915976\n",
      "Stochastic Gradient Descent(786/999): loss=0.11745248566524127\n",
      "Stochastic Gradient Descent(787/999): loss=0.11744656538001463\n",
      "Stochastic Gradient Descent(788/999): loss=0.11744065150416742\n",
      "Stochastic Gradient Descent(789/999): loss=0.11743474402840275\n",
      "Stochastic Gradient Descent(790/999): loss=0.1174288429434391\n",
      "Stochastic Gradient Descent(791/999): loss=0.11742294824001052\n",
      "Stochastic Gradient Descent(792/999): loss=0.1174170599088664\n",
      "Stochastic Gradient Descent(793/999): loss=0.1174111779407715\n",
      "Stochastic Gradient Descent(794/999): loss=0.11740530232650595\n",
      "Stochastic Gradient Descent(795/999): loss=0.11739943305686529\n",
      "Stochastic Gradient Descent(796/999): loss=0.11739357012266025\n",
      "Stochastic Gradient Descent(797/999): loss=0.1173877135147169\n",
      "Stochastic Gradient Descent(798/999): loss=0.11738186322387653\n",
      "Stochastic Gradient Descent(799/999): loss=0.11737601924099567\n",
      "Stochastic Gradient Descent(800/999): loss=0.11737018155694601\n",
      "Stochastic Gradient Descent(801/999): loss=0.1173643501626145\n",
      "Stochastic Gradient Descent(802/999): loss=0.11735852504890314\n",
      "Stochastic Gradient Descent(803/999): loss=0.11735270620672904\n",
      "Stochastic Gradient Descent(804/999): loss=0.11734689362702441\n",
      "Stochastic Gradient Descent(805/999): loss=0.1173410873007366\n",
      "Stochastic Gradient Descent(806/999): loss=0.11733528721882788\n",
      "Stochastic Gradient Descent(807/999): loss=0.1173294933722756\n",
      "Stochastic Gradient Descent(808/999): loss=0.11732370575207204\n",
      "Stochastic Gradient Descent(809/999): loss=0.1173179243492245\n",
      "Stochastic Gradient Descent(810/999): loss=0.1173121491547551\n",
      "Stochastic Gradient Descent(811/999): loss=0.11730638015970096\n",
      "Stochastic Gradient Descent(812/999): loss=0.11730061735511402\n",
      "Stochastic Gradient Descent(813/999): loss=0.11729486073206108\n",
      "Stochastic Gradient Descent(814/999): loss=0.11728911028162375\n",
      "Stochastic Gradient Descent(815/999): loss=0.11728336599489851\n",
      "Stochastic Gradient Descent(816/999): loss=0.11727762786299646\n",
      "Stochastic Gradient Descent(817/999): loss=0.11727189587704354\n",
      "Stochastic Gradient Descent(818/999): loss=0.11726617002818042\n",
      "Stochastic Gradient Descent(819/999): loss=0.11726045030756238\n",
      "Stochastic Gradient Descent(820/999): loss=0.11725473670635944\n",
      "Stochastic Gradient Descent(821/999): loss=0.11724902921575626\n",
      "Stochastic Gradient Descent(822/999): loss=0.11724332782695202\n",
      "Stochastic Gradient Descent(823/999): loss=0.11723763253116057\n",
      "Stochastic Gradient Descent(824/999): loss=0.11723194331961023\n",
      "Stochastic Gradient Descent(825/999): loss=0.117226260183544\n",
      "Stochastic Gradient Descent(826/999): loss=0.11722058311421925\n",
      "Stochastic Gradient Descent(827/999): loss=0.1172149121029079\n",
      "Stochastic Gradient Descent(828/999): loss=0.11720924714089631\n",
      "Stochastic Gradient Descent(829/999): loss=0.11720358821948526\n",
      "Stochastic Gradient Descent(830/999): loss=0.11719793532998991\n",
      "Stochastic Gradient Descent(831/999): loss=0.11719228846373986\n",
      "Stochastic Gradient Descent(832/999): loss=0.117186647612079\n",
      "Stochastic Gradient Descent(833/999): loss=0.11718101276636567\n",
      "Stochastic Gradient Descent(834/999): loss=0.11717538391797232\n",
      "Stochastic Gradient Descent(835/999): loss=0.11716976105828583\n",
      "Stochastic Gradient Descent(836/999): loss=0.11716414417870728\n",
      "Stochastic Gradient Descent(837/999): loss=0.11715853327065193\n",
      "Stochastic Gradient Descent(838/999): loss=0.11715292832554935\n",
      "Stochastic Gradient Descent(839/999): loss=0.11714732933484322\n",
      "Stochastic Gradient Descent(840/999): loss=0.1171417362899913\n",
      "Stochastic Gradient Descent(841/999): loss=0.11713614918246566\n",
      "Stochastic Gradient Descent(842/999): loss=0.11713056800375224\n",
      "Stochastic Gradient Descent(843/999): loss=0.1171249927453513\n",
      "Stochastic Gradient Descent(844/999): loss=0.11711942339877694\n",
      "Stochastic Gradient Descent(845/999): loss=0.1171138599555574\n",
      "Stochastic Gradient Descent(846/999): loss=0.1171083024072349\n",
      "Stochastic Gradient Descent(847/999): loss=0.11710275074536557\n",
      "Stochastic Gradient Descent(848/999): loss=0.11709720496151961\n",
      "Stochastic Gradient Descent(849/999): loss=0.11709166504728112\n",
      "Stochastic Gradient Descent(850/999): loss=0.11708613099424799\n",
      "Stochastic Gradient Descent(851/999): loss=0.11708060279403207\n",
      "Stochastic Gradient Descent(852/999): loss=0.11707508043825912\n",
      "Stochastic Gradient Descent(853/999): loss=0.1170695639185686\n",
      "Stochastic Gradient Descent(854/999): loss=0.1170640532266139\n",
      "Stochastic Gradient Descent(855/999): loss=0.11705854835406207\n",
      "Stochastic Gradient Descent(856/999): loss=0.117053049292594\n",
      "Stochastic Gradient Descent(857/999): loss=0.1170475560339043\n",
      "Stochastic Gradient Descent(858/999): loss=0.1170420685697012\n",
      "Stochastic Gradient Descent(859/999): loss=0.11703658689170675\n",
      "Stochastic Gradient Descent(860/999): loss=0.11703111099165658\n",
      "Stochastic Gradient Descent(861/999): loss=0.11702564086129992\n",
      "Stochastic Gradient Descent(862/999): loss=0.1170201764923997\n",
      "Stochastic Gradient Descent(863/999): loss=0.11701471787673233\n",
      "Stochastic Gradient Descent(864/999): loss=0.11700926500608788\n",
      "Stochastic Gradient Descent(865/999): loss=0.11700381787226985\n",
      "Stochastic Gradient Descent(866/999): loss=0.1169983764670954\n",
      "Stochastic Gradient Descent(867/999): loss=0.11699294078239507\n",
      "Stochastic Gradient Descent(868/999): loss=0.11698751081001284\n",
      "Stochastic Gradient Descent(869/999): loss=0.1169820865418062\n",
      "Stochastic Gradient Descent(870/999): loss=0.11697666796964608\n",
      "Stochastic Gradient Descent(871/999): loss=0.11697125508541671\n",
      "Stochastic Gradient Descent(872/999): loss=0.11696584788101577\n",
      "Stochastic Gradient Descent(873/999): loss=0.11696044634835427\n",
      "Stochastic Gradient Descent(874/999): loss=0.1169550504793565\n",
      "Stochastic Gradient Descent(875/999): loss=0.11694966026596007\n",
      "Stochastic Gradient Descent(876/999): loss=0.11694427570011591\n",
      "Stochastic Gradient Descent(877/999): loss=0.11693889677378815\n",
      "Stochastic Gradient Descent(878/999): loss=0.11693352347895418\n",
      "Stochastic Gradient Descent(879/999): loss=0.11692815580760459\n",
      "Stochastic Gradient Descent(880/999): loss=0.11692279375174311\n",
      "Stochastic Gradient Descent(881/999): loss=0.1169174373033867\n",
      "Stochastic Gradient Descent(882/999): loss=0.11691208645456538\n",
      "Stochastic Gradient Descent(883/999): loss=0.11690674119732235\n",
      "Stochastic Gradient Descent(884/999): loss=0.11690140152371385\n",
      "Stochastic Gradient Descent(885/999): loss=0.11689606742580924\n",
      "Stochastic Gradient Descent(886/999): loss=0.11689073889569088\n",
      "Stochastic Gradient Descent(887/999): loss=0.11688541592545412\n",
      "Stochastic Gradient Descent(888/999): loss=0.11688009850720733\n",
      "Stochastic Gradient Descent(889/999): loss=0.11687478663307199\n",
      "Stochastic Gradient Descent(890/999): loss=0.11686948029518225\n",
      "Stochastic Gradient Descent(891/999): loss=0.11686417948568546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(892/999): loss=0.11685888419674174\n",
      "Stochastic Gradient Descent(893/999): loss=0.11685359442052409\n",
      "Stochastic Gradient Descent(894/999): loss=0.11684831014921843\n",
      "Stochastic Gradient Descent(895/999): loss=0.11684303137502343\n",
      "Stochastic Gradient Descent(896/999): loss=0.11683775809015066\n",
      "Stochastic Gradient Descent(897/999): loss=0.11683249028682445\n",
      "Stochastic Gradient Descent(898/999): loss=0.11682722795728191\n",
      "Stochastic Gradient Descent(899/999): loss=0.11682197109377286\n",
      "Stochastic Gradient Descent(900/999): loss=0.11681671968855985\n",
      "Stochastic Gradient Descent(901/999): loss=0.11681147373391822\n",
      "Stochastic Gradient Descent(902/999): loss=0.11680623322213582\n",
      "Stochastic Gradient Descent(903/999): loss=0.11680099814551333\n",
      "Stochastic Gradient Descent(904/999): loss=0.11679576849636394\n",
      "Stochastic Gradient Descent(905/999): loss=0.11679054426701353\n",
      "Stochastic Gradient Descent(906/999): loss=0.11678532544980055\n",
      "Stochastic Gradient Descent(907/999): loss=0.116780112037076\n",
      "Stochastic Gradient Descent(908/999): loss=0.11677490402120339\n",
      "Stochastic Gradient Descent(909/999): loss=0.11676970139455888\n",
      "Stochastic Gradient Descent(910/999): loss=0.11676450414953096\n",
      "Stochastic Gradient Descent(911/999): loss=0.11675931227852074\n",
      "Stochastic Gradient Descent(912/999): loss=0.11675412577394172\n",
      "Stochastic Gradient Descent(913/999): loss=0.11674894462821986\n",
      "Stochastic Gradient Descent(914/999): loss=0.11674376883379353\n",
      "Stochastic Gradient Descent(915/999): loss=0.11673859838311341\n",
      "Stochastic Gradient Descent(916/999): loss=0.11673343326864273\n",
      "Stochastic Gradient Descent(917/999): loss=0.11672827348285693\n",
      "Stochastic Gradient Descent(918/999): loss=0.11672311901824375\n",
      "Stochastic Gradient Descent(919/999): loss=0.11671796986730333\n",
      "Stochastic Gradient Descent(920/999): loss=0.11671282602254808\n",
      "Stochastic Gradient Descent(921/999): loss=0.11670768747650262\n",
      "Stochastic Gradient Descent(922/999): loss=0.11670255422170378\n",
      "Stochastic Gradient Descent(923/999): loss=0.11669742625070074\n",
      "Stochastic Gradient Descent(924/999): loss=0.11669230355605477\n",
      "Stochastic Gradient Descent(925/999): loss=0.11668718613033935\n",
      "Stochastic Gradient Descent(926/999): loss=0.11668207396614008\n",
      "Stochastic Gradient Descent(927/999): loss=0.11667696705605474\n",
      "Stochastic Gradient Descent(928/999): loss=0.11667186539269317\n",
      "Stochastic Gradient Descent(929/999): loss=0.11666676896867734\n",
      "Stochastic Gradient Descent(930/999): loss=0.11666167777664128\n",
      "Stochastic Gradient Descent(931/999): loss=0.11665659180923107\n",
      "Stochastic Gradient Descent(932/999): loss=0.11665151105910476\n",
      "Stochastic Gradient Descent(933/999): loss=0.11664643551893249\n",
      "Stochastic Gradient Descent(934/999): loss=0.11664136518139631\n",
      "Stochastic Gradient Descent(935/999): loss=0.11663630003919032\n",
      "Stochastic Gradient Descent(936/999): loss=0.11663124008502042\n",
      "Stochastic Gradient Descent(937/999): loss=0.11662618531160454\n",
      "Stochastic Gradient Descent(938/999): loss=0.11662113571167254\n",
      "Stochastic Gradient Descent(939/999): loss=0.11661609127796604\n",
      "Stochastic Gradient Descent(940/999): loss=0.1166110520032386\n",
      "Stochastic Gradient Descent(941/999): loss=0.11660601788025558\n",
      "Stochastic Gradient Descent(942/999): loss=0.11660098890179418\n",
      "Stochastic Gradient Descent(943/999): loss=0.11659596506064338\n",
      "Stochastic Gradient Descent(944/999): loss=0.11659094634960393\n",
      "Stochastic Gradient Descent(945/999): loss=0.1165859327614883\n",
      "Stochastic Gradient Descent(946/999): loss=0.1165809242891208\n",
      "Stochastic Gradient Descent(947/999): loss=0.11657592092533733\n",
      "Stochastic Gradient Descent(948/999): loss=0.11657092266298553\n",
      "Stochastic Gradient Descent(949/999): loss=0.11656592949492477\n",
      "Stochastic Gradient Descent(950/999): loss=0.11656094141402597\n",
      "Stochastic Gradient Descent(951/999): loss=0.11655595841317172\n",
      "Stochastic Gradient Descent(952/999): loss=0.11655098048525621\n",
      "Stochastic Gradient Descent(953/999): loss=0.11654600762318526\n",
      "Stochastic Gradient Descent(954/999): loss=0.1165410398198762\n",
      "Stochastic Gradient Descent(955/999): loss=0.11653607706825794\n",
      "Stochastic Gradient Descent(956/999): loss=0.11653111936127092\n",
      "Stochastic Gradient Descent(957/999): loss=0.11652616669186705\n",
      "Stochastic Gradient Descent(958/999): loss=0.11652121905300981\n",
      "Stochastic Gradient Descent(959/999): loss=0.11651627643767405\n",
      "Stochastic Gradient Descent(960/999): loss=0.1165113388388461\n",
      "Stochastic Gradient Descent(961/999): loss=0.11650640624952374\n",
      "Stochastic Gradient Descent(962/999): loss=0.11650147866271615\n",
      "Stochastic Gradient Descent(963/999): loss=0.11649655607144387\n",
      "Stochastic Gradient Descent(964/999): loss=0.11649163846873883\n",
      "Stochastic Gradient Descent(965/999): loss=0.11648672584764433\n",
      "Stochastic Gradient Descent(966/999): loss=0.1164818182012149\n",
      "Stochastic Gradient Descent(967/999): loss=0.11647691552251646\n",
      "Stochastic Gradient Descent(968/999): loss=0.11647201780462624\n",
      "Stochastic Gradient Descent(969/999): loss=0.11646712504063268\n",
      "Stochastic Gradient Descent(970/999): loss=0.11646223722363543\n",
      "Stochastic Gradient Descent(971/999): loss=0.11645735434674548\n",
      "Stochastic Gradient Descent(972/999): loss=0.11645247640308494\n",
      "Stochastic Gradient Descent(973/999): loss=0.11644760338578711\n",
      "Stochastic Gradient Descent(974/999): loss=0.11644273528799652\n",
      "Stochastic Gradient Descent(975/999): loss=0.1164378721028688\n",
      "Stochastic Gradient Descent(976/999): loss=0.1164330138235707\n",
      "Stochastic Gradient Descent(977/999): loss=0.11642816044328012\n",
      "Stochastic Gradient Descent(978/999): loss=0.116423311955186\n",
      "Stochastic Gradient Descent(979/999): loss=0.11641846835248838\n",
      "Stochastic Gradient Descent(980/999): loss=0.11641362962839838\n",
      "Stochastic Gradient Descent(981/999): loss=0.11640879577613811\n",
      "Stochastic Gradient Descent(982/999): loss=0.11640396678894072\n",
      "Stochastic Gradient Descent(983/999): loss=0.11639914266005026\n",
      "Stochastic Gradient Descent(984/999): loss=0.11639432338272189\n",
      "Stochastic Gradient Descent(985/999): loss=0.11638950895022165\n",
      "Stochastic Gradient Descent(986/999): loss=0.1163846993558265\n",
      "Stochastic Gradient Descent(987/999): loss=0.11637989459282436\n",
      "Stochastic Gradient Descent(988/999): loss=0.11637509465451402\n",
      "Stochastic Gradient Descent(989/999): loss=0.11637029953420522\n",
      "Stochastic Gradient Descent(990/999): loss=0.1163655092252184\n",
      "Stochastic Gradient Descent(991/999): loss=0.11636072372088503\n",
      "Stochastic Gradient Descent(992/999): loss=0.11635594301454716\n",
      "Stochastic Gradient Descent(993/999): loss=0.11635116709955795\n",
      "Stochastic Gradient Descent(994/999): loss=0.11634639596928106\n",
      "Stochastic Gradient Descent(995/999): loss=0.11634162961709112\n",
      "Stochastic Gradient Descent(996/999): loss=0.11633686803637332\n",
      "Stochastic Gradient Descent(997/999): loss=0.11633211122052373\n",
      "Stochastic Gradient Descent(998/999): loss=0.11632735916294906\n",
      "Stochastic Gradient Descent(999/999): loss=0.11632261185706673\n",
      "Stochastic Gradient Descent(0/999): loss=0.23409482421110328\n",
      "Stochastic Gradient Descent(1/999): loss=0.22268728515151393\n",
      "Stochastic Gradient Descent(2/999): loss=0.21247076730859876\n",
      "Stochastic Gradient Descent(3/999): loss=0.20332072804792875\n",
      "Stochastic Gradient Descent(4/999): loss=0.19512564864397453\n",
      "Stochastic Gradient Descent(5/999): loss=0.18778567230875923\n",
      "Stochastic Gradient Descent(6/999): loss=0.18121138464845063\n",
      "Stochastic Gradient Descent(7/999): loss=0.17532272165351004\n",
      "Stochastic Gradient Descent(8/999): loss=0.17004799188559228\n",
      "Stochastic Gradient Descent(9/999): loss=0.16532300091908717\n",
      "Stochastic Gradient Descent(10/999): loss=0.16109026734403603\n",
      "Stochastic Gradient Descent(11/999): loss=0.15729832075540587\n",
      "Stochastic Gradient Descent(12/999): loss=0.15390107315500726\n",
      "Stochastic Gradient Descent(13/999): loss=0.15085725608894024\n",
      "Stochastic Gradient Descent(14/999): loss=0.14812991664628292\n",
      "Stochastic Gradient Descent(15/999): loss=0.14568596616361756\n",
      "Stochastic Gradient Descent(16/999): loss=0.14349577612368733\n",
      "Stochastic Gradient Descent(17/999): loss=0.14153281631286516\n",
      "Stochastic Gradient Descent(18/999): loss=0.1397733308182246\n",
      "Stochastic Gradient Descent(19/999): loss=0.1381960479071421\n",
      "Stochastic Gradient Descent(20/999): loss=0.13678192024616972\n",
      "Stochastic Gradient Descent(21/999): loss=0.13551389228645355\n",
      "Stochastic Gradient Descent(22/999): loss=0.1343766919747603\n",
      "Stochastic Gradient Descent(23/999): loss=0.13335664424626517\n",
      "Stochastic Gradient Descent(24/999): loss=0.1324415040212771\n",
      "Stochastic Gradient Descent(25/999): loss=0.13162030666627944\n",
      "Stochastic Gradient Descent(26/999): loss=0.1308832340929586\n",
      "Stochastic Gradient Descent(27/999): loss=0.1302214948598804\n",
      "Stochastic Gradient Descent(28/999): loss=0.12962721681248981\n",
      "Stochastic Gradient Descent(29/999): loss=0.12909335095024044\n",
      "Stochastic Gradient Descent(30/999): loss=0.1286135853467792\n",
      "Stochastic Gradient Descent(31/999): loss=0.12818226807188965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(32/999): loss=0.12779433817383654\n",
      "Stochastic Gradient Descent(33/999): loss=0.12744526387919636\n",
      "Stochastic Gradient Descent(34/999): loss=0.1271309872554079\n",
      "Stochastic Gradient Descent(35/999): loss=0.12684787466020306\n",
      "Stochastic Gradient Descent(36/999): loss=0.12659267237275879\n",
      "Stochastic Gradient Descent(37/999): loss=0.12636246686469038\n",
      "Stochastic Gradient Descent(38/999): loss=0.12615464922567723\n",
      "Stochastic Gradient Descent(39/999): loss=0.12596688330925082\n",
      "Stochastic Gradient Descent(40/999): loss=0.12579707720971003\n",
      "Stochastic Gradient Descent(41/999): loss=0.1256433577218118\n",
      "Stochastic Gradient Descent(42/999): loss=0.12550404747131444\n",
      "Stochastic Gradient Descent(43/999): loss=0.1253776444370699\n",
      "Stochastic Gradient Descent(44/999): loss=0.12526280361456948\n",
      "Stochastic Gradient Descent(45/999): loss=0.1251583205970008\n",
      "Stochastic Gradient Descent(46/999): loss=0.1250631168732938\n",
      "Stochastic Gradient Descent(47/999): loss=0.12497622666360124\n",
      "Stochastic Gradient Descent(48/999): loss=0.12489678513143773\n",
      "Stochastic Gradient Descent(49/999): loss=0.12482401782851375\n",
      "Stochastic Gradient Descent(50/999): loss=0.12475723124335562\n",
      "Stochastic Gradient Descent(51/999): loss=0.12469580433828421\n",
      "Stochastic Gradient Descent(52/999): loss=0.12463918097139451\n",
      "Stochastic Gradient Descent(53/999): loss=0.12458686311098834\n",
      "Stochastic Gradient Descent(54/999): loss=0.124538404759589\n",
      "Stochastic Gradient Descent(55/999): loss=0.12449340651333485\n",
      "Stochastic Gradient Descent(56/999): loss=0.12445151069030612\n",
      "Stochastic Gradient Descent(57/999): loss=0.12441239696829046\n",
      "Stochastic Gradient Descent(58/999): loss=0.12437577847871213\n",
      "Stochastic Gradient Descent(59/999): loss=0.12434139830902258\n",
      "Stochastic Gradient Descent(60/999): loss=0.12430902637083699\n",
      "Stochastic Gradient Descent(61/999): loss=0.12427845659557071\n",
      "Stochastic Gradient Descent(62/999): loss=0.12424950442332594\n",
      "Stochastic Gradient Descent(63/999): loss=0.1242220045543636\n",
      "Stochastic Gradient Descent(64/999): loss=0.12419580893570022\n",
      "Stochastic Gradient Descent(65/999): loss=0.12417078495824244\n",
      "Stochastic Gradient Descent(66/999): loss=0.1241468138424417\n",
      "Stochastic Gradient Descent(67/999): loss=0.12412378919275581\n",
      "Stochastic Gradient Descent(68/999): loss=0.12410161570326401\n",
      "Stochastic Gradient Descent(69/999): loss=0.12408020799862933\n",
      "Stochastic Gradient Descent(70/999): loss=0.12405948959625433\n",
      "Stochastic Gradient Descent(71/999): loss=0.12403939197695675\n",
      "Stochastic Gradient Descent(72/999): loss=0.12401985375281713\n",
      "Stochastic Gradient Descent(73/999): loss=0.12400081992203632\n",
      "Stochastic Gradient Descent(74/999): loss=0.1239822412017047\n",
      "Stochastic Gradient Descent(75/999): loss=0.12396407343033515\n",
      "Stochastic Gradient Descent(76/999): loss=0.123946277032865\n",
      "Stochastic Gradient Descent(77/999): loss=0.12392881654159409\n",
      "Stochastic Gradient Descent(78/999): loss=0.1239116601672098\n",
      "Stochastic Gradient Descent(79/999): loss=0.12389477941466152\n",
      "Stochastic Gradient Descent(80/999): loss=0.1238781487391943\n",
      "Stochastic Gradient Descent(81/999): loss=0.12386174523834273\n",
      "Stochastic Gradient Descent(82/999): loss=0.12384554837612448\n",
      "Stochastic Gradient Descent(83/999): loss=0.12382953973606603\n",
      "Stochastic Gradient Descent(84/999): loss=0.12381370280004653\n",
      "Stochastic Gradient Descent(85/999): loss=0.1237980227502591\n",
      "Stochastic Gradient Descent(86/999): loss=0.12378248629187324\n",
      "Stochastic Gradient Descent(87/999): loss=0.12376708149423256\n",
      "Stochastic Gradient Descent(88/999): loss=0.12375179764865073\n",
      "Stochastic Gradient Descent(89/999): loss=0.12373662514106938\n",
      "Stochastic Gradient Descent(90/999): loss=0.12372155533802477\n",
      "Stochastic Gradient Descent(91/999): loss=0.12370658048452997\n",
      "Stochastic Gradient Descent(92/999): loss=0.12369169361262883\n",
      "Stochastic Gradient Descent(93/999): loss=0.1236768884595042\n",
      "Stochastic Gradient Descent(94/999): loss=0.12366215939414221\n",
      "Stochastic Gradient Descent(95/999): loss=0.12364750135165777\n",
      "Stochastic Gradient Descent(96/999): loss=0.12363290977448044\n",
      "Stochastic Gradient Descent(97/999): loss=0.12361838055968317\n",
      "Stochastic Gradient Descent(98/999): loss=0.12360391001181191\n",
      "Stochastic Gradient Descent(99/999): loss=0.12358949480064073\n",
      "Stochastic Gradient Descent(100/999): loss=0.12357513192333776\n",
      "Stochastic Gradient Descent(101/999): loss=0.12356081867058064\n",
      "Stochastic Gradient Descent(102/999): loss=0.12354655259620866\n",
      "Stochastic Gradient Descent(103/999): loss=0.12353233149004184\n",
      "Stochastic Gradient Descent(104/999): loss=0.123518153353536\n",
      "Stochastic Gradient Descent(105/999): loss=0.12350401637797724\n",
      "Stochastic Gradient Descent(106/999): loss=0.12348991892495056\n",
      "Stochastic Gradient Descent(107/999): loss=0.12347585950884463\n",
      "Stochastic Gradient Descent(108/999): loss=0.1234618367811802\n",
      "Stochastic Gradient Descent(109/999): loss=0.12344784951657162\n",
      "Stochastic Gradient Descent(110/999): loss=0.12343389660015035\n",
      "Stochastic Gradient Descent(111/999): loss=0.12341997701629799\n",
      "Stochastic Gradient Descent(112/999): loss=0.12340608983855235\n",
      "Stochastic Gradient Descent(113/999): loss=0.12339223422056309\n",
      "Stochastic Gradient Descent(114/999): loss=0.12337840938798832\n",
      "Stochastic Gradient Descent(115/999): loss=0.12336461463123288\n",
      "Stochastic Gradient Descent(116/999): loss=0.12335084929894137\n",
      "Stochastic Gradient Descent(117/999): loss=0.12333711279216598\n",
      "Stochastic Gradient Descent(118/999): loss=0.12332340455914002\n",
      "Stochastic Gradient Descent(119/999): loss=0.12330972409059242\n",
      "Stochastic Gradient Descent(120/999): loss=0.12329607091554795\n",
      "Stochastic Gradient Descent(121/999): loss=0.12328244459756164\n",
      "Stochastic Gradient Descent(122/999): loss=0.12326884473134281\n",
      "Stochastic Gradient Descent(123/999): loss=0.12325527093972712\n",
      "Stochastic Gradient Descent(124/999): loss=0.12324172287096122\n",
      "Stochastic Gradient Descent(125/999): loss=0.12322820019626718\n",
      "Stochastic Gradient Descent(126/999): loss=0.12321470260765681\n",
      "Stochastic Gradient Descent(127/999): loss=0.1232012298159711\n",
      "Stochastic Gradient Descent(128/999): loss=0.12318778154912005\n",
      "Stochastic Gradient Descent(129/999): loss=0.12317435755050289\n",
      "Stochastic Gradient Descent(130/999): loss=0.12316095757758956\n",
      "Stochastic Gradient Descent(131/999): loss=0.12314758140064656\n",
      "Stochastic Gradient Descent(132/999): loss=0.12313422880159237\n",
      "Stochastic Gradient Descent(133/999): loss=0.12312089957296897\n",
      "Stochastic Gradient Descent(134/999): loss=0.12310759351701715\n",
      "Stochastic Gradient Descent(135/999): loss=0.12309431044484494\n",
      "Stochastic Gradient Descent(136/999): loss=0.12308105017567976\n",
      "Stochastic Gradient Descent(137/999): loss=0.12306781253619518\n",
      "Stochastic Gradient Descent(138/999): loss=0.12305459735990469\n",
      "Stochastic Gradient Descent(139/999): loss=0.12304140448661578\n",
      "Stochastic Gradient Descent(140/999): loss=0.12302823376193774\n",
      "Stochastic Gradient Descent(141/999): loss=0.12301508503683789\n",
      "Stochastic Gradient Descent(142/999): loss=0.12300195816724101\n",
      "Stochastic Gradient Descent(143/999): loss=0.12298885301366776\n",
      "Stochastic Gradient Descent(144/999): loss=0.12297576944090773\n",
      "Stochastic Gradient Descent(145/999): loss=0.12296270731772417\n",
      "Stochastic Gradient Descent(146/999): loss=0.12294966651658623\n",
      "Stochastic Gradient Descent(147/999): loss=0.12293664691342683\n",
      "Stochastic Gradient Descent(148/999): loss=0.12292364838742277\n",
      "Stochastic Gradient Descent(149/999): loss=0.12291067082079545\n",
      "Stochastic Gradient Descent(150/999): loss=0.12289771409862955\n",
      "Stochastic Gradient Descent(151/999): loss=0.12288477810870826\n",
      "Stochastic Gradient Descent(152/999): loss=0.12287186274136297\n",
      "Stochastic Gradient Descent(153/999): loss=0.12285896788933645\n",
      "Stochastic Gradient Descent(154/999): loss=0.12284609344765776\n",
      "Stochastic Gradient Descent(155/999): loss=0.12283323931352787\n",
      "Stochastic Gradient Descent(156/999): loss=0.12282040538621479\n",
      "Stochastic Gradient Descent(157/999): loss=0.1228075915669577\n",
      "Stochastic Gradient Descent(158/999): loss=0.12279479775887837\n",
      "Stochastic Gradient Descent(159/999): loss=0.12278202386690001\n",
      "Stochastic Gradient Descent(160/999): loss=0.12276926979767228\n",
      "Stochastic Gradient Descent(161/999): loss=0.12275653545950208\n",
      "Stochastic Gradient Descent(162/999): loss=0.12274382076228929\n",
      "Stochastic Gradient Descent(163/999): loss=0.12273112561746759\n",
      "Stochastic Gradient Descent(164/999): loss=0.12271844993794899\n",
      "Stochastic Gradient Descent(165/999): loss=0.12270579363807262\n",
      "Stochastic Gradient Descent(166/999): loss=0.12269315663355655\n",
      "Stochastic Gradient Descent(167/999): loss=0.12268053884145312\n",
      "Stochastic Gradient Descent(168/999): loss=0.12266794018010685\n",
      "Stochastic Gradient Descent(169/999): loss=0.12265536056911493\n",
      "Stochastic Gradient Descent(170/999): loss=0.12264279992929038\n",
      "Stochastic Gradient Descent(171/999): loss=0.12263025818262675\n",
      "Stochastic Gradient Descent(172/999): loss=0.12261773525226537\n",
      "Stochastic Gradient Descent(173/999): loss=0.12260523106246392\n",
      "Stochastic Gradient Descent(174/999): loss=0.12259274553856692\n",
      "Stochastic Gradient Descent(175/999): loss=0.12258027860697751\n",
      "Stochastic Gradient Descent(176/999): loss=0.12256783019513073\n",
      "Stochastic Gradient Descent(177/999): loss=0.12255540023146792\n",
      "Stochastic Gradient Descent(178/999): loss=0.12254298864541227\n",
      "Stochastic Gradient Descent(179/999): loss=0.12253059536734565\n",
      "Stochastic Gradient Descent(180/999): loss=0.12251822032858604\n",
      "Stochastic Gradient Descent(181/999): loss=0.12250586346136627\n",
      "Stochastic Gradient Descent(182/999): loss=0.12249352469881324\n",
      "Stochastic Gradient Descent(183/999): loss=0.12248120397492814\n",
      "Stochastic Gradient Descent(184/999): loss=0.1224689012245673\n",
      "Stochastic Gradient Descent(185/999): loss=0.12245661638342371\n",
      "Stochastic Gradient Descent(186/999): loss=0.12244434938800926\n",
      "Stochastic Gradient Descent(187/999): loss=0.12243210017563741\n",
      "Stochastic Gradient Descent(188/999): loss=0.12241986868440653\n",
      "Stochastic Gradient Descent(189/999): loss=0.12240765485318376\n",
      "Stochastic Gradient Descent(190/999): loss=0.12239545862158925\n",
      "Stochastic Gradient Descent(191/999): loss=0.12238327992998096\n",
      "Stochastic Gradient Descent(192/999): loss=0.12237111871943991\n",
      "Stochastic Gradient Descent(193/999): loss=0.12235897493175575\n",
      "Stochastic Gradient Descent(194/999): loss=0.12234684850941265\n",
      "Stochastic Gradient Descent(195/999): loss=0.12233473939557597\n",
      "Stochastic Gradient Descent(196/999): loss=0.1223226475340787\n",
      "Stochastic Gradient Descent(197/999): loss=0.12231057286940865\n",
      "Stochastic Gradient Descent(198/999): loss=0.12229851534669586\n",
      "Stochastic Gradient Descent(199/999): loss=0.12228647491170022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(200/999): loss=0.12227445151079958\n",
      "Stochastic Gradient Descent(201/999): loss=0.12226244509097788\n",
      "Stochastic Gradient Descent(202/999): loss=0.1222504555998138\n",
      "Stochastic Gradient Descent(203/999): loss=0.12223848298546958\n",
      "Stochastic Gradient Descent(204/999): loss=0.12222652719668\n",
      "Stochastic Gradient Descent(205/999): loss=0.1222145881827418\n",
      "Stochastic Gradient Descent(206/999): loss=0.12220266589350318\n",
      "Stochastic Gradient Descent(207/999): loss=0.12219076027935352\n",
      "Stochastic Gradient Descent(208/999): loss=0.12217887129121348\n",
      "Stochastic Gradient Descent(209/999): loss=0.12216699888052517\n",
      "Stochastic Gradient Descent(210/999): loss=0.12215514299924254\n",
      "Stochastic Gradient Descent(211/999): loss=0.12214330359982213\n",
      "Stochastic Gradient Descent(212/999): loss=0.12213148063521362\n",
      "Stochastic Gradient Descent(213/999): loss=0.12211967405885123\n",
      "Stochastic Gradient Descent(214/999): loss=0.12210788382464453\n",
      "Stochastic Gradient Descent(215/999): loss=0.12209610988697017\n",
      "Stochastic Gradient Descent(216/999): loss=0.12208435220066315\n",
      "Stochastic Gradient Descent(217/999): loss=0.12207261072100874\n",
      "Stochastic Gradient Descent(218/999): loss=0.12206088540373425\n",
      "Stochastic Gradient Descent(219/999): loss=0.12204917620500118\n",
      "Stochastic Gradient Descent(220/999): loss=0.12203748308139742\n",
      "Stochastic Gradient Descent(221/999): loss=0.1220258059899296\n",
      "Stochastic Gradient Descent(222/999): loss=0.12201414488801567\n",
      "Stochastic Gradient Descent(223/999): loss=0.12200249973347746\n",
      "Stochastic Gradient Descent(224/999): loss=0.12199087048453372\n",
      "Stochastic Gradient Descent(225/999): loss=0.1219792570997928\n",
      "Stochastic Gradient Descent(226/999): loss=0.1219676595382461\n",
      "Stochastic Gradient Descent(227/999): loss=0.12195607775926093\n",
      "Stochastic Gradient Descent(228/999): loss=0.12194451172257414\n",
      "Stochastic Gradient Descent(229/999): loss=0.12193296138828566\n",
      "Stochastic Gradient Descent(230/999): loss=0.12192142671685192\n",
      "Stochastic Gradient Descent(231/999): loss=0.12190990766907976\n",
      "Stochastic Gradient Descent(232/999): loss=0.12189840420612028\n",
      "Stochastic Gradient Descent(233/999): loss=0.12188691628946279\n",
      "Stochastic Gradient Descent(234/999): loss=0.12187544388092901\n",
      "Stochastic Gradient Descent(235/999): loss=0.1218639869426672\n",
      "Stochastic Gradient Descent(236/999): loss=0.12185254543714662\n",
      "Stochastic Gradient Descent(237/999): loss=0.1218411193271519\n",
      "Stochastic Gradient Descent(238/999): loss=0.12182970857577764\n",
      "Stochastic Gradient Descent(239/999): loss=0.12181831314642298\n",
      "Stochastic Gradient Descent(240/999): loss=0.12180693300278655\n",
      "Stochastic Gradient Descent(241/999): loss=0.12179556810886118\n",
      "Stochastic Gradient Descent(242/999): loss=0.1217842184289289\n",
      "Stochastic Gradient Descent(243/999): loss=0.12177288392755614\n",
      "Stochastic Gradient Descent(244/999): loss=0.12176156456958874\n",
      "Stochastic Gradient Descent(245/999): loss=0.1217502603201472\n",
      "Stochastic Gradient Descent(246/999): loss=0.1217389711446222\n",
      "Stochastic Gradient Descent(247/999): loss=0.12172769700866982\n",
      "Stochastic Gradient Descent(248/999): loss=0.1217164378782072\n",
      "Stochastic Gradient Descent(249/999): loss=0.12170519371940816\n",
      "Stochastic Gradient Descent(250/999): loss=0.1216939644986988\n",
      "Stochastic Gradient Descent(251/999): loss=0.12168275018275342\n",
      "Stochastic Gradient Descent(252/999): loss=0.12167155073849026\n",
      "Stochastic Gradient Descent(253/999): loss=0.12166036613306742\n",
      "Stochastic Gradient Descent(254/999): loss=0.12164919633387904\n",
      "Stochastic Gradient Descent(255/999): loss=0.12163804130855121\n",
      "Stochastic Gradient Descent(256/999): loss=0.12162690102493837\n",
      "Stochastic Gradient Descent(257/999): loss=0.12161577545111918\n",
      "Stochastic Gradient Descent(258/999): loss=0.12160466455539326\n",
      "Stochastic Gradient Descent(259/999): loss=0.12159356830627731\n",
      "Stochastic Gradient Descent(260/999): loss=0.12158248667250163\n",
      "Stochastic Gradient Descent(261/999): loss=0.12157141962300672\n",
      "Stochastic Gradient Descent(262/999): loss=0.12156036712693971\n",
      "Stochastic Gradient Descent(263/999): loss=0.12154932915365114\n",
      "Stochastic Gradient Descent(264/999): loss=0.1215383056726917\n",
      "Stochastic Gradient Descent(265/999): loss=0.1215272966538089\n",
      "Stochastic Gradient Descent(266/999): loss=0.12151630206694401\n",
      "Stochastic Gradient Descent(267/999): loss=0.1215053218822289\n",
      "Stochastic Gradient Descent(268/999): loss=0.12149435606998313\n",
      "Stochastic Gradient Descent(269/999): loss=0.12148340460071076\n",
      "Stochastic Gradient Descent(270/999): loss=0.12147246744509756\n",
      "Stochastic Gradient Descent(271/999): loss=0.12146154457400823\n",
      "Stochastic Gradient Descent(272/999): loss=0.12145063595848339\n",
      "Stochastic Gradient Descent(273/999): loss=0.12143974156973697\n",
      "Stochastic Gradient Descent(274/999): loss=0.12142886137915347\n",
      "Stochastic Gradient Descent(275/999): loss=0.12141799535828526\n",
      "Stochastic Gradient Descent(276/999): loss=0.12140714347885002\n",
      "Stochastic Gradient Descent(277/999): loss=0.12139630571272822\n",
      "Stochastic Gradient Descent(278/999): loss=0.12138548203196055\n",
      "Stochastic Gradient Descent(279/999): loss=0.12137467240874548\n",
      "Stochastic Gradient Descent(280/999): loss=0.1213638768154369\n",
      "Stochastic Gradient Descent(281/999): loss=0.12135309522454164\n",
      "Stochastic Gradient Descent(282/999): loss=0.12134232760871737\n",
      "Stochastic Gradient Descent(283/999): loss=0.12133157394077\n",
      "Stochastic Gradient Descent(284/999): loss=0.12132083419365176\n",
      "Stochastic Gradient Descent(285/999): loss=0.12131010834045888\n",
      "Stochastic Gradient Descent(286/999): loss=0.1212993963544294\n",
      "Stochastic Gradient Descent(287/999): loss=0.12128869820894114\n",
      "Stochastic Gradient Descent(288/999): loss=0.12127801387750956\n",
      "Stochastic Gradient Descent(289/999): loss=0.12126734333378579\n",
      "Stochastic Gradient Descent(290/999): loss=0.12125668655155462\n",
      "Stochastic Gradient Descent(291/999): loss=0.12124604350473263\n",
      "Stochastic Gradient Descent(292/999): loss=0.12123541416736604\n",
      "Stochastic Gradient Descent(293/999): loss=0.12122479851362915\n",
      "Stochastic Gradient Descent(294/999): loss=0.12121419651782221\n",
      "Stochastic Gradient Descent(295/999): loss=0.1212036081543698\n",
      "Stochastic Gradient Descent(296/999): loss=0.12119303339781892\n",
      "Stochastic Gradient Descent(297/999): loss=0.12118247222283747\n",
      "Stochastic Gradient Descent(298/999): loss=0.1211719246042122\n",
      "Stochastic Gradient Descent(299/999): loss=0.12116139051684735\n",
      "Stochastic Gradient Descent(300/999): loss=0.12115086993576288\n",
      "Stochastic Gradient Descent(301/999): loss=0.12114036283609278\n",
      "Stochastic Gradient Descent(302/999): loss=0.12112986919308359\n",
      "Stochastic Gradient Descent(303/999): loss=0.12111938898209292\n",
      "Stochastic Gradient Descent(304/999): loss=0.12110892217858765\n",
      "Stochastic Gradient Descent(305/999): loss=0.12109846875814273\n",
      "Stochastic Gradient Descent(306/999): loss=0.12108802869643959\n",
      "Stochastic Gradient Descent(307/999): loss=0.12107760196926463\n",
      "Stochastic Gradient Descent(308/999): loss=0.12106718855250795\n",
      "Stochastic Gradient Descent(309/999): loss=0.12105678842216186\n",
      "Stochastic Gradient Descent(310/999): loss=0.12104640155431952\n",
      "Stochastic Gradient Descent(311/999): loss=0.12103602792517375\n",
      "Stochastic Gradient Descent(312/999): loss=0.12102566751101541\n",
      "Stochastic Gradient Descent(313/999): loss=0.12101532028823249\n",
      "Stochastic Gradient Descent(314/999): loss=0.12100498623330856\n",
      "Stochastic Gradient Descent(315/999): loss=0.12099466532282167\n",
      "Stochastic Gradient Descent(316/999): loss=0.12098435753344312\n",
      "Stochastic Gradient Descent(317/999): loss=0.12097406284193622\n",
      "Stochastic Gradient Descent(318/999): loss=0.12096378122515515\n",
      "Stochastic Gradient Descent(319/999): loss=0.12095351266004381\n",
      "Stochastic Gradient Descent(320/999): loss=0.12094325712363467\n",
      "Stochastic Gradient Descent(321/999): loss=0.12093301459304774\n",
      "Stochastic Gradient Descent(322/999): loss=0.12092278504548931\n",
      "Stochastic Gradient Descent(323/999): loss=0.12091256845825119\n",
      "Stochastic Gradient Descent(324/999): loss=0.12090236480870928\n",
      "Stochastic Gradient Descent(325/999): loss=0.1208921740743228\n",
      "Stochastic Gradient Descent(326/999): loss=0.12088199623263332\n",
      "Stochastic Gradient Descent(327/999): loss=0.12087183126126348\n",
      "Stochastic Gradient Descent(328/999): loss=0.12086167913791633\n",
      "Stochastic Gradient Descent(329/999): loss=0.12085153984037424\n",
      "Stochastic Gradient Descent(330/999): loss=0.12084141334649792\n",
      "Stochastic Gradient Descent(331/999): loss=0.12083129963422556\n",
      "Stochastic Gradient Descent(332/999): loss=0.12082119868157207\n",
      "Stochastic Gradient Descent(333/999): loss=0.1208111104666278\n",
      "Stochastic Gradient Descent(334/999): loss=0.1208010349675581\n",
      "Stochastic Gradient Descent(335/999): loss=0.12079097216260222\n",
      "Stochastic Gradient Descent(336/999): loss=0.12078092203007254\n",
      "Stochastic Gradient Descent(337/999): loss=0.12077088454835373\n",
      "Stochastic Gradient Descent(338/999): loss=0.12076085969590199\n",
      "Stochastic Gradient Descent(339/999): loss=0.12075084745124415\n",
      "Stochastic Gradient Descent(340/999): loss=0.12074084779297707\n",
      "Stochastic Gradient Descent(341/999): loss=0.12073086069976666\n",
      "Stochastic Gradient Descent(342/999): loss=0.12072088615034729\n",
      "Stochastic Gradient Descent(343/999): loss=0.12071092412352098\n",
      "Stochastic Gradient Descent(344/999): loss=0.1207009745981567\n",
      "Stochastic Gradient Descent(345/999): loss=0.12069103755318963\n",
      "Stochastic Gradient Descent(346/999): loss=0.12068111296762057\n",
      "Stochastic Gradient Descent(347/999): loss=0.12067120082051509\n",
      "Stochastic Gradient Descent(348/999): loss=0.1206613010910029\n",
      "Stochastic Gradient Descent(349/999): loss=0.12065141375827733\n",
      "Stochastic Gradient Descent(350/999): loss=0.12064153880159449\n",
      "Stochastic Gradient Descent(351/999): loss=0.12063167620027275\n",
      "Stochastic Gradient Descent(352/999): loss=0.12062182593369214\n",
      "Stochastic Gradient Descent(353/999): loss=0.12061198798129354\n",
      "Stochastic Gradient Descent(354/999): loss=0.12060216232257831\n",
      "Stochastic Gradient Descent(355/999): loss=0.12059234893710755\n",
      "Stochastic Gradient Descent(356/999): loss=0.12058254780450163\n",
      "Stochastic Gradient Descent(357/999): loss=0.1205727589044395\n",
      "Stochastic Gradient Descent(358/999): loss=0.12056298221665818\n",
      "Stochastic Gradient Descent(359/999): loss=0.1205532177209522\n",
      "Stochastic Gradient Descent(360/999): loss=0.1205434653971731\n",
      "Stochastic Gradient Descent(361/999): loss=0.12053372522522882\n",
      "Stochastic Gradient Descent(362/999): loss=0.12052399718508326\n",
      "Stochastic Gradient Descent(363/999): loss=0.12051428125675567\n",
      "Stochastic Gradient Descent(364/999): loss=0.12050457742032021\n",
      "Stochastic Gradient Descent(365/999): loss=0.12049488565590549\n",
      "Stochastic Gradient Descent(366/999): loss=0.12048520594369393\n",
      "Stochastic Gradient Descent(367/999): loss=0.12047553826392149\n",
      "Stochastic Gradient Descent(368/999): loss=0.12046588259687703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(369/999): loss=0.12045623892290193\n",
      "Stochastic Gradient Descent(370/999): loss=0.1204466072223895\n",
      "Stochastic Gradient Descent(371/999): loss=0.12043698747578487\n",
      "Stochastic Gradient Descent(372/999): loss=0.12042737966358405\n",
      "Stochastic Gradient Descent(373/999): loss=0.12041778376633404\n",
      "Stochastic Gradient Descent(374/999): loss=0.1204081997646319\n",
      "Stochastic Gradient Descent(375/999): loss=0.12039862763912476\n",
      "Stochastic Gradient Descent(376/999): loss=0.12038906737050908\n",
      "Stochastic Gradient Descent(377/999): loss=0.12037951893953046\n",
      "Stochastic Gradient Descent(378/999): loss=0.12036998232698314\n",
      "Stochastic Gradient Descent(379/999): loss=0.12036045751370965\n",
      "Stochastic Gradient Descent(380/999): loss=0.12035094448060041\n",
      "Stochastic Gradient Descent(381/999): loss=0.12034144320859334\n",
      "Stochastic Gradient Descent(382/999): loss=0.12033195367867355\n",
      "Stochastic Gradient Descent(383/999): loss=0.12032247587187292\n",
      "Stochastic Gradient Descent(384/999): loss=0.1203130097692697\n",
      "Stochastic Gradient Descent(385/999): loss=0.12030355535198833\n",
      "Stochastic Gradient Descent(386/999): loss=0.1202941126011989\n",
      "Stochastic Gradient Descent(387/999): loss=0.12028468149811686\n",
      "Stochastic Gradient Descent(388/999): loss=0.12027526202400281\n",
      "Stochastic Gradient Descent(389/999): loss=0.12026585416016197\n",
      "Stochastic Gradient Descent(390/999): loss=0.12025645788794403\n",
      "Stochastic Gradient Descent(391/999): loss=0.12024707318874274\n",
      "Stochastic Gradient Descent(392/999): loss=0.12023770004399562\n",
      "Stochastic Gradient Descent(393/999): loss=0.12022833843518362\n",
      "Stochastic Gradient Descent(394/999): loss=0.1202189883438309\n",
      "Stochastic Gradient Descent(395/999): loss=0.1202096497515045\n",
      "Stochastic Gradient Descent(396/999): loss=0.1202003226398139\n",
      "Stochastic Gradient Descent(397/999): loss=0.120191006990411\n",
      "Stochastic Gradient Descent(398/999): loss=0.12018170278498963\n",
      "Stochastic Gradient Descent(399/999): loss=0.12017241000528534\n",
      "Stochastic Gradient Descent(400/999): loss=0.1201631286330751\n",
      "Stochastic Gradient Descent(401/999): loss=0.12015385865017707\n",
      "Stochastic Gradient Descent(402/999): loss=0.12014460003845037\n",
      "Stochastic Gradient Descent(403/999): loss=0.12013535277979462\n",
      "Stochastic Gradient Descent(404/999): loss=0.12012611685614996\n",
      "Stochastic Gradient Descent(405/999): loss=0.12011689224949659\n",
      "Stochastic Gradient Descent(406/999): loss=0.12010767894185462\n",
      "Stochastic Gradient Descent(407/999): loss=0.1200984769152838\n",
      "Stochastic Gradient Descent(408/999): loss=0.12008928615188327\n",
      "Stochastic Gradient Descent(409/999): loss=0.12008010663379126\n",
      "Stochastic Gradient Descent(410/999): loss=0.12007093834318501\n",
      "Stochastic Gradient Descent(411/999): loss=0.12006178126228037\n",
      "Stochastic Gradient Descent(412/999): loss=0.12005263537333173\n",
      "Stochastic Gradient Descent(413/999): loss=0.12004350065863172\n",
      "Stochastic Gradient Descent(414/999): loss=0.12003437710051086\n",
      "Stochastic Gradient Descent(415/999): loss=0.12002526468133767\n",
      "Stochastic Gradient Descent(416/999): loss=0.12001616338351807\n",
      "Stochastic Gradient Descent(417/999): loss=0.12000707318949548\n",
      "Stochastic Gradient Descent(418/999): loss=0.11999799408175044\n",
      "Stochastic Gradient Descent(419/999): loss=0.11998892604280044\n",
      "Stochastic Gradient Descent(420/999): loss=0.11997986905519978\n",
      "Stochastic Gradient Descent(421/999): loss=0.1199708231015393\n",
      "Stochastic Gradient Descent(422/999): loss=0.11996178816444623\n",
      "Stochastic Gradient Descent(423/999): loss=0.11995276422658396\n",
      "Stochastic Gradient Descent(424/999): loss=0.11994375127065185\n",
      "Stochastic Gradient Descent(425/999): loss=0.11993474927938512\n",
      "Stochastic Gradient Descent(426/999): loss=0.11992575823555456\n",
      "Stochastic Gradient Descent(427/999): loss=0.11991677812196637\n",
      "Stochastic Gradient Descent(428/999): loss=0.11990780892146213\n",
      "Stochastic Gradient Descent(429/999): loss=0.11989885061691839\n",
      "Stochastic Gradient Descent(430/999): loss=0.11988990319124662\n",
      "Stochastic Gradient Descent(431/999): loss=0.11988096662739306\n",
      "Stochastic Gradient Descent(432/999): loss=0.11987204090833854\n",
      "Stochastic Gradient Descent(433/999): loss=0.11986312601709823\n",
      "Stochastic Gradient Descent(434/999): loss=0.11985422193672154\n",
      "Stochastic Gradient Descent(435/999): loss=0.11984532865029206\n",
      "Stochastic Gradient Descent(436/999): loss=0.1198364461409272\n",
      "Stochastic Gradient Descent(437/999): loss=0.11982757439177812\n",
      "Stochastic Gradient Descent(438/999): loss=0.11981871338602963\n",
      "Stochastic Gradient Descent(439/999): loss=0.11980986310690002\n",
      "Stochastic Gradient Descent(440/999): loss=0.11980102353764077\n",
      "Stochastic Gradient Descent(441/999): loss=0.11979219466153668\n",
      "Stochastic Gradient Descent(442/999): loss=0.11978337646190534\n",
      "Stochastic Gradient Descent(443/999): loss=0.11977456892209742\n",
      "Stochastic Gradient Descent(444/999): loss=0.11976577202549618\n",
      "Stochastic Gradient Descent(445/999): loss=0.11975698575551745\n",
      "Stochastic Gradient Descent(446/999): loss=0.11974821009560956\n",
      "Stochastic Gradient Descent(447/999): loss=0.11973944502925311\n",
      "Stochastic Gradient Descent(448/999): loss=0.11973069053996085\n",
      "Stochastic Gradient Descent(449/999): loss=0.11972194661127761\n",
      "Stochastic Gradient Descent(450/999): loss=0.11971321322678001\n",
      "Stochastic Gradient Descent(451/999): loss=0.1197044903700766\n",
      "Stochastic Gradient Descent(452/999): loss=0.11969577802480744\n",
      "Stochastic Gradient Descent(453/999): loss=0.11968707617464414\n",
      "Stochastic Gradient Descent(454/999): loss=0.11967838480328971\n",
      "Stochastic Gradient Descent(455/999): loss=0.11966970389447841\n",
      "Stochastic Gradient Descent(456/999): loss=0.11966103343197565\n",
      "Stochastic Gradient Descent(457/999): loss=0.11965237339957784\n",
      "Stochastic Gradient Descent(458/999): loss=0.11964372378111231\n",
      "Stochastic Gradient Descent(459/999): loss=0.11963508456043716\n",
      "Stochastic Gradient Descent(460/999): loss=0.11962645572144119\n",
      "Stochastic Gradient Descent(461/999): loss=0.11961783724804373\n",
      "Stochastic Gradient Descent(462/999): loss=0.11960922912419446\n",
      "Stochastic Gradient Descent(463/999): loss=0.11960063133387355\n",
      "Stochastic Gradient Descent(464/999): loss=0.11959204386109125\n",
      "Stochastic Gradient Descent(465/999): loss=0.11958346668988802\n",
      "Stochastic Gradient Descent(466/999): loss=0.11957489980433422\n",
      "Stochastic Gradient Descent(467/999): loss=0.11956634318853011\n",
      "Stochastic Gradient Descent(468/999): loss=0.1195577968266058\n",
      "Stochastic Gradient Descent(469/999): loss=0.11954926070272104\n",
      "Stochastic Gradient Descent(470/999): loss=0.11954073480106514\n",
      "Stochastic Gradient Descent(471/999): loss=0.1195322191058569\n",
      "Stochastic Gradient Descent(472/999): loss=0.11952371360134448\n",
      "Stochastic Gradient Descent(473/999): loss=0.11951521827180532\n",
      "Stochastic Gradient Descent(474/999): loss=0.11950673310154604\n",
      "Stochastic Gradient Descent(475/999): loss=0.11949825807490229\n",
      "Stochastic Gradient Descent(476/999): loss=0.11948979317623878\n",
      "Stochastic Gradient Descent(477/999): loss=0.11948133838994898\n",
      "Stochastic Gradient Descent(478/999): loss=0.11947289370045532\n",
      "Stochastic Gradient Descent(479/999): loss=0.11946445909220872\n",
      "Stochastic Gradient Descent(480/999): loss=0.11945603454968885\n",
      "Stochastic Gradient Descent(481/999): loss=0.11944762005740385\n",
      "Stochastic Gradient Descent(482/999): loss=0.11943921559989024\n",
      "Stochastic Gradient Descent(483/999): loss=0.1194308211617129\n",
      "Stochastic Gradient Descent(484/999): loss=0.11942243672746498\n",
      "Stochastic Gradient Descent(485/999): loss=0.11941406228176768\n",
      "Stochastic Gradient Descent(486/999): loss=0.11940569780927039\n",
      "Stochastic Gradient Descent(487/999): loss=0.11939734329465039\n",
      "Stochastic Gradient Descent(488/999): loss=0.11938899872261287\n",
      "Stochastic Gradient Descent(489/999): loss=0.11938066407789086\n",
      "Stochastic Gradient Descent(490/999): loss=0.11937233934524508\n",
      "Stochastic Gradient Descent(491/999): loss=0.11936402450946389\n",
      "Stochastic Gradient Descent(492/999): loss=0.11935571955536324\n",
      "Stochastic Gradient Descent(493/999): loss=0.11934742446778651\n",
      "Stochastic Gradient Descent(494/999): loss=0.11933913923160452\n",
      "Stochastic Gradient Descent(495/999): loss=0.11933086383171541\n",
      "Stochastic Gradient Descent(496/999): loss=0.11932259825304452\n",
      "Stochastic Gradient Descent(497/999): loss=0.11931434248054434\n",
      "Stochastic Gradient Descent(498/999): loss=0.1193060964991945\n",
      "Stochastic Gradient Descent(499/999): loss=0.11929786029400163\n",
      "Stochastic Gradient Descent(500/999): loss=0.11928963384999919\n",
      "Stochastic Gradient Descent(501/999): loss=0.11928141715224766\n",
      "Stochastic Gradient Descent(502/999): loss=0.11927321018583416\n",
      "Stochastic Gradient Descent(503/999): loss=0.11926501293587255\n",
      "Stochastic Gradient Descent(504/999): loss=0.11925682538750335\n",
      "Stochastic Gradient Descent(505/999): loss=0.1192486475258936\n",
      "Stochastic Gradient Descent(506/999): loss=0.11924047933623685\n",
      "Stochastic Gradient Descent(507/999): loss=0.1192323208037531\n",
      "Stochastic Gradient Descent(508/999): loss=0.11922417191368863\n",
      "Stochastic Gradient Descent(509/999): loss=0.11921603265131596\n",
      "Stochastic Gradient Descent(510/999): loss=0.11920790300193392\n",
      "Stochastic Gradient Descent(511/999): loss=0.11919978295086742\n",
      "Stochastic Gradient Descent(512/999): loss=0.11919167248346743\n",
      "Stochastic Gradient Descent(513/999): loss=0.11918357158511089\n",
      "Stochastic Gradient Descent(514/999): loss=0.11917548024120074\n",
      "Stochastic Gradient Descent(515/999): loss=0.11916739843716574\n",
      "Stochastic Gradient Descent(516/999): loss=0.11915932615846042\n",
      "Stochastic Gradient Descent(517/999): loss=0.11915126339056507\n",
      "Stochastic Gradient Descent(518/999): loss=0.11914321011898563\n",
      "Stochastic Gradient Descent(519/999): loss=0.11913516632925365\n",
      "Stochastic Gradient Descent(520/999): loss=0.11912713200692623\n",
      "Stochastic Gradient Descent(521/999): loss=0.11911910713758588\n",
      "Stochastic Gradient Descent(522/999): loss=0.11911109170684057\n",
      "Stochastic Gradient Descent(523/999): loss=0.11910308570032363\n",
      "Stochastic Gradient Descent(524/999): loss=0.11909508910369357\n",
      "Stochastic Gradient Descent(525/999): loss=0.11908710190263423\n",
      "Stochastic Gradient Descent(526/999): loss=0.11907912408285456\n",
      "Stochastic Gradient Descent(527/999): loss=0.11907115563008856\n",
      "Stochastic Gradient Descent(528/999): loss=0.11906319653009538\n",
      "Stochastic Gradient Descent(529/999): loss=0.11905524676865903\n",
      "Stochastic Gradient Descent(530/999): loss=0.11904730633158846\n",
      "Stochastic Gradient Descent(531/999): loss=0.11903937520471752\n",
      "Stochastic Gradient Descent(532/999): loss=0.11903145337390476\n",
      "Stochastic Gradient Descent(533/999): loss=0.1190235408250336\n",
      "Stochastic Gradient Descent(534/999): loss=0.11901563754401205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(535/999): loss=0.11900774351677271\n",
      "Stochastic Gradient Descent(536/999): loss=0.11899985872927282\n",
      "Stochastic Gradient Descent(537/999): loss=0.11899198316749406\n",
      "Stochastic Gradient Descent(538/999): loss=0.1189841168174426\n",
      "Stochastic Gradient Descent(539/999): loss=0.11897625966514899\n",
      "Stochastic Gradient Descent(540/999): loss=0.1189684116966681\n",
      "Stochastic Gradient Descent(541/999): loss=0.1189605728980791\n",
      "Stochastic Gradient Descent(542/999): loss=0.11895274325548533\n",
      "Stochastic Gradient Descent(543/999): loss=0.11894492275501435\n",
      "Stochastic Gradient Descent(544/999): loss=0.11893711138281783\n",
      "Stochastic Gradient Descent(545/999): loss=0.11892930912507149\n",
      "Stochastic Gradient Descent(546/999): loss=0.11892151596797501\n",
      "Stochastic Gradient Descent(547/999): loss=0.11891373189775208\n",
      "Stochastic Gradient Descent(548/999): loss=0.11890595690065026\n",
      "Stochastic Gradient Descent(549/999): loss=0.11889819096294095\n",
      "Stochastic Gradient Descent(550/999): loss=0.11889043407091933\n",
      "Stochastic Gradient Descent(551/999): loss=0.11888268621090439\n",
      "Stochastic Gradient Descent(552/999): loss=0.11887494736923869\n",
      "Stochastic Gradient Descent(553/999): loss=0.1188672175322885\n",
      "Stochastic Gradient Descent(554/999): loss=0.11885949668644363\n",
      "Stochastic Gradient Descent(555/999): loss=0.11885178481811742\n",
      "Stochastic Gradient Descent(556/999): loss=0.1188440819137468\n",
      "Stochastic Gradient Descent(557/999): loss=0.11883638795979191\n",
      "Stochastic Gradient Descent(558/999): loss=0.11882870294273647\n",
      "Stochastic Gradient Descent(559/999): loss=0.1188210268490874\n",
      "Stochastic Gradient Descent(560/999): loss=0.11881335966537496\n",
      "Stochastic Gradient Descent(561/999): loss=0.11880570137815258\n",
      "Stochastic Gradient Descent(562/999): loss=0.11879805197399693\n",
      "Stochastic Gradient Descent(563/999): loss=0.11879041143950778\n",
      "Stochastic Gradient Descent(564/999): loss=0.11878277976130795\n",
      "Stochastic Gradient Descent(565/999): loss=0.11877515692604332\n",
      "Stochastic Gradient Descent(566/999): loss=0.11876754292038275\n",
      "Stochastic Gradient Descent(567/999): loss=0.118759937731018\n",
      "Stochastic Gradient Descent(568/999): loss=0.11875234134466373\n",
      "Stochastic Gradient Descent(569/999): loss=0.11874475374805747\n",
      "Stochastic Gradient Descent(570/999): loss=0.11873717492795946\n",
      "Stochastic Gradient Descent(571/999): loss=0.11872960487115275\n",
      "Stochastic Gradient Descent(572/999): loss=0.11872204356444307\n",
      "Stochastic Gradient Descent(573/999): loss=0.11871449099465874\n",
      "Stochastic Gradient Descent(574/999): loss=0.11870694714865074\n",
      "Stochastic Gradient Descent(575/999): loss=0.11869941201329261\n",
      "Stochastic Gradient Descent(576/999): loss=0.11869188557548033\n",
      "Stochastic Gradient Descent(577/999): loss=0.11868436782213235\n",
      "Stochastic Gradient Descent(578/999): loss=0.11867685874018963\n",
      "Stochastic Gradient Descent(579/999): loss=0.11866935831661533\n",
      "Stochastic Gradient Descent(580/999): loss=0.11866186653839508\n",
      "Stochastic Gradient Descent(581/999): loss=0.11865438339253673\n",
      "Stochastic Gradient Descent(582/999): loss=0.11864690886607035\n",
      "Stochastic Gradient Descent(583/999): loss=0.1186394429460482\n",
      "Stochastic Gradient Descent(584/999): loss=0.11863198561954469\n",
      "Stochastic Gradient Descent(585/999): loss=0.11862453687365632\n",
      "Stochastic Gradient Descent(586/999): loss=0.11861709669550159\n",
      "Stochastic Gradient Descent(587/999): loss=0.11860966507222113\n",
      "Stochastic Gradient Descent(588/999): loss=0.11860224199097749\n",
      "Stochastic Gradient Descent(589/999): loss=0.11859482743895498\n",
      "Stochastic Gradient Descent(590/999): loss=0.11858742140336\n",
      "Stochastic Gradient Descent(591/999): loss=0.11858002387142072\n",
      "Stochastic Gradient Descent(592/999): loss=0.11857263483038699\n",
      "Stochastic Gradient Descent(593/999): loss=0.11856525426753053\n",
      "Stochastic Gradient Descent(594/999): loss=0.11855788217014473\n",
      "Stochastic Gradient Descent(595/999): loss=0.11855051852554457\n",
      "Stochastic Gradient Descent(596/999): loss=0.11854316332106678\n",
      "Stochastic Gradient Descent(597/999): loss=0.11853581654406953\n",
      "Stochastic Gradient Descent(598/999): loss=0.11852847818193257\n",
      "Stochastic Gradient Descent(599/999): loss=0.11852114822205717\n",
      "Stochastic Gradient Descent(600/999): loss=0.11851382665186597\n",
      "Stochastic Gradient Descent(601/999): loss=0.1185065134588031\n",
      "Stochastic Gradient Descent(602/999): loss=0.11849920863033403\n",
      "Stochastic Gradient Descent(603/999): loss=0.11849191215394547\n",
      "Stochastic Gradient Descent(604/999): loss=0.11848462401714552\n",
      "Stochastic Gradient Descent(605/999): loss=0.1184773442074634\n",
      "Stochastic Gradient Descent(606/999): loss=0.1184700727124497\n",
      "Stochastic Gradient Descent(607/999): loss=0.11846280951967594\n",
      "Stochastic Gradient Descent(608/999): loss=0.11845555461673495\n",
      "Stochastic Gradient Descent(609/999): loss=0.11844830799124052\n",
      "Stochastic Gradient Descent(610/999): loss=0.11844106963082754\n",
      "Stochastic Gradient Descent(611/999): loss=0.11843383952315178\n",
      "Stochastic Gradient Descent(612/999): loss=0.11842661765589012\n",
      "Stochastic Gradient Descent(613/999): loss=0.1184194040167402\n",
      "Stochastic Gradient Descent(614/999): loss=0.11841219859342064\n",
      "Stochastic Gradient Descent(615/999): loss=0.11840500137367081\n",
      "Stochastic Gradient Descent(616/999): loss=0.11839781234525094\n",
      "Stochastic Gradient Descent(617/999): loss=0.11839063149594198\n",
      "Stochastic Gradient Descent(618/999): loss=0.11838345881354556\n",
      "Stochastic Gradient Descent(619/999): loss=0.118376294285884\n",
      "Stochastic Gradient Descent(620/999): loss=0.11836913790080032\n",
      "Stochastic Gradient Descent(621/999): loss=0.118361989646158\n",
      "Stochastic Gradient Descent(622/999): loss=0.11835484950984121\n",
      "Stochastic Gradient Descent(623/999): loss=0.11834771747975453\n",
      "Stochastic Gradient Descent(624/999): loss=0.11834059354382306\n",
      "Stochastic Gradient Descent(625/999): loss=0.11833347768999233\n",
      "Stochastic Gradient Descent(626/999): loss=0.1183263699062283\n",
      "Stochastic Gradient Descent(627/999): loss=0.11831927018051726\n",
      "Stochastic Gradient Descent(628/999): loss=0.11831217850086577\n",
      "Stochastic Gradient Descent(629/999): loss=0.11830509485530073\n",
      "Stochastic Gradient Descent(630/999): loss=0.11829801923186935\n",
      "Stochastic Gradient Descent(631/999): loss=0.11829095161863884\n",
      "Stochastic Gradient Descent(632/999): loss=0.11828389200369684\n",
      "Stochastic Gradient Descent(633/999): loss=0.11827684037515089\n",
      "Stochastic Gradient Descent(634/999): loss=0.11826979672112879\n",
      "Stochastic Gradient Descent(635/999): loss=0.11826276102977828\n",
      "Stochastic Gradient Descent(636/999): loss=0.1182557332892672\n",
      "Stochastic Gradient Descent(637/999): loss=0.11824871348778326\n",
      "Stochastic Gradient Descent(638/999): loss=0.1182417016135343\n",
      "Stochastic Gradient Descent(639/999): loss=0.11823469765474784\n",
      "Stochastic Gradient Descent(640/999): loss=0.11822770159967143\n",
      "Stochastic Gradient Descent(641/999): loss=0.11822071343657235\n",
      "Stochastic Gradient Descent(642/999): loss=0.11821373315373777\n",
      "Stochastic Gradient Descent(643/999): loss=0.11820676073947452\n",
      "Stochastic Gradient Descent(644/999): loss=0.11819979618210924\n",
      "Stochastic Gradient Descent(645/999): loss=0.11819283946998817\n",
      "Stochastic Gradient Descent(646/999): loss=0.11818589059147723\n",
      "Stochastic Gradient Descent(647/999): loss=0.11817894953496197\n",
      "Stochastic Gradient Descent(648/999): loss=0.11817201628884748\n",
      "Stochastic Gradient Descent(649/999): loss=0.11816509084155846\n",
      "Stochastic Gradient Descent(650/999): loss=0.11815817318153901\n",
      "Stochastic Gradient Descent(651/999): loss=0.11815126329725269\n",
      "Stochastic Gradient Descent(652/999): loss=0.1181443611771826\n",
      "Stochastic Gradient Descent(653/999): loss=0.11813746680983121\n",
      "Stochastic Gradient Descent(654/999): loss=0.11813058018372023\n",
      "Stochastic Gradient Descent(655/999): loss=0.1181237012873908\n",
      "Stochastic Gradient Descent(656/999): loss=0.11811683010940333\n",
      "Stochastic Gradient Descent(657/999): loss=0.11810996663833741\n",
      "Stochastic Gradient Descent(658/999): loss=0.11810311086279196\n",
      "Stochastic Gradient Descent(659/999): loss=0.11809626277138502\n",
      "Stochastic Gradient Descent(660/999): loss=0.11808942235275374\n",
      "Stochastic Gradient Descent(661/999): loss=0.11808258959555443\n",
      "Stochastic Gradient Descent(662/999): loss=0.11807576448846245\n",
      "Stochastic Gradient Descent(663/999): loss=0.1180689470201722\n",
      "Stochastic Gradient Descent(664/999): loss=0.11806213717939713\n",
      "Stochastic Gradient Descent(665/999): loss=0.11805533495486958\n",
      "Stochastic Gradient Descent(666/999): loss=0.11804854033534087\n",
      "Stochastic Gradient Descent(667/999): loss=0.1180417533095812\n",
      "Stochastic Gradient Descent(668/999): loss=0.11803497386637966\n",
      "Stochastic Gradient Descent(669/999): loss=0.11802820199454414\n",
      "Stochastic Gradient Descent(670/999): loss=0.11802143768290135\n",
      "Stochastic Gradient Descent(671/999): loss=0.11801468092029682\n",
      "Stochastic Gradient Descent(672/999): loss=0.11800793169559463\n",
      "Stochastic Gradient Descent(673/999): loss=0.11800118999767777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(674/999): loss=0.1179944558154477\n",
      "Stochastic Gradient Descent(675/999): loss=0.11798772913782461\n",
      "Stochastic Gradient Descent(676/999): loss=0.11798100995374729\n",
      "Stochastic Gradient Descent(677/999): loss=0.11797429825217302\n",
      "Stochastic Gradient Descent(678/999): loss=0.11796759402207771\n",
      "Stochastic Gradient Descent(679/999): loss=0.11796089725245558\n",
      "Stochastic Gradient Descent(680/999): loss=0.11795420793231952\n",
      "Stochastic Gradient Descent(681/999): loss=0.11794752605070066\n",
      "Stochastic Gradient Descent(682/999): loss=0.11794085159664867\n",
      "Stochastic Gradient Descent(683/999): loss=0.11793418455923145\n",
      "Stochastic Gradient Descent(684/999): loss=0.11792752492753528\n",
      "Stochastic Gradient Descent(685/999): loss=0.11792087269066473\n",
      "Stochastic Gradient Descent(686/999): loss=0.11791422783774264\n",
      "Stochastic Gradient Descent(687/999): loss=0.11790759035791003\n",
      "Stochastic Gradient Descent(688/999): loss=0.11790096024032612\n",
      "Stochastic Gradient Descent(689/999): loss=0.11789433747416832\n",
      "Stochastic Gradient Descent(690/999): loss=0.11788772204863217\n",
      "Stochastic Gradient Descent(691/999): loss=0.1178811139529312\n",
      "Stochastic Gradient Descent(692/999): loss=0.11787451317629714\n",
      "Stochastic Gradient Descent(693/999): loss=0.11786791970797965\n",
      "Stochastic Gradient Descent(694/999): loss=0.11786133353724643\n",
      "Stochastic Gradient Descent(695/999): loss=0.11785475465338303\n",
      "Stochastic Gradient Descent(696/999): loss=0.11784818304569314\n",
      "Stochastic Gradient Descent(697/999): loss=0.11784161870349817\n",
      "Stochastic Gradient Descent(698/999): loss=0.11783506161613744\n",
      "Stochastic Gradient Descent(699/999): loss=0.11782851177296812\n",
      "Stochastic Gradient Descent(700/999): loss=0.11782196916336517\n",
      "Stochastic Gradient Descent(701/999): loss=0.1178154337767213\n",
      "Stochastic Gradient Descent(702/999): loss=0.11780890560244696\n",
      "Stochastic Gradient Descent(703/999): loss=0.11780238462997035\n",
      "Stochastic Gradient Descent(704/999): loss=0.11779587084873724\n",
      "Stochastic Gradient Descent(705/999): loss=0.11778936424821117\n",
      "Stochastic Gradient Descent(706/999): loss=0.11778286481787316\n",
      "Stochastic Gradient Descent(707/999): loss=0.11777637254722191\n",
      "Stochastic Gradient Descent(708/999): loss=0.11776988742577357\n",
      "Stochastic Gradient Descent(709/999): loss=0.1177634094430619\n",
      "Stochastic Gradient Descent(710/999): loss=0.11775693858863805\n",
      "Stochastic Gradient Descent(711/999): loss=0.11775047485207067\n",
      "Stochastic Gradient Descent(712/999): loss=0.1177440182229458\n",
      "Stochastic Gradient Descent(713/999): loss=0.11773756869086692\n",
      "Stochastic Gradient Descent(714/999): loss=0.11773112624545481\n",
      "Stochastic Gradient Descent(715/999): loss=0.11772469087634757\n",
      "Stochastic Gradient Descent(716/999): loss=0.11771826257320067\n",
      "Stochastic Gradient Descent(717/999): loss=0.11771184132568674\n",
      "Stochastic Gradient Descent(718/999): loss=0.11770542712349573\n",
      "Stochastic Gradient Descent(719/999): loss=0.11769901995633472\n",
      "Stochastic Gradient Descent(720/999): loss=0.11769261981392798\n",
      "Stochastic Gradient Descent(721/999): loss=0.11768622668601693\n",
      "Stochastic Gradient Descent(722/999): loss=0.11767984056236012\n",
      "Stochastic Gradient Descent(723/999): loss=0.11767346143273316\n",
      "Stochastic Gradient Descent(724/999): loss=0.11766708928692869\n",
      "Stochastic Gradient Descent(725/999): loss=0.11766072411475632\n",
      "Stochastic Gradient Descent(726/999): loss=0.11765436590604277\n",
      "Stochastic Gradient Descent(727/999): loss=0.1176480146506316\n",
      "Stochastic Gradient Descent(728/999): loss=0.11764167033838341\n",
      "Stochastic Gradient Descent(729/999): loss=0.11763533295917554\n",
      "Stochastic Gradient Descent(730/999): loss=0.1176290025029023\n",
      "Stochastic Gradient Descent(731/999): loss=0.11762267895947487\n",
      "Stochastic Gradient Descent(732/999): loss=0.11761636231882111\n",
      "Stochastic Gradient Descent(733/999): loss=0.11761005257088573\n",
      "Stochastic Gradient Descent(734/999): loss=0.11760374970563027\n",
      "Stochastic Gradient Descent(735/999): loss=0.11759745371303273\n",
      "Stochastic Gradient Descent(736/999): loss=0.11759116458308805\n",
      "Stochastic Gradient Descent(737/999): loss=0.11758488230580773\n",
      "Stochastic Gradient Descent(738/999): loss=0.11757860687121986\n",
      "Stochastic Gradient Descent(739/999): loss=0.11757233826936916\n",
      "Stochastic Gradient Descent(740/999): loss=0.11756607649031693\n",
      "Stochastic Gradient Descent(741/999): loss=0.11755982152414099\n",
      "Stochastic Gradient Descent(742/999): loss=0.11755357336093564\n",
      "Stochastic Gradient Descent(743/999): loss=0.11754733199081172\n",
      "Stochastic Gradient Descent(744/999): loss=0.11754109740389644\n",
      "Stochastic Gradient Descent(745/999): loss=0.11753486959033346\n",
      "Stochastic Gradient Descent(746/999): loss=0.11752864854028287\n",
      "Stochastic Gradient Descent(747/999): loss=0.11752243424392107\n",
      "Stochastic Gradient Descent(748/999): loss=0.11751622669144077\n",
      "Stochastic Gradient Descent(749/999): loss=0.11751002587305111\n",
      "Stochastic Gradient Descent(750/999): loss=0.11750383177897729\n",
      "Stochastic Gradient Descent(751/999): loss=0.11749764439946093\n",
      "Stochastic Gradient Descent(752/999): loss=0.11749146372475981\n",
      "Stochastic Gradient Descent(753/999): loss=0.1174852897451479\n",
      "Stochastic Gradient Descent(754/999): loss=0.11747912245091528\n",
      "Stochastic Gradient Descent(755/999): loss=0.11747296183236819\n",
      "Stochastic Gradient Descent(756/999): loss=0.11746680787982901\n",
      "Stochastic Gradient Descent(757/999): loss=0.11746066058363618\n",
      "Stochastic Gradient Descent(758/999): loss=0.11745451993414406\n",
      "Stochastic Gradient Descent(759/999): loss=0.11744838592172319\n",
      "Stochastic Gradient Descent(760/999): loss=0.11744225853676002\n",
      "Stochastic Gradient Descent(761/999): loss=0.11743613776965693\n",
      "Stochastic Gradient Descent(762/999): loss=0.11743002361083228\n",
      "Stochastic Gradient Descent(763/999): loss=0.11742391605072032\n",
      "Stochastic Gradient Descent(764/999): loss=0.11741781507977109\n",
      "Stochastic Gradient Descent(765/999): loss=0.11741172068845063\n",
      "Stochastic Gradient Descent(766/999): loss=0.11740563286724066\n",
      "Stochastic Gradient Descent(767/999): loss=0.11739955160663874\n",
      "Stochastic Gradient Descent(768/999): loss=0.1173934768971582\n",
      "Stochastic Gradient Descent(769/999): loss=0.11738740872932805\n",
      "Stochastic Gradient Descent(770/999): loss=0.11738134709369305\n",
      "Stochastic Gradient Descent(771/999): loss=0.11737529198081365\n",
      "Stochastic Gradient Descent(772/999): loss=0.11736924338126585\n",
      "Stochastic Gradient Descent(773/999): loss=0.11736320128564141\n",
      "Stochastic Gradient Descent(774/999): loss=0.11735716568454754\n",
      "Stochastic Gradient Descent(775/999): loss=0.11735113656860716\n",
      "Stochastic Gradient Descent(776/999): loss=0.11734511392845856\n",
      "Stochastic Gradient Descent(777/999): loss=0.11733909775475573\n",
      "Stochastic Gradient Descent(778/999): loss=0.1173330880381679\n",
      "Stochastic Gradient Descent(779/999): loss=0.11732708476938003\n",
      "Stochastic Gradient Descent(780/999): loss=0.11732108793909231\n",
      "Stochastic Gradient Descent(781/999): loss=0.11731509753802036\n",
      "Stochastic Gradient Descent(782/999): loss=0.11730911355689524\n",
      "Stochastic Gradient Descent(783/999): loss=0.11730313598646329\n",
      "Stochastic Gradient Descent(784/999): loss=0.11729716481748621\n",
      "Stochastic Gradient Descent(785/999): loss=0.11729120004074094\n",
      "Stochastic Gradient Descent(786/999): loss=0.11728524164701971\n",
      "Stochastic Gradient Descent(787/999): loss=0.11727928962713005\n",
      "Stochastic Gradient Descent(788/999): loss=0.11727334397189457\n",
      "Stochastic Gradient Descent(789/999): loss=0.11726740467215117\n",
      "Stochastic Gradient Descent(790/999): loss=0.11726147171875284\n",
      "Stochastic Gradient Descent(791/999): loss=0.11725554510256773\n",
      "Stochastic Gradient Descent(792/999): loss=0.11724962481447906\n",
      "Stochastic Gradient Descent(793/999): loss=0.1172437108453852\n",
      "Stochastic Gradient Descent(794/999): loss=0.11723780318619945\n",
      "Stochastic Gradient Descent(795/999): loss=0.11723190182785025\n",
      "Stochastic Gradient Descent(796/999): loss=0.11722600676128092\n",
      "Stochastic Gradient Descent(797/999): loss=0.11722011797744988\n",
      "Stochastic Gradient Descent(798/999): loss=0.11721423546733042\n",
      "Stochastic Gradient Descent(799/999): loss=0.11720835922191067\n",
      "Stochastic Gradient Descent(800/999): loss=0.11720248923219374\n",
      "Stochastic Gradient Descent(801/999): loss=0.11719662548919768\n",
      "Stochastic Gradient Descent(802/999): loss=0.11719076798395517\n",
      "Stochastic Gradient Descent(803/999): loss=0.11718491670751385\n",
      "Stochastic Gradient Descent(804/999): loss=0.1171790716509361\n",
      "Stochastic Gradient Descent(805/999): loss=0.11717323280529912\n",
      "Stochastic Gradient Descent(806/999): loss=0.1171674001616947\n",
      "Stochastic Gradient Descent(807/999): loss=0.11716157371122946\n",
      "Stochastic Gradient Descent(808/999): loss=0.11715575344502463\n",
      "Stochastic Gradient Descent(809/999): loss=0.1171499393542162\n",
      "Stochastic Gradient Descent(810/999): loss=0.11714413142995461\n",
      "Stochastic Gradient Descent(811/999): loss=0.11713832966340502\n",
      "Stochastic Gradient Descent(812/999): loss=0.1171325340457472\n",
      "Stochastic Gradient Descent(813/999): loss=0.11712674456817536\n",
      "Stochastic Gradient Descent(814/999): loss=0.11712096122189831\n",
      "Stochastic Gradient Descent(815/999): loss=0.11711518399813935\n",
      "Stochastic Gradient Descent(816/999): loss=0.11710941288813623\n",
      "Stochastic Gradient Descent(817/999): loss=0.1171036478831411\n",
      "Stochastic Gradient Descent(818/999): loss=0.11709788897442068\n",
      "Stochastic Gradient Descent(819/999): loss=0.11709213615325595\n",
      "Stochastic Gradient Descent(820/999): loss=0.1170863894109423\n",
      "Stochastic Gradient Descent(821/999): loss=0.11708064873878951\n",
      "Stochastic Gradient Descent(822/999): loss=0.11707491412812158\n",
      "Stochastic Gradient Descent(823/999): loss=0.11706918557027693\n",
      "Stochastic Gradient Descent(824/999): loss=0.11706346305660816\n",
      "Stochastic Gradient Descent(825/999): loss=0.11705774657848213\n",
      "Stochastic Gradient Descent(826/999): loss=0.11705203612727992\n",
      "Stochastic Gradient Descent(827/999): loss=0.11704633169439685\n",
      "Stochastic Gradient Descent(828/999): loss=0.11704063327124233\n",
      "Stochastic Gradient Descent(829/999): loss=0.11703494084923997\n",
      "Stochastic Gradient Descent(830/999): loss=0.11702925441982752\n",
      "Stochastic Gradient Descent(831/999): loss=0.11702357397445676\n",
      "Stochastic Gradient Descent(832/999): loss=0.11701789950459357\n",
      "Stochastic Gradient Descent(833/999): loss=0.11701223100171786\n",
      "Stochastic Gradient Descent(834/999): loss=0.11700656845732359\n",
      "Stochastic Gradient Descent(835/999): loss=0.11700091186291868\n",
      "Stochastic Gradient Descent(836/999): loss=0.11699526121002507\n",
      "Stochastic Gradient Descent(837/999): loss=0.11698961649017854\n",
      "Stochastic Gradient Descent(838/999): loss=0.11698397769492896\n",
      "Stochastic Gradient Descent(839/999): loss=0.11697834481583996\n",
      "Stochastic Gradient Descent(840/999): loss=0.11697271784448905\n",
      "Stochastic Gradient Descent(841/999): loss=0.11696709677246764\n",
      "Stochastic Gradient Descent(842/999): loss=0.11696148159138096\n",
      "Stochastic Gradient Descent(843/999): loss=0.11695587229284803\n",
      "Stochastic Gradient Descent(844/999): loss=0.11695026886850157\n",
      "Stochastic Gradient Descent(845/999): loss=0.11694467130998819\n",
      "Stochastic Gradient Descent(846/999): loss=0.1169390796089681\n",
      "Stochastic Gradient Descent(847/999): loss=0.1169334937571153\n",
      "Stochastic Gradient Descent(848/999): loss=0.1169279137461174\n",
      "Stochastic Gradient Descent(849/999): loss=0.11692233956767568\n",
      "Stochastic Gradient Descent(850/999): loss=0.11691677121350512\n",
      "Stochastic Gradient Descent(851/999): loss=0.11691120867533421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(852/999): loss=0.11690565194490503\n",
      "Stochastic Gradient Descent(853/999): loss=0.11690010101397329\n",
      "Stochastic Gradient Descent(854/999): loss=0.11689455587430818\n",
      "Stochastic Gradient Descent(855/999): loss=0.1168890165176924\n",
      "Stochastic Gradient Descent(856/999): loss=0.11688348293592214\n",
      "Stochastic Gradient Descent(857/999): loss=0.11687795512080706\n",
      "Stochastic Gradient Descent(858/999): loss=0.11687243306417028\n",
      "Stochastic Gradient Descent(859/999): loss=0.1168669167578483\n",
      "Stochastic Gradient Descent(860/999): loss=0.11686140619369097\n",
      "Stochastic Gradient Descent(861/999): loss=0.11685590136356161\n",
      "Stochastic Gradient Descent(862/999): loss=0.11685040225933681\n",
      "Stochastic Gradient Descent(863/999): loss=0.11684490887290651\n",
      "Stochastic Gradient Descent(864/999): loss=0.11683942119617395\n",
      "Stochastic Gradient Descent(865/999): loss=0.11683393922105562\n",
      "Stochastic Gradient Descent(866/999): loss=0.11682846293948126\n",
      "Stochastic Gradient Descent(867/999): loss=0.11682299234339386\n",
      "Stochastic Gradient Descent(868/999): loss=0.11681752742474961\n",
      "Stochastic Gradient Descent(869/999): loss=0.1168120681755179\n",
      "Stochastic Gradient Descent(870/999): loss=0.11680661458768121\n",
      "Stochastic Gradient Descent(871/999): loss=0.11680116665323517\n",
      "Stochastic Gradient Descent(872/999): loss=0.11679572436418859\n",
      "Stochastic Gradient Descent(873/999): loss=0.11679028771256332\n",
      "Stochastic Gradient Descent(874/999): loss=0.11678485669039428\n",
      "Stochastic Gradient Descent(875/999): loss=0.1167794312897294\n",
      "Stochastic Gradient Descent(876/999): loss=0.11677401150262966\n",
      "Stochastic Gradient Descent(877/999): loss=0.11676859732116907\n",
      "Stochastic Gradient Descent(878/999): loss=0.11676318873743459\n",
      "Stochastic Gradient Descent(879/999): loss=0.11675778574352605\n",
      "Stochastic Gradient Descent(880/999): loss=0.11675238833155634\n",
      "Stochastic Gradient Descent(881/999): loss=0.11674699649365114\n",
      "Stochastic Gradient Descent(882/999): loss=0.11674161022194908\n",
      "Stochastic Gradient Descent(883/999): loss=0.11673622950860164\n",
      "Stochastic Gradient Descent(884/999): loss=0.11673085434577317\n",
      "Stochastic Gradient Descent(885/999): loss=0.1167254847256407\n",
      "Stochastic Gradient Descent(886/999): loss=0.1167201206403942\n",
      "Stochastic Gradient Descent(887/999): loss=0.11671476208223634\n",
      "Stochastic Gradient Descent(888/999): loss=0.11670940904338259\n",
      "Stochastic Gradient Descent(889/999): loss=0.11670406151606104\n",
      "Stochastic Gradient Descent(890/999): loss=0.11669871949251258\n",
      "Stochastic Gradient Descent(891/999): loss=0.11669338296499075\n",
      "Stochastic Gradient Descent(892/999): loss=0.11668805192576175\n",
      "Stochastic Gradient Descent(893/999): loss=0.1166827263671044\n",
      "Stochastic Gradient Descent(894/999): loss=0.11667740628131011\n",
      "Stochastic Gradient Descent(895/999): loss=0.11667209166068297\n",
      "Stochastic Gradient Descent(896/999): loss=0.11666678249753955\n",
      "Stochastic Gradient Descent(897/999): loss=0.116661478784209\n",
      "Stochastic Gradient Descent(898/999): loss=0.11665618051303302\n",
      "Stochastic Gradient Descent(899/999): loss=0.11665088767636574\n",
      "Stochastic Gradient Descent(900/999): loss=0.11664560026657386\n",
      "Stochastic Gradient Descent(901/999): loss=0.11664031827603645\n",
      "Stochastic Gradient Descent(902/999): loss=0.1166350416971451\n",
      "Stochastic Gradient Descent(903/999): loss=0.11662977052230378\n",
      "Stochastic Gradient Descent(904/999): loss=0.11662450474392881\n",
      "Stochastic Gradient Descent(905/999): loss=0.11661924435444898\n",
      "Stochastic Gradient Descent(906/999): loss=0.1166139893463053\n",
      "Stochastic Gradient Descent(907/999): loss=0.11660873971195125\n",
      "Stochastic Gradient Descent(908/999): loss=0.11660349544385251\n",
      "Stochastic Gradient Descent(909/999): loss=0.1165982565344871\n",
      "Stochastic Gradient Descent(910/999): loss=0.11659302297634525\n",
      "Stochastic Gradient Descent(911/999): loss=0.1165877947619295\n",
      "Stochastic Gradient Descent(912/999): loss=0.11658257188375454\n",
      "Stochastic Gradient Descent(913/999): loss=0.11657735433434735\n",
      "Stochastic Gradient Descent(914/999): loss=0.11657214210624701\n",
      "Stochastic Gradient Descent(915/999): loss=0.11656693519200474\n",
      "Stochastic Gradient Descent(916/999): loss=0.116561733584184\n",
      "Stochastic Gradient Descent(917/999): loss=0.11655653727536025\n",
      "Stochastic Gradient Descent(918/999): loss=0.11655134625812111\n",
      "Stochastic Gradient Descent(919/999): loss=0.11654616052506622\n",
      "Stochastic Gradient Descent(920/999): loss=0.11654098006880732\n",
      "Stochastic Gradient Descent(921/999): loss=0.1165358048819682\n",
      "Stochastic Gradient Descent(922/999): loss=0.11653063495718456\n",
      "Stochastic Gradient Descent(923/999): loss=0.11652547028710418\n",
      "Stochastic Gradient Descent(924/999): loss=0.11652031086438673\n",
      "Stochastic Gradient Descent(925/999): loss=0.11651515668170388\n",
      "Stochastic Gradient Descent(926/999): loss=0.11651000773173925\n",
      "Stochastic Gradient Descent(927/999): loss=0.11650486400718826\n",
      "Stochastic Gradient Descent(928/999): loss=0.1164997255007583\n",
      "Stochastic Gradient Descent(929/999): loss=0.11649459220516857\n",
      "Stochastic Gradient Descent(930/999): loss=0.11648946411315018\n",
      "Stochastic Gradient Descent(931/999): loss=0.116484341217446\n",
      "Stochastic Gradient Descent(932/999): loss=0.11647922351081064\n",
      "Stochastic Gradient Descent(933/999): loss=0.11647411098601067\n",
      "Stochastic Gradient Descent(934/999): loss=0.11646900363582423\n",
      "Stochastic Gradient Descent(935/999): loss=0.11646390145304131\n",
      "Stochastic Gradient Descent(936/999): loss=0.11645880443046353\n",
      "Stochastic Gradient Descent(937/999): loss=0.11645371256090434\n",
      "Stochastic Gradient Descent(938/999): loss=0.11644862583718872\n",
      "Stochastic Gradient Descent(939/999): loss=0.11644354425215336\n",
      "Stochastic Gradient Descent(940/999): loss=0.11643846779864664\n",
      "Stochastic Gradient Descent(941/999): loss=0.11643339646952847\n",
      "Stochastic Gradient Descent(942/999): loss=0.1164283302576704\n",
      "Stochastic Gradient Descent(943/999): loss=0.11642326915595552\n",
      "Stochastic Gradient Descent(944/999): loss=0.11641821315727854\n",
      "Stochastic Gradient Descent(945/999): loss=0.11641316225454562\n",
      "Stochastic Gradient Descent(946/999): loss=0.11640811644067448\n",
      "Stochastic Gradient Descent(947/999): loss=0.11640307570859434\n",
      "Stochastic Gradient Descent(948/999): loss=0.11639804005124586\n",
      "Stochastic Gradient Descent(949/999): loss=0.11639300946158118\n",
      "Stochastic Gradient Descent(950/999): loss=0.11638798393256386\n",
      "Stochastic Gradient Descent(951/999): loss=0.11638296345716884\n",
      "Stochastic Gradient Descent(952/999): loss=0.11637794802838251\n",
      "Stochastic Gradient Descent(953/999): loss=0.11637293763920263\n",
      "Stochastic Gradient Descent(954/999): loss=0.11636793228263828\n",
      "Stochastic Gradient Descent(955/999): loss=0.11636293195170987\n",
      "Stochastic Gradient Descent(956/999): loss=0.11635793663944913\n",
      "Stochastic Gradient Descent(957/999): loss=0.11635294633889912\n",
      "Stochastic Gradient Descent(958/999): loss=0.1163479610431141\n",
      "Stochastic Gradient Descent(959/999): loss=0.11634298074515965\n",
      "Stochastic Gradient Descent(960/999): loss=0.11633800543811258\n",
      "Stochastic Gradient Descent(961/999): loss=0.11633303511506085\n",
      "Stochastic Gradient Descent(962/999): loss=0.1163280697691037\n",
      "Stochastic Gradient Descent(963/999): loss=0.1163231093933515\n",
      "Stochastic Gradient Descent(964/999): loss=0.11631815398092575\n",
      "Stochastic Gradient Descent(965/999): loss=0.11631320352495914\n",
      "Stochastic Gradient Descent(966/999): loss=0.11630825801859543\n",
      "Stochastic Gradient Descent(967/999): loss=0.11630331745498954\n",
      "Stochastic Gradient Descent(968/999): loss=0.11629838182730735\n",
      "Stochastic Gradient Descent(969/999): loss=0.116293451128726\n",
      "Stochastic Gradient Descent(970/999): loss=0.11628852535243342\n",
      "Stochastic Gradient Descent(971/999): loss=0.11628360449162875\n",
      "Stochastic Gradient Descent(972/999): loss=0.11627868853952204\n",
      "Stochastic Gradient Descent(973/999): loss=0.11627377748933436\n",
      "Stochastic Gradient Descent(974/999): loss=0.11626887133429772\n",
      "Stochastic Gradient Descent(975/999): loss=0.11626397006765508\n",
      "Stochastic Gradient Descent(976/999): loss=0.11625907368266035\n",
      "Stochastic Gradient Descent(977/999): loss=0.1162541821725783\n",
      "Stochastic Gradient Descent(978/999): loss=0.11624929553068458\n",
      "Stochastic Gradient Descent(979/999): loss=0.11624441375026576\n",
      "Stochastic Gradient Descent(980/999): loss=0.11623953682461924\n",
      "Stochastic Gradient Descent(981/999): loss=0.1162346647470532\n",
      "Stochastic Gradient Descent(982/999): loss=0.11622979751088668\n",
      "Stochastic Gradient Descent(983/999): loss=0.11622493510944952\n",
      "Stochastic Gradient Descent(984/999): loss=0.11622007753608231\n",
      "Stochastic Gradient Descent(985/999): loss=0.11621522478413635\n",
      "Stochastic Gradient Descent(986/999): loss=0.11621037684697375\n",
      "Stochastic Gradient Descent(987/999): loss=0.11620553371796734\n",
      "Stochastic Gradient Descent(988/999): loss=0.11620069539050053\n",
      "Stochastic Gradient Descent(989/999): loss=0.11619586185796756\n",
      "Stochastic Gradient Descent(990/999): loss=0.11619103311377321\n",
      "Stochastic Gradient Descent(991/999): loss=0.11618620915133299\n",
      "Stochastic Gradient Descent(992/999): loss=0.11618138996407293\n",
      "Stochastic Gradient Descent(993/999): loss=0.11617657554542982\n",
      "Stochastic Gradient Descent(994/999): loss=0.11617176588885088\n",
      "Stochastic Gradient Descent(995/999): loss=0.11616696098779396\n",
      "Stochastic Gradient Descent(996/999): loss=0.11616216083572746\n",
      "Stochastic Gradient Descent(997/999): loss=0.11615736542613034\n",
      "Stochastic Gradient Descent(998/999): loss=0.11615257475249201\n",
      "Stochastic Gradient Descent(999/999): loss=0.11614778880831238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(0/999): loss=0.23422206561741446\n",
      "Stochastic Gradient Descent(1/999): loss=0.22274826816031182\n",
      "Stochastic Gradient Descent(2/999): loss=0.212478664539309\n",
      "Stochastic Gradient Descent(3/999): loss=0.2032866717495318\n",
      "Stochastic Gradient Descent(4/999): loss=0.19505901383849528\n",
      "Stochastic Gradient Descent(5/999): loss=0.18769432298846023\n",
      "Stochastic Gradient Descent(6/999): loss=0.18110188766168775\n",
      "Stochastic Gradient Descent(7/999): loss=0.17520053234842226\n",
      "Stochastic Gradient Descent(8/999): loss=0.16991761508270403\n",
      "Stochastic Gradient Descent(9/999): loss=0.1651881303455249\n",
      "Stochastic Gradient Descent(10/999): loss=0.16095390627635042\n",
      "Stochastic Gradient Descent(11/999): loss=0.15716288627872857\n",
      "Stochastic Gradient Descent(12/999): loss=0.15376848614795066\n",
      "Stochastic Gradient Descent(13/999): loss=0.15072901878141748\n",
      "Stochastic Gradient Descent(14/999): loss=0.14800717936699515\n",
      "Stochastic Gradient Descent(15/999): loss=0.14556958469153897\n",
      "Stochastic Gradient Descent(16/999): loss=0.14338636088013496\n",
      "Stochastic Gradient Descent(17/999): loss=0.14143077447472235\n",
      "Stochastic Gradient Descent(18/999): loss=0.1396789022959906\n",
      "Stochastic Gradient Descent(19/999): loss=0.13810933601141231\n",
      "Stochastic Gradient Descent(20/999): loss=0.13670291776088797\n",
      "Stochastic Gradient Descent(21/999): loss=0.13544250357503188\n",
      "Stochastic Gradient Descent(22/999): loss=0.13431275166436593\n",
      "Stochastic Gradient Descent(23/999): loss=0.13329993296483575\n",
      "Stochastic Gradient Descent(24/999): loss=0.13239176159992708\n",
      "Stochastic Gradient Descent(25/999): loss=0.13157724316562583\n",
      "Stochastic Gradient Descent(26/999): loss=0.1308465389645734\n",
      "Stochastic Gradient Descent(27/999): loss=0.1301908445127402\n",
      "Stochastic Gradient Descent(28/999): loss=0.12960228081820066\n",
      "Stochastic Gradient Descent(29/999): loss=0.12907379708932806\n",
      "Stochastic Gradient Descent(30/999): loss=0.12859908367087833\n",
      "Stochastic Gradient Descent(31/999): loss=0.12817249413274273\n",
      "Stochastic Gradient Descent(32/999): loss=0.12778897554918575\n",
      "Stochastic Gradient Descent(33/999): loss=0.12744400610753295\n",
      "Stochastic Gradient Descent(34/999): loss=0.1271335392757924\n",
      "Stochastic Gradient Descent(35/999): loss=0.12685395383969397\n",
      "Stochastic Gradient Descent(36/999): loss=0.12660200919211773\n",
      "Stochastic Gradient Descent(37/999): loss=0.12637480532274717\n",
      "Stochastic Gradient Descent(38/999): loss=0.1261697470138322\n",
      "Stochastic Gradient Descent(39/999): loss=0.1259845117998885\n",
      "Stochastic Gradient Descent(40/999): loss=0.1258170212956466\n",
      "Stochastic Gradient Descent(41/999): loss=0.1256654155381586\n",
      "Stochastic Gradient Descent(42/999): loss=0.12552803002619753\n",
      "Stochastic Gradient Descent(43/999): loss=0.12540337517339165\n",
      "Stochastic Gradient Descent(44/999): loss=0.1252901179213491\n",
      "Stochastic Gradient Descent(45/999): loss=0.1251870652857007\n",
      "Stochastic Gradient Descent(46/999): loss=0.1250931496318614\n",
      "Stochastic Gradient Descent(47/999): loss=0.12500741549867167\n",
      "Stochastic Gradient Descent(48/999): loss=0.12492900780719715\n",
      "Stochastic Gradient Descent(49/999): loss=0.12485716130906946\n",
      "Stochastic Gradient Descent(50/999): loss=0.1247911911440618\n",
      "Stochastic Gradient Descent(51/999): loss=0.12473048439028832\n",
      "Stochastic Gradient Descent(52/999): loss=0.12467449250267874\n",
      "Stochastic Gradient Descent(53/999): loss=0.12462272454634588\n",
      "Stochastic Gradient Descent(54/999): loss=0.1245747411412838\n",
      "Stochastic Gradient Descent(55/999): loss=0.12453014904361659\n",
      "Stochastic Gradient Descent(56/999): loss=0.12448859629648011\n",
      "Stochastic Gradient Descent(57/999): loss=0.1244497678906544\n",
      "Stochastic Gradient Descent(58/999): loss=0.12441338188135792\n",
      "Stochastic Gradient Descent(59/999): loss=0.12437918591325021\n",
      "Stochastic Gradient Descent(60/999): loss=0.12434695411072953\n",
      "Stochastic Gradient Descent(61/999): loss=0.12431648429512401\n",
      "Stochastic Gradient Descent(62/999): loss=0.1242875954944113\n",
      "Stochastic Gradient Descent(63/999): loss=0.12426012571471481\n",
      "Stochastic Gradient Descent(64/999): loss=0.1242339299460572\n",
      "Stochastic Gradient Descent(65/999): loss=0.12420887837774515\n",
      "Stochastic Gradient Descent(66/999): loss=0.1241848548013474\n",
      "Stochastic Gradient Descent(67/999): loss=0.12416175518154604\n",
      "Stochastic Gradient Descent(68/999): loss=0.12413948637721306\n",
      "Stochastic Gradient Descent(69/999): loss=0.12411796499692002\n",
      "Stochastic Gradient Descent(70/999): loss=0.12409711637474867\n",
      "Stochastic Gradient Descent(71/999): loss=0.12407687365375601\n",
      "Stochastic Gradient Descent(72/999): loss=0.12405717696577673\n",
      "Stochastic Gradient Descent(73/999): loss=0.12403797269743579\n",
      "Stochastic Gradient Descent(74/999): loss=0.12401921283330845\n",
      "Stochastic Gradient Descent(75/999): loss=0.12400085436811784\n",
      "Stochastic Gradient Descent(76/999): loss=0.12398285878071258\n",
      "Stochastic Gradient Descent(77/999): loss=0.12396519156333002\n",
      "Stochastic Gradient Descent(78/999): loss=0.12394782180033355\n",
      "Stochastic Gradient Descent(79/999): loss=0.123930721791223\n",
      "Stochastic Gradient Descent(80/999): loss=0.12391386671326421\n",
      "Stochastic Gradient Descent(81/999): loss=0.123897234319573\n",
      "Stochastic Gradient Descent(82/999): loss=0.12388080466892656\n",
      "Stochastic Gradient Descent(83/999): loss=0.12386455988396709\n",
      "Stochastic Gradient Descent(84/999): loss=0.12384848393481329\n",
      "Stochastic Gradient Descent(85/999): loss=0.12383256244540865\n",
      "Stochastic Gradient Descent(86/999): loss=0.12381678252021655\n",
      "Stochastic Gradient Descent(87/999): loss=0.12380113258912404\n",
      "Stochastic Gradient Descent(88/999): loss=0.12378560226863888\n",
      "Stochastic Gradient Descent(89/999): loss=0.12377018223766893\n",
      "Stochastic Gradient Descent(90/999): loss=0.12375486412634976\n",
      "Stochastic Gradient Descent(91/999): loss=0.12373964041654956\n",
      "Stochastic Gradient Descent(92/999): loss=0.12372450435282387\n",
      "Stochastic Gradient Descent(93/999): loss=0.12370944986272187\n",
      "Stochastic Gradient Descent(94/999): loss=0.12369447148546099\n",
      "Stochastic Gradient Descent(95/999): loss=0.12367956430809063\n",
      "Stochastic Gradient Descent(96/999): loss=0.12366472390835771\n",
      "Stochastic Gradient Descent(97/999): loss=0.12364994630356983\n",
      "Stochastic Gradient Descent(98/999): loss=0.12363522790482537\n",
      "Stochastic Gradient Descent(99/999): loss=0.12362056547604697\n",
      "Stochastic Gradient Descent(100/999): loss=0.12360595609731308\n",
      "Stochastic Gradient Descent(101/999): loss=0.12359139713203648\n",
      "Stochastic Gradient Descent(102/999): loss=0.12357688619758499\n",
      "Stochastic Gradient Descent(103/999): loss=0.12356242113898275\n",
      "Stochastic Gradient Descent(104/999): loss=0.12354800000536888\n",
      "Stochastic Gradient Descent(105/999): loss=0.12353362102892304\n",
      "Stochastic Gradient Descent(106/999): loss=0.12351928260599929\n",
      "Stochastic Gradient Descent(107/999): loss=0.12350498328023594\n",
      "Stochastic Gradient Descent(108/999): loss=0.12349072172743383\n",
      "Stochastic Gradient Descent(109/999): loss=0.12347649674201754\n",
      "Stochastic Gradient Descent(110/999): loss=0.12346230722491278\n",
      "Stochastic Gradient Descent(111/999): loss=0.12344815217269169\n",
      "Stochastic Gradient Descent(112/999): loss=0.12343403066785259\n",
      "Stochastic Gradient Descent(113/999): loss=0.12341994187011512\n",
      "Stochastic Gradient Descent(114/999): loss=0.1234058850086243\n",
      "Stochastic Gradient Descent(115/999): loss=0.12339185937496779\n",
      "Stochastic Gradient Descent(116/999): loss=0.12337786431692124\n",
      "Stochastic Gradient Descent(117/999): loss=0.12336389923284519\n",
      "Stochastic Gradient Descent(118/999): loss=0.12334996356666515\n",
      "Stochastic Gradient Descent(119/999): loss=0.12333605680337383\n",
      "Stochastic Gradient Descent(120/999): loss=0.12332217846500029\n",
      "Stochastic Gradient Descent(121/999): loss=0.12330832810699756\n",
      "Stochastic Gradient Descent(122/999): loss=0.12329450531500467\n",
      "Stochastic Gradient Descent(123/999): loss=0.12328070970194352\n",
      "Stochastic Gradient Descent(124/999): loss=0.12326694090541605\n",
      "Stochastic Gradient Descent(125/999): loss=0.12325319858536977\n",
      "Stochastic Gradient Descent(126/999): loss=0.12323948242200371\n",
      "Stochastic Gradient Descent(127/999): loss=0.1232257921138898\n",
      "Stochastic Gradient Descent(128/999): loss=0.12321212737628673\n",
      "Stochastic Gradient Descent(129/999): loss=0.12319848793962644\n",
      "Stochastic Gradient Descent(130/999): loss=0.12318487354815524\n",
      "Stochastic Gradient Descent(131/999): loss=0.12317128395871306\n",
      "Stochastic Gradient Descent(132/999): loss=0.12315771893963663\n",
      "Stochastic Gradient Descent(133/999): loss=0.1231441782697739\n",
      "Stochastic Gradient Descent(134/999): loss=0.1231306617375974\n",
      "Stochastic Gradient Descent(135/999): loss=0.12311716914040688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(136/999): loss=0.12310370028361164\n",
      "Stochastic Gradient Descent(137/999): loss=0.12309025498008423\n",
      "Stochastic Gradient Descent(138/999): loss=0.12307683304957828\n",
      "Stochastic Gradient Descent(139/999): loss=0.12306343431820346\n",
      "Stochastic Gradient Descent(140/999): loss=0.12305005861795214\n",
      "Stochastic Gradient Descent(141/999): loss=0.12303670578627174\n",
      "Stochastic Gradient Descent(142/999): loss=0.12302337566567867\n",
      "Stochastic Gradient Descent(143/999): loss=0.12301006810340931\n",
      "Stochastic Gradient Descent(144/999): loss=0.12299678295110399\n",
      "Stochastic Gradient Descent(145/999): loss=0.12298352006452073\n",
      "Stochastic Gradient Descent(146/999): loss=0.12297027930327604\n",
      "Stochastic Gradient Descent(147/999): loss=0.12295706053060938\n",
      "Stochastic Gradient Descent(148/999): loss=0.12294386361316906\n",
      "Stochastic Gradient Descent(149/999): loss=0.12293068842081792\n",
      "Stochastic Gradient Descent(150/999): loss=0.12291753482645588\n",
      "Stochastic Gradient Descent(151/999): loss=0.12290440270585826\n",
      "Stochastic Gradient Descent(152/999): loss=0.1228912919375281\n",
      "Stochastic Gradient Descent(153/999): loss=0.12287820240256114\n",
      "Stochastic Gradient Descent(154/999): loss=0.1228651339845221\n",
      "Stochastic Gradient Descent(155/999): loss=0.12285208656933094\n",
      "Stochastic Gradient Descent(156/999): loss=0.1228390600451587\n",
      "Stochastic Gradient Descent(157/999): loss=0.12282605430233114\n",
      "Stochastic Gradient Descent(158/999): loss=0.1228130692332402\n",
      "Stochastic Gradient Descent(159/999): loss=0.12280010473226209\n",
      "Stochastic Gradient Descent(160/999): loss=0.12278716069568123\n",
      "Stochastic Gradient Descent(161/999): loss=0.12277423702161995\n",
      "Stochastic Gradient Descent(162/999): loss=0.12276133360997295\n",
      "Stochastic Gradient Descent(163/999): loss=0.1227484503623463\n",
      "Stochastic Gradient Descent(164/999): loss=0.12273558718200052\n",
      "Stochastic Gradient Descent(165/999): loss=0.1227227439737973\n",
      "Stochastic Gradient Descent(166/999): loss=0.12270992064414947\n",
      "Stochastic Gradient Descent(167/999): loss=0.12269711710097421\n",
      "Stochastic Gradient Descent(168/999): loss=0.12268433325364876\n",
      "Stochastic Gradient Descent(169/999): loss=0.1226715690129687\n",
      "Stochastic Gradient Descent(170/999): loss=0.12265882429110883\n",
      "Stochastic Gradient Descent(171/999): loss=0.12264609900158553\n",
      "Stochastic Gradient Descent(172/999): loss=0.12263339305922152\n",
      "Stochastic Gradient Descent(173/999): loss=0.12262070638011226\n",
      "Stochastic Gradient Descent(174/999): loss=0.12260803888159363\n",
      "Stochastic Gradient Descent(175/999): loss=0.12259539048221174\n",
      "Stochastic Gradient Descent(176/999): loss=0.1225827611016933\n",
      "Stochastic Gradient Descent(177/999): loss=0.12257015066091786\n",
      "Stochastic Gradient Descent(178/999): loss=0.1225575590818908\n",
      "Stochastic Gradient Descent(179/999): loss=0.12254498628771766\n",
      "Stochastic Gradient Descent(180/999): loss=0.1225324322025793\n",
      "Stochastic Gradient Descent(181/999): loss=0.12251989675170807\n",
      "Stochastic Gradient Descent(182/999): loss=0.12250737986136459\n",
      "Stochastic Gradient Descent(183/999): loss=0.1224948814588158\n",
      "Stochastic Gradient Descent(184/999): loss=0.12248240147231321\n",
      "Stochastic Gradient Descent(185/999): loss=0.12246993983107224\n",
      "Stochastic Gradient Descent(186/999): loss=0.12245749646525206\n",
      "Stochastic Gradient Descent(187/999): loss=0.12244507130593603\n",
      "Stochastic Gradient Descent(188/999): loss=0.12243266428511271\n",
      "Stochastic Gradient Descent(189/999): loss=0.12242027533565757\n",
      "Stochastic Gradient Descent(190/999): loss=0.12240790439131506\n",
      "Stochastic Gradient Descent(191/999): loss=0.1223955513866811\n",
      "Stochastic Gradient Descent(192/999): loss=0.1223832162571863\n",
      "Stochastic Gradient Descent(193/999): loss=0.12237089893907943\n",
      "Stochastic Gradient Descent(194/999): loss=0.12235859936941128\n",
      "Stochastic Gradient Descent(195/999): loss=0.12234631748601914\n",
      "Stochastic Gradient Descent(196/999): loss=0.12233405322751133\n",
      "Stochastic Gradient Descent(197/999): loss=0.12232180653325253\n",
      "Stochastic Gradient Descent(198/999): loss=0.12230957734334902\n",
      "Stochastic Gradient Descent(199/999): loss=0.1222973655986346\n",
      "Stochastic Gradient Descent(200/999): loss=0.1222851712406567\n",
      "Stochastic Gradient Descent(201/999): loss=0.1222729942116627\n",
      "Stochastic Gradient Descent(202/999): loss=0.12226083445458684\n",
      "Stochastic Gradient Descent(203/999): loss=0.12224869191303715\n",
      "Stochastic Gradient Descent(204/999): loss=0.12223656653128283\n",
      "Stochastic Gradient Descent(205/999): loss=0.12222445825424175\n",
      "Stochastic Gradient Descent(206/999): loss=0.12221236702746846\n",
      "Stochastic Gradient Descent(207/999): loss=0.12220029279714224\n",
      "Stochastic Gradient Descent(208/999): loss=0.12218823551005542\n",
      "Stochastic Gradient Descent(209/999): loss=0.1221761951136021\n",
      "Stochastic Gradient Descent(210/999): loss=0.12216417155576703\n",
      "Stochastic Gradient Descent(211/999): loss=0.12215216478511456\n",
      "Stochastic Gradient Descent(212/999): loss=0.12214017475077815\n",
      "Stochastic Gradient Descent(213/999): loss=0.12212820140244969\n",
      "Stochastic Gradient Descent(214/999): loss=0.12211624469036947\n",
      "Stochastic Gradient Descent(215/999): loss=0.12210430456531599\n",
      "Stochastic Gradient Descent(216/999): loss=0.12209238097859619\n",
      "Stochastic Gradient Descent(217/999): loss=0.12208047388203579\n",
      "Stochastic Gradient Descent(218/999): loss=0.1220685832279699\n",
      "Stochastic Gradient Descent(219/999): loss=0.12205670896923362\n",
      "Stochastic Gradient Descent(220/999): loss=0.12204485105915318\n",
      "Stochastic Gradient Descent(221/999): loss=0.1220330094515369\n",
      "Stochastic Gradient Descent(222/999): loss=0.1220211841006665\n",
      "Stochastic Gradient Descent(223/999): loss=0.12200937496128858\n",
      "Stochastic Gradient Descent(224/999): loss=0.1219975819886064\n",
      "Stochastic Gradient Descent(225/999): loss=0.12198580513827131\n",
      "Stochastic Gradient Descent(226/999): loss=0.1219740443663751\n",
      "Stochastic Gradient Descent(227/999): loss=0.12196229962944184\n",
      "Stochastic Gradient Descent(228/999): loss=0.12195057088442035\n",
      "Stochastic Gradient Descent(229/999): loss=0.12193885808867645\n",
      "Stochastic Gradient Descent(230/999): loss=0.12192716119998571\n",
      "Stochastic Gradient Descent(231/999): loss=0.12191548017652591\n",
      "Stochastic Gradient Descent(232/999): loss=0.12190381497687025\n",
      "Stochastic Gradient Descent(233/999): loss=0.12189216555998005\n",
      "Stochastic Gradient Descent(234/999): loss=0.12188053188519794\n",
      "Stochastic Gradient Descent(235/999): loss=0.12186891391224132\n",
      "Stochastic Gradient Descent(236/999): loss=0.12185731160119552\n",
      "Stochastic Gradient Descent(237/999): loss=0.12184572491250749\n",
      "Stochastic Gradient Descent(238/999): loss=0.12183415380697932\n",
      "Stochastic Gradient Descent(239/999): loss=0.12182259824576218\n",
      "Stochastic Gradient Descent(240/999): loss=0.12181105819035007\n",
      "Stochastic Gradient Descent(241/999): loss=0.12179953360257391\n",
      "Stochastic Gradient Descent(242/999): loss=0.12178802444459572\n",
      "Stochastic Gradient Descent(243/999): loss=0.1217765306789027\n",
      "Stochastic Gradient Descent(244/999): loss=0.12176505226830178\n",
      "Stochastic Gradient Descent(245/999): loss=0.121753589175914\n",
      "Stochastic Gradient Descent(246/999): loss=0.121742141365169\n",
      "Stochastic Gradient Descent(247/999): loss=0.1217307087997999\n",
      "Stochastic Gradient Descent(248/999): loss=0.12171929144383795\n",
      "Stochastic Gradient Descent(249/999): loss=0.12170788926160735\n",
      "Stochastic Gradient Descent(250/999): loss=0.12169650221772038\n",
      "Stochastic Gradient Descent(251/999): loss=0.12168513027707242\n",
      "Stochastic Gradient Descent(252/999): loss=0.1216737734048371\n",
      "Stochastic Gradient Descent(253/999): loss=0.12166243156646155\n",
      "Stochastic Gradient Descent(254/999): loss=0.12165110472766183\n",
      "Stochastic Gradient Descent(255/999): loss=0.12163979285441838\n",
      "Stochastic Gradient Descent(256/999): loss=0.12162849591297145\n",
      "Stochastic Gradient Descent(257/999): loss=0.1216172138698168\n",
      "Stochastic Gradient Descent(258/999): loss=0.12160594669170136\n",
      "Stochastic Gradient Descent(259/999): loss=0.12159469434561916\n",
      "Stochastic Gradient Descent(260/999): loss=0.121583456798807\n",
      "Stochastic Gradient Descent(261/999): loss=0.12157223401874048\n",
      "Stochastic Gradient Descent(262/999): loss=0.1215610259731301\n",
      "Stochastic Gradient Descent(263/999): loss=0.12154983262991724\n",
      "Stochastic Gradient Descent(264/999): loss=0.12153865395727044\n",
      "Stochastic Gradient Descent(265/999): loss=0.12152748992358157\n",
      "Stochastic Gradient Descent(266/999): loss=0.12151634049746225\n",
      "Stochastic Gradient Descent(267/999): loss=0.12150520564774013\n",
      "Stochastic Gradient Descent(268/999): loss=0.12149408534345549\n",
      "Stochastic Gradient Descent(269/999): loss=0.12148297955385749\n",
      "Stochastic Gradient Descent(270/999): loss=0.12147188824840126\n",
      "Stochastic Gradient Descent(271/999): loss=0.12146081139674404\n",
      "Stochastic Gradient Descent(272/999): loss=0.12144974896874229\n",
      "Stochastic Gradient Descent(273/999): loss=0.12143870093444821\n",
      "Stochastic Gradient Descent(274/999): loss=0.12142766726410675\n",
      "Stochastic Gradient Descent(275/999): loss=0.12141664792815256\n",
      "Stochastic Gradient Descent(276/999): loss=0.12140564289720676\n",
      "Stochastic Gradient Descent(277/999): loss=0.12139465214207414\n",
      "Stochastic Gradient Descent(278/999): loss=0.12138367563374022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(279/999): loss=0.12137271334336834\n",
      "Stochastic Gradient Descent(280/999): loss=0.1213617652422969\n",
      "Stochastic Gradient Descent(281/999): loss=0.12135083130203658\n",
      "Stochastic Gradient Descent(282/999): loss=0.12133991149426761\n",
      "Stochastic Gradient Descent(283/999): loss=0.12132900579083725\n",
      "Stochastic Gradient Descent(284/999): loss=0.12131811416375707\n",
      "Stochastic Gradient Descent(285/999): loss=0.12130723658520041\n",
      "Stochastic Gradient Descent(286/999): loss=0.12129637302749995\n",
      "Stochastic Gradient Descent(287/999): loss=0.12128552346314522\n",
      "Stochastic Gradient Descent(288/999): loss=0.12127468786478018\n",
      "Stochastic Gradient Descent(289/999): loss=0.12126386620520095\n",
      "Stochastic Gradient Descent(290/999): loss=0.1212530584573533\n",
      "Stochastic Gradient Descent(291/999): loss=0.12124226459433066\n",
      "Stochastic Gradient Descent(292/999): loss=0.12123148458937165\n",
      "Stochastic Gradient Descent(293/999): loss=0.12122071841585792\n",
      "Stochastic Gradient Descent(294/999): loss=0.12120996604731225\n",
      "Stochastic Gradient Descent(295/999): loss=0.12119922745739616\n",
      "Stochastic Gradient Descent(296/999): loss=0.121188502619908\n",
      "Stochastic Gradient Descent(297/999): loss=0.12117779150878082\n",
      "Stochastic Gradient Descent(298/999): loss=0.1211670940980806\n",
      "Stochastic Gradient Descent(299/999): loss=0.12115641036200402\n",
      "Stochastic Gradient Descent(300/999): loss=0.1211457402748767\n",
      "Stochastic Gradient Descent(301/999): loss=0.12113508381115139\n",
      "Stochastic Gradient Descent(302/999): loss=0.12112444094540602\n",
      "Stochastic Gradient Descent(303/999): loss=0.1211138116523419\n",
      "Stochastic Gradient Descent(304/999): loss=0.12110319590678201\n",
      "Stochastic Gradient Descent(305/999): loss=0.12109259368366926\n",
      "Stochastic Gradient Descent(306/999): loss=0.12108200495806469\n",
      "Stochastic Gradient Descent(307/999): loss=0.12107142970514595\n",
      "Stochastic Gradient Descent(308/999): loss=0.12106086790020555\n",
      "Stochastic Gradient Descent(309/999): loss=0.12105031951864922\n",
      "Stochastic Gradient Descent(310/999): loss=0.12103978453599451\n",
      "Stochastic Gradient Descent(311/999): loss=0.12102926292786902\n",
      "Stochastic Gradient Descent(312/999): loss=0.12101875467000907\n",
      "Stochastic Gradient Descent(313/999): loss=0.12100825973825802\n",
      "Stochastic Gradient Descent(314/999): loss=0.120997778108565\n",
      "Stochastic Gradient Descent(315/999): loss=0.12098730975698335\n",
      "Stochastic Gradient Descent(316/999): loss=0.12097685465966924\n",
      "Stochastic Gradient Descent(317/999): loss=0.1209664127928803\n",
      "Stochastic Gradient Descent(318/999): loss=0.12095598413297419\n",
      "Stochastic Gradient Descent(319/999): loss=0.12094556865640745\n",
      "Stochastic Gradient Descent(320/999): loss=0.12093516633973402\n",
      "Stochastic Gradient Descent(321/999): loss=0.12092477715960395\n",
      "Stochastic Gradient Descent(322/999): loss=0.12091440109276232\n",
      "Stochastic Gradient Descent(323/999): loss=0.12090403811604779\n",
      "Stochastic Gradient Descent(324/999): loss=0.12089368820639157\n",
      "Stochastic Gradient Descent(325/999): loss=0.12088335134081613\n",
      "Stochastic Gradient Descent(326/999): loss=0.12087302749643396\n",
      "Stochastic Gradient Descent(327/999): loss=0.12086271665044669\n",
      "Stochastic Gradient Descent(328/999): loss=0.1208524187801436\n",
      "Stochastic Gradient Descent(329/999): loss=0.1208421338629009\n",
      "Stochastic Gradient Descent(330/999): loss=0.12083186187618027\n",
      "Stochastic Gradient Descent(331/999): loss=0.12082160279752813\n",
      "Stochastic Gradient Descent(332/999): loss=0.1208113566045744\n",
      "Stochastic Gradient Descent(333/999): loss=0.12080112327503152\n",
      "Stochastic Gradient Descent(334/999): loss=0.12079090278669345\n",
      "Stochastic Gradient Descent(335/999): loss=0.12078069511743465\n",
      "Stochastic Gradient Descent(336/999): loss=0.12077050024520927\n",
      "Stochastic Gradient Descent(337/999): loss=0.12076031814804991\n",
      "Stochastic Gradient Descent(338/999): loss=0.120750148804067\n",
      "Stochastic Gradient Descent(339/999): loss=0.12073999219144763\n",
      "Stochastic Gradient Descent(340/999): loss=0.12072984828845483\n",
      "Stochastic Gradient Descent(341/999): loss=0.1207197170734266\n",
      "Stochastic Gradient Descent(342/999): loss=0.120709598524775\n",
      "Stochastic Gradient Descent(343/999): loss=0.12069949262098546\n",
      "Stochastic Gradient Descent(344/999): loss=0.12068939934061579\n",
      "Stochastic Gradient Descent(345/999): loss=0.12067931866229537\n",
      "Stochastic Gradient Descent(346/999): loss=0.12066925056472447\n",
      "Stochastic Gradient Descent(347/999): loss=0.12065919502667335\n",
      "Stochastic Gradient Descent(348/999): loss=0.12064915202698148\n",
      "Stochastic Gradient Descent(349/999): loss=0.12063912154455689\n",
      "Stochastic Gradient Descent(350/999): loss=0.12062910355837522\n",
      "Stochastic Gradient Descent(351/999): loss=0.12061909804747925\n",
      "Stochastic Gradient Descent(352/999): loss=0.1206091049909779\n",
      "Stochastic Gradient Descent(353/999): loss=0.12059912436804578\n",
      "Stochastic Gradient Descent(354/999): loss=0.12058915615792234\n",
      "Stochastic Gradient Descent(355/999): loss=0.12057920033991117\n",
      "Stochastic Gradient Descent(356/999): loss=0.1205692568933794\n",
      "Stochastic Gradient Descent(357/999): loss=0.12055932579775709\n",
      "Stochastic Gradient Descent(358/999): loss=0.12054940703253642\n",
      "Stochastic Gradient Descent(359/999): loss=0.12053950057727125\n",
      "Stochastic Gradient Descent(360/999): loss=0.12052960641157631\n",
      "Stochastic Gradient Descent(361/999): loss=0.12051972451512669\n",
      "Stochastic Gradient Descent(362/999): loss=0.12050985486765728\n",
      "Stochastic Gradient Descent(363/999): loss=0.12049999744896206\n",
      "Stochastic Gradient Descent(364/999): loss=0.12049015223889363\n",
      "Stochastic Gradient Descent(365/999): loss=0.12048031921736255\n",
      "Stochastic Gradient Descent(366/999): loss=0.12047049836433686\n",
      "Stochastic Gradient Descent(367/999): loss=0.12046068965984146\n",
      "Stochastic Gradient Descent(368/999): loss=0.12045089308395762\n",
      "Stochastic Gradient Descent(369/999): loss=0.12044110861682242\n",
      "Stochastic Gradient Descent(370/999): loss=0.1204313362386282\n",
      "Stochastic Gradient Descent(371/999): loss=0.12042157592962223\n",
      "Stochastic Gradient Descent(372/999): loss=0.12041182767010589\n",
      "Stochastic Gradient Descent(373/999): loss=0.1204020914404344\n",
      "Stochastic Gradient Descent(374/999): loss=0.12039236722101639\n",
      "Stochastic Gradient Descent(375/999): loss=0.12038265499231317\n",
      "Stochastic Gradient Descent(376/999): loss=0.12037295473483849\n",
      "Stochastic Gradient Descent(377/999): loss=0.12036326642915791\n",
      "Stochastic Gradient Descent(378/999): loss=0.12035359005588857\n",
      "Stochastic Gradient Descent(379/999): loss=0.12034392559569851\n",
      "Stochastic Gradient Descent(380/999): loss=0.12033427302930634\n",
      "Stochastic Gradient Descent(381/999): loss=0.12032463233748081\n",
      "Stochastic Gradient Descent(382/999): loss=0.12031500350104038\n",
      "Stochastic Gradient Descent(383/999): loss=0.1203053865008528\n",
      "Stochastic Gradient Descent(384/999): loss=0.12029578131783474\n",
      "Stochastic Gradient Descent(385/999): loss=0.1202861879329512\n",
      "Stochastic Gradient Descent(386/999): loss=0.12027660632721555\n",
      "Stochastic Gradient Descent(387/999): loss=0.12026703648168857\n",
      "Stochastic Gradient Descent(388/999): loss=0.12025747837747854\n",
      "Stochastic Gradient Descent(389/999): loss=0.1202479319957406\n",
      "Stochastic Gradient Descent(390/999): loss=0.12023839731767655\n",
      "Stochastic Gradient Descent(391/999): loss=0.12022887432453429\n",
      "Stochastic Gradient Descent(392/999): loss=0.12021936299760766\n",
      "Stochastic Gradient Descent(393/999): loss=0.12020986331823603\n",
      "Stochastic Gradient Descent(394/999): loss=0.12020037526780385\n",
      "Stochastic Gradient Descent(395/999): loss=0.12019089882774045\n",
      "Stochastic Gradient Descent(396/999): loss=0.12018143397951966\n",
      "Stochastic Gradient Descent(397/999): loss=0.12017198070465947\n",
      "Stochastic Gradient Descent(398/999): loss=0.12016253898472165\n",
      "Stochastic Gradient Descent(399/999): loss=0.12015310880131161\n",
      "Stochastic Gradient Descent(400/999): loss=0.12014369013607792\n",
      "Stochastic Gradient Descent(401/999): loss=0.12013428297071205\n",
      "Stochastic Gradient Descent(402/999): loss=0.12012488728694808\n",
      "Stochastic Gradient Descent(403/999): loss=0.12011550306656242\n",
      "Stochastic Gradient Descent(404/999): loss=0.1201061302913735\n",
      "Stochastic Gradient Descent(405/999): loss=0.12009676894324153\n",
      "Stochastic Gradient Descent(406/999): loss=0.12008741900406805\n",
      "Stochastic Gradient Descent(407/999): loss=0.12007808045579588\n",
      "Stochastic Gradient Descent(408/999): loss=0.12006875328040877\n",
      "Stochastic Gradient Descent(409/999): loss=0.12005943745993095\n",
      "Stochastic Gradient Descent(410/999): loss=0.1200501329764272\n",
      "Stochastic Gradient Descent(411/999): loss=0.12004083981200224\n",
      "Stochastic Gradient Descent(412/999): loss=0.12003155794880083\n",
      "Stochastic Gradient Descent(413/999): loss=0.12002228736900718\n",
      "Stochastic Gradient Descent(414/999): loss=0.12001302805484496\n",
      "Stochastic Gradient Descent(415/999): loss=0.12000377998857681\n",
      "Stochastic Gradient Descent(416/999): loss=0.1199945431525044\n",
      "Stochastic Gradient Descent(417/999): loss=0.11998531752896793\n",
      "Stochastic Gradient Descent(418/999): loss=0.11997610310034601\n",
      "Stochastic Gradient Descent(419/999): loss=0.11996689984905551\n",
      "Stochastic Gradient Descent(420/999): loss=0.11995770775755113\n",
      "Stochastic Gradient Descent(421/999): loss=0.11994852680832534\n",
      "Stochastic Gradient Descent(422/999): loss=0.11993935698390809\n",
      "Stochastic Gradient Descent(423/999): loss=0.11993019826686668\n",
      "Stochastic Gradient Descent(424/999): loss=0.11992105063980549\n",
      "Stochastic Gradient Descent(425/999): loss=0.11991191408536564\n",
      "Stochastic Gradient Descent(426/999): loss=0.11990278858622506\n",
      "Stochastic Gradient Descent(427/999): loss=0.11989367412509808\n",
      "Stochastic Gradient Descent(428/999): loss=0.11988457068473528\n",
      "Stochastic Gradient Descent(429/999): loss=0.11987547824792331\n",
      "Stochastic Gradient Descent(430/999): loss=0.11986639679748469\n",
      "Stochastic Gradient Descent(431/999): loss=0.11985732631627766\n",
      "Stochastic Gradient Descent(432/999): loss=0.11984826678719587\n",
      "Stochastic Gradient Descent(433/999): loss=0.11983921819316831\n",
      "Stochastic Gradient Descent(434/999): loss=0.11983018051715917\n",
      "Stochastic Gradient Descent(435/999): loss=0.11982115374216748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(436/999): loss=0.11981213785122707\n",
      "Stochastic Gradient Descent(437/999): loss=0.11980313282740636\n",
      "Stochastic Gradient Descent(438/999): loss=0.1197941386538082\n",
      "Stochastic Gradient Descent(439/999): loss=0.11978515531356965\n",
      "Stochastic Gradient Descent(440/999): loss=0.11977618278986195\n",
      "Stochastic Gradient Descent(441/999): loss=0.11976722106589015\n",
      "Stochastic Gradient Descent(442/999): loss=0.11975827012489308\n",
      "Stochastic Gradient Descent(443/999): loss=0.11974932995014324\n",
      "Stochastic Gradient Descent(444/999): loss=0.11974040052494644\n",
      "Stochastic Gradient Descent(445/999): loss=0.11973148183264183\n",
      "Stochastic Gradient Descent(446/999): loss=0.11972257385660175\n",
      "Stochastic Gradient Descent(447/999): loss=0.11971367658023144\n",
      "Stochastic Gradient Descent(448/999): loss=0.11970478998696894\n",
      "Stochastic Gradient Descent(449/999): loss=0.11969591406028503\n",
      "Stochastic Gradient Descent(450/999): loss=0.11968704878368301\n",
      "Stochastic Gradient Descent(451/999): loss=0.11967819414069857\n",
      "Stochastic Gradient Descent(452/999): loss=0.11966935011489958\n",
      "Stochastic Gradient Descent(453/999): loss=0.11966051668988612\n",
      "Stochastic Gradient Descent(454/999): loss=0.11965169384929018\n",
      "Stochastic Gradient Descent(455/999): loss=0.11964288157677556\n",
      "Stochastic Gradient Descent(456/999): loss=0.11963407985603787\n",
      "Stochastic Gradient Descent(457/999): loss=0.11962528867080412\n",
      "Stochastic Gradient Descent(458/999): loss=0.1196165080048329\n",
      "Stochastic Gradient Descent(459/999): loss=0.119607737841914\n",
      "Stochastic Gradient Descent(460/999): loss=0.11959897816586847\n",
      "Stochastic Gradient Descent(461/999): loss=0.11959022896054837\n",
      "Stochastic Gradient Descent(462/999): loss=0.11958149020983672\n",
      "Stochastic Gradient Descent(463/999): loss=0.11957276189764729\n",
      "Stochastic Gradient Descent(464/999): loss=0.11956404400792459\n",
      "Stochastic Gradient Descent(465/999): loss=0.11955533652464365\n",
      "Stochastic Gradient Descent(466/999): loss=0.11954663943181\n",
      "Stochastic Gradient Descent(467/999): loss=0.11953795271345947\n",
      "Stochastic Gradient Descent(468/999): loss=0.11952927635365806\n",
      "Stochastic Gradient Descent(469/999): loss=0.11952061033650194\n",
      "Stochastic Gradient Descent(470/999): loss=0.11951195464611729\n",
      "Stochastic Gradient Descent(471/999): loss=0.11950330926666003\n",
      "Stochastic Gradient Descent(472/999): loss=0.11949467418231602\n",
      "Stochastic Gradient Descent(473/999): loss=0.11948604937730062\n",
      "Stochastic Gradient Descent(474/999): loss=0.11947743483585888\n",
      "Stochastic Gradient Descent(475/999): loss=0.1194688305422652\n",
      "Stochastic Gradient Descent(476/999): loss=0.11946023648082334\n",
      "Stochastic Gradient Descent(477/999): loss=0.11945165263586635\n",
      "Stochastic Gradient Descent(478/999): loss=0.11944307899175638\n",
      "Stochastic Gradient Descent(479/999): loss=0.11943451553288455\n",
      "Stochastic Gradient Descent(480/999): loss=0.11942596224367101\n",
      "Stochastic Gradient Descent(481/999): loss=0.11941741910856471\n",
      "Stochastic Gradient Descent(482/999): loss=0.1194088861120433\n",
      "Stochastic Gradient Descent(483/999): loss=0.11940036323861314\n",
      "Stochastic Gradient Descent(484/999): loss=0.11939185047280902\n",
      "Stochastic Gradient Descent(485/999): loss=0.11938334779919432\n",
      "Stochastic Gradient Descent(486/999): loss=0.11937485520236073\n",
      "Stochastic Gradient Descent(487/999): loss=0.11936637266692812\n",
      "Stochastic Gradient Descent(488/999): loss=0.11935790017754462\n",
      "Stochastic Gradient Descent(489/999): loss=0.11934943771888641\n",
      "Stochastic Gradient Descent(490/999): loss=0.11934098527565763\n",
      "Stochastic Gradient Descent(491/999): loss=0.11933254283259045\n",
      "Stochastic Gradient Descent(492/999): loss=0.11932411037444467\n",
      "Stochastic Gradient Descent(493/999): loss=0.11931568788600788\n",
      "Stochastic Gradient Descent(494/999): loss=0.11930727535209544\n",
      "Stochastic Gradient Descent(495/999): loss=0.11929887275755009\n",
      "Stochastic Gradient Descent(496/999): loss=0.11929048008724213\n",
      "Stochastic Gradient Descent(497/999): loss=0.11928209732606926\n",
      "Stochastic Gradient Descent(498/999): loss=0.11927372445895638\n",
      "Stochastic Gradient Descent(499/999): loss=0.11926536147085574\n",
      "Stochastic Gradient Descent(500/999): loss=0.11925700834674668\n",
      "Stochastic Gradient Descent(501/999): loss=0.11924866507163562\n",
      "Stochastic Gradient Descent(502/999): loss=0.11924033163055588\n",
      "Stochastic Gradient Descent(503/999): loss=0.11923200800856779\n",
      "Stochastic Gradient Descent(504/999): loss=0.11922369419075844\n",
      "Stochastic Gradient Descent(505/999): loss=0.11921539016224171\n",
      "Stochastic Gradient Descent(506/999): loss=0.11920709590815812\n",
      "Stochastic Gradient Descent(507/999): loss=0.11919881141367476\n",
      "Stochastic Gradient Descent(508/999): loss=0.11919053666398532\n",
      "Stochastic Gradient Descent(509/999): loss=0.1191822716443099\n",
      "Stochastic Gradient Descent(510/999): loss=0.11917401633989488\n",
      "Stochastic Gradient Descent(511/999): loss=0.11916577073601312\n",
      "Stochastic Gradient Descent(512/999): loss=0.11915753481796353\n",
      "Stochastic Gradient Descent(513/999): loss=0.11914930857107123\n",
      "Stochastic Gradient Descent(514/999): loss=0.1191410919806875\n",
      "Stochastic Gradient Descent(515/999): loss=0.11913288503218955\n",
      "Stochastic Gradient Descent(516/999): loss=0.11912468771098052\n",
      "Stochastic Gradient Descent(517/999): loss=0.1191165000024895\n",
      "Stochastic Gradient Descent(518/999): loss=0.11910832189217128\n",
      "Stochastic Gradient Descent(519/999): loss=0.11910015336550647\n",
      "Stochastic Gradient Descent(520/999): loss=0.11909199440800136\n",
      "Stochastic Gradient Descent(521/999): loss=0.11908384500518766\n",
      "Stochastic Gradient Descent(522/999): loss=0.11907570514262289\n",
      "Stochastic Gradient Descent(523/999): loss=0.11906757480588981\n",
      "Stochastic Gradient Descent(524/999): loss=0.11905945398059668\n",
      "Stochastic Gradient Descent(525/999): loss=0.11905134265237707\n",
      "Stochastic Gradient Descent(526/999): loss=0.11904324080688988\n",
      "Stochastic Gradient Descent(527/999): loss=0.11903514842981908\n",
      "Stochastic Gradient Descent(528/999): loss=0.11902706550687388\n",
      "Stochastic Gradient Descent(529/999): loss=0.11901899202378864\n",
      "Stochastic Gradient Descent(530/999): loss=0.11901092796632255\n",
      "Stochastic Gradient Descent(531/999): loss=0.1190028733202599\n",
      "Stochastic Gradient Descent(532/999): loss=0.11899482807140982\n",
      "Stochastic Gradient Descent(533/999): loss=0.11898679220560629\n",
      "Stochastic Gradient Descent(534/999): loss=0.11897876570870806\n",
      "Stochastic Gradient Descent(535/999): loss=0.11897074856659857\n",
      "Stochastic Gradient Descent(536/999): loss=0.11896274076518587\n",
      "Stochastic Gradient Descent(537/999): loss=0.11895474229040273\n",
      "Stochastic Gradient Descent(538/999): loss=0.11894675312820632\n",
      "Stochastic Gradient Descent(539/999): loss=0.11893877326457834\n",
      "Stochastic Gradient Descent(540/999): loss=0.1189308026855249\n",
      "Stochastic Gradient Descent(541/999): loss=0.11892284137707645\n",
      "Stochastic Gradient Descent(542/999): loss=0.11891488932528776\n",
      "Stochastic Gradient Descent(543/999): loss=0.11890694651623783\n",
      "Stochastic Gradient Descent(544/999): loss=0.11889901293602982\n",
      "Stochastic Gradient Descent(545/999): loss=0.11889108857079106\n",
      "Stochastic Gradient Descent(546/999): loss=0.11888317340667293\n",
      "Stochastic Gradient Descent(547/999): loss=0.11887526742985079\n",
      "Stochastic Gradient Descent(548/999): loss=0.11886737062652405\n",
      "Stochastic Gradient Descent(549/999): loss=0.11885948298291596\n",
      "Stochastic Gradient Descent(550/999): loss=0.11885160448527357\n",
      "Stochastic Gradient Descent(551/999): loss=0.11884373511986784\n",
      "Stochastic Gradient Descent(552/999): loss=0.11883587487299345\n",
      "Stochastic Gradient Descent(553/999): loss=0.11882802373096862\n",
      "Stochastic Gradient Descent(554/999): loss=0.1188201816801354\n",
      "Stochastic Gradient Descent(555/999): loss=0.11881234870685936\n",
      "Stochastic Gradient Descent(556/999): loss=0.11880452479752954\n",
      "Stochastic Gradient Descent(557/999): loss=0.11879670993855845\n",
      "Stochastic Gradient Descent(558/999): loss=0.11878890411638213\n",
      "Stochastic Gradient Descent(559/999): loss=0.11878110731745992\n",
      "Stochastic Gradient Descent(560/999): loss=0.11877331952827448\n",
      "Stochastic Gradient Descent(561/999): loss=0.1187655407353317\n",
      "Stochastic Gradient Descent(562/999): loss=0.11875777092516082\n",
      "Stochastic Gradient Descent(563/999): loss=0.1187500100843141\n",
      "Stochastic Gradient Descent(564/999): loss=0.11874225819936703\n",
      "Stochastic Gradient Descent(565/999): loss=0.11873451525691804\n",
      "Stochastic Gradient Descent(566/999): loss=0.11872678124358871\n",
      "Stochastic Gradient Descent(567/999): loss=0.11871905614602349\n",
      "Stochastic Gradient Descent(568/999): loss=0.11871133995088981\n",
      "Stochastic Gradient Descent(569/999): loss=0.11870363264487789\n",
      "Stochastic Gradient Descent(570/999): loss=0.1186959342147009\n",
      "Stochastic Gradient Descent(571/999): loss=0.11868824464709464\n",
      "Stochastic Gradient Descent(572/999): loss=0.11868056392881772\n",
      "Stochastic Gradient Descent(573/999): loss=0.11867289204665138\n",
      "Stochastic Gradient Descent(574/999): loss=0.11866522898739952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(575/999): loss=0.11865757473788857\n",
      "Stochastic Gradient Descent(576/999): loss=0.11864992928496758\n",
      "Stochastic Gradient Descent(577/999): loss=0.11864229261550795\n",
      "Stochastic Gradient Descent(578/999): loss=0.11863466471640363\n",
      "Stochastic Gradient Descent(579/999): loss=0.1186270455745709\n",
      "Stochastic Gradient Descent(580/999): loss=0.11861943517694844\n",
      "Stochastic Gradient Descent(581/999): loss=0.11861183351049714\n",
      "Stochastic Gradient Descent(582/999): loss=0.1186042405622002\n",
      "Stochastic Gradient Descent(583/999): loss=0.11859665631906305\n",
      "Stochastic Gradient Descent(584/999): loss=0.11858908076811323\n",
      "Stochastic Gradient Descent(585/999): loss=0.11858151389640037\n",
      "Stochastic Gradient Descent(586/999): loss=0.11857395569099623\n",
      "Stochastic Gradient Descent(587/999): loss=0.11856640613899455\n",
      "Stochastic Gradient Descent(588/999): loss=0.11855886522751109\n",
      "Stochastic Gradient Descent(589/999): loss=0.11855133294368347\n",
      "Stochastic Gradient Descent(590/999): loss=0.11854380927467133\n",
      "Stochastic Gradient Descent(591/999): loss=0.11853629420765595\n",
      "Stochastic Gradient Descent(592/999): loss=0.11852878772984063\n",
      "Stochastic Gradient Descent(593/999): loss=0.11852128982845025\n",
      "Stochastic Gradient Descent(594/999): loss=0.11851380049073156\n",
      "Stochastic Gradient Descent(595/999): loss=0.11850631970395284\n",
      "Stochastic Gradient Descent(596/999): loss=0.11849884745540405\n",
      "Stochastic Gradient Descent(597/999): loss=0.1184913837323967\n",
      "Stochastic Gradient Descent(598/999): loss=0.11848392852226398\n",
      "Stochastic Gradient Descent(599/999): loss=0.11847648181236037\n",
      "Stochastic Gradient Descent(600/999): loss=0.1184690435900619\n",
      "Stochastic Gradient Descent(601/999): loss=0.11846161384276606\n",
      "Stochastic Gradient Descent(602/999): loss=0.1184541925578916\n",
      "Stochastic Gradient Descent(603/999): loss=0.11844677972287868\n",
      "Stochastic Gradient Descent(604/999): loss=0.11843937532518871\n",
      "Stochastic Gradient Descent(605/999): loss=0.11843197935230437\n",
      "Stochastic Gradient Descent(606/999): loss=0.11842459179172947\n",
      "Stochastic Gradient Descent(607/999): loss=0.11841721263098907\n",
      "Stochastic Gradient Descent(608/999): loss=0.11840984185762932\n",
      "Stochastic Gradient Descent(609/999): loss=0.11840247945921736\n",
      "Stochastic Gradient Descent(610/999): loss=0.11839512542334144\n",
      "Stochastic Gradient Descent(611/999): loss=0.11838777973761079\n",
      "Stochastic Gradient Descent(612/999): loss=0.11838044238965564\n",
      "Stochastic Gradient Descent(613/999): loss=0.1183731133671271\n",
      "Stochastic Gradient Descent(614/999): loss=0.11836579265769706\n",
      "Stochastic Gradient Descent(615/999): loss=0.11835848024905835\n",
      "Stochastic Gradient Descent(616/999): loss=0.11835117612892458\n",
      "Stochastic Gradient Descent(617/999): loss=0.11834388028503004\n",
      "Stochastic Gradient Descent(618/999): loss=0.1183365927051298\n",
      "Stochastic Gradient Descent(619/999): loss=0.11832931337699959\n",
      "Stochastic Gradient Descent(620/999): loss=0.11832204228843574\n",
      "Stochastic Gradient Descent(621/999): loss=0.11831477942725514\n",
      "Stochastic Gradient Descent(622/999): loss=0.11830752478129536\n",
      "Stochastic Gradient Descent(623/999): loss=0.11830027833841435\n",
      "Stochastic Gradient Descent(624/999): loss=0.11829304008649057\n",
      "Stochastic Gradient Descent(625/999): loss=0.11828581001342295\n",
      "Stochastic Gradient Descent(626/999): loss=0.11827858810713075\n",
      "Stochastic Gradient Descent(627/999): loss=0.11827137435555365\n",
      "Stochastic Gradient Descent(628/999): loss=0.11826416874665156\n",
      "Stochastic Gradient Descent(629/999): loss=0.1182569712684048\n",
      "Stochastic Gradient Descent(630/999): loss=0.11824978190881386\n",
      "Stochastic Gradient Descent(631/999): loss=0.1182426006558993\n",
      "Stochastic Gradient Descent(632/999): loss=0.11823542749770208\n",
      "Stochastic Gradient Descent(633/999): loss=0.11822826242228313\n",
      "Stochastic Gradient Descent(634/999): loss=0.1182211054177235\n",
      "Stochastic Gradient Descent(635/999): loss=0.11821395647212429\n",
      "Stochastic Gradient Descent(636/999): loss=0.11820681557360663\n",
      "Stochastic Gradient Descent(637/999): loss=0.11819968271031159\n",
      "Stochastic Gradient Descent(638/999): loss=0.11819255787040019\n",
      "Stochastic Gradient Descent(639/999): loss=0.11818544104205334\n",
      "Stochastic Gradient Descent(640/999): loss=0.11817833221347185\n",
      "Stochastic Gradient Descent(641/999): loss=0.11817123137287629\n",
      "Stochastic Gradient Descent(642/999): loss=0.118164138508507\n",
      "Stochastic Gradient Descent(643/999): loss=0.1181570536086242\n",
      "Stochastic Gradient Descent(644/999): loss=0.11814997666150767\n",
      "Stochastic Gradient Descent(645/999): loss=0.118142907655457\n",
      "Stochastic Gradient Descent(646/999): loss=0.11813584657879125\n",
      "Stochastic Gradient Descent(647/999): loss=0.11812879341984923\n",
      "Stochastic Gradient Descent(648/999): loss=0.1181217481669893\n",
      "Stochastic Gradient Descent(649/999): loss=0.11811471080858921\n",
      "Stochastic Gradient Descent(650/999): loss=0.11810768133304639\n",
      "Stochastic Gradient Descent(651/999): loss=0.11810065972877758\n",
      "Stochastic Gradient Descent(652/999): loss=0.11809364598421906\n",
      "Stochastic Gradient Descent(653/999): loss=0.11808664008782635\n",
      "Stochastic Gradient Descent(654/999): loss=0.11807964202807444\n",
      "Stochastic Gradient Descent(655/999): loss=0.11807265179345759\n",
      "Stochastic Gradient Descent(656/999): loss=0.11806566937248937\n",
      "Stochastic Gradient Descent(657/999): loss=0.11805869475370245\n",
      "Stochastic Gradient Descent(658/999): loss=0.1180517279256489\n",
      "Stochastic Gradient Descent(659/999): loss=0.11804476887689982\n",
      "Stochastic Gradient Descent(660/999): loss=0.11803781759604552\n",
      "Stochastic Gradient Descent(661/999): loss=0.11803087407169535\n",
      "Stochastic Gradient Descent(662/999): loss=0.1180239382924778\n",
      "Stochastic Gradient Descent(663/999): loss=0.11801701024704028\n",
      "Stochastic Gradient Descent(664/999): loss=0.11801008992404927\n",
      "Stochastic Gradient Descent(665/999): loss=0.11800317731219023\n",
      "Stochastic Gradient Descent(666/999): loss=0.11799627240016743\n",
      "Stochastic Gradient Descent(667/999): loss=0.11798937517670417\n",
      "Stochastic Gradient Descent(668/999): loss=0.11798248563054248\n",
      "Stochastic Gradient Descent(669/999): loss=0.11797560375044332\n",
      "Stochastic Gradient Descent(670/999): loss=0.11796872952518633\n",
      "Stochastic Gradient Descent(671/999): loss=0.11796186294356997\n",
      "Stochastic Gradient Descent(672/999): loss=0.11795500399441142\n",
      "Stochastic Gradient Descent(673/999): loss=0.11794815266654644\n",
      "Stochastic Gradient Descent(674/999): loss=0.11794130894882958\n",
      "Stochastic Gradient Descent(675/999): loss=0.11793447283013393\n",
      "Stochastic Gradient Descent(676/999): loss=0.11792764429935111\n",
      "Stochastic Gradient Descent(677/999): loss=0.11792082334539139\n",
      "Stochastic Gradient Descent(678/999): loss=0.11791400995718351\n",
      "Stochastic Gradient Descent(679/999): loss=0.11790720412367463\n",
      "Stochastic Gradient Descent(680/999): loss=0.11790040583383044\n",
      "Stochastic Gradient Descent(681/999): loss=0.117893615076635\n",
      "Stochastic Gradient Descent(682/999): loss=0.11788683184109075\n",
      "Stochastic Gradient Descent(683/999): loss=0.11788005611621845\n",
      "Stochastic Gradient Descent(684/999): loss=0.1178732878910572\n",
      "Stochastic Gradient Descent(685/999): loss=0.11786652715466438\n",
      "Stochastic Gradient Descent(686/999): loss=0.11785977389611561\n",
      "Stochastic Gradient Descent(687/999): loss=0.11785302810450474\n",
      "Stochastic Gradient Descent(688/999): loss=0.1178462897689437\n",
      "Stochastic Gradient Descent(689/999): loss=0.11783955887856268\n",
      "Stochastic Gradient Descent(690/999): loss=0.11783283542250993\n",
      "Stochastic Gradient Descent(691/999): loss=0.11782611938995176\n",
      "Stochastic Gradient Descent(692/999): loss=0.1178194107700726\n",
      "Stochastic Gradient Descent(693/999): loss=0.11781270955207479\n",
      "Stochastic Gradient Descent(694/999): loss=0.11780601572517876\n",
      "Stochastic Gradient Descent(695/999): loss=0.11779932927862281\n",
      "Stochastic Gradient Descent(696/999): loss=0.11779265020166313\n",
      "Stochastic Gradient Descent(697/999): loss=0.11778597848357389\n",
      "Stochastic Gradient Descent(698/999): loss=0.11777931411364706\n",
      "Stochastic Gradient Descent(699/999): loss=0.11777265708119242\n",
      "Stochastic Gradient Descent(700/999): loss=0.11776600737553752\n",
      "Stochastic Gradient Descent(701/999): loss=0.11775936498602775\n",
      "Stochastic Gradient Descent(702/999): loss=0.11775272990202611\n",
      "Stochastic Gradient Descent(703/999): loss=0.11774610211291335\n",
      "Stochastic Gradient Descent(704/999): loss=0.1177394816080879\n",
      "Stochastic Gradient Descent(705/999): loss=0.11773286837696577\n",
      "Stochastic Gradient Descent(706/999): loss=0.11772626240898056\n",
      "Stochastic Gradient Descent(707/999): loss=0.1177196636935835\n",
      "Stochastic Gradient Descent(708/999): loss=0.11771307222024327\n",
      "Stochastic Gradient Descent(709/999): loss=0.11770648797844609\n",
      "Stochastic Gradient Descent(710/999): loss=0.11769991095769564\n",
      "Stochastic Gradient Descent(711/999): loss=0.11769334114751305\n",
      "Stochastic Gradient Descent(712/999): loss=0.11768677853743684\n",
      "Stochastic Gradient Descent(713/999): loss=0.1176802231170229\n",
      "Stochastic Gradient Descent(714/999): loss=0.11767367487584456\n",
      "Stochastic Gradient Descent(715/999): loss=0.11766713380349225\n",
      "Stochastic Gradient Descent(716/999): loss=0.11766059988957385\n",
      "Stochastic Gradient Descent(717/999): loss=0.1176540731237145\n",
      "Stochastic Gradient Descent(718/999): loss=0.11764755349555643\n",
      "Stochastic Gradient Descent(719/999): loss=0.11764104099475918\n",
      "Stochastic Gradient Descent(720/999): loss=0.11763453561099937\n",
      "Stochastic Gradient Descent(721/999): loss=0.11762803733397081\n",
      "Stochastic Gradient Descent(722/999): loss=0.11762154615338435\n",
      "Stochastic Gradient Descent(723/999): loss=0.11761506205896792\n",
      "Stochastic Gradient Descent(724/999): loss=0.11760858504046652\n",
      "Stochastic Gradient Descent(725/999): loss=0.1176021150876421\n",
      "Stochastic Gradient Descent(726/999): loss=0.11759565219027361\n",
      "Stochastic Gradient Descent(727/999): loss=0.11758919633815693\n",
      "Stochastic Gradient Descent(728/999): loss=0.1175827475211049\n",
      "Stochastic Gradient Descent(729/999): loss=0.11757630572894719\n",
      "Stochastic Gradient Descent(730/999): loss=0.11756987095153032\n",
      "Stochastic Gradient Descent(731/999): loss=0.1175634431787177\n",
      "Stochastic Gradient Descent(732/999): loss=0.11755702240038941\n",
      "Stochastic Gradient Descent(733/999): loss=0.11755060860644244\n",
      "Stochastic Gradient Descent(734/999): loss=0.11754420178679041\n",
      "Stochastic Gradient Descent(735/999): loss=0.11753780193136365\n",
      "Stochastic Gradient Descent(736/999): loss=0.11753140903010922\n",
      "Stochastic Gradient Descent(737/999): loss=0.11752502307299072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(738/999): loss=0.1175186440499885\n",
      "Stochastic Gradient Descent(739/999): loss=0.1175122719510994\n",
      "Stochastic Gradient Descent(740/999): loss=0.11750590676633682\n",
      "Stochastic Gradient Descent(741/999): loss=0.11749954848573071\n",
      "Stochastic Gradient Descent(742/999): loss=0.1174931970993275\n",
      "Stochastic Gradient Descent(743/999): loss=0.11748685259719008\n",
      "Stochastic Gradient Descent(744/999): loss=0.11748051496939782\n",
      "Stochastic Gradient Descent(745/999): loss=0.11747418420604641\n",
      "Stochastic Gradient Descent(746/999): loss=0.11746786029724803\n",
      "Stochastic Gradient Descent(747/999): loss=0.11746154323313111\n",
      "Stochastic Gradient Descent(748/999): loss=0.11745523300384045\n",
      "Stochastic Gradient Descent(749/999): loss=0.1174489295995371\n",
      "Stochastic Gradient Descent(750/999): loss=0.11744263301039842\n",
      "Stochastic Gradient Descent(751/999): loss=0.11743634322661797\n",
      "Stochastic Gradient Descent(752/999): loss=0.11743006023840556\n",
      "Stochastic Gradient Descent(753/999): loss=0.1174237840359871\n",
      "Stochastic Gradient Descent(754/999): loss=0.1174175146096047\n",
      "Stochastic Gradient Descent(755/999): loss=0.11741125194951657\n",
      "Stochastic Gradient Descent(756/999): loss=0.117404996045997\n",
      "Stochastic Gradient Descent(757/999): loss=0.11739874688933634\n",
      "Stochastic Gradient Descent(758/999): loss=0.117392504469841\n",
      "Stochastic Gradient Descent(759/999): loss=0.11738626877783336\n",
      "Stochastic Gradient Descent(760/999): loss=0.1173800398036518\n",
      "Stochastic Gradient Descent(761/999): loss=0.11737381753765064\n",
      "Stochastic Gradient Descent(762/999): loss=0.11736760197020008\n",
      "Stochastic Gradient Descent(763/999): loss=0.11736139309168622\n",
      "Stochastic Gradient Descent(764/999): loss=0.11735519089251109\n",
      "Stochastic Gradient Descent(765/999): loss=0.1173489953630925\n",
      "Stochastic Gradient Descent(766/999): loss=0.11734280649386403\n",
      "Stochastic Gradient Descent(767/999): loss=0.1173366242752751\n",
      "Stochastic Gradient Descent(768/999): loss=0.11733044869779087\n",
      "Stochastic Gradient Descent(769/999): loss=0.11732427975189215\n",
      "Stochastic Gradient Descent(770/999): loss=0.11731811742807555\n",
      "Stochastic Gradient Descent(771/999): loss=0.11731196171685328\n",
      "Stochastic Gradient Descent(772/999): loss=0.11730581260875317\n",
      "Stochastic Gradient Descent(773/999): loss=0.11729967009431869\n",
      "Stochastic Gradient Descent(774/999): loss=0.11729353416410895\n",
      "Stochastic Gradient Descent(775/999): loss=0.11728740480869847\n",
      "Stochastic Gradient Descent(776/999): loss=0.11728128201867742\n",
      "Stochastic Gradient Descent(777/999): loss=0.11727516578465148\n",
      "Stochastic Gradient Descent(778/999): loss=0.11726905609724167\n",
      "Stochastic Gradient Descent(779/999): loss=0.11726295294708457\n",
      "Stochastic Gradient Descent(780/999): loss=0.11725685632483213\n",
      "Stochastic Gradient Descent(781/999): loss=0.11725076622115173\n",
      "Stochastic Gradient Descent(782/999): loss=0.11724468262672605\n",
      "Stochastic Gradient Descent(783/999): loss=0.1172386055322532\n",
      "Stochastic Gradient Descent(784/999): loss=0.11723253492844644\n",
      "Stochastic Gradient Descent(785/999): loss=0.11722647080603449\n",
      "Stochastic Gradient Descent(786/999): loss=0.11722041315576125\n",
      "Stochastic Gradient Descent(787/999): loss=0.11721436196838576\n",
      "Stochastic Gradient Descent(788/999): loss=0.11720831723468238\n",
      "Stochastic Gradient Descent(789/999): loss=0.11720227894544062\n",
      "Stochastic Gradient Descent(790/999): loss=0.1171962470914651\n",
      "Stochastic Gradient Descent(791/999): loss=0.11719022166357558\n",
      "Stochastic Gradient Descent(792/999): loss=0.11718420265260693\n",
      "Stochastic Gradient Descent(793/999): loss=0.11717819004940903\n",
      "Stochastic Gradient Descent(794/999): loss=0.11717218384484684\n",
      "Stochastic Gradient Descent(795/999): loss=0.11716618402980035\n",
      "Stochastic Gradient Descent(796/999): loss=0.11716019059516455\n",
      "Stochastic Gradient Descent(797/999): loss=0.11715420353184923\n",
      "Stochastic Gradient Descent(798/999): loss=0.11714822283077937\n",
      "Stochastic Gradient Descent(799/999): loss=0.11714224848289466\n",
      "Stochastic Gradient Descent(800/999): loss=0.11713628047914972\n",
      "Stochastic Gradient Descent(801/999): loss=0.1171303188105141\n",
      "Stochastic Gradient Descent(802/999): loss=0.11712436346797205\n",
      "Stochastic Gradient Descent(803/999): loss=0.11711841444252274\n",
      "Stochastic Gradient Descent(804/999): loss=0.11711247172518004\n",
      "Stochastic Gradient Descent(805/999): loss=0.11710653530697257\n",
      "Stochastic Gradient Descent(806/999): loss=0.11710060517894375\n",
      "Stochastic Gradient Descent(807/999): loss=0.11709468133215162\n",
      "Stochastic Gradient Descent(808/999): loss=0.11708876375766888\n",
      "Stochastic Gradient Descent(809/999): loss=0.11708285244658298\n",
      "Stochastic Gradient Descent(810/999): loss=0.11707694738999587\n",
      "Stochastic Gradient Descent(811/999): loss=0.11707104857902416\n",
      "Stochastic Gradient Descent(812/999): loss=0.11706515600479903\n",
      "Stochastic Gradient Descent(813/999): loss=0.11705926965846616\n",
      "Stochastic Gradient Descent(814/999): loss=0.1170533895311858\n",
      "Stochastic Gradient Descent(815/999): loss=0.11704751561413267\n",
      "Stochastic Gradient Descent(816/999): loss=0.11704164789849593\n",
      "Stochastic Gradient Descent(817/999): loss=0.11703578637547916\n",
      "Stochastic Gradient Descent(818/999): loss=0.11702993103630052\n",
      "Stochastic Gradient Descent(819/999): loss=0.11702408187219229\n",
      "Stochastic Gradient Descent(820/999): loss=0.11701823887440133\n",
      "Stochastic Gradient Descent(821/999): loss=0.11701240203418878\n",
      "Stochastic Gradient Descent(822/999): loss=0.11700657134283005\n",
      "Stochastic Gradient Descent(823/999): loss=0.11700074679161492\n",
      "Stochastic Gradient Descent(824/999): loss=0.1169949283718473\n",
      "Stochastic Gradient Descent(825/999): loss=0.1169891160748455\n",
      "Stochastic Gradient Descent(826/999): loss=0.11698330989194188\n",
      "Stochastic Gradient Descent(827/999): loss=0.11697750981448314\n",
      "Stochastic Gradient Descent(828/999): loss=0.11697171583383006\n",
      "Stochastic Gradient Descent(829/999): loss=0.11696592794135757\n",
      "Stochastic Gradient Descent(830/999): loss=0.1169601461284547\n",
      "Stochastic Gradient Descent(831/999): loss=0.1169543703865246\n",
      "Stochastic Gradient Descent(832/999): loss=0.11694860070698444\n",
      "Stochastic Gradient Descent(833/999): loss=0.11694283708126547\n",
      "Stochastic Gradient Descent(834/999): loss=0.11693707950081297\n",
      "Stochastic Gradient Descent(835/999): loss=0.1169313279570861\n",
      "Stochastic Gradient Descent(836/999): loss=0.11692558244155814\n",
      "Stochastic Gradient Descent(837/999): loss=0.11691984294571618\n",
      "Stochastic Gradient Descent(838/999): loss=0.11691410946106127\n",
      "Stochastic Gradient Descent(839/999): loss=0.11690838197910841\n",
      "Stochastic Gradient Descent(840/999): loss=0.11690266049138637\n",
      "Stochastic Gradient Descent(841/999): loss=0.11689694498943783\n",
      "Stochastic Gradient Descent(842/999): loss=0.11689123546481923\n",
      "Stochastic Gradient Descent(843/999): loss=0.11688553190910089\n",
      "Stochastic Gradient Descent(844/999): loss=0.11687983431386678\n",
      "Stochastic Gradient Descent(845/999): loss=0.11687414267071473\n",
      "Stochastic Gradient Descent(846/999): loss=0.11686845697125622\n",
      "Stochastic Gradient Descent(847/999): loss=0.11686277720711645\n",
      "Stochastic Gradient Descent(848/999): loss=0.11685710336993428\n",
      "Stochastic Gradient Descent(849/999): loss=0.11685143545136228\n",
      "Stochastic Gradient Descent(850/999): loss=0.11684577344306654\n",
      "Stochastic Gradient Descent(851/999): loss=0.1168401173367268\n",
      "Stochastic Gradient Descent(852/999): loss=0.11683446712403643\n",
      "Stochastic Gradient Descent(853/999): loss=0.11682882279670223\n",
      "Stochastic Gradient Descent(854/999): loss=0.11682318434644468\n",
      "Stochastic Gradient Descent(855/999): loss=0.11681755176499761\n",
      "Stochastic Gradient Descent(856/999): loss=0.11681192504410848\n",
      "Stochastic Gradient Descent(857/999): loss=0.11680630417553807\n",
      "Stochastic Gradient Descent(858/999): loss=0.11680068915106072\n",
      "Stochastic Gradient Descent(859/999): loss=0.11679507996246412\n",
      "Stochastic Gradient Descent(860/999): loss=0.11678947660154929\n",
      "Stochastic Gradient Descent(861/999): loss=0.11678387906013069\n",
      "Stochastic Gradient Descent(862/999): loss=0.11677828733003612\n",
      "Stochastic Gradient Descent(863/999): loss=0.1167727014031067\n",
      "Stochastic Gradient Descent(864/999): loss=0.11676712127119673\n",
      "Stochastic Gradient Descent(865/999): loss=0.11676154692617395\n",
      "Stochastic Gradient Descent(866/999): loss=0.11675597835991927\n",
      "Stochastic Gradient Descent(867/999): loss=0.11675041556432676\n",
      "Stochastic Gradient Descent(868/999): loss=0.1167448585313038\n",
      "Stochastic Gradient Descent(869/999): loss=0.11673930725277086\n",
      "Stochastic Gradient Descent(870/999): loss=0.11673376172066165\n",
      "Stochastic Gradient Descent(871/999): loss=0.11672822192692293\n",
      "Stochastic Gradient Descent(872/999): loss=0.11672268786351457\n",
      "Stochastic Gradient Descent(873/999): loss=0.11671715952240957\n",
      "Stochastic Gradient Descent(874/999): loss=0.116711636895594\n",
      "Stochastic Gradient Descent(875/999): loss=0.11670611997506693\n",
      "Stochastic Gradient Descent(876/999): loss=0.11670060875284044\n",
      "Stochastic Gradient Descent(877/999): loss=0.11669510322093964\n",
      "Stochastic Gradient Descent(878/999): loss=0.11668960337140262\n",
      "Stochastic Gradient Descent(879/999): loss=0.1166841091962803\n",
      "Stochastic Gradient Descent(880/999): loss=0.1166786206876367\n",
      "Stochastic Gradient Descent(881/999): loss=0.11667313783754861\n",
      "Stochastic Gradient Descent(882/999): loss=0.11666766063810578\n",
      "Stochastic Gradient Descent(883/999): loss=0.11666218908141075\n",
      "Stochastic Gradient Descent(884/999): loss=0.11665672315957892\n",
      "Stochastic Gradient Descent(885/999): loss=0.11665126286473855\n",
      "Stochastic Gradient Descent(886/999): loss=0.11664580818903061\n",
      "Stochastic Gradient Descent(887/999): loss=0.11664035912460886\n",
      "Stochastic Gradient Descent(888/999): loss=0.11663491566363987\n",
      "Stochastic Gradient Descent(889/999): loss=0.11662947779830285\n",
      "Stochastic Gradient Descent(890/999): loss=0.11662404552078977\n",
      "Stochastic Gradient Descent(891/999): loss=0.11661861882330517\n",
      "Stochastic Gradient Descent(892/999): loss=0.11661319769806641\n",
      "Stochastic Gradient Descent(893/999): loss=0.11660778213730334\n",
      "Stochastic Gradient Descent(894/999): loss=0.11660237213325853\n",
      "Stochastic Gradient Descent(895/999): loss=0.11659696767818704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(896/999): loss=0.11659156876435657\n",
      "Stochastic Gradient Descent(897/999): loss=0.11658617538404732\n",
      "Stochastic Gradient Descent(898/999): loss=0.11658078752955203\n",
      "Stochastic Gradient Descent(899/999): loss=0.11657540519317598\n",
      "Stochastic Gradient Descent(900/999): loss=0.11657002836723686\n",
      "Stochastic Gradient Descent(901/999): loss=0.1165646570440648\n",
      "Stochastic Gradient Descent(902/999): loss=0.11655929121600253\n",
      "Stochastic Gradient Descent(903/999): loss=0.116553930875405\n",
      "Stochastic Gradient Descent(904/999): loss=0.11654857601463962\n",
      "Stochastic Gradient Descent(905/999): loss=0.11654322662608625\n",
      "Stochastic Gradient Descent(906/999): loss=0.11653788270213693\n",
      "Stochastic Gradient Descent(907/999): loss=0.1165325442351962\n",
      "Stochastic Gradient Descent(908/999): loss=0.11652721121768085\n",
      "Stochastic Gradient Descent(909/999): loss=0.11652188364201986\n",
      "Stochastic Gradient Descent(910/999): loss=0.11651656150065459\n",
      "Stochastic Gradient Descent(911/999): loss=0.11651124478603862\n",
      "Stochastic Gradient Descent(912/999): loss=0.11650593349063763\n",
      "Stochastic Gradient Descent(913/999): loss=0.11650062760692972\n",
      "Stochastic Gradient Descent(914/999): loss=0.116495327127405\n",
      "Stochastic Gradient Descent(915/999): loss=0.11649003204456573\n",
      "Stochastic Gradient Descent(916/999): loss=0.11648474235092639\n",
      "Stochastic Gradient Descent(917/999): loss=0.11647945803901355\n",
      "Stochastic Gradient Descent(918/999): loss=0.11647417910136583\n",
      "Stochastic Gradient Descent(919/999): loss=0.11646890553053395\n",
      "Stochastic Gradient Descent(920/999): loss=0.1164636373190807\n",
      "Stochastic Gradient Descent(921/999): loss=0.11645837445958088\n",
      "Stochastic Gradient Descent(922/999): loss=0.11645311694462124\n",
      "Stochastic Gradient Descent(923/999): loss=0.11644786476680061\n",
      "Stochastic Gradient Descent(924/999): loss=0.11644261791872977\n",
      "Stochastic Gradient Descent(925/999): loss=0.11643737639303137\n",
      "Stochastic Gradient Descent(926/999): loss=0.11643214018234004\n",
      "Stochastic Gradient Descent(927/999): loss=0.11642690927930231\n",
      "Stochastic Gradient Descent(928/999): loss=0.11642168367657661\n",
      "Stochastic Gradient Descent(929/999): loss=0.11641646336683319\n",
      "Stochastic Gradient Descent(930/999): loss=0.11641124834275418\n",
      "Stochastic Gradient Descent(931/999): loss=0.11640603859703348\n",
      "Stochastic Gradient Descent(932/999): loss=0.11640083412237684\n",
      "Stochastic Gradient Descent(933/999): loss=0.11639563491150176\n",
      "Stochastic Gradient Descent(934/999): loss=0.1163904409571375\n",
      "Stochastic Gradient Descent(935/999): loss=0.11638525225202508\n",
      "Stochastic Gradient Descent(936/999): loss=0.11638006878891723\n",
      "Stochastic Gradient Descent(937/999): loss=0.11637489056057833\n",
      "Stochastic Gradient Descent(938/999): loss=0.11636971755978445\n",
      "Stochastic Gradient Descent(939/999): loss=0.11636454977932342\n",
      "Stochastic Gradient Descent(940/999): loss=0.11635938721199456\n",
      "Stochastic Gradient Descent(941/999): loss=0.11635422985060892\n",
      "Stochastic Gradient Descent(942/999): loss=0.11634907768798904\n",
      "Stochastic Gradient Descent(943/999): loss=0.11634393071696912\n",
      "Stochastic Gradient Descent(944/999): loss=0.11633878893039486\n",
      "Stochastic Gradient Descent(945/999): loss=0.1163336523211235\n",
      "Stochastic Gradient Descent(946/999): loss=0.11632852088202388\n",
      "Stochastic Gradient Descent(947/999): loss=0.11632339460597622\n",
      "Stochastic Gradient Descent(948/999): loss=0.11631827348587223\n",
      "Stochastic Gradient Descent(949/999): loss=0.11631315751461518\n",
      "Stochastic Gradient Descent(950/999): loss=0.11630804668511958\n",
      "Stochastic Gradient Descent(951/999): loss=0.11630294099031159\n",
      "Stochastic Gradient Descent(952/999): loss=0.11629784042312857\n",
      "Stochastic Gradient Descent(953/999): loss=0.11629274497651935\n",
      "Stochastic Gradient Descent(954/999): loss=0.11628765464344405\n",
      "Stochastic Gradient Descent(955/999): loss=0.11628256941687425\n",
      "Stochastic Gradient Descent(956/999): loss=0.1162774892897927\n",
      "Stochastic Gradient Descent(957/999): loss=0.11627241425519351\n",
      "Stochastic Gradient Descent(958/999): loss=0.11626734430608207\n",
      "Stochastic Gradient Descent(959/999): loss=0.11626227943547504\n",
      "Stochastic Gradient Descent(960/999): loss=0.11625721963640022\n",
      "Stochastic Gradient Descent(961/999): loss=0.11625216490189676\n",
      "Stochastic Gradient Descent(962/999): loss=0.11624711522501495\n",
      "Stochastic Gradient Descent(963/999): loss=0.11624207059881622\n",
      "Stochastic Gradient Descent(964/999): loss=0.11623703101637317\n",
      "Stochastic Gradient Descent(965/999): loss=0.11623199647076962\n",
      "Stochastic Gradient Descent(966/999): loss=0.11622696695510043\n",
      "Stochastic Gradient Descent(967/999): loss=0.11622194246247151\n",
      "Stochastic Gradient Descent(968/999): loss=0.116216922986\n",
      "Stochastic Gradient Descent(969/999): loss=0.11621190851881397\n",
      "Stochastic Gradient Descent(970/999): loss=0.1162068990540526\n",
      "Stochastic Gradient Descent(971/999): loss=0.11620189458486609\n",
      "Stochastic Gradient Descent(972/999): loss=0.11619689510441558\n",
      "Stochastic Gradient Descent(973/999): loss=0.11619190060587326\n",
      "Stochastic Gradient Descent(974/999): loss=0.11618691108242224\n",
      "Stochastic Gradient Descent(975/999): loss=0.11618192652725663\n",
      "Stochastic Gradient Descent(976/999): loss=0.11617694693358145\n",
      "Stochastic Gradient Descent(977/999): loss=0.11617197229461255\n",
      "Stochastic Gradient Descent(978/999): loss=0.11616700260357683\n",
      "Stochastic Gradient Descent(979/999): loss=0.11616203785371187\n",
      "Stochastic Gradient Descent(980/999): loss=0.11615707803826623\n",
      "Stochastic Gradient Descent(981/999): loss=0.11615212315049925\n",
      "Stochastic Gradient Descent(982/999): loss=0.1161471731836811\n",
      "Stochastic Gradient Descent(983/999): loss=0.11614222813109275\n",
      "Stochastic Gradient Descent(984/999): loss=0.11613728798602593\n",
      "Stochastic Gradient Descent(985/999): loss=0.11613235274178314\n",
      "Stochastic Gradient Descent(986/999): loss=0.11612742239167759\n",
      "Stochastic Gradient Descent(987/999): loss=0.1161224969290333\n",
      "Stochastic Gradient Descent(988/999): loss=0.11611757634718481\n",
      "Stochastic Gradient Descent(989/999): loss=0.11611266063947753\n",
      "Stochastic Gradient Descent(990/999): loss=0.11610774979926744\n",
      "Stochastic Gradient Descent(991/999): loss=0.11610284381992117\n",
      "Stochastic Gradient Descent(992/999): loss=0.11609794269481599\n",
      "Stochastic Gradient Descent(993/999): loss=0.1160930464173398\n",
      "Stochastic Gradient Descent(994/999): loss=0.116088154980891\n",
      "Stochastic Gradient Descent(995/999): loss=0.11608326837887868\n",
      "Stochastic Gradient Descent(996/999): loss=0.11607838660472243\n",
      "Stochastic Gradient Descent(997/999): loss=0.11607350965185233\n",
      "Stochastic Gradient Descent(998/999): loss=0.11606863751370906\n",
      "Stochastic Gradient Descent(999/999): loss=0.11606377018374371\n",
      "Stochastic Gradient Descent(0/999): loss=0.23372288779265524\n",
      "Stochastic Gradient Descent(1/999): loss=0.22237253440338314\n",
      "Stochastic Gradient Descent(2/999): loss=0.2122075944543592\n",
      "Stochastic Gradient Descent(3/999): loss=0.20310406813183957\n",
      "Stochastic Gradient Descent(4/999): loss=0.19495092737822503\n",
      "Stochastic Gradient Descent(5/999): loss=0.18764875889105354\n",
      "Stochastic Gradient Descent(6/999): loss=0.18110854908076265\n",
      "Stochastic Gradient Descent(7/999): loss=0.17525059613660818\n",
      "Stochastic Gradient Descent(8/999): loss=0.17000353590367917\n",
      "Stochastic Gradient Descent(9/999): loss=0.16530346966498505\n",
      "Stochastic Gradient Descent(10/999): loss=0.16109318316810528\n",
      "Stochastic Gradient Descent(11/999): loss=0.15732144735111034\n",
      "Stochastic Gradient Descent(12/999): loss=0.15394239222101697\n",
      "Stochastic Gradient Descent(13/999): loss=0.1509149462321336\n",
      "Stochastic Gradient Descent(14/999): loss=0.14820233431221078\n",
      "Stochastic Gradient Descent(15/999): loss=0.1457716284011233\n",
      "Stochastic Gradient Descent(16/999): loss=0.14359334500863546\n",
      "Stochastic Gradient Descent(17/999): loss=0.1416410848724815\n",
      "Stochastic Gradient Descent(18/999): loss=0.1398912103125566\n",
      "Stochastic Gradient Descent(19/999): loss=0.1383225563377477\n",
      "Stochastic Gradient Descent(20/999): loss=0.13691617197446798\n",
      "Stochastic Gradient Descent(21/999): loss=0.13565508865533712\n",
      "Stochastic Gradient Descent(22/999): loss=0.13452411283718843\n",
      "Stochastic Gradient Descent(23/999): loss=0.13350964031372048\n",
      "Stochastic Gradient Descent(24/999): loss=0.13259948995327017\n",
      "Stochastic Gradient Descent(25/999): loss=0.1317827548296041\n",
      "Stochastic Gradient Descent(26/999): loss=0.1310496689262072\n",
      "Stochastic Gradient Descent(27/999): loss=0.13039148778489215\n",
      "Stochastic Gradient Descent(28/999): loss=0.1298003816399846\n",
      "Stochastic Gradient Descent(29/999): loss=0.12926933973194144\n",
      "Stochastic Gradient Descent(30/999): loss=0.12879208463089808\n",
      "Stochastic Gradient Descent(31/999): loss=0.12836299552298525\n",
      "Stochastic Gradient Descent(32/999): loss=0.12797703952180053\n",
      "Stochastic Gradient Descent(33/999): loss=0.12762971016550717\n",
      "Stochastic Gradient Descent(34/999): loss=0.1273169723478556\n",
      "Stochastic Gradient Descent(35/999): loss=0.1270352130100629\n",
      "Stochastic Gradient Descent(36/999): loss=0.12678119699089382\n",
      "Stochastic Gradient Descent(37/999): loss=0.12655202749533376\n",
      "Stochastic Gradient Descent(38/999): loss=0.1263451106986933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(39/999): loss=0.12615812405352833\n",
      "Stochastic Gradient Descent(40/999): loss=0.1259889879120161\n",
      "Stochastic Gradient Descent(41/999): loss=0.12583584011695204\n",
      "Stochastic Gradient Descent(42/999): loss=0.12569701325081234\n",
      "Stochastic Gradient Descent(43/999): loss=0.1255710142648179\n",
      "Stochastic Gradient Descent(44/999): loss=0.12545650623902266\n",
      "Stochastic Gradient Descent(45/999): loss=0.12535229205049545\n",
      "Stochastic Gradient Descent(46/999): loss=0.12525729974998634\n",
      "Stochastic Gradient Descent(47/999): loss=0.12517056946834956\n",
      "Stochastic Gradient Descent(48/999): loss=0.1250912416926921\n",
      "Stochastic Gradient Descent(49/999): loss=0.12501854676895877\n",
      "Stochastic Gradient Descent(50/999): loss=0.12495179550265376\n",
      "Stochastic Gradient Descent(51/999): loss=0.12489037074282028\n",
      "Stochastic Gradient Descent(52/999): loss=0.12483371984641894\n",
      "Stochastic Gradient Descent(53/999): loss=0.1247813479310038\n",
      "Stochastic Gradient Descent(54/999): loss=0.12473281183323132\n",
      "Stochastic Gradient Descent(55/999): loss=0.12468771469936434\n",
      "Stochastic Gradient Descent(56/999): loss=0.12464570114165603\n",
      "Stochastic Gradient Descent(57/999): loss=0.12460645290141749\n",
      "Stochastic Gradient Descent(58/999): loss=0.12456968496576311\n",
      "Stochastic Gradient Descent(59/999): loss=0.12453514209057409\n",
      "Stochastic Gradient Descent(60/999): loss=0.12450259568718558\n",
      "Stochastic Gradient Descent(61/999): loss=0.12447184103474711\n",
      "Stochastic Gradient Descent(62/999): loss=0.12444269478418767\n",
      "Stochastic Gradient Descent(63/999): loss=0.12441499272328048\n",
      "Stochastic Gradient Descent(64/999): loss=0.12438858777549305\n",
      "Stochastic Gradient Descent(65/999): loss=0.12436334820816677\n",
      "Stochastic Gradient Descent(66/999): loss=0.12433915602812719\n",
      "Stochastic Gradient Descent(67/999): loss=0.12431590554511818\n",
      "Stochastic Gradient Descent(68/999): loss=0.12429350208550394\n",
      "Stochastic Gradient Descent(69/999): loss=0.1242718608405189\n",
      "Stochastic Gradient Descent(70/999): loss=0.12425090583499068\n",
      "Stochastic Gradient Descent(71/999): loss=0.12423056900393356\n",
      "Stochastic Gradient Descent(72/999): loss=0.12421078936572787\n",
      "Stochastic Gradient Descent(73/999): loss=0.12419151228178156\n",
      "Stochastic Gradient Descent(74/999): loss=0.12417268879362757\n",
      "Stochastic Gradient Descent(75/999): loss=0.12415427502935578\n",
      "Stochastic Gradient Descent(76/999): loss=0.12413623167212706\n",
      "Stochastic Gradient Descent(77/999): loss=0.12411852348427503\n",
      "Stochastic Gradient Descent(78/999): loss=0.12410111888118072\n",
      "Stochastic Gradient Descent(79/999): loss=0.12408398954971292\n",
      "Stochastic Gradient Descent(80/999): loss=0.12406711010657323\n",
      "Stochastic Gradient Descent(81/999): loss=0.12405045779237098\n",
      "Stochastic Gradient Descent(82/999): loss=0.12403401219769042\n",
      "Stochastic Gradient Descent(83/999): loss=0.12401775501780386\n",
      "Stochastic Gradient Descent(84/999): loss=0.12400166983303411\n",
      "Stochastic Gradient Descent(85/999): loss=0.12398574191208328\n",
      "Stochastic Gradient Descent(86/999): loss=0.1239699580359255\n",
      "Stochastic Gradient Descent(87/999): loss=0.12395430634011277\n",
      "Stochastic Gradient Descent(88/999): loss=0.12393877617356709\n",
      "Stochastic Gradient Descent(89/999): loss=0.12392335797213602\n",
      "Stochastic Gradient Descent(90/999): loss=0.12390804314536531\n",
      "Stochastic Gradient Descent(91/999): loss=0.12389282397510797\n",
      "Stochastic Gradient Descent(92/999): loss=0.12387769352473084\n",
      "Stochastic Gradient Descent(93/999): loss=0.12386264555780994\n",
      "Stochastic Gradient Descent(94/999): loss=0.12384767446532309\n",
      "Stochastic Gradient Descent(95/999): loss=0.12383277520045008\n",
      "Stochastic Gradient Descent(96/999): loss=0.12381794322018538\n",
      "Stochastic Gradient Descent(97/999): loss=0.1238031744330506\n",
      "Stochastic Gradient Descent(98/999): loss=0.1237884651522685\n",
      "Stochastic Gradient Descent(99/999): loss=0.1237738120538282\n",
      "Stochastic Gradient Descent(100/999): loss=0.12375921213892892\n",
      "Stochastic Gradient Descent(101/999): loss=0.1237446627003454\n",
      "Stochastic Gradient Descent(102/999): loss=0.12373016129230423\n",
      "Stochastic Gradient Descent(103/999): loss=0.12371570570350414\n",
      "Stochastic Gradient Descent(104/999): loss=0.12370129393295169\n",
      "Stochastic Gradient Descent(105/999): loss=0.12368692416831763\n",
      "Stochastic Gradient Descent(106/999): loss=0.12367259476655067\n",
      "Stochastic Gradient Descent(107/999): loss=0.12365830423651239\n",
      "Stochastic Gradient Descent(108/999): loss=0.12364405122342234\n",
      "Stochastic Gradient Descent(109/999): loss=0.12362983449492365\n",
      "Stochastic Gradient Descent(110/999): loss=0.12361565292860013\n",
      "Stochastic Gradient Descent(111/999): loss=0.12360150550079281\n",
      "Stochastic Gradient Descent(112/999): loss=0.12358739127658047\n",
      "Stochastic Gradient Descent(113/999): loss=0.1235733094008023\n",
      "Stochastic Gradient Descent(114/999): loss=0.12355925909001354\n",
      "Stochastic Gradient Descent(115/999): loss=0.12354523962527737\n",
      "Stochastic Gradient Descent(116/999): loss=0.12353125034570468\n",
      "Stochastic Gradient Descent(117/999): loss=0.12351729064266445\n",
      "Stochastic Gradient Descent(118/999): loss=0.1235033599545943\n",
      "Stochastic Gradient Descent(119/999): loss=0.12348945776234839\n",
      "Stochastic Gradient Descent(120/999): loss=0.12347558358502699\n",
      "Stochastic Gradient Descent(121/999): loss=0.12346173697623716\n",
      "Stochastic Gradient Descent(122/999): loss=0.12344791752073939\n",
      "Stochastic Gradient Descent(123/999): loss=0.12343412483144028\n",
      "Stochastic Gradient Descent(124/999): loss=0.1234203585466951\n",
      "Stochastic Gradient Descent(125/999): loss=0.12340661832788737\n",
      "Stochastic Gradient Descent(126/999): loss=0.1233929038572576\n",
      "Stochastic Gradient Descent(127/999): loss=0.12337921483595396\n",
      "Stochastic Gradient Descent(128/999): loss=0.12336555098228297\n",
      "Stochastic Gradient Descent(129/999): loss=0.12335191203013854\n",
      "Stochastic Gradient Descent(130/999): loss=0.12333829772759097\n",
      "Stochastic Gradient Descent(131/999): loss=0.12332470783561958\n",
      "Stochastic Gradient Descent(132/999): loss=0.12331114212697367\n",
      "Stochastic Gradient Descent(133/999): loss=0.12329760038514848\n",
      "Stochastic Gradient Descent(134/999): loss=0.12328408240346464\n",
      "Stochastic Gradient Descent(135/999): loss=0.12327058798423962\n",
      "Stochastic Gradient Descent(136/999): loss=0.12325711693804228\n",
      "Stochastic Gradient Descent(137/999): loss=0.1232436690830218\n",
      "Stochastic Gradient Descent(138/999): loss=0.12323024424430261\n",
      "Stochastic Gradient Descent(139/999): loss=0.12321684225343973\n",
      "Stochastic Gradient Descent(140/999): loss=0.12320346294792701\n",
      "Stochastic Gradient Descent(141/999): loss=0.12319010617075357\n",
      "Stochastic Gradient Descent(142/999): loss=0.12317677177000333\n",
      "Stochastic Gradient Descent(143/999): loss=0.12316345959849288\n",
      "Stochastic Gradient Descent(144/999): loss=0.1231501695134442\n",
      "Stochastic Gradient Descent(145/999): loss=0.12313690137618832\n",
      "Stochastic Gradient Descent(146/999): loss=0.12312365505189689\n",
      "Stochastic Gradient Descent(147/999): loss=0.12311043040933864\n",
      "Stochastic Gradient Descent(148/999): loss=0.12309722732065843\n",
      "Stochastic Gradient Descent(149/999): loss=0.12308404566117649\n",
      "Stochastic Gradient Descent(150/999): loss=0.12307088530920565\n",
      "Stochastic Gradient Descent(151/999): loss=0.12305774614588495\n",
      "Stochastic Gradient Descent(152/999): loss=0.1230446280550277\n",
      "Stochastic Gradient Descent(153/999): loss=0.12303153092298298\n",
      "Stochastic Gradient Descent(154/999): loss=0.12301845463850873\n",
      "Stochastic Gradient Descent(155/999): loss=0.1230053990926557\n",
      "Stochastic Gradient Descent(156/999): loss=0.1229923641786606\n",
      "Stochastic Gradient Descent(157/999): loss=0.12297934979184828\n",
      "Stochastic Gradient Descent(158/999): loss=0.1229663558295416\n",
      "Stochastic Gradient Descent(159/999): loss=0.12295338219097812\n",
      "Stochastic Gradient Descent(160/999): loss=0.12294042877723328\n",
      "Stochastic Gradient Descent(161/999): loss=0.12292749549114934\n",
      "Stochastic Gradient Descent(162/999): loss=0.12291458223726917\n",
      "Stochastic Gradient Descent(163/999): loss=0.12290168892177515\n",
      "Stochastic Gradient Descent(164/999): loss=0.122888815452432\n",
      "Stochastic Gradient Descent(165/999): loss=0.12287596173853334\n",
      "Stochastic Gradient Descent(166/999): loss=0.12286312769085209\n",
      "Stochastic Gradient Descent(167/999): loss=0.12285031322159358\n",
      "Stochastic Gradient Descent(168/999): loss=0.12283751824435185\n",
      "Stochastic Gradient Descent(169/999): loss=0.1228247426740684\n",
      "Stochastic Gradient Descent(170/999): loss=0.12281198642699317\n",
      "Stochastic Gradient Descent(171/999): loss=0.12279924942064803\n",
      "Stochastic Gradient Descent(172/999): loss=0.12278653157379191\n",
      "Stochastic Gradient Descent(173/999): loss=0.1227738328063877\n",
      "Stochastic Gradient Descent(174/999): loss=0.12276115303957127\n",
      "Stochastic Gradient Descent(175/999): loss=0.1227484921956213\n",
      "Stochastic Gradient Descent(176/999): loss=0.12273585019793107\n",
      "Stochastic Gradient Descent(177/999): loss=0.12272322697098124\n",
      "Stochastic Gradient Descent(178/999): loss=0.12271062244031385\n",
      "Stochastic Gradient Descent(179/999): loss=0.12269803653250745\n",
      "Stochastic Gradient Descent(180/999): loss=0.12268546917515316\n",
      "Stochastic Gradient Descent(181/999): loss=0.12267292029683179\n",
      "Stochastic Gradient Descent(182/999): loss=0.12266038982709153\n",
      "Stochastic Gradient Descent(183/999): loss=0.1226478776964269\n",
      "Stochastic Gradient Descent(184/999): loss=0.12263538383625806\n",
      "Stochastic Gradient Descent(185/999): loss=0.12262290817891087\n",
      "Stochastic Gradient Descent(186/999): loss=0.1226104506575978\n",
      "Stochastic Gradient Descent(187/999): loss=0.1225980112063993\n",
      "Stochastic Gradient Descent(188/999): loss=0.12258558976024574\n",
      "Stochastic Gradient Descent(189/999): loss=0.12257318625489996\n",
      "Stochastic Gradient Descent(190/999): loss=0.12256080062694027\n",
      "Stochastic Gradient Descent(191/999): loss=0.12254843281374408\n",
      "Stochastic Gradient Descent(192/999): loss=0.12253608275347164\n",
      "Stochastic Gradient Descent(193/999): loss=0.12252375038505069\n",
      "Stochastic Gradient Descent(194/999): loss=0.12251143564816112\n",
      "Stochastic Gradient Descent(195/999): loss=0.12249913848322025\n",
      "Stochastic Gradient Descent(196/999): loss=0.12248685883136841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(197/999): loss=0.1224745966344549\n",
      "Stochastic Gradient Descent(198/999): loss=0.12246235183502424\n",
      "Stochastic Gradient Descent(199/999): loss=0.12245012437630284\n",
      "Stochastic Gradient Descent(200/999): loss=0.12243791420218594\n",
      "Stochastic Gradient Descent(201/999): loss=0.12242572125722483\n",
      "Stochastic Gradient Descent(202/999): loss=0.12241354548661436\n",
      "Stochastic Gradient Descent(203/999): loss=0.12240138683618082\n",
      "Stochastic Gradient Descent(204/999): loss=0.12238924525237005\n",
      "Stochastic Gradient Descent(205/999): loss=0.12237712068223573\n",
      "Stochastic Gradient Descent(206/999): loss=0.12236501307342802\n",
      "Stochastic Gradient Descent(207/999): loss=0.12235292237418247\n",
      "Stochastic Gradient Descent(208/999): loss=0.12234084853330907\n",
      "Stochastic Gradient Descent(209/999): loss=0.12232879150018151\n",
      "Stochastic Gradient Descent(210/999): loss=0.122316751224727\n",
      "Stochastic Gradient Descent(211/999): loss=0.12230472765741573\n",
      "Stochastic Gradient Descent(212/999): loss=0.12229272074925111\n",
      "Stochastic Gradient Descent(213/999): loss=0.12228073045175984\n",
      "Stochastic Gradient Descent(214/999): loss=0.12226875671698233\n",
      "Stochastic Gradient Descent(215/999): loss=0.12225679949746336\n",
      "Stochastic Gradient Descent(216/999): loss=0.12224485874624284\n",
      "Stochastic Gradient Descent(217/999): loss=0.12223293441684677\n",
      "Stochastic Gradient Descent(218/999): loss=0.12222102646327845\n",
      "Stochastic Gradient Descent(219/999): loss=0.12220913484000981\n",
      "Stochastic Gradient Descent(220/999): loss=0.1221972595019729\n",
      "Stochastic Gradient Descent(221/999): loss=0.12218540040455153\n",
      "Stochastic Gradient Descent(222/999): loss=0.12217355750357335\n",
      "Stochastic Gradient Descent(223/999): loss=0.12216173075530155\n",
      "Stochastic Gradient Descent(224/999): loss=0.1221499201164273\n",
      "Stochastic Gradient Descent(225/999): loss=0.12213812554406187\n",
      "Stochastic Gradient Descent(226/999): loss=0.1221263469957293\n",
      "Stochastic Gradient Descent(227/999): loss=0.12211458442935895\n",
      "Stochastic Gradient Descent(228/999): loss=0.12210283780327821\n",
      "Stochastic Gradient Descent(229/999): loss=0.12209110707620557\n",
      "Stochastic Gradient Descent(230/999): loss=0.12207939220724351\n",
      "Stochastic Gradient Descent(231/999): loss=0.12206769315587185\n",
      "Stochastic Gradient Descent(232/999): loss=0.12205600988194096\n",
      "Stochastic Gradient Descent(233/999): loss=0.12204434234566523\n",
      "Stochastic Gradient Descent(234/999): loss=0.12203269050761685\n",
      "Stochastic Gradient Descent(235/999): loss=0.1220210543287192\n",
      "Stochastic Gradient Descent(236/999): loss=0.12200943377024093\n",
      "Stochastic Gradient Descent(237/999): loss=0.12199782879378983\n",
      "Stochastic Gradient Descent(238/999): loss=0.1219862393613069\n",
      "Stochastic Gradient Descent(239/999): loss=0.1219746654350606\n",
      "Stochastic Gradient Descent(240/999): loss=0.121963106977641\n",
      "Stochastic Gradient Descent(241/999): loss=0.12195156395195443\n",
      "Stochastic Gradient Descent(242/999): loss=0.12194003632121776\n",
      "Stochastic Gradient Descent(243/999): loss=0.1219285240489532\n",
      "Stochastic Gradient Descent(244/999): loss=0.12191702709898301\n",
      "Stochastic Gradient Descent(245/999): loss=0.12190554543542417\n",
      "Stochastic Gradient Descent(246/999): loss=0.12189407902268352\n",
      "Stochastic Gradient Descent(247/999): loss=0.12188262782545274\n",
      "Stochastic Gradient Descent(248/999): loss=0.12187119180870337\n",
      "Stochastic Gradient Descent(249/999): loss=0.12185977093768219\n",
      "Stochastic Gradient Descent(250/999): loss=0.12184836517790645\n",
      "Stochastic Gradient Descent(251/999): loss=0.12183697449515926\n",
      "Stochastic Gradient Descent(252/999): loss=0.12182559885548512\n",
      "Stochastic Gradient Descent(253/999): loss=0.12181423822518553\n",
      "Stochastic Gradient Descent(254/999): loss=0.12180289257081464\n",
      "Stochastic Gradient Descent(255/999): loss=0.12179156185917486\n",
      "Stochastic Gradient Descent(256/999): loss=0.12178024605731298\n",
      "Stochastic Gradient Descent(257/999): loss=0.12176894513251585\n",
      "Stochastic Gradient Descent(258/999): loss=0.12175765905230641\n",
      "Stochastic Gradient Descent(259/999): loss=0.12174638778443994\n",
      "Stochastic Gradient Descent(260/999): loss=0.12173513129689988\n",
      "Stochastic Gradient Descent(261/999): loss=0.12172388955789434\n",
      "Stochastic Gradient Descent(262/999): loss=0.12171266253585229\n",
      "Stochastic Gradient Descent(263/999): loss=0.1217014501994199\n",
      "Stochastic Gradient Descent(264/999): loss=0.12169025251745699\n",
      "Stochastic Gradient Descent(265/999): loss=0.12167906945903352\n",
      "Stochastic Gradient Descent(266/999): loss=0.12166790099342624\n",
      "Stochastic Gradient Descent(267/999): loss=0.12165674709011522\n",
      "Stochastic Gradient Descent(268/999): loss=0.12164560771878054\n",
      "Stochastic Gradient Descent(269/999): loss=0.12163448284929927\n",
      "Stochastic Gradient Descent(270/999): loss=0.12162337245174194\n",
      "Stochastic Gradient Descent(271/999): loss=0.12161227649636978\n",
      "Stochastic Gradient Descent(272/999): loss=0.12160119495363149\n",
      "Stochastic Gradient Descent(273/999): loss=0.12159012779416013\n",
      "Stochastic Gradient Descent(274/999): loss=0.12157907498877049\n",
      "Stochastic Gradient Descent(275/999): loss=0.1215680365084559\n",
      "Stochastic Gradient Descent(276/999): loss=0.12155701232438565\n",
      "Stochastic Gradient Descent(277/999): loss=0.12154600240790207\n",
      "Stochastic Gradient Descent(278/999): loss=0.1215350067305178\n",
      "Stochastic Gradient Descent(279/999): loss=0.12152402526391323\n",
      "Stochastic Gradient Descent(280/999): loss=0.12151305797993386\n",
      "Stochastic Gradient Descent(281/999): loss=0.12150210485058766\n",
      "Stochastic Gradient Descent(282/999): loss=0.12149116584804255\n",
      "Stochastic Gradient Descent(283/999): loss=0.12148024094462406\n",
      "Stochastic Gradient Descent(284/999): loss=0.12146933011281283\n",
      "Stochastic Gradient Descent(285/999): loss=0.12145843332524218\n",
      "Stochastic Gradient Descent(286/999): loss=0.1214475505546959\n",
      "Stochastic Gradient Descent(287/999): loss=0.12143668177410578\n",
      "Stochastic Gradient Descent(288/999): loss=0.12142582695654966\n",
      "Stochastic Gradient Descent(289/999): loss=0.12141498607524889\n",
      "Stochastic Gradient Descent(290/999): loss=0.12140415910356642\n",
      "Stochastic Gradient Descent(291/999): loss=0.12139334601500465\n",
      "Stochastic Gradient Descent(292/999): loss=0.1213825467832032\n",
      "Stochastic Gradient Descent(293/999): loss=0.12137176138193709\n",
      "Stochastic Gradient Descent(294/999): loss=0.12136098978511453\n",
      "Stochastic Gradient Descent(295/999): loss=0.12135023196677519\n",
      "Stochastic Gradient Descent(296/999): loss=0.12133948790108802\n",
      "Stochastic Gradient Descent(297/999): loss=0.12132875756234962\n",
      "Stochastic Gradient Descent(298/999): loss=0.12131804092498213\n",
      "Stochastic Gradient Descent(299/999): loss=0.12130733796353171\n",
      "Stochastic Gradient Descent(300/999): loss=0.12129664865266646\n",
      "Stochastic Gradient Descent(301/999): loss=0.1212859729671749\n",
      "Stochastic Gradient Descent(302/999): loss=0.12127531088196411\n",
      "Stochastic Gradient Descent(303/999): loss=0.12126466237205819\n",
      "Stochastic Gradient Descent(304/999): loss=0.12125402741259637\n",
      "Stochastic Gradient Descent(305/999): loss=0.12124340597883175\n",
      "Stochastic Gradient Descent(306/999): loss=0.12123279804612941\n",
      "Stochastic Gradient Descent(307/999): loss=0.12122220358996491\n",
      "Stochastic Gradient Descent(308/999): loss=0.12121162258592282\n",
      "Stochastic Gradient Descent(309/999): loss=0.12120105500969529\n",
      "Stochastic Gradient Descent(310/999): loss=0.12119050083708043\n",
      "Stochastic Gradient Descent(311/999): loss=0.12117996004398088\n",
      "Stochastic Gradient Descent(312/999): loss=0.12116943260640252\n",
      "Stochastic Gradient Descent(313/999): loss=0.12115891850045295\n",
      "Stochastic Gradient Descent(314/999): loss=0.12114841770234021\n",
      "Stochastic Gradient Descent(315/999): loss=0.12113793018837136\n",
      "Stochastic Gradient Descent(316/999): loss=0.12112745593495125\n",
      "Stochastic Gradient Descent(317/999): loss=0.12111699491858122\n",
      "Stochastic Gradient Descent(318/999): loss=0.1211065471158577\n",
      "Stochastic Gradient Descent(319/999): loss=0.1210961125034712\n",
      "Stochastic Gradient Descent(320/999): loss=0.12108569105820488\n",
      "Stochastic Gradient Descent(321/999): loss=0.12107528275693352\n",
      "Stochastic Gradient Descent(322/999): loss=0.1210648875766222\n",
      "Stochastic Gradient Descent(323/999): loss=0.12105450549432523\n",
      "Stochastic Gradient Descent(324/999): loss=0.12104413648718505\n",
      "Stochastic Gradient Descent(325/999): loss=0.12103378053243108\n",
      "Stochastic Gradient Descent(326/999): loss=0.12102343760737856\n",
      "Stochastic Gradient Descent(327/999): loss=0.12101310768942762\n",
      "Stochastic Gradient Descent(328/999): loss=0.12100279075606218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(329/999): loss=0.12099248678484884\n",
      "Stochastic Gradient Descent(330/999): loss=0.12098219575343595\n",
      "Stochastic Gradient Descent(331/999): loss=0.1209719176395527\n",
      "Stochastic Gradient Descent(332/999): loss=0.12096165242100793\n",
      "Stochastic Gradient Descent(333/999): loss=0.12095140007568934\n",
      "Stochastic Gradient Descent(334/999): loss=0.12094116058156255\n",
      "Stochastic Gradient Descent(335/999): loss=0.12093093391667012\n",
      "Stochastic Gradient Descent(336/999): loss=0.12092072005913063\n",
      "Stochastic Gradient Descent(337/999): loss=0.1209105189871379\n",
      "Stochastic Gradient Descent(338/999): loss=0.12090033067895996\n",
      "Stochastic Gradient Descent(339/999): loss=0.12089015511293841\n",
      "Stochastic Gradient Descent(340/999): loss=0.12087999226748734\n",
      "Stochastic Gradient Descent(341/999): loss=0.1208698421210927\n",
      "Stochastic Gradient Descent(342/999): loss=0.1208597046523114\n",
      "Stochastic Gradient Descent(343/999): loss=0.12084957983977054\n",
      "Stochastic Gradient Descent(344/999): loss=0.12083946766216658\n",
      "Stochastic Gradient Descent(345/999): loss=0.12082936809826468\n",
      "Stochastic Gradient Descent(346/999): loss=0.1208192811268979\n",
      "Stochastic Gradient Descent(347/999): loss=0.12080920672696634\n",
      "Stochastic Gradient Descent(348/999): loss=0.12079914487743666\n",
      "Stochastic Gradient Descent(349/999): loss=0.12078909555734123\n",
      "Stochastic Gradient Descent(350/999): loss=0.12077905874577734\n",
      "Stochastic Gradient Descent(351/999): loss=0.12076903442190665\n",
      "Stochastic Gradient Descent(352/999): loss=0.12075902256495455\n",
      "Stochastic Gradient Descent(353/999): loss=0.1207490231542094\n",
      "Stochastic Gradient Descent(354/999): loss=0.12073903616902189\n",
      "Stochastic Gradient Descent(355/999): loss=0.12072906158880442\n",
      "Stochastic Gradient Descent(356/999): loss=0.1207190993930305\n",
      "Stochastic Gradient Descent(357/999): loss=0.12070914956123414\n",
      "Stochastic Gradient Descent(358/999): loss=0.12069921207300914\n",
      "Stochastic Gradient Descent(359/999): loss=0.12068928690800872\n",
      "Stochastic Gradient Descent(360/999): loss=0.12067937404594463\n",
      "Stochastic Gradient Descent(361/999): loss=0.12066947346658685\n",
      "Stochastic Gradient Descent(362/999): loss=0.12065958514976292\n",
      "Stochastic Gradient Descent(363/999): loss=0.12064970907535733\n",
      "Stochastic Gradient Descent(364/999): loss=0.12063984522331113\n",
      "Stochastic Gradient Descent(365/999): loss=0.12062999357362124\n",
      "Stochastic Gradient Descent(366/999): loss=0.12062015410633999\n",
      "Stochastic Gradient Descent(367/999): loss=0.12061032680157467\n",
      "Stochastic Gradient Descent(368/999): loss=0.12060051163948694\n",
      "Stochastic Gradient Descent(369/999): loss=0.12059070860029229\n",
      "Stochastic Gradient Descent(370/999): loss=0.12058091766425974\n",
      "Stochastic Gradient Descent(371/999): loss=0.12057113881171119\n",
      "Stochastic Gradient Descent(372/999): loss=0.12056137202302095\n",
      "Stochastic Gradient Descent(373/999): loss=0.12055161727861546\n",
      "Stochastic Gradient Descent(374/999): loss=0.12054187455897258\n",
      "Stochastic Gradient Descent(375/999): loss=0.12053214384462134\n",
      "Stochastic Gradient Descent(376/999): loss=0.12052242511614145\n",
      "Stochastic Gradient Descent(377/999): loss=0.12051271835416279\n",
      "Stochastic Gradient Descent(378/999): loss=0.12050302353936508\n",
      "Stochastic Gradient Descent(379/999): loss=0.12049334065247748\n",
      "Stochastic Gradient Descent(380/999): loss=0.12048366967427804\n",
      "Stochastic Gradient Descent(381/999): loss=0.12047401058559347\n",
      "Stochastic Gradient Descent(382/999): loss=0.12046436336729865\n",
      "Stochastic Gradient Descent(383/999): loss=0.12045472800031627\n",
      "Stochastic Gradient Descent(384/999): loss=0.12044510446561636\n",
      "Stochastic Gradient Descent(385/999): loss=0.1204354927442161\n",
      "Stochastic Gradient Descent(386/999): loss=0.12042589281717929\n",
      "Stochastic Gradient Descent(387/999): loss=0.12041630466561606\n",
      "Stochastic Gradient Descent(388/999): loss=0.12040672827068244\n",
      "Stochastic Gradient Descent(389/999): loss=0.12039716361358013\n",
      "Stochastic Gradient Descent(390/999): loss=0.12038761067555608\n",
      "Stochastic Gradient Descent(391/999): loss=0.12037806943790211\n",
      "Stochastic Gradient Descent(392/999): loss=0.12036853988195467\n",
      "Stochastic Gradient Descent(393/999): loss=0.12035902198909453\n",
      "Stochastic Gradient Descent(394/999): loss=0.12034951574074625\n",
      "Stochastic Gradient Descent(395/999): loss=0.12034002111837815\n",
      "Stochastic Gradient Descent(396/999): loss=0.12033053810350171\n",
      "Stochastic Gradient Descent(397/999): loss=0.12032106667767162\n",
      "Stochastic Gradient Descent(398/999): loss=0.12031160682248507\n",
      "Stochastic Gradient Descent(399/999): loss=0.1203021585195818\n",
      "Stochastic Gradient Descent(400/999): loss=0.12029272175064357\n",
      "Stochastic Gradient Descent(401/999): loss=0.12028329649739401\n",
      "Stochastic Gradient Descent(402/999): loss=0.12027388274159825\n",
      "Stochastic Gradient Descent(403/999): loss=0.12026448046506276\n",
      "Stochastic Gradient Descent(404/999): loss=0.12025508964963497\n",
      "Stochastic Gradient Descent(405/999): loss=0.12024571027720297\n",
      "Stochastic Gradient Descent(406/999): loss=0.12023634232969545\n",
      "Stochastic Gradient Descent(407/999): loss=0.12022698578908113\n",
      "Stochastic Gradient Descent(408/999): loss=0.12021764063736882\n",
      "Stochastic Gradient Descent(409/999): loss=0.12020830685660697\n",
      "Stochastic Gradient Descent(410/999): loss=0.12019898442888342\n",
      "Stochastic Gradient Descent(411/999): loss=0.12018967333632524\n",
      "Stochastic Gradient Descent(412/999): loss=0.12018037356109855\n",
      "Stochastic Gradient Descent(413/999): loss=0.12017108508540805\n",
      "Stochastic Gradient Descent(414/999): loss=0.12016180789149707\n",
      "Stochastic Gradient Descent(415/999): loss=0.12015254196164701\n",
      "Stochastic Gradient Descent(416/999): loss=0.12014328727817752\n",
      "Stochastic Gradient Descent(417/999): loss=0.12013404382344593\n",
      "Stochastic Gradient Descent(418/999): loss=0.12012481157984718\n",
      "Stochastic Gradient Descent(419/999): loss=0.12011559052981366\n",
      "Stochastic Gradient Descent(420/999): loss=0.1201063806558148\n",
      "Stochastic Gradient Descent(421/999): loss=0.12009718194035714\n",
      "Stochastic Gradient Descent(422/999): loss=0.12008799436598384\n",
      "Stochastic Gradient Descent(423/999): loss=0.12007881791527465\n",
      "Stochastic Gradient Descent(424/999): loss=0.12006965257084573\n",
      "Stochastic Gradient Descent(425/999): loss=0.12006049831534933\n",
      "Stochastic Gradient Descent(426/999): loss=0.12005135513147368\n",
      "Stochastic Gradient Descent(427/999): loss=0.12004222300194275\n",
      "Stochastic Gradient Descent(428/999): loss=0.12003310190951613\n",
      "Stochastic Gradient Descent(429/999): loss=0.12002399183698881\n",
      "Stochastic Gradient Descent(430/999): loss=0.12001489276719093\n",
      "Stochastic Gradient Descent(431/999): loss=0.12000580468298773\n",
      "Stochastic Gradient Descent(432/999): loss=0.11999672756727932\n",
      "Stochastic Gradient Descent(433/999): loss=0.1199876614030004\n",
      "Stochastic Gradient Descent(434/999): loss=0.1199786061731203\n",
      "Stochastic Gradient Descent(435/999): loss=0.11996956186064256\n",
      "Stochastic Gradient Descent(436/999): loss=0.119960528448605\n",
      "Stochastic Gradient Descent(437/999): loss=0.11995150592007943\n",
      "Stochastic Gradient Descent(438/999): loss=0.11994249425817141\n",
      "Stochastic Gradient Descent(439/999): loss=0.11993349344602033\n",
      "Stochastic Gradient Descent(440/999): loss=0.119924503466799\n",
      "Stochastic Gradient Descent(441/999): loss=0.1199155243037137\n",
      "Stochastic Gradient Descent(442/999): loss=0.11990655594000382\n",
      "Stochastic Gradient Descent(443/999): loss=0.11989759835894184\n",
      "Stochastic Gradient Descent(444/999): loss=0.11988865154383326\n",
      "Stochastic Gradient Descent(445/999): loss=0.11987971547801621\n",
      "Stochastic Gradient Descent(446/999): loss=0.11987079014486152\n",
      "Stochastic Gradient Descent(447/999): loss=0.1198618755277725\n",
      "Stochastic Gradient Descent(448/999): loss=0.11985297161018475\n",
      "Stochastic Gradient Descent(449/999): loss=0.11984407837556624\n",
      "Stochastic Gradient Descent(450/999): loss=0.11983519580741672\n",
      "Stochastic Gradient Descent(451/999): loss=0.11982632388926809\n",
      "Stochastic Gradient Descent(452/999): loss=0.119817462604684\n",
      "Stochastic Gradient Descent(453/999): loss=0.11980861193725967\n",
      "Stochastic Gradient Descent(454/999): loss=0.11979977187062198\n",
      "Stochastic Gradient Descent(455/999): loss=0.1197909423884291\n",
      "Stochastic Gradient Descent(456/999): loss=0.11978212347437053\n",
      "Stochastic Gradient Descent(457/999): loss=0.11977331511216692\n",
      "Stochastic Gradient Descent(458/999): loss=0.11976451728556989\n",
      "Stochastic Gradient Descent(459/999): loss=0.11975572997836209\n",
      "Stochastic Gradient Descent(460/999): loss=0.11974695317435678\n",
      "Stochastic Gradient Descent(461/999): loss=0.11973818685739798\n",
      "Stochastic Gradient Descent(462/999): loss=0.11972943101136028\n",
      "Stochastic Gradient Descent(463/999): loss=0.11972068562014862\n",
      "Stochastic Gradient Descent(464/999): loss=0.11971195066769831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(465/999): loss=0.11970322613797482\n",
      "Stochastic Gradient Descent(466/999): loss=0.11969451201497372\n",
      "Stochastic Gradient Descent(467/999): loss=0.11968580828272055\n",
      "Stochastic Gradient Descent(468/999): loss=0.11967711492527072\n",
      "Stochastic Gradient Descent(469/999): loss=0.11966843192670941\n",
      "Stochastic Gradient Descent(470/999): loss=0.11965975927115145\n",
      "Stochastic Gradient Descent(471/999): loss=0.11965109694274123\n",
      "Stochastic Gradient Descent(472/999): loss=0.11964244492565244\n",
      "Stochastic Gradient Descent(473/999): loss=0.11963380320408831\n",
      "Stochastic Gradient Descent(474/999): loss=0.11962517176228123\n",
      "Stochastic Gradient Descent(475/999): loss=0.11961655058449262\n",
      "Stochastic Gradient Descent(476/999): loss=0.1196079396550131\n",
      "Stochastic Gradient Descent(477/999): loss=0.11959933895816209\n",
      "Stochastic Gradient Descent(478/999): loss=0.11959074847828793\n",
      "Stochastic Gradient Descent(479/999): loss=0.11958216819976766\n",
      "Stochastic Gradient Descent(480/999): loss=0.11957359810700693\n",
      "Stochastic Gradient Descent(481/999): loss=0.11956503818444009\n",
      "Stochastic Gradient Descent(482/999): loss=0.11955648841652976\n",
      "Stochastic Gradient Descent(483/999): loss=0.11954794878776702\n",
      "Stochastic Gradient Descent(484/999): loss=0.11953941928267124\n",
      "Stochastic Gradient Descent(485/999): loss=0.1195308998857899\n",
      "Stochastic Gradient Descent(486/999): loss=0.11952239058169864\n",
      "Stochastic Gradient Descent(487/999): loss=0.11951389135500108\n",
      "Stochastic Gradient Descent(488/999): loss=0.11950540219032875\n",
      "Stochastic Gradient Descent(489/999): loss=0.11949692307234103\n",
      "Stochastic Gradient Descent(490/999): loss=0.119488453985725\n",
      "Stochastic Gradient Descent(491/999): loss=0.11947999491519544\n",
      "Stochastic Gradient Descent(492/999): loss=0.1194715458454947\n",
      "Stochastic Gradient Descent(493/999): loss=0.11946310676139266\n",
      "Stochastic Gradient Descent(494/999): loss=0.1194546776476865\n",
      "Stochastic Gradient Descent(495/999): loss=0.11944625848920083\n",
      "Stochastic Gradient Descent(496/999): loss=0.11943784927078745\n",
      "Stochastic Gradient Descent(497/999): loss=0.11942944997732535\n",
      "Stochastic Gradient Descent(498/999): loss=0.1194210605937206\n",
      "Stochastic Gradient Descent(499/999): loss=0.11941268110490631\n",
      "Stochastic Gradient Descent(500/999): loss=0.11940431149584249\n",
      "Stochastic Gradient Descent(501/999): loss=0.11939595175151596\n",
      "Stochastic Gradient Descent(502/999): loss=0.11938760185694039\n",
      "Stochastic Gradient Descent(503/999): loss=0.1193792617971561\n",
      "Stochastic Gradient Descent(504/999): loss=0.11937093155723014\n",
      "Stochastic Gradient Descent(505/999): loss=0.11936261112225595\n",
      "Stochastic Gradient Descent(506/999): loss=0.11935430047735353\n",
      "Stochastic Gradient Descent(507/999): loss=0.11934599960766937\n",
      "Stochastic Gradient Descent(508/999): loss=0.11933770849837612\n",
      "Stochastic Gradient Descent(509/999): loss=0.1193294271346728\n",
      "Stochastic Gradient Descent(510/999): loss=0.11932115550178463\n",
      "Stochastic Gradient Descent(511/999): loss=0.11931289358496291\n",
      "Stochastic Gradient Descent(512/999): loss=0.11930464136948497\n",
      "Stochastic Gradient Descent(513/999): loss=0.1192963988406542\n",
      "Stochastic Gradient Descent(514/999): loss=0.11928816598379978\n",
      "Stochastic Gradient Descent(515/999): loss=0.11927994278427687\n",
      "Stochastic Gradient Descent(516/999): loss=0.11927172922746629\n",
      "Stochastic Gradient Descent(517/999): loss=0.11926352529877465\n",
      "Stochastic Gradient Descent(518/999): loss=0.11925533098363417\n",
      "Stochastic Gradient Descent(519/999): loss=0.1192471462675026\n",
      "Stochastic Gradient Descent(520/999): loss=0.1192389711358633\n",
      "Stochastic Gradient Descent(521/999): loss=0.11923080557422497\n",
      "Stochastic Gradient Descent(522/999): loss=0.11922264956812183\n",
      "Stochastic Gradient Descent(523/999): loss=0.11921450310311321\n",
      "Stochastic Gradient Descent(524/999): loss=0.11920636616478389\n",
      "Stochastic Gradient Descent(525/999): loss=0.11919823873874372\n",
      "Stochastic Gradient Descent(526/999): loss=0.11919012081062777\n",
      "Stochastic Gradient Descent(527/999): loss=0.11918201236609607\n",
      "Stochastic Gradient Descent(528/999): loss=0.11917391339083376\n",
      "Stochastic Gradient Descent(529/999): loss=0.11916582387055084\n",
      "Stochastic Gradient Descent(530/999): loss=0.11915774379098222\n",
      "Stochastic Gradient Descent(531/999): loss=0.11914967313788766\n",
      "Stochastic Gradient Descent(532/999): loss=0.11914161189705164\n",
      "Stochastic Gradient Descent(533/999): loss=0.11913356005428336\n",
      "Stochastic Gradient Descent(534/999): loss=0.11912551759541666\n",
      "Stochastic Gradient Descent(535/999): loss=0.11911748450631\n",
      "Stochastic Gradient Descent(536/999): loss=0.1191094607728463\n",
      "Stochastic Gradient Descent(537/999): loss=0.11910144638093302\n",
      "Stochastic Gradient Descent(538/999): loss=0.11909344131650196\n",
      "Stochastic Gradient Descent(539/999): loss=0.11908544556550932\n",
      "Stochastic Gradient Descent(540/999): loss=0.1190774591139356\n",
      "Stochastic Gradient Descent(541/999): loss=0.11906948194778552\n",
      "Stochastic Gradient Descent(542/999): loss=0.11906151405308799\n",
      "Stochastic Gradient Descent(543/999): loss=0.11905355541589605\n",
      "Stochastic Gradient Descent(544/999): loss=0.11904560602228681\n",
      "Stochastic Gradient Descent(545/999): loss=0.11903766585836144\n",
      "Stochastic Gradient Descent(546/999): loss=0.11902973491024499\n",
      "Stochastic Gradient Descent(547/999): loss=0.11902181316408653\n",
      "Stochastic Gradient Descent(548/999): loss=0.11901390060605885\n",
      "Stochastic Gradient Descent(549/999): loss=0.11900599722235869\n",
      "Stochastic Gradient Descent(550/999): loss=0.11899810299920645\n",
      "Stochastic Gradient Descent(551/999): loss=0.11899021792284624\n",
      "Stochastic Gradient Descent(552/999): loss=0.11898234197954582\n",
      "Stochastic Gradient Descent(553/999): loss=0.11897447515559655\n",
      "Stochastic Gradient Descent(554/999): loss=0.11896661743731327\n",
      "Stochastic Gradient Descent(555/999): loss=0.11895876881103447\n",
      "Stochastic Gradient Descent(556/999): loss=0.11895092926312185\n",
      "Stochastic Gradient Descent(557/999): loss=0.11894309877996066\n",
      "Stochastic Gradient Descent(558/999): loss=0.11893527734795944\n",
      "Stochastic Gradient Descent(559/999): loss=0.11892746495355004\n",
      "Stochastic Gradient Descent(560/999): loss=0.11891966158318744\n",
      "Stochastic Gradient Descent(561/999): loss=0.11891186722334991\n",
      "Stochastic Gradient Descent(562/999): loss=0.11890408186053884\n",
      "Stochastic Gradient Descent(563/999): loss=0.11889630548127865\n",
      "Stochastic Gradient Descent(564/999): loss=0.11888853807211683\n",
      "Stochastic Gradient Descent(565/999): loss=0.11888077961962387\n",
      "Stochastic Gradient Descent(566/999): loss=0.11887303011039316\n",
      "Stochastic Gradient Descent(567/999): loss=0.11886528953104102\n",
      "Stochastic Gradient Descent(568/999): loss=0.11885755786820658\n",
      "Stochastic Gradient Descent(569/999): loss=0.11884983510855175\n",
      "Stochastic Gradient Descent(570/999): loss=0.11884212123876121\n",
      "Stochastic Gradient Descent(571/999): loss=0.11883441624554233\n",
      "Stochastic Gradient Descent(572/999): loss=0.11882672011562515\n",
      "Stochastic Gradient Descent(573/999): loss=0.11881903283576226\n",
      "Stochastic Gradient Descent(574/999): loss=0.11881135439272886\n",
      "Stochastic Gradient Descent(575/999): loss=0.11880368477332261\n",
      "Stochastic Gradient Descent(576/999): loss=0.11879602396436367\n",
      "Stochastic Gradient Descent(577/999): loss=0.1187883719526946\n",
      "Stochastic Gradient Descent(578/999): loss=0.11878072872518033\n",
      "Stochastic Gradient Descent(579/999): loss=0.11877309426870813\n",
      "Stochastic Gradient Descent(580/999): loss=0.11876546857018752\n",
      "Stochastic Gradient Descent(581/999): loss=0.11875785161655023\n",
      "Stochastic Gradient Descent(582/999): loss=0.11875024339475025\n",
      "Stochastic Gradient Descent(583/999): loss=0.11874264389176371\n",
      "Stochastic Gradient Descent(584/999): loss=0.11873505309458873\n",
      "Stochastic Gradient Descent(585/999): loss=0.1187274709902456\n",
      "Stochastic Gradient Descent(586/999): loss=0.11871989756577657\n",
      "Stochastic Gradient Descent(587/999): loss=0.11871233280824586\n",
      "Stochastic Gradient Descent(588/999): loss=0.1187047767047396\n",
      "Stochastic Gradient Descent(589/999): loss=0.1186972292423658\n",
      "Stochastic Gradient Descent(590/999): loss=0.1186896904082543\n",
      "Stochastic Gradient Descent(591/999): loss=0.11868216018955678\n",
      "Stochastic Gradient Descent(592/999): loss=0.11867463857344658\n",
      "Stochastic Gradient Descent(593/999): loss=0.11866712554711882\n",
      "Stochastic Gradient Descent(594/999): loss=0.11865962109779023\n",
      "Stochastic Gradient Descent(595/999): loss=0.11865212521269913\n",
      "Stochastic Gradient Descent(596/999): loss=0.11864463787910555\n",
      "Stochastic Gradient Descent(597/999): loss=0.11863715908429089\n",
      "Stochastic Gradient Descent(598/999): loss=0.11862968881555808\n",
      "Stochastic Gradient Descent(599/999): loss=0.11862222706023158\n",
      "Stochastic Gradient Descent(600/999): loss=0.11861477380565717\n",
      "Stochastic Gradient Descent(601/999): loss=0.11860732903920203\n",
      "Stochastic Gradient Descent(602/999): loss=0.11859989274825466\n",
      "Stochastic Gradient Descent(603/999): loss=0.11859246492022484\n",
      "Stochastic Gradient Descent(604/999): loss=0.11858504554254351\n",
      "Stochastic Gradient Descent(605/999): loss=0.11857763460266295\n",
      "Stochastic Gradient Descent(606/999): loss=0.1185702320880565\n",
      "Stochastic Gradient Descent(607/999): loss=0.11856283798621868\n",
      "Stochastic Gradient Descent(608/999): loss=0.11855545228466495\n",
      "Stochastic Gradient Descent(609/999): loss=0.11854807497093199\n",
      "Stochastic Gradient Descent(610/999): loss=0.11854070603257733\n",
      "Stochastic Gradient Descent(611/999): loss=0.1185333454571795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(612/999): loss=0.11852599323233795\n",
      "Stochastic Gradient Descent(613/999): loss=0.11851864934567302\n",
      "Stochastic Gradient Descent(614/999): loss=0.1185113137848258\n",
      "Stochastic Gradient Descent(615/999): loss=0.1185039865374583\n",
      "Stochastic Gradient Descent(616/999): loss=0.11849666759125316\n",
      "Stochastic Gradient Descent(617/999): loss=0.1184893569339138\n",
      "Stochastic Gradient Descent(618/999): loss=0.11848205455316434\n",
      "Stochastic Gradient Descent(619/999): loss=0.1184747604367494\n",
      "Stochastic Gradient Descent(620/999): loss=0.11846747457243437\n",
      "Stochastic Gradient Descent(621/999): loss=0.1184601969480051\n",
      "Stochastic Gradient Descent(622/999): loss=0.11845292755126795\n",
      "Stochastic Gradient Descent(623/999): loss=0.1184456663700498\n",
      "Stochastic Gradient Descent(624/999): loss=0.11843841339219792\n",
      "Stochastic Gradient Descent(625/999): loss=0.11843116860558003\n",
      "Stochastic Gradient Descent(626/999): loss=0.11842393199808421\n",
      "Stochastic Gradient Descent(627/999): loss=0.11841670355761882\n",
      "Stochastic Gradient Descent(628/999): loss=0.11840948327211252\n",
      "Stochastic Gradient Descent(629/999): loss=0.11840227112951424\n",
      "Stochastic Gradient Descent(630/999): loss=0.11839506711779314\n",
      "Stochastic Gradient Descent(631/999): loss=0.11838787122493846\n",
      "Stochastic Gradient Descent(632/999): loss=0.1183806834389597\n",
      "Stochastic Gradient Descent(633/999): loss=0.11837350374788634\n",
      "Stochastic Gradient Descent(634/999): loss=0.11836633213976802\n",
      "Stochastic Gradient Descent(635/999): loss=0.11835916860267429\n",
      "Stochastic Gradient Descent(636/999): loss=0.11835201312469475\n",
      "Stochastic Gradient Descent(637/999): loss=0.11834486569393898\n",
      "Stochastic Gradient Descent(638/999): loss=0.11833772629853644\n",
      "Stochastic Gradient Descent(639/999): loss=0.11833059492663639\n",
      "Stochastic Gradient Descent(640/999): loss=0.11832347156640806\n",
      "Stochastic Gradient Descent(641/999): loss=0.11831635620604034\n",
      "Stochastic Gradient Descent(642/999): loss=0.118309248833742\n",
      "Stochastic Gradient Descent(643/999): loss=0.11830214943774146\n",
      "Stochastic Gradient Descent(644/999): loss=0.11829505800628687\n",
      "Stochastic Gradient Descent(645/999): loss=0.118287974527646\n",
      "Stochastic Gradient Descent(646/999): loss=0.11828089899010623\n",
      "Stochastic Gradient Descent(647/999): loss=0.11827383138197461\n",
      "Stochastic Gradient Descent(648/999): loss=0.11826677169157758\n",
      "Stochastic Gradient Descent(649/999): loss=0.11825971990726124\n",
      "Stochastic Gradient Descent(650/999): loss=0.11825267601739109\n",
      "Stochastic Gradient Descent(651/999): loss=0.11824564001035207\n",
      "Stochastic Gradient Descent(652/999): loss=0.11823861187454847\n",
      "Stochastic Gradient Descent(653/999): loss=0.11823159159840407\n",
      "Stochastic Gradient Descent(654/999): loss=0.11822457917036185\n",
      "Stochastic Gradient Descent(655/999): loss=0.1182175745788842\n",
      "Stochastic Gradient Descent(656/999): loss=0.11821057781245264\n",
      "Stochastic Gradient Descent(657/999): loss=0.11820358885956803\n",
      "Stochastic Gradient Descent(658/999): loss=0.11819660770875036\n",
      "Stochastic Gradient Descent(659/999): loss=0.11818963434853876\n",
      "Stochastic Gradient Descent(660/999): loss=0.1181826687674915\n",
      "Stochastic Gradient Descent(661/999): loss=0.118175710954186\n",
      "Stochastic Gradient Descent(662/999): loss=0.11816876089721856\n",
      "Stochastic Gradient Descent(663/999): loss=0.11816181858520468\n",
      "Stochastic Gradient Descent(664/999): loss=0.1181548840067787\n",
      "Stochastic Gradient Descent(665/999): loss=0.11814795715059398\n",
      "Stochastic Gradient Descent(666/999): loss=0.11814103800532276\n",
      "Stochastic Gradient Descent(667/999): loss=0.11813412655965617\n",
      "Stochastic Gradient Descent(668/999): loss=0.1181272228023042\n",
      "Stochastic Gradient Descent(669/999): loss=0.11812032672199553\n",
      "Stochastic Gradient Descent(670/999): loss=0.1181134383074778\n",
      "Stochastic Gradient Descent(671/999): loss=0.11810655754751723\n",
      "Stochastic Gradient Descent(672/999): loss=0.11809968443089883\n",
      "Stochastic Gradient Descent(673/999): loss=0.11809281894642625\n",
      "Stochastic Gradient Descent(674/999): loss=0.11808596108292177\n",
      "Stochastic Gradient Descent(675/999): loss=0.1180791108292263\n",
      "Stochastic Gradient Descent(676/999): loss=0.11807226817419932\n",
      "Stochastic Gradient Descent(677/999): loss=0.11806543310671878\n",
      "Stochastic Gradient Descent(678/999): loss=0.11805860561568125\n",
      "Stochastic Gradient Descent(679/999): loss=0.11805178569000167\n",
      "Stochastic Gradient Descent(680/999): loss=0.11804497331861341\n",
      "Stochastic Gradient Descent(681/999): loss=0.11803816849046832\n",
      "Stochastic Gradient Descent(682/999): loss=0.11803137119453658\n",
      "Stochastic Gradient Descent(683/999): loss=0.11802458141980667\n",
      "Stochastic Gradient Descent(684/999): loss=0.11801779915528547\n",
      "Stochastic Gradient Descent(685/999): loss=0.11801102438999798\n",
      "Stochastic Gradient Descent(686/999): loss=0.11800425711298758\n",
      "Stochastic Gradient Descent(687/999): loss=0.11799749731331582\n",
      "Stochastic Gradient Descent(688/999): loss=0.11799074498006235\n",
      "Stochastic Gradient Descent(689/999): loss=0.11798400010232497\n",
      "Stochastic Gradient Descent(690/999): loss=0.11797726266921969\n",
      "Stochastic Gradient Descent(691/999): loss=0.11797053266988054\n",
      "Stochastic Gradient Descent(692/999): loss=0.11796381009345952\n",
      "Stochastic Gradient Descent(693/999): loss=0.11795709492912673\n",
      "Stochastic Gradient Descent(694/999): loss=0.11795038716607023\n",
      "Stochastic Gradient Descent(695/999): loss=0.11794368679349594\n",
      "Stochastic Gradient Descent(696/999): loss=0.1179369938006278\n",
      "Stochastic Gradient Descent(697/999): loss=0.11793030817670762\n",
      "Stochastic Gradient Descent(698/999): loss=0.11792362991099495\n",
      "Stochastic Gradient Descent(699/999): loss=0.11791695899276732\n",
      "Stochastic Gradient Descent(700/999): loss=0.1179102954113199\n",
      "Stochastic Gradient Descent(701/999): loss=0.11790363915596566\n",
      "Stochastic Gradient Descent(702/999): loss=0.11789699021603534\n",
      "Stochastic Gradient Descent(703/999): loss=0.11789034858087725\n",
      "Stochastic Gradient Descent(704/999): loss=0.11788371423985752\n",
      "Stochastic Gradient Descent(705/999): loss=0.11787708718235977\n",
      "Stochastic Gradient Descent(706/999): loss=0.1178704673977853\n",
      "Stochastic Gradient Descent(707/999): loss=0.1178638548755529\n",
      "Stochastic Gradient Descent(708/999): loss=0.11785724960509893\n",
      "Stochastic Gradient Descent(709/999): loss=0.11785065157587725\n",
      "Stochastic Gradient Descent(710/999): loss=0.11784406077735922\n",
      "Stochastic Gradient Descent(711/999): loss=0.11783747719903356\n",
      "Stochastic Gradient Descent(712/999): loss=0.11783090083040644\n",
      "Stochastic Gradient Descent(713/999): loss=0.11782433166100141\n",
      "Stochastic Gradient Descent(714/999): loss=0.11781776968035934\n",
      "Stochastic Gradient Descent(715/999): loss=0.1178112148780385\n",
      "Stochastic Gradient Descent(716/999): loss=0.11780466724361434\n",
      "Stochastic Gradient Descent(717/999): loss=0.11779812676667954\n",
      "Stochastic Gradient Descent(718/999): loss=0.11779159343684414\n",
      "Stochastic Gradient Descent(719/999): loss=0.11778506724373526\n",
      "Stochastic Gradient Descent(720/999): loss=0.11777854817699715\n",
      "Stochastic Gradient Descent(721/999): loss=0.11777203622629133\n",
      "Stochastic Gradient Descent(722/999): loss=0.11776553138129632\n",
      "Stochastic Gradient Descent(723/999): loss=0.11775903363170769\n",
      "Stochastic Gradient Descent(724/999): loss=0.1177525429672381\n",
      "Stochastic Gradient Descent(725/999): loss=0.1177460593776172\n",
      "Stochastic Gradient Descent(726/999): loss=0.11773958285259166\n",
      "Stochastic Gradient Descent(727/999): loss=0.11773311338192499\n",
      "Stochastic Gradient Descent(728/999): loss=0.11772665095539772\n",
      "Stochastic Gradient Descent(729/999): loss=0.11772019556280723\n",
      "Stochastic Gradient Descent(730/999): loss=0.11771374719396778\n",
      "Stochastic Gradient Descent(731/999): loss=0.11770730583871039\n",
      "Stochastic Gradient Descent(732/999): loss=0.11770087148688298\n",
      "Stochastic Gradient Descent(733/999): loss=0.11769444412835012\n",
      "Stochastic Gradient Descent(734/999): loss=0.11768802375299321\n",
      "Stochastic Gradient Descent(735/999): loss=0.11768161035071034\n",
      "Stochastic Gradient Descent(736/999): loss=0.11767520391141627\n",
      "Stochastic Gradient Descent(737/999): loss=0.1176688044250424\n",
      "Stochastic Gradient Descent(738/999): loss=0.11766241188153678\n",
      "Stochastic Gradient Descent(739/999): loss=0.11765602627086401\n",
      "Stochastic Gradient Descent(740/999): loss=0.11764964758300525\n",
      "Stochastic Gradient Descent(741/999): loss=0.11764327580795823\n",
      "Stochastic Gradient Descent(742/999): loss=0.11763691093573718\n",
      "Stochastic Gradient Descent(743/999): loss=0.11763055295637277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(744/999): loss=0.11762420185991213\n",
      "Stochastic Gradient Descent(745/999): loss=0.11761785763641884\n",
      "Stochastic Gradient Descent(746/999): loss=0.11761152027597278\n",
      "Stochastic Gradient Descent(747/999): loss=0.11760518976867028\n",
      "Stochastic Gradient Descent(748/999): loss=0.11759886610462396\n",
      "Stochastic Gradient Descent(749/999): loss=0.1175925492739627\n",
      "Stochastic Gradient Descent(750/999): loss=0.11758623926683173\n",
      "Stochastic Gradient Descent(751/999): loss=0.11757993607339248\n",
      "Stochastic Gradient Descent(752/999): loss=0.11757363968382256\n",
      "Stochastic Gradient Descent(753/999): loss=0.11756735008831584\n",
      "Stochastic Gradient Descent(754/999): loss=0.11756106727708229\n",
      "Stochastic Gradient Descent(755/999): loss=0.11755479124034802\n",
      "Stochastic Gradient Descent(756/999): loss=0.11754852196835525\n",
      "Stochastic Gradient Descent(757/999): loss=0.11754225945136222\n",
      "Stochastic Gradient Descent(758/999): loss=0.1175360036796433\n",
      "Stochastic Gradient Descent(759/999): loss=0.11752975464348883\n",
      "Stochastic Gradient Descent(760/999): loss=0.11752351233320511\n",
      "Stochastic Gradient Descent(761/999): loss=0.11751727673911441\n",
      "Stochastic Gradient Descent(762/999): loss=0.11751104785155499\n",
      "Stochastic Gradient Descent(763/999): loss=0.11750482566088091\n",
      "Stochastic Gradient Descent(764/999): loss=0.11749861015746219\n",
      "Stochastic Gradient Descent(765/999): loss=0.11749240133168466\n",
      "Stochastic Gradient Descent(766/999): loss=0.11748619917394991\n",
      "Stochastic Gradient Descent(767/999): loss=0.11748000367467548\n",
      "Stochastic Gradient Descent(768/999): loss=0.1174738148242945\n",
      "Stochastic Gradient Descent(769/999): loss=0.11746763261325589\n",
      "Stochastic Gradient Descent(770/999): loss=0.11746145703202435\n",
      "Stochastic Gradient Descent(771/999): loss=0.11745528807108015\n",
      "Stochastic Gradient Descent(772/999): loss=0.11744912572091927\n",
      "Stochastic Gradient Descent(773/999): loss=0.1174429699720533\n",
      "Stochastic Gradient Descent(774/999): loss=0.11743682081500942\n",
      "Stochastic Gradient Descent(775/999): loss=0.11743067824033038\n",
      "Stochastic Gradient Descent(776/999): loss=0.11742454223857453\n",
      "Stochastic Gradient Descent(777/999): loss=0.11741841280031563\n",
      "Stochastic Gradient Descent(778/999): loss=0.117412289916143\n",
      "Stochastic Gradient Descent(779/999): loss=0.11740617357666139\n",
      "Stochastic Gradient Descent(780/999): loss=0.11740006377249095\n",
      "Stochastic Gradient Descent(781/999): loss=0.11739396049426737\n",
      "Stochastic Gradient Descent(782/999): loss=0.11738786373264154\n",
      "Stochastic Gradient Descent(783/999): loss=0.11738177347827977\n",
      "Stochastic Gradient Descent(784/999): loss=0.11737568972186375\n",
      "Stochastic Gradient Descent(785/999): loss=0.11736961245409043\n",
      "Stochastic Gradient Descent(786/999): loss=0.11736354166567199\n",
      "Stochastic Gradient Descent(787/999): loss=0.11735747734733588\n",
      "Stochastic Gradient Descent(788/999): loss=0.11735141948982479\n",
      "Stochastic Gradient Descent(789/999): loss=0.1173453680838966\n",
      "Stochastic Gradient Descent(790/999): loss=0.11733932312032432\n",
      "Stochastic Gradient Descent(791/999): loss=0.11733328458989606\n",
      "Stochastic Gradient Descent(792/999): loss=0.11732725248341516\n",
      "Stochastic Gradient Descent(793/999): loss=0.11732122679169994\n",
      "Stochastic Gradient Descent(794/999): loss=0.11731520750558383\n",
      "Stochastic Gradient Descent(795/999): loss=0.11730919461591523\n",
      "Stochastic Gradient Descent(796/999): loss=0.11730318811355762\n",
      "Stochastic Gradient Descent(797/999): loss=0.11729718798938939\n",
      "Stochastic Gradient Descent(798/999): loss=0.11729119423430392\n",
      "Stochastic Gradient Descent(799/999): loss=0.11728520683920957\n",
      "Stochastic Gradient Descent(800/999): loss=0.11727922579502942\n",
      "Stochastic Gradient Descent(801/999): loss=0.11727325109270162\n",
      "Stochastic Gradient Descent(802/999): loss=0.11726728272317906\n",
      "Stochastic Gradient Descent(803/999): loss=0.1172613206774295\n",
      "Stochastic Gradient Descent(804/999): loss=0.11725536494643544\n",
      "Stochastic Gradient Descent(805/999): loss=0.11724941552119418\n",
      "Stochastic Gradient Descent(806/999): loss=0.11724347239271779\n",
      "Stochastic Gradient Descent(807/999): loss=0.11723753555203299\n",
      "Stochastic Gradient Descent(808/999): loss=0.11723160499018127\n",
      "Stochastic Gradient Descent(809/999): loss=0.1172256806982187\n",
      "Stochastic Gradient Descent(810/999): loss=0.11721976266721605\n",
      "Stochastic Gradient Descent(811/999): loss=0.11721385088825871\n",
      "Stochastic Gradient Descent(812/999): loss=0.11720794535244661\n",
      "Stochastic Gradient Descent(813/999): loss=0.11720204605089428\n",
      "Stochastic Gradient Descent(814/999): loss=0.11719615297473077\n",
      "Stochastic Gradient Descent(815/999): loss=0.11719026611509965\n",
      "Stochastic Gradient Descent(816/999): loss=0.11718438546315897\n",
      "Stochastic Gradient Descent(817/999): loss=0.11717851101008128\n",
      "Stochastic Gradient Descent(818/999): loss=0.11717264274705348\n",
      "Stochastic Gradient Descent(819/999): loss=0.11716678066527694\n",
      "Stochastic Gradient Descent(820/999): loss=0.11716092475596743\n",
      "Stochastic Gradient Descent(821/999): loss=0.11715507501035506\n",
      "Stochastic Gradient Descent(822/999): loss=0.1171492314196842\n",
      "Stochastic Gradient Descent(823/999): loss=0.11714339397521369\n",
      "Stochastic Gradient Descent(824/999): loss=0.11713756266821651\n",
      "Stochastic Gradient Descent(825/999): loss=0.11713173748998\n",
      "Stochastic Gradient Descent(826/999): loss=0.11712591843180564\n",
      "Stochastic Gradient Descent(827/999): loss=0.11712010548500922\n",
      "Stochastic Gradient Descent(828/999): loss=0.11711429864092063\n",
      "Stochastic Gradient Descent(829/999): loss=0.11710849789088393\n",
      "Stochastic Gradient Descent(830/999): loss=0.11710270322625738\n",
      "Stochastic Gradient Descent(831/999): loss=0.11709691463841329\n",
      "Stochastic Gradient Descent(832/999): loss=0.11709113211873809\n",
      "Stochastic Gradient Descent(833/999): loss=0.11708535565863226\n",
      "Stochastic Gradient Descent(834/999): loss=0.1170795852495103\n",
      "Stochastic Gradient Descent(835/999): loss=0.11707382088280073\n",
      "Stochastic Gradient Descent(836/999): loss=0.11706806254994606\n",
      "Stochastic Gradient Descent(837/999): loss=0.11706231024240281\n",
      "Stochastic Gradient Descent(838/999): loss=0.11705656395164134\n",
      "Stochastic Gradient Descent(839/999): loss=0.11705082366914603\n",
      "Stochastic Gradient Descent(840/999): loss=0.11704508938641504\n",
      "Stochastic Gradient Descent(841/999): loss=0.11703936109496049\n",
      "Stochastic Gradient Descent(842/999): loss=0.11703363878630833\n",
      "Stochastic Gradient Descent(843/999): loss=0.11702792245199825\n",
      "Stochastic Gradient Descent(844/999): loss=0.11702221208358382\n",
      "Stochastic Gradient Descent(845/999): loss=0.11701650767263234\n",
      "Stochastic Gradient Descent(846/999): loss=0.11701080921072489\n",
      "Stochastic Gradient Descent(847/999): loss=0.11700511668945619\n",
      "Stochastic Gradient Descent(848/999): loss=0.11699943010043475\n",
      "Stochastic Gradient Descent(849/999): loss=0.11699374943528268\n",
      "Stochastic Gradient Descent(850/999): loss=0.11698807468563578\n",
      "Stochastic Gradient Descent(851/999): loss=0.11698240584314348\n",
      "Stochastic Gradient Descent(852/999): loss=0.1169767428994688\n",
      "Stochastic Gradient Descent(853/999): loss=0.11697108584628833\n",
      "Stochastic Gradient Descent(854/999): loss=0.1169654346752922\n",
      "Stochastic Gradient Descent(855/999): loss=0.11695978937818413\n",
      "Stochastic Gradient Descent(856/999): loss=0.11695414994668128\n",
      "Stochastic Gradient Descent(857/999): loss=0.11694851637251434\n",
      "Stochastic Gradient Descent(858/999): loss=0.11694288864742744\n",
      "Stochastic Gradient Descent(859/999): loss=0.11693726676317809\n",
      "Stochastic Gradient Descent(860/999): loss=0.11693165071153735\n",
      "Stochastic Gradient Descent(861/999): loss=0.11692604048428956\n",
      "Stochastic Gradient Descent(862/999): loss=0.11692043607323244\n",
      "Stochastic Gradient Descent(863/999): loss=0.11691483747017709\n",
      "Stochastic Gradient Descent(864/999): loss=0.11690924466694787\n",
      "Stochastic Gradient Descent(865/999): loss=0.11690365765538252\n",
      "Stochastic Gradient Descent(866/999): loss=0.11689807642733198\n",
      "Stochastic Gradient Descent(867/999): loss=0.11689250097466043\n",
      "Stochastic Gradient Descent(868/999): loss=0.11688693128924539\n",
      "Stochastic Gradient Descent(869/999): loss=0.11688136736297747\n",
      "Stochastic Gradient Descent(870/999): loss=0.11687580918776047\n",
      "Stochastic Gradient Descent(871/999): loss=0.11687025675551144\n",
      "Stochastic Gradient Descent(872/999): loss=0.11686471005816045\n",
      "Stochastic Gradient Descent(873/999): loss=0.11685916908765072\n",
      "Stochastic Gradient Descent(874/999): loss=0.11685363383593858\n",
      "Stochastic Gradient Descent(875/999): loss=0.11684810429499344\n",
      "Stochastic Gradient Descent(876/999): loss=0.11684258045679773\n",
      "Stochastic Gradient Descent(877/999): loss=0.11683706231334685\n",
      "Stochastic Gradient Descent(878/999): loss=0.11683154985664929\n",
      "Stochastic Gradient Descent(879/999): loss=0.11682604307872646\n",
      "Stochastic Gradient Descent(880/999): loss=0.11682054197161275\n",
      "Stochastic Gradient Descent(881/999): loss=0.11681504652735543\n",
      "Stochastic Gradient Descent(882/999): loss=0.11680955673801477\n",
      "Stochastic Gradient Descent(883/999): loss=0.11680407259566379\n",
      "Stochastic Gradient Descent(884/999): loss=0.11679859409238852\n",
      "Stochastic Gradient Descent(885/999): loss=0.1167931212202877\n",
      "Stochastic Gradient Descent(886/999): loss=0.11678765397147303\n",
      "Stochastic Gradient Descent(887/999): loss=0.1167821923380688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(888/999): loss=0.11677673631221235\n",
      "Stochastic Gradient Descent(889/999): loss=0.1167712858860535\n",
      "Stochastic Gradient Descent(890/999): loss=0.11676584105175493\n",
      "Stochastic Gradient Descent(891/999): loss=0.11676040180149205\n",
      "Stochastic Gradient Descent(892/999): loss=0.11675496812745292\n",
      "Stochastic Gradient Descent(893/999): loss=0.11674954002183822\n",
      "Stochastic Gradient Descent(894/999): loss=0.11674411747686134\n",
      "Stochastic Gradient Descent(895/999): loss=0.11673870048474826\n",
      "Stochastic Gradient Descent(896/999): loss=0.1167332890377375\n",
      "Stochastic Gradient Descent(897/999): loss=0.11672788312808027\n",
      "Stochastic Gradient Descent(898/999): loss=0.11672248274804023\n",
      "Stochastic Gradient Descent(899/999): loss=0.11671708788989368\n",
      "Stochastic Gradient Descent(900/999): loss=0.11671169854592925\n",
      "Stochastic Gradient Descent(901/999): loss=0.11670631470844826\n",
      "Stochastic Gradient Descent(902/999): loss=0.11670093636976435\n",
      "Stochastic Gradient Descent(903/999): loss=0.11669556352220369\n",
      "Stochastic Gradient Descent(904/999): loss=0.1166901961581048\n",
      "Stochastic Gradient Descent(905/999): loss=0.1166848342698187\n",
      "Stochastic Gradient Descent(906/999): loss=0.11667947784970864\n",
      "Stochastic Gradient Descent(907/999): loss=0.11667412689015037\n",
      "Stochastic Gradient Descent(908/999): loss=0.1166687813835319\n",
      "Stochastic Gradient Descent(909/999): loss=0.1166634413222536\n",
      "Stochastic Gradient Descent(910/999): loss=0.11665810669872809\n",
      "Stochastic Gradient Descent(911/999): loss=0.11665277750538028\n",
      "Stochastic Gradient Descent(912/999): loss=0.11664745373464731\n",
      "Stochastic Gradient Descent(913/999): loss=0.11664213537897856\n",
      "Stochastic Gradient Descent(914/999): loss=0.11663682243083569\n",
      "Stochastic Gradient Descent(915/999): loss=0.11663151488269241\n",
      "Stochastic Gradient Descent(916/999): loss=0.1166262127270347\n",
      "Stochastic Gradient Descent(917/999): loss=0.11662091595636065\n",
      "Stochastic Gradient Descent(918/999): loss=0.11661562456318045\n",
      "Stochastic Gradient Descent(919/999): loss=0.11661033854001646\n",
      "Stochastic Gradient Descent(920/999): loss=0.11660505787940302\n",
      "Stochastic Gradient Descent(921/999): loss=0.11659978257388662\n",
      "Stochastic Gradient Descent(922/999): loss=0.11659451261602573\n",
      "Stochastic Gradient Descent(923/999): loss=0.11658924799839086\n",
      "Stochastic Gradient Descent(924/999): loss=0.1165839887135646\n",
      "Stochastic Gradient Descent(925/999): loss=0.11657873475414131\n",
      "Stochastic Gradient Descent(926/999): loss=0.11657348611272751\n",
      "Stochastic Gradient Descent(927/999): loss=0.11656824278194156\n",
      "Stochastic Gradient Descent(928/999): loss=0.11656300475441372\n",
      "Stochastic Gradient Descent(929/999): loss=0.11655777202278622\n",
      "Stochastic Gradient Descent(930/999): loss=0.11655254457971305\n",
      "Stochastic Gradient Descent(931/999): loss=0.1165473224178602\n",
      "Stochastic Gradient Descent(932/999): loss=0.11654210552990532\n",
      "Stochastic Gradient Descent(933/999): loss=0.11653689390853801\n",
      "Stochastic Gradient Descent(934/999): loss=0.11653168754645961\n",
      "Stochastic Gradient Descent(935/999): loss=0.11652648643638322\n",
      "Stochastic Gradient Descent(936/999): loss=0.11652129057103365\n",
      "Stochastic Gradient Descent(937/999): loss=0.11651609994314757\n",
      "Stochastic Gradient Descent(938/999): loss=0.1165109145454732\n",
      "Stochastic Gradient Descent(939/999): loss=0.1165057343707706\n",
      "Stochastic Gradient Descent(940/999): loss=0.11650055941181132\n",
      "Stochastic Gradient Descent(941/999): loss=0.11649538966137875\n",
      "Stochastic Gradient Descent(942/999): loss=0.11649022511226777\n",
      "Stochastic Gradient Descent(943/999): loss=0.11648506575728493\n",
      "Stochastic Gradient Descent(944/999): loss=0.11647991158924834\n",
      "Stochastic Gradient Descent(945/999): loss=0.11647476260098767\n",
      "Stochastic Gradient Descent(946/999): loss=0.11646961878534419\n",
      "Stochastic Gradient Descent(947/999): loss=0.11646448013517062\n",
      "Stochastic Gradient Descent(948/999): loss=0.11645934664333125\n",
      "Stochastic Gradient Descent(949/999): loss=0.11645421830270188\n",
      "Stochastic Gradient Descent(950/999): loss=0.11644909510616966\n",
      "Stochastic Gradient Descent(951/999): loss=0.11644397704663331\n",
      "Stochastic Gradient Descent(952/999): loss=0.11643886411700285\n",
      "Stochastic Gradient Descent(953/999): loss=0.11643375631019988\n",
      "Stochastic Gradient Descent(954/999): loss=0.1164286536191572\n",
      "Stochastic Gradient Descent(955/999): loss=0.11642355603681917\n",
      "Stochastic Gradient Descent(956/999): loss=0.11641846355614131\n",
      "Stochastic Gradient Descent(957/999): loss=0.11641337617009058\n",
      "Stochastic Gradient Descent(958/999): loss=0.11640829387164522\n",
      "Stochastic Gradient Descent(959/999): loss=0.11640321665379483\n",
      "Stochastic Gradient Descent(960/999): loss=0.11639814450954013\n",
      "Stochastic Gradient Descent(961/999): loss=0.1163930774318932\n",
      "Stochastic Gradient Descent(962/999): loss=0.11638801541387732\n",
      "Stochastic Gradient Descent(963/999): loss=0.11638295844852699\n",
      "Stochastic Gradient Descent(964/999): loss=0.11637790652888791\n",
      "Stochastic Gradient Descent(965/999): loss=0.11637285964801693\n",
      "Stochastic Gradient Descent(966/999): loss=0.11636781779898205\n",
      "Stochastic Gradient Descent(967/999): loss=0.11636278097486244\n",
      "Stochastic Gradient Descent(968/999): loss=0.11635774916874832\n",
      "Stochastic Gradient Descent(969/999): loss=0.11635272237374107\n",
      "Stochastic Gradient Descent(970/999): loss=0.11634770058295316\n",
      "Stochastic Gradient Descent(971/999): loss=0.116342683789508\n",
      "Stochastic Gradient Descent(972/999): loss=0.11633767198654016\n",
      "Stochastic Gradient Descent(973/999): loss=0.11633266516719519\n",
      "Stochastic Gradient Descent(974/999): loss=0.11632766332462964\n",
      "Stochastic Gradient Descent(975/999): loss=0.11632266645201098\n",
      "Stochastic Gradient Descent(976/999): loss=0.11631767454251778\n",
      "Stochastic Gradient Descent(977/999): loss=0.11631268758933935\n",
      "Stochastic Gradient Descent(978/999): loss=0.11630770558567616\n",
      "Stochastic Gradient Descent(979/999): loss=0.11630272852473938\n",
      "Stochastic Gradient Descent(980/999): loss=0.1162977563997512\n",
      "Stochastic Gradient Descent(981/999): loss=0.1162927892039446\n",
      "Stochastic Gradient Descent(982/999): loss=0.11628782693056344\n",
      "Stochastic Gradient Descent(983/999): loss=0.1162828695728624\n",
      "Stochastic Gradient Descent(984/999): loss=0.11627791712410702\n",
      "Stochastic Gradient Descent(985/999): loss=0.1162729695775735\n",
      "Stochastic Gradient Descent(986/999): loss=0.11626802692654897\n",
      "Stochastic Gradient Descent(987/999): loss=0.11626308916433119\n",
      "Stochastic Gradient Descent(988/999): loss=0.11625815628422875\n",
      "Stochastic Gradient Descent(989/999): loss=0.11625322827956092\n",
      "Stochastic Gradient Descent(990/999): loss=0.11624830514365762\n",
      "Stochastic Gradient Descent(991/999): loss=0.11624338686985951\n",
      "Stochastic Gradient Descent(992/999): loss=0.11623847345151796\n",
      "Stochastic Gradient Descent(993/999): loss=0.1162335648819948\n",
      "Stochastic Gradient Descent(994/999): loss=0.1162286611546627\n",
      "Stochastic Gradient Descent(995/999): loss=0.11622376226290479\n",
      "Stochastic Gradient Descent(996/999): loss=0.11621886820011486\n",
      "Stochastic Gradient Descent(997/999): loss=0.11621397895969723\n",
      "Stochastic Gradient Descent(998/999): loss=0.11620909453506685\n",
      "Stochastic Gradient Descent(999/999): loss=0.11620421491964905\n",
      "Stochastic Gradient Descent(0/999): loss=0.14802841371423545\n",
      "Stochastic Gradient Descent(1/999): loss=0.14733369692909587\n",
      "Stochastic Gradient Descent(2/999): loss=0.14665012610258044\n",
      "Stochastic Gradient Descent(3/999): loss=0.14597752155645172\n",
      "Stochastic Gradient Descent(4/999): loss=0.1453157065092214\n",
      "Stochastic Gradient Descent(5/999): loss=0.14466450702944947\n",
      "Stochastic Gradient Descent(6/999): loss=0.14402375198979586\n",
      "Stochastic Gradient Descent(7/999): loss=0.143393273021813\n",
      "Stochastic Gradient Descent(8/999): loss=0.14277290447146757\n",
      "Stochastic Gradient Descent(9/999): loss=0.14216248335537834\n",
      "Stochastic Gradient Descent(10/999): loss=0.14156184931776092\n",
      "Stochastic Gradient Descent(11/999): loss=0.14097084458806525\n",
      "Stochastic Gradient Descent(12/999): loss=0.1403893139392972\n",
      "Stochastic Gradient Descent(13/999): loss=0.13981710464701136\n",
      "Stochastic Gradient Descent(14/999): loss=0.13925406644896535\n",
      "Stochastic Gradient Descent(15/999): loss=0.13870005150542414\n",
      "Stochastic Gradient Descent(16/999): loss=0.13815491436010494\n",
      "Stochastic Gradient Descent(17/999): loss=0.13761851190175148\n",
      "Stochastic Gradient Descent(18/999): loss=0.13709070332632728\n",
      "Stochastic Gradient Descent(19/999): loss=0.13657135009981938\n",
      "Stochastic Gradient Descent(20/999): loss=0.13606031592164092\n",
      "Stochastic Gradient Descent(21/999): loss=0.13555746668862417\n",
      "Stochastic Gradient Descent(22/999): loss=0.13506267045959405\n",
      "Stochastic Gradient Descent(23/999): loss=0.13457579742051257\n",
      "Stochastic Gradient Descent(24/999): loss=0.1340967198501853\n",
      "Stochastic Gradient Descent(25/999): loss=0.13362531208652104\n",
      "Stochastic Gradient Descent(26/999): loss=0.13316145049333483\n",
      "Stochastic Gradient Descent(27/999): loss=0.13270501342768673\n",
      "Stochastic Gradient Descent(28/999): loss=0.13225588120774662\n",
      "Stochastic Gradient Descent(29/999): loss=0.13181393608117786\n",
      "Stochastic Gradient Descent(30/999): loss=0.13137906219402973\n",
      "Stochastic Gradient Descent(31/999): loss=0.13095114556013213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(32/999): loss=0.13053007403098313\n",
      "Stochastic Gradient Descent(33/999): loss=0.13011573726612277\n",
      "Stochastic Gradient Descent(34/999): loss=0.12970802670398332\n",
      "Stochastic Gradient Descent(35/999): loss=0.1293068355332108\n",
      "Stochastic Gradient Descent(36/999): loss=0.1289120586644477\n",
      "Stochastic Gradient Descent(37/999): loss=0.1285235927025714\n",
      "Stochastic Gradient Descent(38/999): loss=0.12814133591937985\n",
      "Stochastic Gradient Descent(39/999): loss=0.1277651882267176\n",
      "Stochastic Gradient Descent(40/999): loss=0.12739505115003516\n",
      "Stochastic Gradient Descent(41/999): loss=0.12703082780237465\n",
      "Stochastic Gradient Descent(42/999): loss=0.12667242285877464\n",
      "Stochastic Gradient Descent(43/999): loss=0.12631974253108802\n",
      "Stochastic Gradient Descent(44/999): loss=0.12597269454320545\n",
      "Stochastic Gradient Descent(45/999): loss=0.1256311881066786\n",
      "Stochastic Gradient Descent(46/999): loss=0.12529513389673608\n",
      "Stochastic Gradient Descent(47/999): loss=0.12496444402868627\n",
      "Stochastic Gradient Descent(48/999): loss=0.1246390320347005\n",
      "Stochastic Gradient Descent(49/999): loss=0.12431881284097054\n",
      "Stochastic Gradient Descent(50/999): loss=0.12400370274523434\n",
      "Stochastic Gradient Descent(51/999): loss=0.12369361939466393\n",
      "Stochastic Gradient Descent(52/999): loss=0.12338848176411023\n",
      "Stochastic Gradient Descent(53/999): loss=0.12308821013469828\n",
      "Stochastic Gradient Descent(54/999): loss=0.12279272607276746\n",
      "Stochastic Gradient Descent(55/999): loss=0.12250195240915168\n",
      "Stochastic Gradient Descent(56/999): loss=0.12221581321879299\n",
      "Stochastic Gradient Descent(57/999): loss=0.12193423380068454\n",
      "Stochastic Gradient Descent(58/999): loss=0.1216571406581362\n",
      "Stochastic Gradient Descent(59/999): loss=0.12138446147935897\n",
      "Stochastic Gradient Descent(60/999): loss=0.12111612511836202\n",
      "Stochastic Gradient Descent(61/999): loss=0.12085206157615778\n",
      "Stochastic Gradient Descent(62/999): loss=0.12059220198227012\n",
      "Stochastic Gradient Descent(63/999): loss=0.12033647857654055\n",
      "Stochastic Gradient Descent(64/999): loss=0.12008482469122779\n",
      "Stochastic Gradient Descent(65/999): loss=0.1198371747333961\n",
      "Stochastic Gradient Descent(66/999): loss=0.11959346416758711\n",
      "Stochastic Gradient Descent(67/999): loss=0.1193536294987714\n",
      "Stochastic Gradient Descent(68/999): loss=0.11911760825557491\n",
      "Stochastic Gradient Descent(69/999): loss=0.1188853389737755\n",
      "Stochastic Gradient Descent(70/999): loss=0.11865676118006582\n",
      "Stochastic Gradient Descent(71/999): loss=0.11843181537607791\n",
      "Stochastic Gradient Descent(72/999): loss=0.11821044302266531\n",
      "Stochastic Gradient Descent(73/999): loss=0.11799258652443853\n",
      "Stochastic Gradient Descent(74/999): loss=0.11777818921455006\n",
      "Stochastic Gradient Descent(75/999): loss=0.11756719533972425\n",
      "Stochastic Gradient Descent(76/999): loss=0.11735955004552903\n",
      "Stochastic Gradient Descent(77/999): loss=0.1171551993618847\n",
      "Stochastic Gradient Descent(78/999): loss=0.11695409018880669\n",
      "Stochastic Gradient Descent(79/999): loss=0.11675617028237761\n",
      "Stochastic Gradient Descent(80/999): loss=0.11656138824094624\n",
      "Stochastic Gradient Descent(81/999): loss=0.11636969349154837\n",
      "Stochastic Gradient Descent(82/999): loss=0.11618103627654668\n",
      "Stochastic Gradient Descent(83/999): loss=0.11599536764048646\n",
      "Stochastic Gradient Descent(84/999): loss=0.11581263941716265\n",
      "Stochastic Gradient Descent(85/999): loss=0.11563280421689566\n",
      "Stochastic Gradient Descent(86/999): loss=0.1154558154140124\n",
      "Stochastic Gradient Descent(87/999): loss=0.11528162713452896\n",
      "Stochastic Gradient Descent(88/999): loss=0.11511019424403203\n",
      "Stochastic Gradient Descent(89/999): loss=0.11494147233575554\n",
      "Stochastic Gradient Descent(90/999): loss=0.11477541771884961\n",
      "Stochastic Gradient Descent(91/999): loss=0.11461198740683873\n",
      "Stochastic Gradient Descent(92/999): loss=0.11445113910626582\n",
      "Stochastic Gradient Descent(93/999): loss=0.11429283120551938\n",
      "Stochastic Gradient Descent(94/999): loss=0.11413702276384105\n",
      "Stochastic Gradient Descent(95/999): loss=0.11398367350050999\n",
      "Stochastic Gradient Descent(96/999): loss=0.11383274378420195\n",
      "Stochastic Gradient Descent(97/999): loss=0.11368419462251972\n",
      "Stochastic Gradient Descent(98/999): loss=0.1135379876516923\n",
      "Stochastic Gradient Descent(99/999): loss=0.11339408512644057\n",
      "Stochastic Gradient Descent(100/999): loss=0.1132524499100056\n",
      "Stochastic Gradient Descent(101/999): loss=0.11311304546433855\n",
      "Stochastic Gradient Descent(102/999): loss=0.11297583584044801\n",
      "Stochastic Gradient Descent(103/999): loss=0.1128407856689035\n",
      "Stochastic Gradient Descent(104/999): loss=0.11270786015049178\n",
      "Stochastic Gradient Descent(105/999): loss=0.11257702504702372\n",
      "Stochastic Gradient Descent(106/999): loss=0.11244824667228981\n",
      "Stochastic Gradient Descent(107/999): loss=0.11232149188316098\n",
      "Stochastic Gradient Descent(108/999): loss=0.11219672807083315\n",
      "Stochastic Gradient Descent(109/999): loss=0.11207392315221301\n",
      "Stochastic Gradient Descent(110/999): loss=0.1119530455614424\n",
      "Stochastic Gradient Descent(111/999): loss=0.11183406424155963\n",
      "Stochastic Gradient Descent(112/999): loss=0.11171694863629503\n",
      "Stochastic Gradient Descent(113/999): loss=0.11160166868199896\n",
      "Stochastic Gradient Descent(114/999): loss=0.11148819479969956\n",
      "Stochastic Gradient Descent(115/999): loss=0.11137649788728923\n",
      "Stochastic Gradient Descent(116/999): loss=0.11126654931183631\n",
      "Stochastic Gradient Descent(117/999): loss=0.11115832090202132\n",
      "Stochastic Gradient Descent(118/999): loss=0.11105178494069476\n",
      "Stochastic Gradient Descent(119/999): loss=0.11094691415755505\n",
      "Stochastic Gradient Descent(120/999): loss=0.11084368172194456\n",
      "Stochastic Gradient Descent(121/999): loss=0.11074206123576162\n",
      "Stochastic Gradient Descent(122/999): loss=0.11064202672648706\n",
      "Stochastic Gradient Descent(123/999): loss=0.11054355264032267\n",
      "Stochastic Gradient Descent(124/999): loss=0.11044661383544115\n",
      "Stochastic Gradient Descent(125/999): loss=0.11035118557534385\n",
      "Stochastic Gradient Descent(126/999): loss=0.1102572435223264\n",
      "Stochastic Gradient Descent(127/999): loss=0.11016476373104929\n",
      "Stochastic Gradient Descent(128/999): loss=0.11007372264221223\n",
      "Stochastic Gradient Descent(129/999): loss=0.10998409707633047\n",
      "Stochastic Gradient Descent(130/999): loss=0.10989586422761141\n",
      "Stochastic Gradient Descent(131/999): loss=0.10980900165793002\n",
      "Stochastic Gradient Descent(132/999): loss=0.10972348729090138\n",
      "Stochastic Gradient Descent(133/999): loss=0.1096392994060486\n",
      "Stochastic Gradient Descent(134/999): loss=0.10955641663306509\n",
      "Stochastic Gradient Descent(135/999): loss=0.10947481794616898\n",
      "Stochastic Gradient Descent(136/999): loss=0.1093944826585489\n",
      "Stochastic Gradient Descent(137/999): loss=0.10931539041689894\n",
      "Stochastic Gradient Descent(138/999): loss=0.10923752119604212\n",
      "Stochastic Gradient Descent(139/999): loss=0.1091608552936402\n",
      "Stochastic Gradient Descent(140/999): loss=0.10908537332498888\n",
      "Stochastic Gradient Descent(141/999): loss=0.109011056217897\n",
      "Stochastic Gradient Descent(142/999): loss=0.10893788520764826\n",
      "Stochastic Gradient Descent(143/999): loss=0.10886584183204415\n",
      "Stochastic Gradient Descent(144/999): loss=0.10879490792652667\n",
      "Stochastic Gradient Descent(145/999): loss=0.10872506561937989\n",
      "Stochastic Gradient Descent(146/999): loss=0.10865629732700886\n",
      "Stochastic Gradient Descent(147/999): loss=0.10858858574929438\n",
      "Stochastic Gradient Descent(148/999): loss=0.10852191386502294\n",
      "Stochastic Gradient Descent(149/999): loss=0.10845626492739027\n",
      "Stochastic Gradient Descent(150/999): loss=0.10839162245957722\n",
      "Stochastic Gradient Descent(151/999): loss=0.10832797025039718\n",
      "Stochastic Gradient Descent(152/999): loss=0.10826529235001356\n",
      "Stochastic Gradient Descent(153/999): loss=0.10820357306572635\n",
      "Stochastic Gradient Descent(154/999): loss=0.10814279695782658\n",
      "Stochastic Gradient Descent(155/999): loss=0.10808294883551751\n",
      "Stochastic Gradient Descent(156/999): loss=0.108024013752902\n",
      "Stochastic Gradient Descent(157/999): loss=0.10796597700503387\n",
      "Stochastic Gradient Descent(158/999): loss=0.10790882412403359\n",
      "Stochastic Gradient Descent(159/999): loss=0.10785254087526633\n",
      "Stochastic Gradient Descent(160/999): loss=0.10779711325358136\n",
      "Stochastic Gradient Descent(161/999): loss=0.10774252747961258\n",
      "Stochastic Gradient Descent(162/999): loss=0.1076887699961384\n",
      "Stochastic Gradient Descent(163/999): loss=0.10763582746450027\n",
      "Stochastic Gradient Descent(164/999): loss=0.10758368676107899\n",
      "Stochastic Gradient Descent(165/999): loss=0.1075323349738281\n",
      "Stochastic Gradient Descent(166/999): loss=0.10748175939886263\n",
      "Stochastic Gradient Descent(167/999): loss=0.10743194753710344\n",
      "Stochastic Gradient Descent(168/999): loss=0.10738288709097506\n",
      "Stochastic Gradient Descent(169/999): loss=0.1073345659611574\n",
      "Stochastic Gradient Descent(170/999): loss=0.10728697224338912\n",
      "Stochastic Gradient Descent(171/999): loss=0.1072400942253233\n",
      "Stochastic Gradient Descent(172/999): loss=0.10719392038343309\n",
      "Stochastic Gradient Descent(173/999): loss=0.10714843937996779\n",
      "Stochastic Gradient Descent(174/999): loss=0.10710364005995773\n",
      "Stochastic Gradient Descent(175/999): loss=0.10705951144826742\n",
      "Stochastic Gradient Descent(176/999): loss=0.10701604274669636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(177/999): loss=0.10697322333112659\n",
      "Stochastic Gradient Descent(178/999): loss=0.10693104274871587\n",
      "Stochastic Gradient Descent(179/999): loss=0.10688949071513676\n",
      "Stochastic Gradient Descent(180/999): loss=0.10684855711185953\n",
      "Stochastic Gradient Descent(181/999): loss=0.10680823198347938\n",
      "Stochastic Gradient Descent(182/999): loss=0.10676850553508654\n",
      "Stochastic Gradient Descent(183/999): loss=0.10672936812967866\n",
      "Stochastic Gradient Descent(184/999): loss=0.10669081028561513\n",
      "Stochastic Gradient Descent(185/999): loss=0.10665282267411244\n",
      "Stochastic Gradient Descent(186/999): loss=0.10661539611677967\n",
      "Stochastic Gradient Descent(187/999): loss=0.106578521583194\n",
      "Stochastic Gradient Descent(188/999): loss=0.10654219018851507\n",
      "Stochastic Gradient Descent(189/999): loss=0.10650639319113817\n",
      "Stochastic Gradient Descent(190/999): loss=0.10647112199038457\n",
      "Stochastic Gradient Descent(191/999): loss=0.10643636812422995\n",
      "Stochastic Gradient Descent(192/999): loss=0.1064021232670689\n",
      "Stochastic Gradient Descent(193/999): loss=0.10636837922751546\n",
      "Stochastic Gradient Descent(194/999): loss=0.10633512794623938\n",
      "Stochastic Gradient Descent(195/999): loss=0.10630236149383708\n",
      "Stochastic Gradient Descent(196/999): loss=0.10627007206873701\n",
      "Stochastic Gradient Descent(197/999): loss=0.10623825199513884\n",
      "Stochastic Gradient Descent(198/999): loss=0.10620689372098552\n",
      "Stochastic Gradient Descent(199/999): loss=0.10617598981596874\n",
      "Stochastic Gradient Descent(200/999): loss=0.10614553296956558\n",
      "Stochastic Gradient Descent(201/999): loss=0.1061155159891078\n",
      "Stochastic Gradient Descent(202/999): loss=0.10608593179788163\n",
      "Stochastic Gradient Descent(203/999): loss=0.10605677343325831\n",
      "Stochastic Gradient Descent(204/999): loss=0.10602803404485485\n",
      "Stochastic Gradient Descent(205/999): loss=0.10599970689272446\n",
      "Stochastic Gradient Descent(206/999): loss=0.10597178534557614\n",
      "Stochastic Gradient Descent(207/999): loss=0.10594426287902287\n",
      "Stochastic Gradient Descent(208/999): loss=0.10591713307385806\n",
      "Stochastic Gradient Descent(209/999): loss=0.10589038961436008\n",
      "Stochastic Gradient Descent(210/999): loss=0.1058640262866236\n",
      "Stochastic Gradient Descent(211/999): loss=0.10583803697691833\n",
      "Stochastic Gradient Descent(212/999): loss=0.10581241567007395\n",
      "Stochastic Gradient Descent(213/999): loss=0.10578715644789101\n",
      "Stochastic Gradient Descent(214/999): loss=0.10576225348757784\n",
      "Stochastic Gradient Descent(215/999): loss=0.10573770106021212\n",
      "Stochastic Gradient Descent(216/999): loss=0.10571349352922774\n",
      "Stochastic Gradient Descent(217/999): loss=0.10568962534892572\n",
      "Stochastic Gradient Descent(218/999): loss=0.1056660910630093\n",
      "Stochastic Gradient Descent(219/999): loss=0.10564288530314263\n",
      "Stochastic Gradient Descent(220/999): loss=0.10562000278753261\n",
      "Stochastic Gradient Descent(221/999): loss=0.10559743831953382\n",
      "Stochastic Gradient Descent(222/999): loss=0.10557518678627552\n",
      "Stochastic Gradient Descent(223/999): loss=0.10555324315731128\n",
      "Stochastic Gradient Descent(224/999): loss=0.10553160248329004\n",
      "Stochastic Gradient Descent(225/999): loss=0.10551025989464867\n",
      "Stochastic Gradient Descent(226/999): loss=0.10548921060032576\n",
      "Stochastic Gradient Descent(227/999): loss=0.10546844988649581\n",
      "Stochastic Gradient Descent(228/999): loss=0.1054479731153244\n",
      "Stochastic Gradient Descent(229/999): loss=0.10542777572374264\n",
      "Stochastic Gradient Descent(230/999): loss=0.10540785322224212\n",
      "Stochastic Gradient Descent(231/999): loss=0.10538820119368891\n",
      "Stochastic Gradient Descent(232/999): loss=0.10536881529215666\n",
      "Stochastic Gradient Descent(233/999): loss=0.10534969124177866\n",
      "Stochastic Gradient Descent(234/999): loss=0.10533082483561836\n",
      "Stochastic Gradient Descent(235/999): loss=0.10531221193455807\n",
      "Stochastic Gradient Descent(236/999): loss=0.10529384846620558\n",
      "Stochastic Gradient Descent(237/999): loss=0.10527573042381849\n",
      "Stochastic Gradient Descent(238/999): loss=0.10525785386524565\n",
      "Stochastic Gradient Descent(239/999): loss=0.1052402149118861\n",
      "Stochastic Gradient Descent(240/999): loss=0.10522280974766421\n",
      "Stochastic Gradient Descent(241/999): loss=0.10520563461802186\n",
      "Stochastic Gradient Descent(242/999): loss=0.10518868582892654\n",
      "Stochastic Gradient Descent(243/999): loss=0.1051719597458958\n",
      "Stochastic Gradient Descent(244/999): loss=0.10515545279303683\n",
      "Stochastic Gradient Descent(245/999): loss=0.10513916145210224\n",
      "Stochastic Gradient Descent(246/999): loss=0.10512308226156038\n",
      "Stochastic Gradient Descent(247/999): loss=0.10510721181568128\n",
      "Stochastic Gradient Descent(248/999): loss=0.10509154676363681\n",
      "Stochastic Gradient Descent(249/999): loss=0.1050760838086157\n",
      "Stochastic Gradient Descent(250/999): loss=0.10506081970695252\n",
      "Stochastic Gradient Descent(251/999): loss=0.10504575126727107\n",
      "Stochastic Gradient Descent(252/999): loss=0.1050308753496413\n",
      "Stochastic Gradient Descent(253/999): loss=0.10501618886474977\n",
      "Stochastic Gradient Descent(254/999): loss=0.10500168877308376\n",
      "Stochastic Gradient Descent(255/999): loss=0.10498737208412848\n",
      "Stochastic Gradient Descent(256/999): loss=0.10497323585557687\n",
      "Stochastic Gradient Descent(257/999): loss=0.10495927719255263\n",
      "Stochastic Gradient Descent(258/999): loss=0.10494549324684543\n",
      "Stochastic Gradient Descent(259/999): loss=0.10493188121615861\n",
      "Stochastic Gradient Descent(260/999): loss=0.10491843834336896\n",
      "Stochastic Gradient Descent(261/999): loss=0.10490516191579848\n",
      "Stochastic Gradient Descent(262/999): loss=0.10489204926449766\n",
      "Stochastic Gradient Descent(263/999): loss=0.10487909776354079\n",
      "Stochastic Gradient Descent(264/999): loss=0.1048663048293321\n",
      "Stochastic Gradient Descent(265/999): loss=0.10485366791992341\n",
      "Stochastic Gradient Descent(266/999): loss=0.10484118453434273\n",
      "Stochastic Gradient Descent(267/999): loss=0.10482885221193354\n",
      "Stochastic Gradient Descent(268/999): loss=0.10481666853170493\n",
      "Stochastic Gradient Descent(269/999): loss=0.10480463111169211\n",
      "Stochastic Gradient Descent(270/999): loss=0.10479273760832711\n",
      "Stochastic Gradient Descent(271/999): loss=0.10478098571582002\n",
      "Stochastic Gradient Descent(272/999): loss=0.10476937316554957\n",
      "Stochastic Gradient Descent(273/999): loss=0.10475789772546426\n",
      "Stochastic Gradient Descent(274/999): loss=0.10474655719949257\n",
      "Stochastic Gradient Descent(275/999): loss=0.1047353494269629\n",
      "Stochastic Gradient Descent(276/999): loss=0.10472427228203303\n",
      "Stochastic Gradient Descent(277/999): loss=0.10471332367312838\n",
      "Stochastic Gradient Descent(278/999): loss=0.10470250154238978\n",
      "Stochastic Gradient Descent(279/999): loss=0.10469180386512968\n",
      "Stochastic Gradient Descent(280/999): loss=0.10468122864929755\n",
      "Stochastic Gradient Descent(281/999): loss=0.10467077393495357\n",
      "Stochastic Gradient Descent(282/999): loss=0.10466043779375112\n",
      "Stochastic Gradient Descent(283/999): loss=0.10465021832842732\n",
      "Stochastic Gradient Descent(284/999): loss=0.10464011367230179\n",
      "Stochastic Gradient Descent(285/999): loss=0.10463012198878391\n",
      "Stochastic Gradient Descent(286/999): loss=0.10462024147088746\n",
      "Stochastic Gradient Descent(287/999): loss=0.10461047034075338\n",
      "Stochastic Gradient Descent(288/999): loss=0.10460080684918033\n",
      "Stochastic Gradient Descent(289/999): loss=0.10459124927516252\n",
      "Stochastic Gradient Descent(290/999): loss=0.10458179592543518\n",
      "Stochastic Gradient Descent(291/999): loss=0.10457244513402741\n",
      "Stochastic Gradient Descent(292/999): loss=0.10456319526182209\n",
      "Stochastic Gradient Descent(293/999): loss=0.10455404469612287\n",
      "Stochastic Gradient Descent(294/999): loss=0.10454499185022834\n",
      "Stochastic Gradient Descent(295/999): loss=0.10453603516301292\n",
      "Stochastic Gradient Descent(296/999): loss=0.10452717309851446\n",
      "Stochastic Gradient Descent(297/999): loss=0.10451840414552874\n",
      "Stochastic Gradient Descent(298/999): loss=0.10450972681721003\n",
      "Stochastic Gradient Descent(299/999): loss=0.10450113965067863\n",
      "Stochastic Gradient Descent(300/999): loss=0.10449264120663446\n",
      "Stochastic Gradient Descent(301/999): loss=0.10448423006897682\n",
      "Stochastic Gradient Descent(302/999): loss=0.10447590484443044\n",
      "Stochastic Gradient Descent(303/999): loss=0.10446766416217748\n",
      "Stochastic Gradient Descent(304/999): loss=0.10445950667349549\n",
      "Stochastic Gradient Descent(305/999): loss=0.10445143105140116\n",
      "Stochastic Gradient Descent(306/999): loss=0.10444343599029982\n",
      "Stochastic Gradient Descent(307/999): loss=0.10443552020564081\n",
      "Stochastic Gradient Descent(308/999): loss=0.10442768243357786\n",
      "Stochastic Gradient Descent(309/999): loss=0.10441992143063565\n",
      "Stochastic Gradient Descent(310/999): loss=0.10441223597338121\n",
      "Stochastic Gradient Descent(311/999): loss=0.10440462485810077\n",
      "Stochastic Gradient Descent(312/999): loss=0.10439708690048194\n",
      "Stochastic Gradient Descent(313/999): loss=0.10438962093530105\n",
      "Stochastic Gradient Descent(314/999): loss=0.10438222581611503\n",
      "Stochastic Gradient Descent(315/999): loss=0.10437490041495895\n",
      "Stochastic Gradient Descent(316/999): loss=0.10436764362204808\n",
      "Stochastic Gradient Descent(317/999): loss=0.10436045434548463\n",
      "Stochastic Gradient Descent(318/999): loss=0.10435333151096969\n",
      "Stochastic Gradient Descent(319/999): loss=0.10434627406151906\n",
      "Stochastic Gradient Descent(320/999): loss=0.10433928095718449\n",
      "Stochastic Gradient Descent(321/999): loss=0.10433235117477888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(322/999): loss=0.10432548370760607\n",
      "Stochastic Gradient Descent(323/999): loss=0.10431867756519493\n",
      "Stochastic Gradient Descent(324/999): loss=0.10431193177303792\n",
      "Stochastic Gradient Descent(325/999): loss=0.10430524537233356\n",
      "Stochastic Gradient Descent(326/999): loss=0.10429861741973333\n",
      "Stochastic Gradient Descent(327/999): loss=0.10429204698709253\n",
      "Stochastic Gradient Descent(328/999): loss=0.10428553316122512\n",
      "Stochastic Gradient Descent(329/999): loss=0.10427907504366259\n",
      "Stochastic Gradient Descent(330/999): loss=0.10427267175041675\n",
      "Stochastic Gradient Descent(331/999): loss=0.10426632241174616\n",
      "Stochastic Gradient Descent(332/999): loss=0.10426002617192645\n",
      "Stochastic Gradient Descent(333/999): loss=0.10425378218902448\n",
      "Stochastic Gradient Descent(334/999): loss=0.10424758963467595\n",
      "Stochastic Gradient Descent(335/999): loss=0.10424144769386653\n",
      "Stochastic Gradient Descent(336/999): loss=0.10423535556471675\n",
      "Stochastic Gradient Descent(337/999): loss=0.10422931245827016\n",
      "Stochastic Gradient Descent(338/999): loss=0.1042233175982852\n",
      "Stochastic Gradient Descent(339/999): loss=0.10421737022102984\n",
      "Stochastic Gradient Descent(340/999): loss=0.10421146957508023\n",
      "Stochastic Gradient Descent(341/999): loss=0.10420561492112222\n",
      "Stochastic Gradient Descent(342/999): loss=0.10419980553175594\n",
      "Stochastic Gradient Descent(343/999): loss=0.10419404069130389\n",
      "Stochastic Gradient Descent(344/999): loss=0.10418831969562199\n",
      "Stochastic Gradient Descent(345/999): loss=0.10418264185191348\n",
      "Stochastic Gradient Descent(346/999): loss=0.10417700647854608\n",
      "Stochastic Gradient Descent(347/999): loss=0.10417141290487204\n",
      "Stochastic Gradient Descent(348/999): loss=0.10416586047105093\n",
      "Stochastic Gradient Descent(349/999): loss=0.10416034852787552\n",
      "Stochastic Gradient Descent(350/999): loss=0.10415487643660029\n",
      "Stochastic Gradient Descent(351/999): loss=0.10414944356877283\n",
      "Stochastic Gradient Descent(352/999): loss=0.10414404930606776\n",
      "Stochastic Gradient Descent(353/999): loss=0.10413869304012367\n",
      "Stochastic Gradient Descent(354/999): loss=0.1041333741723823\n",
      "Stochastic Gradient Descent(355/999): loss=0.10412809211393065\n",
      "Stochastic Gradient Descent(356/999): loss=0.10412284628534527\n",
      "Stochastic Gradient Descent(357/999): loss=0.10411763611653953\n",
      "Stochastic Gradient Descent(358/999): loss=0.10411246104661291\n",
      "Stochastic Gradient Descent(359/999): loss=0.10410732052370303\n",
      "Stochastic Gradient Descent(360/999): loss=0.1041022140048398\n",
      "Stochastic Gradient Descent(361/999): loss=0.10409714095580214\n",
      "Stochastic Gradient Descent(362/999): loss=0.10409210085097698\n",
      "Stochastic Gradient Descent(363/999): loss=0.10408709317322039\n",
      "Stochastic Gradient Descent(364/999): loss=0.10408211741372106\n",
      "Stochastic Gradient Descent(365/999): loss=0.10407717307186612\n",
      "Stochastic Gradient Descent(366/999): loss=0.10407225965510862\n",
      "Stochastic Gradient Descent(367/999): loss=0.10406737667883793\n",
      "Stochastic Gradient Descent(368/999): loss=0.10406252366625139\n",
      "Stochastic Gradient Descent(369/999): loss=0.1040577001482288\n",
      "Stochastic Gradient Descent(370/999): loss=0.10405290566320823\n",
      "Stochastic Gradient Descent(371/999): loss=0.10404813975706434\n",
      "Stochastic Gradient Descent(372/999): loss=0.10404340198298852\n",
      "Stochastic Gradient Descent(373/999): loss=0.10403869190137094\n",
      "Stochastic Gradient Descent(374/999): loss=0.10403400907968426\n",
      "Stochastic Gradient Descent(375/999): loss=0.10402935309236981\n",
      "Stochastic Gradient Descent(376/999): loss=0.10402472352072491\n",
      "Stochastic Gradient Descent(377/999): loss=0.10402011995279259\n",
      "Stochastic Gradient Descent(378/999): loss=0.10401554198325275\n",
      "Stochastic Gradient Descent(379/999): loss=0.10401098921331499\n",
      "Stochastic Gradient Descent(380/999): loss=0.10400646125061369\n",
      "Stochastic Gradient Descent(381/999): loss=0.10400195770910418\n",
      "Stochastic Gradient Descent(382/999): loss=0.10399747820896088\n",
      "Stochastic Gradient Descent(383/999): loss=0.103993022376477\n",
      "Stochastic Gradient Descent(384/999): loss=0.10398858984396611\n",
      "Stochastic Gradient Descent(385/999): loss=0.10398418024966476\n",
      "Stochastic Gradient Descent(386/999): loss=0.10397979323763729\n",
      "Stochastic Gradient Descent(387/999): loss=0.10397542845768164\n",
      "Stochastic Gradient Descent(388/999): loss=0.10397108556523707\n",
      "Stochastic Gradient Descent(389/999): loss=0.1039667642212932\n",
      "Stochastic Gradient Descent(390/999): loss=0.10396246409230038\n",
      "Stochastic Gradient Descent(391/999): loss=0.10395818485008188\n",
      "Stochastic Gradient Descent(392/999): loss=0.10395392617174712\n",
      "Stochastic Gradient Descent(393/999): loss=0.10394968773960643\n",
      "Stochastic Gradient Descent(394/999): loss=0.10394546924108736\n",
      "Stochastic Gradient Descent(395/999): loss=0.10394127036865197\n",
      "Stochastic Gradient Descent(396/999): loss=0.10393709081971592\n",
      "Stochastic Gradient Descent(397/999): loss=0.10393293029656832\n",
      "Stochastic Gradient Descent(398/999): loss=0.10392878850629338\n",
      "Stochastic Gradient Descent(399/999): loss=0.10392466516069306\n",
      "Stochastic Gradient Descent(400/999): loss=0.10392055997621094\n",
      "Stochastic Gradient Descent(401/999): loss=0.10391647267385751\n",
      "Stochastic Gradient Descent(402/999): loss=0.10391240297913633\n",
      "Stochastic Gradient Descent(403/999): loss=0.1039083506219719\n",
      "Stochastic Gradient Descent(404/999): loss=0.10390431533663815\n",
      "Stochastic Gradient Descent(405/999): loss=0.10390029686168839\n",
      "Stochastic Gradient Descent(406/999): loss=0.10389629493988638\n",
      "Stochastic Gradient Descent(407/999): loss=0.1038923093181383\n",
      "Stochastic Gradient Descent(408/999): loss=0.10388833974742621\n",
      "Stochastic Gradient Descent(409/999): loss=0.10388438598274208\n",
      "Stochastic Gradient Descent(410/999): loss=0.10388044778302329\n",
      "Stochastic Gradient Descent(411/999): loss=0.10387652491108902\n",
      "Stochastic Gradient Descent(412/999): loss=0.10387261713357768\n",
      "Stochastic Gradient Descent(413/999): loss=0.10386872422088521\n",
      "Stochastic Gradient Descent(414/999): loss=0.10386484594710467\n",
      "Stochastic Gradient Descent(415/999): loss=0.10386098208996654\n",
      "Stochastic Gradient Descent(416/999): loss=0.10385713243078008\n",
      "Stochastic Gradient Descent(417/999): loss=0.10385329675437575\n",
      "Stochastic Gradient Descent(418/999): loss=0.10384947484904827\n",
      "Stochastic Gradient Descent(419/999): loss=0.1038456665065009\n",
      "Stochastic Gradient Descent(420/999): loss=0.10384187152179039\n",
      "Stochastic Gradient Descent(421/999): loss=0.10383808969327309\n",
      "Stochastic Gradient Descent(422/999): loss=0.10383432082255142\n",
      "Stochastic Gradient Descent(423/999): loss=0.10383056471442202\n",
      "Stochastic Gradient Descent(424/999): loss=0.10382682117682383\n",
      "Stochastic Gradient Descent(425/999): loss=0.10382309002078766\n",
      "Stochastic Gradient Descent(426/999): loss=0.10381937106038615\n",
      "Stochastic Gradient Descent(427/999): loss=0.1038156641126851\n",
      "Stochastic Gradient Descent(428/999): loss=0.10381196899769472\n",
      "Stochastic Gradient Descent(429/999): loss=0.10380828553832261\n",
      "Stochastic Gradient Descent(430/999): loss=0.1038046135603268\n",
      "Stochastic Gradient Descent(431/999): loss=0.10380095289226983\n",
      "Stochastic Gradient Descent(432/999): loss=0.10379730336547359\n",
      "Stochastic Gradient Descent(433/999): loss=0.10379366481397476\n",
      "Stochastic Gradient Descent(434/999): loss=0.10379003707448121\n",
      "Stochastic Gradient Descent(435/999): loss=0.10378641998632868\n",
      "Stochastic Gradient Descent(436/999): loss=0.10378281339143873\n",
      "Stochastic Gradient Descent(437/999): loss=0.1037792171342767\n",
      "Stochastic Gradient Descent(438/999): loss=0.10377563106181101\n",
      "Stochastic Gradient Descent(439/999): loss=0.10377205502347257\n",
      "Stochastic Gradient Descent(440/999): loss=0.10376848887111535\n",
      "Stochastic Gradient Descent(441/999): loss=0.10376493245897704\n",
      "Stochastic Gradient Descent(442/999): loss=0.10376138564364078\n",
      "Stochastic Gradient Descent(443/999): loss=0.10375784828399723\n",
      "Stochastic Gradient Descent(444/999): loss=0.1037543202412076\n",
      "Stochastic Gradient Descent(445/999): loss=0.10375080137866684\n",
      "Stochastic Gradient Descent(446/999): loss=0.10374729156196755\n",
      "Stochastic Gradient Descent(447/999): loss=0.10374379065886492\n",
      "Stochastic Gradient Descent(448/999): loss=0.10374029853924142\n",
      "Stochastic Gradient Descent(449/999): loss=0.10373681507507289\n",
      "Stochastic Gradient Descent(450/999): loss=0.10373334014039451\n",
      "Stochastic Gradient Descent(451/999): loss=0.1037298736112678\n",
      "Stochastic Gradient Descent(452/999): loss=0.10372641536574777\n",
      "Stochastic Gradient Descent(453/999): loss=0.10372296528385096\n",
      "Stochastic Gradient Descent(454/999): loss=0.10371952324752366\n",
      "Stochastic Gradient Descent(455/999): loss=0.10371608914061105\n",
      "Stochastic Gradient Descent(456/999): loss=0.10371266284882617\n",
      "Stochastic Gradient Descent(457/999): loss=0.1037092442597203\n",
      "Stochastic Gradient Descent(458/999): loss=0.10370583326265288\n",
      "Stochastic Gradient Descent(459/999): loss=0.10370242974876265\n",
      "Stochastic Gradient Descent(460/999): loss=0.10369903361093893\n",
      "Stochastic Gradient Descent(461/999): loss=0.1036956447437933\n",
      "Stochastic Gradient Descent(462/999): loss=0.10369226304363187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(463/999): loss=0.10368888840842794\n",
      "Stochastic Gradient Descent(464/999): loss=0.10368552073779527\n",
      "Stochastic Gradient Descent(465/999): loss=0.10368215993296137\n",
      "Stochastic Gradient Descent(466/999): loss=0.10367880589674186\n",
      "Stochastic Gradient Descent(467/999): loss=0.10367545853351443\n",
      "Stochastic Gradient Descent(468/999): loss=0.10367211774919398\n",
      "Stochastic Gradient Descent(469/999): loss=0.10366878345120774\n",
      "Stochastic Gradient Descent(470/999): loss=0.10366545554847084\n",
      "Stochastic Gradient Descent(471/999): loss=0.1036621339513624\n",
      "Stochastic Gradient Descent(472/999): loss=0.1036588185717019\n",
      "Stochastic Gradient Descent(473/999): loss=0.10365550932272592\n",
      "Stochastic Gradient Descent(474/999): loss=0.10365220611906542\n",
      "Stochastic Gradient Descent(475/999): loss=0.10364890887672308\n",
      "Stochastic Gradient Descent(476/999): loss=0.10364561751305136\n",
      "Stochastic Gradient Descent(477/999): loss=0.1036423319467306\n",
      "Stochastic Gradient Descent(478/999): loss=0.10363905209774771\n",
      "Stochastic Gradient Descent(479/999): loss=0.10363577788737512\n",
      "Stochastic Gradient Descent(480/999): loss=0.1036325092381499\n",
      "Stochastic Gradient Descent(481/999): loss=0.10362924607385357\n",
      "Stochastic Gradient Descent(482/999): loss=0.10362598831949198\n",
      "Stochastic Gradient Descent(483/999): loss=0.10362273590127542\n",
      "Stochastic Gradient Descent(484/999): loss=0.10361948874659953\n",
      "Stochastic Gradient Descent(485/999): loss=0.10361624678402574\n",
      "Stochastic Gradient Descent(486/999): loss=0.10361300994326297\n",
      "Stochastic Gradient Descent(487/999): loss=0.10360977815514877\n",
      "Stochastic Gradient Descent(488/999): loss=0.10360655135163124\n",
      "Stochastic Gradient Descent(489/999): loss=0.10360332946575117\n",
      "Stochastic Gradient Descent(490/999): loss=0.1036001124316244\n",
      "Stochastic Gradient Descent(491/999): loss=0.10359690018442445\n",
      "Stochastic Gradient Descent(492/999): loss=0.10359369266036553\n",
      "Stochastic Gradient Descent(493/999): loss=0.10359048979668575\n",
      "Stochastic Gradient Descent(494/999): loss=0.10358729153163049\n",
      "Stochastic Gradient Descent(495/999): loss=0.10358409780443645\n",
      "Stochastic Gradient Descent(496/999): loss=0.10358090855531531\n",
      "Stochastic Gradient Descent(497/999): loss=0.10357772372543837\n",
      "Stochastic Gradient Descent(498/999): loss=0.10357454325692075\n",
      "Stochastic Gradient Descent(499/999): loss=0.10357136709280648\n",
      "Stochastic Gradient Descent(500/999): loss=0.1035681951770532\n",
      "Stochastic Gradient Descent(501/999): loss=0.10356502745451775\n",
      "Stochastic Gradient Descent(502/999): loss=0.10356186387094149\n",
      "Stochastic Gradient Descent(503/999): loss=0.10355870437293614\n",
      "Stochastic Gradient Descent(504/999): loss=0.10355554890796953\n",
      "Stochastic Gradient Descent(505/999): loss=0.1035523974243522\n",
      "Stochastic Gradient Descent(506/999): loss=0.10354924987122344\n",
      "Stochastic Gradient Descent(507/999): loss=0.10354610619853803\n",
      "Stochastic Gradient Descent(508/999): loss=0.1035429663570533\n",
      "Stochastic Gradient Descent(509/999): loss=0.1035398302983159\n",
      "Stochastic Gradient Descent(510/999): loss=0.10353669797464922\n",
      "Stochastic Gradient Descent(511/999): loss=0.1035335693391409\n",
      "Stochastic Gradient Descent(512/999): loss=0.10353044434563045\n",
      "Stochastic Gradient Descent(513/999): loss=0.10352732294869714\n",
      "Stochastic Gradient Descent(514/999): loss=0.10352420510364806\n",
      "Stochastic Gradient Descent(515/999): loss=0.10352109076650647\n",
      "Stochastic Gradient Descent(516/999): loss=0.10351797989400013\n",
      "Stochastic Gradient Descent(517/999): loss=0.10351487244355008\n",
      "Stochastic Gradient Descent(518/999): loss=0.1035117683732593\n",
      "Stochastic Gradient Descent(519/999): loss=0.10350866764190192\n",
      "Stochastic Gradient Descent(520/999): loss=0.1035055702089122\n",
      "Stochastic Gradient Descent(521/999): loss=0.10350247603437406\n",
      "Stochastic Gradient Descent(522/999): loss=0.10349938507901044\n",
      "Stochastic Gradient Descent(523/999): loss=0.10349629730417319\n",
      "Stochastic Gradient Descent(524/999): loss=0.10349321267183278\n",
      "Stochastic Gradient Descent(525/999): loss=0.10349013114456837\n",
      "Stochastic Gradient Descent(526/999): loss=0.10348705268555804\n",
      "Stochastic Gradient Descent(527/999): loss=0.1034839772585691\n",
      "Stochastic Gradient Descent(528/999): loss=0.10348090482794856\n",
      "Stochastic Gradient Descent(529/999): loss=0.10347783535861385\n",
      "Stochastic Gradient Descent(530/999): loss=0.10347476881604364\n",
      "Stochastic Gradient Descent(531/999): loss=0.10347170516626872\n",
      "Stochastic Gradient Descent(532/999): loss=0.10346864437586312\n",
      "Stochastic Gradient Descent(533/999): loss=0.10346558641193532\n",
      "Stochastic Gradient Descent(534/999): loss=0.10346253124211977\n",
      "Stochastic Gradient Descent(535/999): loss=0.1034594788345683\n",
      "Stochastic Gradient Descent(536/999): loss=0.10345642915794183\n",
      "Stochastic Gradient Descent(537/999): loss=0.10345338218140203\n",
      "Stochastic Gradient Descent(538/999): loss=0.10345033787460352\n",
      "Stochastic Gradient Descent(539/999): loss=0.1034472962076856\n",
      "Stochastic Gradient Descent(540/999): loss=0.10344425715126465\n",
      "Stochastic Gradient Descent(541/999): loss=0.10344122067642651\n",
      "Stochastic Gradient Descent(542/999): loss=0.10343818675471868\n",
      "Stochastic Gradient Descent(543/999): loss=0.10343515535814295\n",
      "Stochastic Gradient Descent(544/999): loss=0.10343212645914822\n",
      "Stochastic Gradient Descent(545/999): loss=0.10342910003062313\n",
      "Stochastic Gradient Descent(546/999): loss=0.10342607604588906\n",
      "Stochastic Gradient Descent(547/999): loss=0.10342305447869317\n",
      "Stochastic Gradient Descent(548/999): loss=0.10342003530320139\n",
      "Stochastic Gradient Descent(549/999): loss=0.10341701849399187\n",
      "Stochastic Gradient Descent(550/999): loss=0.10341400402604826\n",
      "Stochastic Gradient Descent(551/999): loss=0.1034109918747531\n",
      "Stochastic Gradient Descent(552/999): loss=0.1034079820158814\n",
      "Stochastic Gradient Descent(553/999): loss=0.1034049744255945\n",
      "Stochastic Gradient Descent(554/999): loss=0.10340196908043361\n",
      "Stochastic Gradient Descent(555/999): loss=0.10339896595731385\n",
      "Stochastic Gradient Descent(556/999): loss=0.10339596503351808\n",
      "Stochastic Gradient Descent(557/999): loss=0.10339296628669108\n",
      "Stochastic Gradient Descent(558/999): loss=0.10338996969483373\n",
      "Stochastic Gradient Descent(559/999): loss=0.10338697523629718\n",
      "Stochastic Gradient Descent(560/999): loss=0.10338398288977727\n",
      "Stochastic Gradient Descent(561/999): loss=0.10338099263430897\n",
      "Stochastic Gradient Descent(562/999): loss=0.10337800444926086\n",
      "Stochastic Gradient Descent(563/999): loss=0.10337501831432988\n",
      "Stochastic Gradient Descent(564/999): loss=0.10337203420953583\n",
      "Stochastic Gradient Descent(565/999): loss=0.10336905211521641\n",
      "Stochastic Gradient Descent(566/999): loss=0.10336607201202194\n",
      "Stochastic Gradient Descent(567/999): loss=0.10336309388091025\n",
      "Stochastic Gradient Descent(568/999): loss=0.10336011770314205\n",
      "Stochastic Gradient Descent(569/999): loss=0.1033571434602756\n",
      "Stochastic Gradient Descent(570/999): loss=0.10335417113416233\n",
      "Stochastic Gradient Descent(571/999): loss=0.10335120070694177\n",
      "Stochastic Gradient Descent(572/999): loss=0.10334823216103724\n",
      "Stochastic Gradient Descent(573/999): loss=0.10334526547915093\n",
      "Stochastic Gradient Descent(574/999): loss=0.10334230064425975\n",
      "Stochastic Gradient Descent(575/999): loss=0.10333933763961059\n",
      "Stochastic Gradient Descent(576/999): loss=0.10333637644871613\n",
      "Stochastic Gradient Descent(577/999): loss=0.10333341705535067\n",
      "Stochastic Gradient Descent(578/999): loss=0.10333045944354566\n",
      "Stochastic Gradient Descent(579/999): loss=0.10332750359758565\n",
      "Stochastic Gradient Descent(580/999): loss=0.10332454950200433\n",
      "Stochastic Gradient Descent(581/999): loss=0.10332159714158042\n",
      "Stochastic Gradient Descent(582/999): loss=0.10331864650133366\n",
      "Stochastic Gradient Descent(583/999): loss=0.10331569756652102\n",
      "Stochastic Gradient Descent(584/999): loss=0.1033127503226329\n",
      "Stochastic Gradient Descent(585/999): loss=0.10330980475538923\n",
      "Stochastic Gradient Descent(586/999): loss=0.10330686085073591\n",
      "Stochastic Gradient Descent(587/999): loss=0.10330391859484112\n",
      "Stochastic Gradient Descent(588/999): loss=0.10330097797409174\n",
      "Stochastic Gradient Descent(589/999): loss=0.10329803897508973\n",
      "Stochastic Gradient Descent(590/999): loss=0.1032951015846489\n",
      "Stochastic Gradient Descent(591/999): loss=0.10329216578979125\n",
      "Stochastic Gradient Descent(592/999): loss=0.10328923157774375\n",
      "Stochastic Gradient Descent(593/999): loss=0.10328629893593504\n",
      "Stochastic Gradient Descent(594/999): loss=0.10328336785199205\n",
      "Stochastic Gradient Descent(595/999): loss=0.10328043831373694\n",
      "Stochastic Gradient Descent(596/999): loss=0.10327751030918399\n",
      "Stochastic Gradient Descent(597/999): loss=0.10327458382653629\n",
      "Stochastic Gradient Descent(598/999): loss=0.10327165885418295\n",
      "Stochastic Gradient Descent(599/999): loss=0.10326873538069599\n",
      "Stochastic Gradient Descent(600/999): loss=0.10326581339482725\n",
      "Stochastic Gradient Descent(601/999): loss=0.10326289288550575\n",
      "Stochastic Gradient Descent(602/999): loss=0.10325997384183475\n",
      "Stochastic Gradient Descent(603/999): loss=0.10325705625308883\n",
      "Stochastic Gradient Descent(604/999): loss=0.10325414010871121\n",
      "Stochastic Gradient Descent(605/999): loss=0.10325122539831112\n",
      "Stochastic Gradient Descent(606/999): loss=0.10324831211166094\n",
      "Stochastic Gradient Descent(607/999): loss=0.10324540023869372\n",
      "Stochastic Gradient Descent(608/999): loss=0.1032424897695005\n",
      "Stochastic Gradient Descent(609/999): loss=0.10323958069432787\n",
      "Stochastic Gradient Descent(610/999): loss=0.10323667300357539\n",
      "Stochastic Gradient Descent(611/999): loss=0.10323376668779305\n",
      "Stochastic Gradient Descent(612/999): loss=0.10323086173767904\n",
      "Stochastic Gradient Descent(613/999): loss=0.1032279581440772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(614/999): loss=0.10322505589797472\n",
      "Stochastic Gradient Descent(615/999): loss=0.10322215499049986\n",
      "Stochastic Gradient Descent(616/999): loss=0.10321925541291967\n",
      "Stochastic Gradient Descent(617/999): loss=0.10321635715663766\n",
      "Stochastic Gradient Descent(618/999): loss=0.1032134602131918\n",
      "Stochastic Gradient Descent(619/999): loss=0.10321056457425218\n",
      "Stochastic Gradient Descent(620/999): loss=0.10320767023161885\n",
      "Stochastic Gradient Descent(621/999): loss=0.10320477717721999\n",
      "Stochastic Gradient Descent(622/999): loss=0.10320188540310948\n",
      "Stochastic Gradient Descent(623/999): loss=0.10319899490146525\n",
      "Stochastic Gradient Descent(624/999): loss=0.10319610566458697\n",
      "Stochastic Gradient Descent(625/999): loss=0.10319321768489419\n",
      "Stochastic Gradient Descent(626/999): loss=0.10319033095492457\n",
      "Stochastic Gradient Descent(627/999): loss=0.10318744546733176\n",
      "Stochastic Gradient Descent(628/999): loss=0.10318456121488352\n",
      "Stochastic Gradient Descent(629/999): loss=0.10318167819046017\n",
      "Stochastic Gradient Descent(630/999): loss=0.10317879638705245\n",
      "Stochastic Gradient Descent(631/999): loss=0.10317591579775989\n",
      "Stochastic Gradient Descent(632/999): loss=0.10317303641578908\n",
      "Stochastic Gradient Descent(633/999): loss=0.10317015823445189\n",
      "Stochastic Gradient Descent(634/999): loss=0.10316728124716375\n",
      "Stochastic Gradient Descent(635/999): loss=0.10316440544744208\n",
      "Stochastic Gradient Descent(636/999): loss=0.10316153082890456\n",
      "Stochastic Gradient Descent(637/999): loss=0.10315865738526754\n",
      "Stochastic Gradient Descent(638/999): loss=0.1031557851103445\n",
      "Stochastic Gradient Descent(639/999): loss=0.10315291399804441\n",
      "Stochastic Gradient Descent(640/999): loss=0.10315004404237019\n",
      "Stochastic Gradient Descent(641/999): loss=0.10314717523741727\n",
      "Stochastic Gradient Descent(642/999): loss=0.10314430757737211\n",
      "Stochastic Gradient Descent(643/999): loss=0.10314144105651059\n",
      "Stochastic Gradient Descent(644/999): loss=0.10313857566919668\n",
      "Stochastic Gradient Descent(645/999): loss=0.10313571140988112\n",
      "Stochastic Gradient Descent(646/999): loss=0.10313284827309976\n",
      "Stochastic Gradient Descent(647/999): loss=0.10312998625347243\n",
      "Stochastic Gradient Descent(648/999): loss=0.10312712534570144\n",
      "Stochastic Gradient Descent(649/999): loss=0.1031242655445704\n",
      "Stochastic Gradient Descent(650/999): loss=0.10312140684494271\n",
      "Stochastic Gradient Descent(651/999): loss=0.10311854924176043\n",
      "Stochastic Gradient Descent(652/999): loss=0.10311569273004302\n",
      "Stochastic Gradient Descent(653/999): loss=0.10311283730488596\n",
      "Stochastic Gradient Descent(654/999): loss=0.10310998296145964\n",
      "Stochastic Gradient Descent(655/999): loss=0.10310712969500814\n",
      "Stochastic Gradient Descent(656/999): loss=0.10310427750084801\n",
      "Stochastic Gradient Descent(657/999): loss=0.1031014263743671\n",
      "Stochastic Gradient Descent(658/999): loss=0.10309857631102347\n",
      "Stochastic Gradient Descent(659/999): loss=0.10309572730634423\n",
      "Stochastic Gradient Descent(660/999): loss=0.10309287935592432\n",
      "Stochastic Gradient Descent(661/999): loss=0.10309003245542565\n",
      "Stochastic Gradient Descent(662/999): loss=0.1030871866005758\n",
      "Stochastic Gradient Descent(663/999): loss=0.10308434178716708\n",
      "Stochastic Gradient Descent(664/999): loss=0.10308149801105543\n",
      "Stochastic Gradient Descent(665/999): loss=0.10307865526815949\n",
      "Stochastic Gradient Descent(666/999): loss=0.10307581355445951\n",
      "Stochastic Gradient Descent(667/999): loss=0.10307297286599633\n",
      "Stochastic Gradient Descent(668/999): loss=0.10307013319887044\n",
      "Stochastic Gradient Descent(669/999): loss=0.1030672945492411\n",
      "Stochastic Gradient Descent(670/999): loss=0.10306445691332533\n",
      "Stochastic Gradient Descent(671/999): loss=0.10306162028739684\n",
      "Stochastic Gradient Descent(672/999): loss=0.1030587846677854\n",
      "Stochastic Gradient Descent(673/999): loss=0.10305595005087574\n",
      "Stochastic Gradient Descent(674/999): loss=0.10305311643310665\n",
      "Stochastic Gradient Descent(675/999): loss=0.10305028381097033\n",
      "Stochastic Gradient Descent(676/999): loss=0.10304745218101116\n",
      "Stochastic Gradient Descent(677/999): loss=0.10304462153982531\n",
      "Stochastic Gradient Descent(678/999): loss=0.1030417918840595\n",
      "Stochastic Gradient Descent(679/999): loss=0.10303896321041042\n",
      "Stochastic Gradient Descent(680/999): loss=0.10303613551562382\n",
      "Stochastic Gradient Descent(681/999): loss=0.10303330879649392\n",
      "Stochastic Gradient Descent(682/999): loss=0.10303048304986222\n",
      "Stochastic Gradient Descent(683/999): loss=0.10302765827261724\n",
      "Stochastic Gradient Descent(684/999): loss=0.1030248344616934\n",
      "Stochastic Gradient Descent(685/999): loss=0.1030220116140704\n",
      "Stochastic Gradient Descent(686/999): loss=0.1030191897267725\n",
      "Stochastic Gradient Descent(687/999): loss=0.10301636879686785\n",
      "Stochastic Gradient Descent(688/999): loss=0.10301354882146763\n",
      "Stochastic Gradient Descent(689/999): loss=0.10301072979772553\n",
      "Stochastic Gradient Descent(690/999): loss=0.10300791172283695\n",
      "Stochastic Gradient Descent(691/999): loss=0.10300509459403848\n",
      "Stochastic Gradient Descent(692/999): loss=0.103002278408607\n",
      "Stochastic Gradient Descent(693/999): loss=0.10299946316385918\n",
      "Stochastic Gradient Descent(694/999): loss=0.10299664885715085\n",
      "Stochastic Gradient Descent(695/999): loss=0.10299383548587637\n",
      "Stochastic Gradient Descent(696/999): loss=0.10299102304746796\n",
      "Stochastic Gradient Descent(697/999): loss=0.1029882115393951\n",
      "Stochastic Gradient Descent(698/999): loss=0.10298540095916395\n",
      "Stochastic Gradient Descent(699/999): loss=0.10298259130431675\n",
      "Stochastic Gradient Descent(700/999): loss=0.10297978257243129\n",
      "Stochastic Gradient Descent(701/999): loss=0.10297697476112022\n",
      "Stochastic Gradient Descent(702/999): loss=0.10297416786803062\n",
      "Stochastic Gradient Descent(703/999): loss=0.10297136189084333\n",
      "Stochastic Gradient Descent(704/999): loss=0.1029685568272726\n",
      "Stochastic Gradient Descent(705/999): loss=0.10296575267506523\n",
      "Stochastic Gradient Descent(706/999): loss=0.10296294943200039\n",
      "Stochastic Gradient Descent(707/999): loss=0.10296014709588885\n",
      "Stochastic Gradient Descent(708/999): loss=0.10295734566457261\n",
      "Stochastic Gradient Descent(709/999): loss=0.10295454513592438\n",
      "Stochastic Gradient Descent(710/999): loss=0.10295174550784704\n",
      "Stochastic Gradient Descent(711/999): loss=0.10294894677827311\n",
      "Stochastic Gradient Descent(712/999): loss=0.10294614894516449\n",
      "Stochastic Gradient Descent(713/999): loss=0.10294335200651172\n",
      "Stochastic Gradient Descent(714/999): loss=0.10294055596033365\n",
      "Stochastic Gradient Descent(715/999): loss=0.10293776080467698\n",
      "Stochastic Gradient Descent(716/999): loss=0.10293496653761593\n",
      "Stochastic Gradient Descent(717/999): loss=0.10293217315725148\n",
      "Stochastic Gradient Descent(718/999): loss=0.10292938066171123\n",
      "Stochastic Gradient Descent(719/999): loss=0.10292658904914892\n",
      "Stochastic Gradient Descent(720/999): loss=0.10292379831774387\n",
      "Stochastic Gradient Descent(721/999): loss=0.10292100846570074\n",
      "Stochastic Gradient Descent(722/999): loss=0.10291821949124902\n",
      "Stochastic Gradient Descent(723/999): loss=0.1029154313926427\n",
      "Stochastic Gradient Descent(724/999): loss=0.10291264416815973\n",
      "Stochastic Gradient Descent(725/999): loss=0.10290985781610192\n",
      "Stochastic Gradient Descent(726/999): loss=0.10290707233479424\n",
      "Stochastic Gradient Descent(727/999): loss=0.10290428772258457\n",
      "Stochastic Gradient Descent(728/999): loss=0.10290150397784345\n",
      "Stochastic Gradient Descent(729/999): loss=0.10289872109896361\n",
      "Stochastic Gradient Descent(730/999): loss=0.1028959390843595\n",
      "Stochastic Gradient Descent(731/999): loss=0.10289315793246721\n",
      "Stochastic Gradient Descent(732/999): loss=0.10289037764174389\n",
      "Stochastic Gradient Descent(733/999): loss=0.10288759821066752\n",
      "Stochastic Gradient Descent(734/999): loss=0.10288481963773656\n",
      "Stochastic Gradient Descent(735/999): loss=0.1028820419214695\n",
      "Stochastic Gradient Descent(736/999): loss=0.10287926506040491\n",
      "Stochastic Gradient Descent(737/999): loss=0.1028764890531006\n",
      "Stochastic Gradient Descent(738/999): loss=0.10287371389813371\n",
      "Stochastic Gradient Descent(739/999): loss=0.1028709395941002\n",
      "Stochastic Gradient Descent(740/999): loss=0.10286816613961466\n",
      "Stochastic Gradient Descent(741/999): loss=0.10286539353330985\n",
      "Stochastic Gradient Descent(742/999): loss=0.10286262177383676\n",
      "Stochastic Gradient Descent(743/999): loss=0.10285985085986375\n",
      "Stochastic Gradient Descent(744/999): loss=0.1028570807900769\n",
      "Stochastic Gradient Descent(745/999): loss=0.10285431156317919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(746/999): loss=0.10285154317789062\n",
      "Stochastic Gradient Descent(747/999): loss=0.10284877563294771\n",
      "Stochastic Gradient Descent(748/999): loss=0.10284600892710334\n",
      "Stochastic Gradient Descent(749/999): loss=0.10284324305912643\n",
      "Stochastic Gradient Descent(750/999): loss=0.10284047802780165\n",
      "Stochastic Gradient Descent(751/999): loss=0.10283771383192933\n",
      "Stochastic Gradient Descent(752/999): loss=0.10283495047032506\n",
      "Stochastic Gradient Descent(753/999): loss=0.10283218794181945\n",
      "Stochastic Gradient Descent(754/999): loss=0.10282942624525798\n",
      "Stochastic Gradient Descent(755/999): loss=0.10282666537950072\n",
      "Stochastic Gradient Descent(756/999): loss=0.10282390534342199\n",
      "Stochastic Gradient Descent(757/999): loss=0.10282114613591034\n",
      "Stochastic Gradient Descent(758/999): loss=0.10281838775586813\n",
      "Stochastic Gradient Descent(759/999): loss=0.10281563020221149\n",
      "Stochastic Gradient Descent(760/999): loss=0.10281287347386985\n",
      "Stochastic Gradient Descent(761/999): loss=0.102810117569786\n",
      "Stochastic Gradient Descent(762/999): loss=0.10280736248891566\n",
      "Stochastic Gradient Descent(763/999): loss=0.10280460823022744\n",
      "Stochastic Gradient Descent(764/999): loss=0.10280185479270255\n",
      "Stochastic Gradient Descent(765/999): loss=0.10279910217533453\n",
      "Stochastic Gradient Descent(766/999): loss=0.10279635037712923\n",
      "Stochastic Gradient Descent(767/999): loss=0.10279359939710446\n",
      "Stochastic Gradient Descent(768/999): loss=0.10279084923428981\n",
      "Stochastic Gradient Descent(769/999): loss=0.1027880998877266\n",
      "Stochastic Gradient Descent(770/999): loss=0.10278535135646752\n",
      "Stochastic Gradient Descent(771/999): loss=0.10278260363957654\n",
      "Stochastic Gradient Descent(772/999): loss=0.10277985673612876\n",
      "Stochastic Gradient Descent(773/999): loss=0.10277711064521008\n",
      "Stochastic Gradient Descent(774/999): loss=0.10277436536591728\n",
      "Stochastic Gradient Descent(775/999): loss=0.10277162089735752\n",
      "Stochastic Gradient Descent(776/999): loss=0.1027688772386485\n",
      "Stochastic Gradient Descent(777/999): loss=0.1027661343889181\n",
      "Stochastic Gradient Descent(778/999): loss=0.10276339234730424\n",
      "Stochastic Gradient Descent(779/999): loss=0.10276065111295472\n",
      "Stochastic Gradient Descent(780/999): loss=0.10275791068502718\n",
      "Stochastic Gradient Descent(781/999): loss=0.10275517106268872\n",
      "Stochastic Gradient Descent(782/999): loss=0.10275243224511602\n",
      "Stochastic Gradient Descent(783/999): loss=0.10274969423149485\n",
      "Stochastic Gradient Descent(784/999): loss=0.10274695702102032\n",
      "Stochastic Gradient Descent(785/999): loss=0.10274422061289641\n",
      "Stochastic Gradient Descent(786/999): loss=0.10274148500633595\n",
      "Stochastic Gradient Descent(787/999): loss=0.1027387502005605\n",
      "Stochastic Gradient Descent(788/999): loss=0.10273601619480026\n",
      "Stochastic Gradient Descent(789/999): loss=0.10273328298829368\n",
      "Stochastic Gradient Descent(790/999): loss=0.10273055058028772\n",
      "Stochastic Gradient Descent(791/999): loss=0.10272781897003722\n",
      "Stochastic Gradient Descent(792/999): loss=0.10272508815680537\n",
      "Stochastic Gradient Descent(793/999): loss=0.1027223581398631\n",
      "Stochastic Gradient Descent(794/999): loss=0.10271962891848913\n",
      "Stochastic Gradient Descent(795/999): loss=0.10271690049196983\n",
      "Stochastic Gradient Descent(796/999): loss=0.10271417285959908\n",
      "Stochastic Gradient Descent(797/999): loss=0.10271144602067829\n",
      "Stochastic Gradient Descent(798/999): loss=0.10270871997451603\n",
      "Stochastic Gradient Descent(799/999): loss=0.10270599472042813\n",
      "Stochastic Gradient Descent(800/999): loss=0.10270327025773747\n",
      "Stochastic Gradient Descent(801/999): loss=0.10270054658577381\n",
      "Stochastic Gradient Descent(802/999): loss=0.10269782370387388\n",
      "Stochastic Gradient Descent(803/999): loss=0.10269510161138105\n",
      "Stochastic Gradient Descent(804/999): loss=0.10269238030764535\n",
      "Stochastic Gradient Descent(805/999): loss=0.10268965979202332\n",
      "Stochastic Gradient Descent(806/999): loss=0.10268694006387791\n",
      "Stochastic Gradient Descent(807/999): loss=0.10268422112257843\n",
      "Stochastic Gradient Descent(808/999): loss=0.10268150296750028\n",
      "Stochastic Gradient Descent(809/999): loss=0.10267878559802518\n",
      "Stochastic Gradient Descent(810/999): loss=0.10267606901354069\n",
      "Stochastic Gradient Descent(811/999): loss=0.10267335321344037\n",
      "Stochastic Gradient Descent(812/999): loss=0.10267063819712362\n",
      "Stochastic Gradient Descent(813/999): loss=0.10266792396399552\n",
      "Stochastic Gradient Descent(814/999): loss=0.10266521051346682\n",
      "Stochastic Gradient Descent(815/999): loss=0.1026624978449539\n",
      "Stochastic Gradient Descent(816/999): loss=0.10265978595787846\n",
      "Stochastic Gradient Descent(817/999): loss=0.10265707485166774\n",
      "Stochastic Gradient Descent(818/999): loss=0.10265436452575412\n",
      "Stochastic Gradient Descent(819/999): loss=0.10265165497957536\n",
      "Stochastic Gradient Descent(820/999): loss=0.1026489462125742\n",
      "Stochastic Gradient Descent(821/999): loss=0.10264623822419855\n",
      "Stochastic Gradient Descent(822/999): loss=0.10264353101390117\n",
      "Stochastic Gradient Descent(823/999): loss=0.10264082458113984\n",
      "Stochastic Gradient Descent(824/999): loss=0.10263811892537703\n",
      "Stochastic Gradient Descent(825/999): loss=0.10263541404607997\n",
      "Stochastic Gradient Descent(826/999): loss=0.10263270994272064\n",
      "Stochastic Gradient Descent(827/999): loss=0.10263000661477553\n",
      "Stochastic Gradient Descent(828/999): loss=0.1026273040617257\n",
      "Stochastic Gradient Descent(829/999): loss=0.10262460228305655\n",
      "Stochastic Gradient Descent(830/999): loss=0.10262190127825797\n",
      "Stochastic Gradient Descent(831/999): loss=0.10261920104682411\n",
      "Stochastic Gradient Descent(832/999): loss=0.10261650158825332\n",
      "Stochastic Gradient Descent(833/999): loss=0.10261380290204819\n",
      "Stochastic Gradient Descent(834/999): loss=0.10261110498771535\n",
      "Stochastic Gradient Descent(835/999): loss=0.10260840784476553\n",
      "Stochastic Gradient Descent(836/999): loss=0.10260571147271337\n",
      "Stochastic Gradient Descent(837/999): loss=0.10260301587107748\n",
      "Stochastic Gradient Descent(838/999): loss=0.10260032103938029\n",
      "Stochastic Gradient Descent(839/999): loss=0.10259762697714804\n",
      "Stochastic Gradient Descent(840/999): loss=0.10259493368391069\n",
      "Stochastic Gradient Descent(841/999): loss=0.10259224115920185\n",
      "Stochastic Gradient Descent(842/999): loss=0.10258954940255882\n",
      "Stochastic Gradient Descent(843/999): loss=0.10258685841352233\n",
      "Stochastic Gradient Descent(844/999): loss=0.1025841681916368\n",
      "Stochastic Gradient Descent(845/999): loss=0.1025814787364499\n",
      "Stochastic Gradient Descent(846/999): loss=0.10257879004751279\n",
      "Stochastic Gradient Descent(847/999): loss=0.10257610212438002\n",
      "Stochastic Gradient Descent(848/999): loss=0.10257341496660931\n",
      "Stochastic Gradient Descent(849/999): loss=0.10257072857376162\n",
      "Stochastic Gradient Descent(850/999): loss=0.1025680429454013\n",
      "Stochastic Gradient Descent(851/999): loss=0.10256535808109558\n",
      "Stochastic Gradient Descent(852/999): loss=0.10256267398041488\n",
      "Stochastic Gradient Descent(853/999): loss=0.1025599906429327\n",
      "Stochastic Gradient Descent(854/999): loss=0.10255730806822545\n",
      "Stochastic Gradient Descent(855/999): loss=0.1025546262558726\n",
      "Stochastic Gradient Descent(856/999): loss=0.10255194520545632\n",
      "Stochastic Gradient Descent(857/999): loss=0.10254926491656184\n",
      "Stochastic Gradient Descent(858/999): loss=0.10254658538877708\n",
      "Stochastic Gradient Descent(859/999): loss=0.10254390662169273\n",
      "Stochastic Gradient Descent(860/999): loss=0.1025412286149023\n",
      "Stochastic Gradient Descent(861/999): loss=0.10253855136800183\n",
      "Stochastic Gradient Descent(862/999): loss=0.10253587488059014\n",
      "Stochastic Gradient Descent(863/999): loss=0.10253319915226848\n",
      "Stochastic Gradient Descent(864/999): loss=0.10253052418264082\n",
      "Stochastic Gradient Descent(865/999): loss=0.1025278499713136\n",
      "Stochastic Gradient Descent(866/999): loss=0.10252517651789564\n",
      "Stochastic Gradient Descent(867/999): loss=0.10252250382199835\n",
      "Stochastic Gradient Descent(868/999): loss=0.1025198318832354\n",
      "Stochastic Gradient Descent(869/999): loss=0.1025171607012229\n",
      "Stochastic Gradient Descent(870/999): loss=0.10251449027557928\n",
      "Stochastic Gradient Descent(871/999): loss=0.10251182060592524\n",
      "Stochastic Gradient Descent(872/999): loss=0.10250915169188374\n",
      "Stochastic Gradient Descent(873/999): loss=0.10250648353308002\n",
      "Stochastic Gradient Descent(874/999): loss=0.10250381612914136\n",
      "Stochastic Gradient Descent(875/999): loss=0.10250114947969738\n",
      "Stochastic Gradient Descent(876/999): loss=0.10249848358437962\n",
      "Stochastic Gradient Descent(877/999): loss=0.10249581844282189\n",
      "Stochastic Gradient Descent(878/999): loss=0.10249315405465992\n",
      "Stochastic Gradient Descent(879/999): loss=0.10249049041953155\n",
      "Stochastic Gradient Descent(880/999): loss=0.10248782753707651\n",
      "Stochastic Gradient Descent(881/999): loss=0.10248516540693664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(882/999): loss=0.10248250402875553\n",
      "Stochastic Gradient Descent(883/999): loss=0.1024798434021788\n",
      "Stochastic Gradient Descent(884/999): loss=0.10247718352685391\n",
      "Stochastic Gradient Descent(885/999): loss=0.1024745244024302\n",
      "Stochastic Gradient Descent(886/999): loss=0.10247186602855868\n",
      "Stochastic Gradient Descent(887/999): loss=0.10246920840489228\n",
      "Stochastic Gradient Descent(888/999): loss=0.10246655153108575\n",
      "Stochastic Gradient Descent(889/999): loss=0.1024638954067953\n",
      "Stochastic Gradient Descent(890/999): loss=0.10246124003167918\n",
      "Stochastic Gradient Descent(891/999): loss=0.10245858540539703\n",
      "Stochastic Gradient Descent(892/999): loss=0.10245593152761036\n",
      "Stochastic Gradient Descent(893/999): loss=0.10245327839798221\n",
      "Stochastic Gradient Descent(894/999): loss=0.10245062601617722\n",
      "Stochastic Gradient Descent(895/999): loss=0.10244797438186161\n",
      "Stochastic Gradient Descent(896/999): loss=0.10244532349470319\n",
      "Stochastic Gradient Descent(897/999): loss=0.10244267335437127\n",
      "Stochastic Gradient Descent(898/999): loss=0.10244002396053668\n",
      "Stochastic Gradient Descent(899/999): loss=0.10243737531287175\n",
      "Stochastic Gradient Descent(900/999): loss=0.10243472741105024\n",
      "Stochastic Gradient Descent(901/999): loss=0.10243208025474736\n",
      "Stochastic Gradient Descent(902/999): loss=0.10242943384363978\n",
      "Stochastic Gradient Descent(903/999): loss=0.10242678817740551\n",
      "Stochastic Gradient Descent(904/999): loss=0.10242414325572394\n",
      "Stochastic Gradient Descent(905/999): loss=0.10242149907827586\n",
      "Stochastic Gradient Descent(906/999): loss=0.10241885564474336\n",
      "Stochastic Gradient Descent(907/999): loss=0.10241621295480986\n",
      "Stochastic Gradient Descent(908/999): loss=0.10241357100816008\n",
      "Stochastic Gradient Descent(909/999): loss=0.10241092980447998\n",
      "Stochastic Gradient Descent(910/999): loss=0.10240828934345682\n",
      "Stochastic Gradient Descent(911/999): loss=0.10240564962477908\n",
      "Stochastic Gradient Descent(912/999): loss=0.10240301064813642\n",
      "Stochastic Gradient Descent(913/999): loss=0.10240037241321984\n",
      "Stochastic Gradient Descent(914/999): loss=0.10239773491972129\n",
      "Stochastic Gradient Descent(915/999): loss=0.1023950981673341\n",
      "Stochastic Gradient Descent(916/999): loss=0.10239246215575265\n",
      "Stochastic Gradient Descent(917/999): loss=0.10238982688467244\n",
      "Stochastic Gradient Descent(918/999): loss=0.10238719235379014\n",
      "Stochastic Gradient Descent(919/999): loss=0.10238455856280351\n",
      "Stochastic Gradient Descent(920/999): loss=0.10238192551141129\n",
      "Stochastic Gradient Descent(921/999): loss=0.10237929319931345\n",
      "Stochastic Gradient Descent(922/999): loss=0.10237666162621084\n",
      "Stochastic Gradient Descent(923/999): loss=0.10237403079180551\n",
      "Stochastic Gradient Descent(924/999): loss=0.10237140069580039\n",
      "Stochastic Gradient Descent(925/999): loss=0.10236877133789948\n",
      "Stochastic Gradient Descent(926/999): loss=0.10236614271780772\n",
      "Stochastic Gradient Descent(927/999): loss=0.10236351483523116\n",
      "Stochastic Gradient Descent(928/999): loss=0.10236088768987663\n",
      "Stochastic Gradient Descent(929/999): loss=0.10235826128145203\n",
      "Stochastic Gradient Descent(930/999): loss=0.10235563560966608\n",
      "Stochastic Gradient Descent(931/999): loss=0.10235301067422858\n",
      "Stochastic Gradient Descent(932/999): loss=0.10235038647485004\n",
      "Stochastic Gradient Descent(933/999): loss=0.10234776301124207\n",
      "Stochastic Gradient Descent(934/999): loss=0.10234514028311699\n",
      "Stochastic Gradient Descent(935/999): loss=0.10234251829018803\n",
      "Stochastic Gradient Descent(936/999): loss=0.10233989703216935\n",
      "Stochastic Gradient Descent(937/999): loss=0.10233727650877587\n",
      "Stochastic Gradient Descent(938/999): loss=0.10233465671972335\n",
      "Stochastic Gradient Descent(939/999): loss=0.10233203766472838\n",
      "Stochastic Gradient Descent(940/999): loss=0.10232941934350837\n",
      "Stochastic Gradient Descent(941/999): loss=0.10232680175578149\n",
      "Stochastic Gradient Descent(942/999): loss=0.10232418490126673\n",
      "Stochastic Gradient Descent(943/999): loss=0.10232156877968379\n",
      "Stochastic Gradient Descent(944/999): loss=0.10231895339075321\n",
      "Stochastic Gradient Descent(945/999): loss=0.10231633873419624\n",
      "Stochastic Gradient Descent(946/999): loss=0.10231372480973483\n",
      "Stochastic Gradient Descent(947/999): loss=0.10231111161709172\n",
      "Stochastic Gradient Descent(948/999): loss=0.10230849915599034\n",
      "Stochastic Gradient Descent(949/999): loss=0.10230588742615486\n",
      "Stochastic Gradient Descent(950/999): loss=0.10230327642731005\n",
      "Stochastic Gradient Descent(951/999): loss=0.10230066615918153\n",
      "Stochastic Gradient Descent(952/999): loss=0.10229805662149538\n",
      "Stochastic Gradient Descent(953/999): loss=0.10229544781397859\n",
      "Stochastic Gradient Descent(954/999): loss=0.1022928397363586\n",
      "Stochastic Gradient Descent(955/999): loss=0.10229023238836361\n",
      "Stochastic Gradient Descent(956/999): loss=0.1022876257697225\n",
      "Stochastic Gradient Descent(957/999): loss=0.10228501988016464\n",
      "Stochastic Gradient Descent(958/999): loss=0.10228241471942018\n",
      "Stochastic Gradient Descent(959/999): loss=0.10227981028721973\n",
      "Stochastic Gradient Descent(960/999): loss=0.10227720658329464\n",
      "Stochastic Gradient Descent(961/999): loss=0.10227460360737675\n",
      "Stochastic Gradient Descent(962/999): loss=0.10227200135919859\n",
      "Stochastic Gradient Descent(963/999): loss=0.1022693998384932\n",
      "Stochastic Gradient Descent(964/999): loss=0.10226679904499421\n",
      "Stochastic Gradient Descent(965/999): loss=0.1022641989784358\n",
      "Stochastic Gradient Descent(966/999): loss=0.10226159963855277\n",
      "Stochastic Gradient Descent(967/999): loss=0.10225900102508036\n",
      "Stochastic Gradient Descent(968/999): loss=0.10225640313775447\n",
      "Stochastic Gradient Descent(969/999): loss=0.10225380597631148\n",
      "Stochastic Gradient Descent(970/999): loss=0.10225120954048826\n",
      "Stochastic Gradient Descent(971/999): loss=0.10224861383002229\n",
      "Stochastic Gradient Descent(972/999): loss=0.10224601884465145\n",
      "Stochastic Gradient Descent(973/999): loss=0.10224342458411423\n",
      "Stochastic Gradient Descent(974/999): loss=0.10224083104814957\n",
      "Stochastic Gradient Descent(975/999): loss=0.10223823823649693\n",
      "Stochastic Gradient Descent(976/999): loss=0.10223564614889621\n",
      "Stochastic Gradient Descent(977/999): loss=0.10223305478508782\n",
      "Stochastic Gradient Descent(978/999): loss=0.10223046414481265\n",
      "Stochastic Gradient Descent(979/999): loss=0.10222787422781206\n",
      "Stochastic Gradient Descent(980/999): loss=0.10222528503382783\n",
      "Stochastic Gradient Descent(981/999): loss=0.10222269656260222\n",
      "Stochastic Gradient Descent(982/999): loss=0.10222010881387798\n",
      "Stochastic Gradient Descent(983/999): loss=0.10221752178739824\n",
      "Stochastic Gradient Descent(984/999): loss=0.10221493548290658\n",
      "Stochastic Gradient Descent(985/999): loss=0.10221234990014708\n",
      "Stochastic Gradient Descent(986/999): loss=0.10220976503886414\n",
      "Stochastic Gradient Descent(987/999): loss=0.10220718089880268\n",
      "Stochastic Gradient Descent(988/999): loss=0.10220459747970792\n",
      "Stochastic Gradient Descent(989/999): loss=0.1022020147813256\n",
      "Stochastic Gradient Descent(990/999): loss=0.10219943280340181\n",
      "Stochastic Gradient Descent(991/999): loss=0.10219685154568306\n",
      "Stochastic Gradient Descent(992/999): loss=0.10219427100791627\n",
      "Stochastic Gradient Descent(993/999): loss=0.10219169118984865\n",
      "Stochastic Gradient Descent(994/999): loss=0.10218911209122795\n",
      "Stochastic Gradient Descent(995/999): loss=0.10218653371180215\n",
      "Stochastic Gradient Descent(996/999): loss=0.10218395605131973\n",
      "Stochastic Gradient Descent(997/999): loss=0.10218137910952946\n",
      "Stochastic Gradient Descent(998/999): loss=0.10217880288618048\n",
      "Stochastic Gradient Descent(999/999): loss=0.10217622738102235\n",
      "Stochastic Gradient Descent(0/999): loss=0.1474419593440508\n",
      "Stochastic Gradient Descent(1/999): loss=0.14675243087194023\n",
      "Stochastic Gradient Descent(2/999): loss=0.14607397366219915\n",
      "Stochastic Gradient Descent(3/999): loss=0.1454064090911196\n",
      "Stochastic Gradient Descent(4/999): loss=0.1447495614171568\n",
      "Stochastic Gradient Descent(5/999): loss=0.1441032577344238\n",
      "Stochastic Gradient Descent(6/999): loss=0.14346732792693725\n",
      "Stochastic Gradient Descent(7/999): loss=0.14284160462360107\n",
      "Stochastic Gradient Descent(8/999): loss=0.14222592315391644\n",
      "Stochastic Gradient Descent(9/999): loss=0.14162012150440667\n",
      "Stochastic Gradient Descent(10/999): loss=0.14102404027574508\n",
      "Stochastic Gradient Descent(11/999): loss=0.14043752264057438\n",
      "Stochastic Gradient Descent(12/999): loss=0.1398604143020069\n",
      "Stochastic Gradient Descent(13/999): loss=0.13929256345279475\n",
      "Stochastic Gradient Descent(14/999): loss=0.13873382073515814\n",
      "Stochastic Gradient Descent(15/999): loss=0.138184039201262\n",
      "Stochastic Gradient Descent(16/999): loss=0.137643074274331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(17/999): loss=0.1371107837103913\n",
      "Stochastic Gradient Descent(18/999): loss=0.13658702756062954\n",
      "Stochastic Gradient Descent(19/999): loss=0.13607166813436\n",
      "Stochastic Gradient Descent(20/999): loss=0.13556456996258825\n",
      "Stochastic Gradient Descent(21/999): loss=0.13506559976216373\n",
      "Stochastic Gradient Descent(22/999): loss=0.1345746264005097\n",
      "Stochastic Gradient Descent(23/999): loss=0.1340915208609234\n",
      "Stochastic Gradient Descent(24/999): loss=0.13361615620843506\n",
      "Stochastic Gradient Descent(25/999): loss=0.13314840755621848\n",
      "Stochastic Gradient Descent(26/999): loss=0.13268815203254353\n",
      "Stochastic Gradient Descent(27/999): loss=0.13223526874826144\n",
      "Stochastic Gradient Descent(28/999): loss=0.1317896387648152\n",
      "Stochastic Gradient Descent(29/999): loss=0.13135114506276588\n",
      "Stochastic Gradient Descent(30/999): loss=0.13091967251082687\n",
      "Stochastic Gradient Descent(31/999): loss=0.13049510783539783\n",
      "Stochastic Gradient Descent(32/999): loss=0.13007733959059004\n",
      "Stochastic Gradient Descent(33/999): loss=0.1296662581287355\n",
      "Stochastic Gradient Descent(34/999): loss=0.12926175557137212\n",
      "Stochastic Gradient Descent(35/999): loss=0.12886372578069646\n",
      "Stochastic Gradient Descent(36/999): loss=0.1284720643314778\n",
      "Stochastic Gradient Descent(37/999): loss=0.12808666848342493\n",
      "Stochastic Gradient Descent(38/999): loss=0.12770743715399893\n",
      "Stochastic Gradient Descent(39/999): loss=0.12733427089166469\n",
      "Stochastic Gradient Descent(40/999): loss=0.1269670718495741\n",
      "Stochastic Gradient Descent(41/999): loss=0.12660574375967332\n",
      "Stochastic Gradient Descent(42/999): loss=0.12625019190722897\n",
      "Stochastic Gradient Descent(43/999): loss=0.12590032310576402\n",
      "Stochastic Gradient Descent(44/999): loss=0.12555604567239909\n",
      "Stochastic Gradient Descent(45/999): loss=0.12521726940359104\n",
      "Stochastic Gradient Descent(46/999): loss=0.12488390555126362\n",
      "Stochastic Gradient Descent(47/999): loss=0.12455586679932253\n",
      "Stochastic Gradient Descent(48/999): loss=0.1242330672405499\n",
      "Stochastic Gradient Descent(49/999): loss=0.12391542235387172\n",
      "Stochastic Gradient Descent(50/999): loss=0.12360284898199142\n",
      "Stochastic Gradient Descent(51/999): loss=0.12329526530938512\n",
      "Stochastic Gradient Descent(52/999): loss=0.12299259084065135\n",
      "Stochastic Gradient Descent(53/999): loss=0.12269474637921045\n",
      "Stochastic Gradient Descent(54/999): loss=0.12240165400634741\n",
      "Stochastic Gradient Descent(55/999): loss=0.12211323706059309\n",
      "Stochastic Gradient Descent(56/999): loss=0.121829420117438\n",
      "Stochastic Gradient Descent(57/999): loss=0.12155012896937352\n",
      "Stochastic Gradient Descent(58/999): loss=0.12127529060625492\n",
      "Stochastic Gradient Descent(59/999): loss=0.12100483319598149\n",
      "Stochastic Gradient Descent(60/999): loss=0.12073868606548842\n",
      "Stochastic Gradient Descent(61/999): loss=0.120476779682045\n",
      "Stochastic Gradient Descent(62/999): loss=0.12021904563485504\n",
      "Stochastic Gradient Descent(63/999): loss=0.11996541661695413\n",
      "Stochastic Gradient Descent(64/999): loss=0.11971582640739849\n",
      "Stochastic Gradient Descent(65/999): loss=0.11947020985374185\n",
      "Stochastic Gradient Descent(66/999): loss=0.1192285028547947\n",
      "Stochastic Gradient Descent(67/999): loss=0.11899064234366187\n",
      "Stochastic Gradient Descent(68/999): loss=0.11875656627105372\n",
      "Stochastic Gradient Descent(69/999): loss=0.11852621358886663\n",
      "Stochastic Gradient Descent(70/999): loss=0.11829952423402858\n",
      "Stochastic Gradient Descent(71/999): loss=0.11807643911260499\n",
      "Stochastic Gradient Descent(72/999): loss=0.11785690008416161\n",
      "Stochastic Gradient Descent(73/999): loss=0.11764084994637916\n",
      "Stochastic Gradient Descent(74/999): loss=0.11742823241991657\n",
      "Stochastic Gradient Descent(75/999): loss=0.1172189921335182\n",
      "Stochastic Gradient Descent(76/999): loss=0.11701307460936171\n",
      "Stochastic Gradient Descent(77/999): loss=0.11681042624864203\n",
      "Stochastic Gradient Descent(78/999): loss=0.11661099431738803\n",
      "Stochastic Gradient Descent(79/999): loss=0.11641472693250805\n",
      "Stochastic Gradient Descent(80/999): loss=0.11622157304806077\n",
      "Stochastic Gradient Descent(81/999): loss=0.11603148244174719\n",
      "Stochastic Gradient Descent(82/999): loss=0.115844405701621\n",
      "Stochastic Gradient Descent(83/999): loss=0.11566029421301317\n",
      "Stochastic Gradient Descent(84/999): loss=0.11547910014566755\n",
      "Stochastic Gradient Descent(85/999): loss=0.11530077644108394\n",
      "Stochastic Gradient Descent(86/999): loss=0.11512527680006561\n",
      "Stochastic Gradient Descent(87/999): loss=0.11495255567046765\n",
      "Stochastic Gradient Descent(88/999): loss=0.11478256823514274\n",
      "Stochastic Gradient Descent(89/999): loss=0.11461527040008207\n",
      "Stochastic Gradient Descent(90/999): loss=0.11445061878274673\n",
      "Stochastic Gradient Descent(91/999): loss=0.11428857070058823\n",
      "Stochastic Gradient Descent(92/999): loss=0.11412908415975366\n",
      "Stochastic Gradient Descent(93/999): loss=0.11397211784397325\n",
      "Stochastic Gradient Descent(94/999): loss=0.11381763110362754\n",
      "Stochastic Gradient Descent(95/999): loss=0.11366558394499053\n",
      "Stochastic Gradient Descent(96/999): loss=0.11351593701964649\n",
      "Stochastic Gradient Descent(97/999): loss=0.1133686516140777\n",
      "Stochastic Gradient Descent(98/999): loss=0.11322368963942026\n",
      "Stochastic Gradient Descent(99/999): loss=0.11308101362138473\n",
      "Stochastic Gradient Descent(100/999): loss=0.11294058669033993\n",
      "Stochastic Gradient Descent(101/999): loss=0.11280237257155656\n",
      "Stochastic Gradient Descent(102/999): loss=0.11266633557560796\n",
      "Stochastic Gradient Descent(103/999): loss=0.11253244058892631\n",
      "Stochastic Gradient Descent(104/999): loss=0.11240065306451079\n",
      "Stochastic Gradient Descent(105/999): loss=0.11227093901278562\n",
      "Stochastic Gradient Descent(106/999): loss=0.11214326499260605\n",
      "Stochastic Gradient Descent(107/999): loss=0.11201759810240906\n",
      "Stochastic Gradient Descent(108/999): loss=0.11189390597150702\n",
      "Stochastic Gradient Descent(109/999): loss=0.11177215675152166\n",
      "Stochastic Gradient Descent(110/999): loss=0.11165231910795663\n",
      "Stochastic Gradient Descent(111/999): loss=0.1115343622119057\n",
      "Stochastic Gradient Descent(112/999): loss=0.11141825573189465\n",
      "Stochastic Gradient Descent(113/999): loss=0.11130396982585541\n",
      "Stochastic Gradient Descent(114/999): loss=0.11119147513322897\n",
      "Stochastic Gradient Descent(115/999): loss=0.111080742767196\n",
      "Stochastic Gradient Descent(116/999): loss=0.11097174430703319\n",
      "Stochastic Gradient Descent(117/999): loss=0.11086445179059227\n",
      "Stochastic Gradient Descent(118/999): loss=0.11075883770690087\n",
      "Stochastic Gradient Descent(119/999): loss=0.11065487498888264\n",
      "Stochastic Gradient Descent(120/999): loss=0.11055253700619459\n",
      "Stochastic Gradient Descent(121/999): loss=0.1104517975581805\n",
      "Stochastic Gradient Descent(122/999): loss=0.11035263086693758\n",
      "Stochastic Gradient Descent(123/999): loss=0.11025501157049518\n",
      "Stochastic Gradient Descent(124/999): loss=0.11015891471610359\n",
      "Stochastic Gradient Descent(125/999): loss=0.11006431575363108\n",
      "Stochastic Gradient Descent(126/999): loss=0.10997119052906747\n",
      "Stochastic Gradient Descent(127/999): loss=0.10987951527813264\n",
      "Stochastic Gradient Descent(128/999): loss=0.10978926661998785\n",
      "Stochastic Gradient Descent(129/999): loss=0.10970042155104907\n",
      "Stochastic Gradient Descent(130/999): loss=0.10961295743889955\n",
      "Stochastic Gradient Descent(131/999): loss=0.109526852016301\n",
      "Stochastic Gradient Descent(132/999): loss=0.10944208337530129\n",
      "Stochastic Gradient Descent(133/999): loss=0.10935862996143722\n",
      "Stochastic Gradient Descent(134/999): loss=0.10927647056803076\n",
      "Stochastic Gradient Descent(135/999): loss=0.10919558433057745\n",
      "Stochastic Gradient Descent(136/999): loss=0.10911595072122533\n",
      "Stochastic Gradient Descent(137/999): loss=0.10903754954334281\n",
      "Stochastic Gradient Descent(138/999): loss=0.10896036092617441\n",
      "Stochastic Gradient Descent(139/999): loss=0.10888436531958243\n",
      "Stochastic Gradient Descent(140/999): loss=0.108809543488874\n",
      "Stochastic Gradient Descent(141/999): loss=0.10873587650971071\n",
      "Stochastic Gradient Descent(142/999): loss=0.10866334576310153\n",
      "Stochastic Gradient Descent(143/999): loss=0.10859193293047537\n",
      "Stochastic Gradient Descent(144/999): loss=0.10852161998883403\n",
      "Stochastic Gradient Descent(145/999): loss=0.10845238920598287\n",
      "Stochastic Gradient Descent(146/999): loss=0.10838422313583855\n",
      "Stochastic Gradient Descent(147/999): loss=0.10831710461381254\n",
      "Stochastic Gradient Descent(148/999): loss=0.10825101675226917\n",
      "Stochastic Gradient Descent(149/999): loss=0.10818594293605677\n",
      "Stochastic Gradient Descent(150/999): loss=0.10812186681811102\n",
      "Stochastic Gradient Descent(151/999): loss=0.10805877231512945\n",
      "Stochastic Gradient Descent(152/999): loss=0.10799664360331512\n",
      "Stochastic Gradient Descent(153/999): loss=0.1079354651141899\n",
      "Stochastic Gradient Descent(154/999): loss=0.1078752215304744\n",
      "Stochastic Gradient Descent(155/999): loss=0.10781589778203499\n",
      "Stochastic Gradient Descent(156/999): loss=0.10775747904189587\n",
      "Stochastic Gradient Descent(157/999): loss=0.1076999507223158\n",
      "Stochastic Gradient Descent(158/999): loss=0.1076432984709277\n",
      "Stochastic Gradient Descent(159/999): loss=0.10758750816694103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(160/999): loss=0.10753256591740508\n",
      "Stochastic Gradient Descent(161/999): loss=0.10747845805353265\n",
      "Stochastic Gradient Descent(162/999): loss=0.1074251711270832\n",
      "Stochastic Gradient Descent(163/999): loss=0.10737269190680415\n",
      "Stochastic Gradient Descent(164/999): loss=0.10732100737492972\n",
      "Stochastic Gradient Descent(165/999): loss=0.10727010472373626\n",
      "Stochastic Gradient Descent(166/999): loss=0.10721997135215303\n",
      "Stochastic Gradient Descent(167/999): loss=0.10717059486242793\n",
      "Stochastic Gradient Descent(168/999): loss=0.10712196305684668\n",
      "Stochastic Gradient Descent(169/999): loss=0.10707406393450537\n",
      "Stochastic Gradient Descent(170/999): loss=0.10702688568813462\n",
      "Stochastic Gradient Descent(171/999): loss=0.10698041670097529\n",
      "Stochastic Gradient Descent(172/999): loss=0.10693464554370452\n",
      "Stochastic Gradient Descent(173/999): loss=0.10688956097141146\n",
      "Stochastic Gradient Descent(174/999): loss=0.10684515192062155\n",
      "Stochastic Gradient Descent(175/999): loss=0.106801407506369\n",
      "Stochastic Gradient Descent(176/999): loss=0.10675831701931664\n",
      "Stochastic Gradient Descent(177/999): loss=0.1067158699229219\n",
      "Stochastic Gradient Descent(178/999): loss=0.1066740558506487\n",
      "Stochastic Gradient Descent(179/999): loss=0.10663286460322448\n",
      "Stochastic Gradient Descent(180/999): loss=0.1065922861459412\n",
      "Stochastic Gradient Descent(181/999): loss=0.10655231060600012\n",
      "Stochastic Gradient Descent(182/999): loss=0.10651292826989919\n",
      "Stochastic Gradient Descent(183/999): loss=0.10647412958086286\n",
      "Stochastic Gradient Descent(184/999): loss=0.1064359051363132\n",
      "Stochastic Gradient Descent(185/999): loss=0.10639824568538184\n",
      "Stochastic Gradient Descent(186/999): loss=0.1063611421264621\n",
      "Stochastic Gradient Descent(187/999): loss=0.10632458550480056\n",
      "Stochastic Gradient Descent(188/999): loss=0.10628856701012752\n",
      "Stochastic Gradient Descent(189/999): loss=0.10625307797432577\n",
      "Stochastic Gradient Descent(190/999): loss=0.10621810986913684\n",
      "Stochastic Gradient Descent(191/999): loss=0.10618365430390432\n",
      "Stochastic Gradient Descent(192/999): loss=0.10614970302335358\n",
      "Stochastic Gradient Descent(193/999): loss=0.10611624790540741\n",
      "Stochastic Gradient Descent(194/999): loss=0.10608328095903674\n",
      "Stochastic Gradient Descent(195/999): loss=0.1060507943221462\n",
      "Stochastic Gradient Descent(196/999): loss=0.10601878025949361\n",
      "Stochastic Gradient Descent(197/999): loss=0.10598723116064328\n",
      "Stochastic Gradient Descent(198/999): loss=0.10595613953795213\n",
      "Stochastic Gradient Descent(199/999): loss=0.10592549802458846\n",
      "Stochastic Gradient Descent(200/999): loss=0.1058952993725825\n",
      "Stochastic Gradient Descent(201/999): loss=0.10586553645090856\n",
      "Stochastic Gradient Descent(202/999): loss=0.10583620224359817\n",
      "Stochastic Gradient Descent(203/999): loss=0.10580728984788344\n",
      "Stochastic Gradient Descent(204/999): loss=0.1057787924723707\n",
      "Stochastic Gradient Descent(205/999): loss=0.10575070343524333\n",
      "Stochastic Gradient Descent(206/999): loss=0.10572301616249365\n",
      "Stochastic Gradient Descent(207/999): loss=0.1056957241861834\n",
      "Stochastic Gradient Descent(208/999): loss=0.10566882114273246\n",
      "Stochastic Gradient Descent(209/999): loss=0.10564230077123464\n",
      "Stochastic Gradient Descent(210/999): loss=0.1056161569118013\n",
      "Stochastic Gradient Descent(211/999): loss=0.10559038350393125\n",
      "Stochastic Gradient Descent(212/999): loss=0.1055649745849071\n",
      "Stochastic Gradient Descent(213/999): loss=0.10553992428821744\n",
      "Stochastic Gradient Descent(214/999): loss=0.10551522684200476\n",
      "Stochastic Gradient Descent(215/999): loss=0.10549087656753778\n",
      "Stochastic Gradient Descent(216/999): loss=0.10546686787770916\n",
      "Stochastic Gradient Descent(217/999): loss=0.10544319527555697\n",
      "Stochastic Gradient Descent(218/999): loss=0.10541985335281008\n",
      "Stochastic Gradient Descent(219/999): loss=0.10539683678845728\n",
      "Stochastic Gradient Descent(220/999): loss=0.10537414034733919\n",
      "Stochastic Gradient Descent(221/999): loss=0.105351758878763\n",
      "Stochastic Gradient Descent(222/999): loss=0.10532968731513966\n",
      "Stochastic Gradient Descent(223/999): loss=0.10530792067064283\n",
      "Stochastic Gradient Descent(224/999): loss=0.10528645403988972\n",
      "Stochastic Gradient Descent(225/999): loss=0.10526528259664313\n",
      "Stochastic Gradient Descent(226/999): loss=0.10524440159253423\n",
      "Stochastic Gradient Descent(227/999): loss=0.1052238063558063\n",
      "Stochastic Gradient Descent(228/999): loss=0.10520349229007854\n",
      "Stochastic Gradient Descent(229/999): loss=0.10518345487312968\n",
      "Stochastic Gradient Descent(230/999): loss=0.10516368965570158\n",
      "Stochastic Gradient Descent(231/999): loss=0.10514419226032191\n",
      "Stochastic Gradient Descent(232/999): loss=0.10512495838014577\n",
      "Stochastic Gradient Descent(233/999): loss=0.10510598377781624\n",
      "Stochastic Gradient Descent(234/999): loss=0.1050872642843429\n",
      "Stochastic Gradient Descent(235/999): loss=0.10506879579799905\n",
      "Stochastic Gradient Descent(236/999): loss=0.10505057428323603\n",
      "Stochastic Gradient Descent(237/999): loss=0.10503259576961578\n",
      "Stochastic Gradient Descent(238/999): loss=0.10501485635075995\n",
      "Stochastic Gradient Descent(239/999): loss=0.10499735218331642\n",
      "Stochastic Gradient Descent(240/999): loss=0.10498007948594229\n",
      "Stochastic Gradient Descent(241/999): loss=0.10496303453830343\n",
      "Stochastic Gradient Descent(242/999): loss=0.10494621368008991\n",
      "Stochastic Gradient Descent(243/999): loss=0.10492961331004773\n",
      "Stochastic Gradient Descent(244/999): loss=0.10491322988502566\n",
      "Stochastic Gradient Descent(245/999): loss=0.10489705991903789\n",
      "Stochastic Gradient Descent(246/999): loss=0.10488109998234166\n",
      "Stochastic Gradient Descent(247/999): loss=0.10486534670052969\n",
      "Stochastic Gradient Descent(248/999): loss=0.1048497967536373\n",
      "Stochastic Gradient Descent(249/999): loss=0.10483444687526408\n",
      "Stochastic Gradient Descent(250/999): loss=0.1048192938517094\n",
      "Stochastic Gradient Descent(251/999): loss=0.10480433452112237\n",
      "Stochastic Gradient Descent(252/999): loss=0.10478956577266492\n",
      "Stochastic Gradient Descent(253/999): loss=0.10477498454568891\n",
      "Stochastic Gradient Descent(254/999): loss=0.10476058782892621\n",
      "Stochastic Gradient Descent(255/999): loss=0.1047463726596919\n",
      "Stochastic Gradient Descent(256/999): loss=0.10473233612310046\n",
      "Stochastic Gradient Descent(257/999): loss=0.10471847535129436\n",
      "Stochastic Gradient Descent(258/999): loss=0.10470478752268547\n",
      "Stochastic Gradient Descent(259/999): loss=0.10469126986120826\n",
      "Stochastic Gradient Descent(260/999): loss=0.1046779196355854\n",
      "Stochastic Gradient Descent(261/999): loss=0.10466473415860503\n",
      "Stochastic Gradient Descent(262/999): loss=0.10465171078640984\n",
      "Stochastic Gradient Descent(263/999): loss=0.10463884691779735\n",
      "Stochastic Gradient Descent(264/999): loss=0.10462613999353186\n",
      "Stochastic Gradient Descent(265/999): loss=0.10461358749566718\n",
      "Stochastic Gradient Descent(266/999): loss=0.10460118694688046\n",
      "Stochastic Gradient Descent(267/999): loss=0.10458893590981663\n",
      "Stochastic Gradient Descent(268/999): loss=0.10457683198644363\n",
      "Stochastic Gradient Descent(269/999): loss=0.10456487281741782\n",
      "Stochastic Gradient Descent(270/999): loss=0.1045530560814598\n",
      "Stochastic Gradient Descent(271/999): loss=0.10454137949474022\n",
      "Stochastic Gradient Descent(272/999): loss=0.1045298408102754\n",
      "Stochastic Gradient Descent(273/999): loss=0.10451843781733298\n",
      "Stochastic Gradient Descent(274/999): loss=0.10450716834084685\n",
      "Stochastic Gradient Descent(275/999): loss=0.10449603024084174\n",
      "Stochastic Gradient Descent(276/999): loss=0.10448502141186701\n",
      "Stochastic Gradient Descent(277/999): loss=0.10447413978243962\n",
      "Stochastic Gradient Descent(278/999): loss=0.10446338331449588\n",
      "Stochastic Gradient Descent(279/999): loss=0.10445275000285242\n",
      "Stochastic Gradient Descent(280/999): loss=0.10444223787467556\n",
      "Stochastic Gradient Descent(281/999): loss=0.10443184498895933\n",
      "Stochastic Gradient Descent(282/999): loss=0.10442156943601197\n",
      "Stochastic Gradient Descent(283/999): loss=0.1044114093369506\n",
      "Stochastic Gradient Descent(284/999): loss=0.10440136284320413\n",
      "Stochastic Gradient Descent(285/999): loss=0.1043914281360242\n",
      "Stochastic Gradient Descent(286/999): loss=0.10438160342600399\n",
      "Stochastic Gradient Descent(287/999): loss=0.10437188695260467\n",
      "Stochastic Gradient Descent(288/999): loss=0.10436227698368974\n",
      "Stochastic Gradient Descent(289/999): loss=0.10435277181506666\n",
      "Stochastic Gradient Descent(290/999): loss=0.10434336977003605\n",
      "Stochastic Gradient Descent(291/999): loss=0.10433406919894798\n",
      "Stochastic Gradient Descent(292/999): loss=0.10432486847876557\n",
      "Stochastic Gradient Descent(293/999): loss=0.10431576601263559\n",
      "Stochastic Gradient Descent(294/999): loss=0.10430676022946593\n",
      "Stochastic Gradient Descent(295/999): loss=0.10429784958351\n",
      "Stochastic Gradient Descent(296/999): loss=0.10428903255395774\n",
      "Stochastic Gradient Descent(297/999): loss=0.10428030764453322\n",
      "Stochastic Gradient Descent(298/999): loss=0.10427167338309883\n",
      "Stochastic Gradient Descent(299/999): loss=0.10426312832126582\n",
      "Stochastic Gradient Descent(300/999): loss=0.104254671034011\n",
      "Stochastic Gradient Descent(301/999): loss=0.10424630011929983\n",
      "Stochastic Gradient Descent(302/999): loss=0.10423801419771546\n",
      "Stochastic Gradient Descent(303/999): loss=0.10422981191209378\n",
      "Stochastic Gradient Descent(304/999): loss=0.10422169192716432\n",
      "Stochastic Gradient Descent(305/999): loss=0.10421365292919711\n",
      "Stochastic Gradient Descent(306/999): loss=0.104205693625655\n",
      "Stochastic Gradient Descent(307/999): loss=0.10419781274485175\n",
      "Stochastic Gradient Descent(308/999): loss=0.10419000903561558\n",
      "Stochastic Gradient Descent(309/999): loss=0.10418228126695818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(310/999): loss=0.10417462822774905\n",
      "Stochastic Gradient Descent(311/999): loss=0.10416704872639508\n",
      "Stochastic Gradient Descent(312/999): loss=0.1041595415905253\n",
      "Stochastic Gradient Descent(313/999): loss=0.1041521056666807\n",
      "Stochastic Gradient Descent(314/999): loss=0.10414473982000913\n",
      "Stochastic Gradient Descent(315/999): loss=0.1041374429339651\n",
      "Stochastic Gradient Descent(316/999): loss=0.10413021391001423\n",
      "Stochastic Gradient Descent(317/999): loss=0.10412305166734281\n",
      "Stochastic Gradient Descent(318/999): loss=0.1041159551425719\n",
      "Stochastic Gradient Descent(319/999): loss=0.10410892328947574\n",
      "Stochastic Gradient Descent(320/999): loss=0.10410195507870534\n",
      "Stochastic Gradient Descent(321/999): loss=0.10409504949751591\n",
      "Stochastic Gradient Descent(322/999): loss=0.10408820554949902\n",
      "Stochastic Gradient Descent(323/999): loss=0.10408142225431907\n",
      "Stochastic Gradient Descent(324/999): loss=0.10407469864745385\n",
      "Stochastic Gradient Descent(325/999): loss=0.10406803377993946\n",
      "Stochastic Gradient Descent(326/999): loss=0.10406142671811924\n",
      "Stochastic Gradient Descent(327/999): loss=0.10405487654339678\n",
      "Stochastic Gradient Descent(328/999): loss=0.10404838235199292\n",
      "Stochastic Gradient Descent(329/999): loss=0.10404194325470671\n",
      "Stochastic Gradient Descent(330/999): loss=0.10403555837668013\n",
      "Stochastic Gradient Descent(331/999): loss=0.10402922685716666\n",
      "Stochastic Gradient Descent(332/999): loss=0.10402294784930367\n",
      "Stochastic Gradient Descent(333/999): loss=0.10401672051988832\n",
      "Stochastic Gradient Descent(334/999): loss=0.10401054404915715\n",
      "Stochastic Gradient Descent(335/999): loss=0.10400441763056929\n",
      "Stochastic Gradient Descent(336/999): loss=0.10399834047059307\n",
      "Stochastic Gradient Descent(337/999): loss=0.10399231178849609\n",
      "Stochastic Gradient Descent(338/999): loss=0.10398633081613883\n",
      "Stochastic Gradient Descent(339/999): loss=0.10398039679777123\n",
      "Stochastic Gradient Descent(340/999): loss=0.103974508989833\n",
      "Stochastic Gradient Descent(341/999): loss=0.10396866666075685\n",
      "Stochastic Gradient Descent(342/999): loss=0.10396286909077493\n",
      "Stochastic Gradient Descent(343/999): loss=0.10395711557172856\n",
      "Stochastic Gradient Descent(344/999): loss=0.10395140540688079\n",
      "Stochastic Gradient Descent(345/999): loss=0.10394573791073217\n",
      "Stochastic Gradient Descent(346/999): loss=0.10394011240883948\n",
      "Stochastic Gradient Descent(347/999): loss=0.10393452823763719\n",
      "Stochastic Gradient Descent(348/999): loss=0.10392898474426211\n",
      "Stochastic Gradient Descent(349/999): loss=0.10392348128638057\n",
      "Stochastic Gradient Descent(350/999): loss=0.10391801723201867\n",
      "Stochastic Gradient Descent(351/999): loss=0.10391259195939492\n",
      "Stochastic Gradient Descent(352/999): loss=0.103907204856756\n",
      "Stochastic Gradient Descent(353/999): loss=0.10390185532221477\n",
      "Stochastic Gradient Descent(354/999): loss=0.10389654276359124\n",
      "Stochastic Gradient Descent(355/999): loss=0.1038912665982558\n",
      "Stochastic Gradient Descent(356/999): loss=0.10388602625297519\n",
      "Stochastic Gradient Descent(357/999): loss=0.10388082116376092\n",
      "Stochastic Gradient Descent(358/999): loss=0.10387565077572002\n",
      "Stochastic Gradient Descent(359/999): loss=0.1038705145429084\n",
      "Stochastic Gradient Descent(360/999): loss=0.10386541192818628\n",
      "Stochastic Gradient Descent(361/999): loss=0.10386034240307626\n",
      "Stochastic Gradient Descent(362/999): loss=0.10385530544762366\n",
      "Stochastic Gradient Descent(363/999): loss=0.10385030055025868\n",
      "Stochastic Gradient Descent(364/999): loss=0.1038453272076614\n",
      "Stochastic Gradient Descent(365/999): loss=0.10384038492462858\n",
      "Stochastic Gradient Descent(366/999): loss=0.10383547321394267\n",
      "Stochastic Gradient Descent(367/999): loss=0.10383059159624289\n",
      "Stochastic Gradient Descent(368/999): loss=0.10382573959989866\n",
      "Stochastic Gradient Descent(369/999): loss=0.10382091676088467\n",
      "Stochastic Gradient Descent(370/999): loss=0.10381612262265826\n",
      "Stochastic Gradient Descent(371/999): loss=0.10381135673603872\n",
      "Stochastic Gradient Descent(372/999): loss=0.10380661865908844\n",
      "Stochastic Gradient Descent(373/999): loss=0.10380190795699602\n",
      "Stochastic Gradient Descent(374/999): loss=0.1037972242019614\n",
      "Stochastic Gradient Descent(375/999): loss=0.10379256697308255\n",
      "Stochastic Gradient Descent(376/999): loss=0.10378793585624443\n",
      "Stochastic Gradient Descent(377/999): loss=0.10378333044400924\n",
      "Stochastic Gradient Descent(378/999): loss=0.1037787503355087\n",
      "Stochastic Gradient Descent(379/999): loss=0.10377419513633829\n",
      "Stochastic Gradient Descent(380/999): loss=0.10376966445845258\n",
      "Stochastic Gradient Descent(381/999): loss=0.10376515792006308\n",
      "Stochastic Gradient Descent(382/999): loss=0.10376067514553686\n",
      "Stochastic Gradient Descent(383/999): loss=0.10375621576529745\n",
      "Stochastic Gradient Descent(384/999): loss=0.10375177941572704\n",
      "Stochastic Gradient Descent(385/999): loss=0.10374736573907042\n",
      "Stochastic Gradient Descent(386/999): loss=0.10374297438334035\n",
      "Stochastic Gradient Descent(387/999): loss=0.10373860500222447\n",
      "Stochastic Gradient Descent(388/999): loss=0.1037342572549936\n",
      "Stochastic Gradient Descent(389/999): loss=0.10372993080641206\n",
      "Stochastic Gradient Descent(390/999): loss=0.10372562532664857\n",
      "Stochastic Gradient Descent(391/999): loss=0.10372134049118933\n",
      "Stochastic Gradient Descent(392/999): loss=0.10371707598075211\n",
      "Stochastic Gradient Descent(393/999): loss=0.1037128314812019\n",
      "Stochastic Gradient Descent(394/999): loss=0.10370860668346786\n",
      "Stochastic Gradient Descent(395/999): loss=0.1037044012834615\n",
      "Stochastic Gradient Descent(396/999): loss=0.10370021498199637\n",
      "Stochastic Gradient Descent(397/999): loss=0.1036960474847091\n",
      "Stochastic Gradient Descent(398/999): loss=0.10369189850198127\n",
      "Stochastic Gradient Descent(399/999): loss=0.10368776774886315\n",
      "Stochastic Gradient Descent(400/999): loss=0.10368365494499816\n",
      "Stochastic Gradient Descent(401/999): loss=0.10367955981454888\n",
      "Stochastic Gradient Descent(402/999): loss=0.10367548208612412\n",
      "Stochastic Gradient Descent(403/999): loss=0.10367142149270711\n",
      "Stochastic Gradient Descent(404/999): loss=0.10366737777158497\n",
      "Stochastic Gradient Descent(405/999): loss=0.1036633506642794\n",
      "Stochastic Gradient Descent(406/999): loss=0.10365933991647797\n",
      "Stochastic Gradient Descent(407/999): loss=0.10365534527796733\n",
      "Stochastic Gradient Descent(408/999): loss=0.10365136650256677\n",
      "Stochastic Gradient Descent(409/999): loss=0.10364740334806351\n",
      "Stochastic Gradient Descent(410/999): loss=0.10364345557614814\n",
      "Stochastic Gradient Descent(411/999): loss=0.10363952295235217\n",
      "Stochastic Gradient Descent(412/999): loss=0.10363560524598568\n",
      "Stochastic Gradient Descent(413/999): loss=0.1036317022300766\n",
      "Stochastic Gradient Descent(414/999): loss=0.10362781368131062\n",
      "Stochastic Gradient Descent(415/999): loss=0.1036239393799721\n",
      "Stochastic Gradient Descent(416/999): loss=0.1036200791098862\n",
      "Stochastic Gradient Descent(417/999): loss=0.1036162326583615\n",
      "Stochastic Gradient Descent(418/999): loss=0.10361239981613406\n",
      "Stochastic Gradient Descent(419/999): loss=0.10360858037731188\n",
      "Stochastic Gradient Descent(420/999): loss=0.10360477413932068\n",
      "Stochastic Gradient Descent(421/999): loss=0.10360098090285022\n",
      "Stochastic Gradient Descent(422/999): loss=0.10359720047180174\n",
      "Stochastic Gradient Descent(423/999): loss=0.10359343265323619\n",
      "Stochastic Gradient Descent(424/999): loss=0.10358967725732292\n",
      "Stochastic Gradient Descent(425/999): loss=0.10358593409729006\n",
      "Stochastic Gradient Descent(426/999): loss=0.10358220298937472\n",
      "Stochastic Gradient Descent(427/999): loss=0.10357848375277463\n",
      "Stochastic Gradient Descent(428/999): loss=0.1035747762096004\n",
      "Stochastic Gradient Descent(429/999): loss=0.10357108018482833\n",
      "Stochastic Gradient Descent(430/999): loss=0.1035673955062544\n",
      "Stochastic Gradient Descent(431/999): loss=0.10356372200444872\n",
      "Stochastic Gradient Descent(432/999): loss=0.10356005951271062\n",
      "Stochastic Gradient Descent(433/999): loss=0.10355640786702477\n",
      "Stochastic Gradient Descent(434/999): loss=0.10355276690601786\n",
      "Stochastic Gradient Descent(435/999): loss=0.10354913647091589\n",
      "Stochastic Gradient Descent(436/999): loss=0.10354551640550222\n",
      "Stochastic Gradient Descent(437/999): loss=0.10354190655607631\n",
      "Stochastic Gradient Descent(438/999): loss=0.10353830677141335\n",
      "Stochastic Gradient Descent(439/999): loss=0.10353471690272392\n",
      "Stochastic Gradient Descent(440/999): loss=0.10353113680361496\n",
      "Stochastic Gradient Descent(441/999): loss=0.10352756633005114\n",
      "Stochastic Gradient Descent(442/999): loss=0.10352400534031665\n",
      "Stochastic Gradient Descent(443/999): loss=0.10352045369497781\n",
      "Stochastic Gradient Descent(444/999): loss=0.1035169112568464\n",
      "Stochastic Gradient Descent(445/999): loss=0.10351337789094317\n",
      "Stochastic Gradient Descent(446/999): loss=0.1035098534644625\n",
      "Stochastic Gradient Descent(447/999): loss=0.1035063378467371\n",
      "Stochastic Gradient Descent(448/999): loss=0.1035028309092036\n",
      "Stochastic Gradient Descent(449/999): loss=0.10349933252536861\n",
      "Stochastic Gradient Descent(450/999): loss=0.10349584257077524\n",
      "Stochastic Gradient Descent(451/999): loss=0.10349236092297039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(452/999): loss=0.10348888746147232\n",
      "Stochastic Gradient Descent(453/999): loss=0.10348542206773881\n",
      "Stochastic Gradient Descent(454/999): loss=0.10348196462513592\n",
      "Stochastic Gradient Descent(455/999): loss=0.10347851501890724\n",
      "Stochastic Gradient Descent(456/999): loss=0.10347507313614353\n",
      "Stochastic Gradient Descent(457/999): loss=0.10347163886575293\n",
      "Stochastic Gradient Descent(458/999): loss=0.10346821209843167\n",
      "Stochastic Gradient Descent(459/999): loss=0.10346479272663515\n",
      "Stochastic Gradient Descent(460/999): loss=0.10346138064454966\n",
      "Stochastic Gradient Descent(461/999): loss=0.10345797574806438\n",
      "Stochastic Gradient Descent(462/999): loss=0.1034545779347439\n",
      "Stochastic Gradient Descent(463/999): loss=0.10345118710380126\n",
      "Stochastic Gradient Descent(464/999): loss=0.1034478031560712\n",
      "Stochastic Gradient Descent(465/999): loss=0.10344442599398423\n",
      "Stochastic Gradient Descent(466/999): loss=0.10344105552154066\n",
      "Stochastic Gradient Descent(467/999): loss=0.10343769164428543\n",
      "Stochastic Gradient Descent(468/999): loss=0.10343433426928302\n",
      "Stochastic Gradient Descent(469/999): loss=0.10343098330509318\n",
      "Stochastic Gradient Descent(470/999): loss=0.10342763866174656\n",
      "Stochastic Gradient Descent(471/999): loss=0.10342430025072102\n",
      "Stochastic Gradient Descent(472/999): loss=0.10342096798491854\n",
      "Stochastic Gradient Descent(473/999): loss=0.10341764177864188\n",
      "Stochastic Gradient Descent(474/999): loss=0.10341432154757219\n",
      "Stochastic Gradient Descent(475/999): loss=0.10341100720874666\n",
      "Stochastic Gradient Descent(476/999): loss=0.10340769868053683\n",
      "Stochastic Gradient Descent(477/999): loss=0.10340439588262684\n",
      "Stochastic Gradient Descent(478/999): loss=0.10340109873599239\n",
      "Stochastic Gradient Descent(479/999): loss=0.10339780716287987\n",
      "Stochastic Gradient Descent(480/999): loss=0.10339452108678576\n",
      "Stochastic Gradient Descent(481/999): loss=0.10339124043243665\n",
      "Stochastic Gradient Descent(482/999): loss=0.1033879651257693\n",
      "Stochastic Gradient Descent(483/999): loss=0.10338469509391097\n",
      "Stochastic Gradient Descent(484/999): loss=0.10338143026516051\n",
      "Stochastic Gradient Descent(485/999): loss=0.10337817056896921\n",
      "Stochastic Gradient Descent(486/999): loss=0.10337491593592232\n",
      "Stochastic Gradient Descent(487/999): loss=0.10337166629772074\n",
      "Stochastic Gradient Descent(488/999): loss=0.10336842158716299\n",
      "Stochastic Gradient Descent(489/999): loss=0.10336518173812752\n",
      "Stochastic Gradient Descent(490/999): loss=0.10336194668555525\n",
      "Stochastic Gradient Descent(491/999): loss=0.10335871636543253\n",
      "Stochastic Gradient Descent(492/999): loss=0.10335549071477426\n",
      "Stochastic Gradient Descent(493/999): loss=0.10335226967160711\n",
      "Stochastic Gradient Descent(494/999): loss=0.10334905317495342\n",
      "Stochastic Gradient Descent(495/999): loss=0.10334584116481496\n",
      "Stochastic Gradient Descent(496/999): loss=0.10334263358215734\n",
      "Stochastic Gradient Descent(497/999): loss=0.10333943036889424\n",
      "Stochastic Gradient Descent(498/999): loss=0.10333623146787227\n",
      "Stochastic Gradient Descent(499/999): loss=0.10333303682285579\n",
      "Stochastic Gradient Descent(500/999): loss=0.1033298463785122\n",
      "Stochastic Gradient Descent(501/999): loss=0.10332666008039738\n",
      "Stochastic Gradient Descent(502/999): loss=0.10332347787494127\n",
      "Stochastic Gradient Descent(503/999): loss=0.10332029970943384\n",
      "Stochastic Gradient Descent(504/999): loss=0.10331712553201115\n",
      "Stochastic Gradient Descent(505/999): loss=0.10331395529164183\n",
      "Stochastic Gradient Descent(506/999): loss=0.10331078893811348\n",
      "Stochastic Gradient Descent(507/999): loss=0.1033076264220195\n",
      "Stochastic Gradient Descent(508/999): loss=0.10330446769474629\n",
      "Stochastic Gradient Descent(509/999): loss=0.10330131270846007\n",
      "Stochastic Gradient Descent(510/999): loss=0.1032981614160947\n",
      "Stochastic Gradient Descent(511/999): loss=0.103295013771339\n",
      "Stochastic Gradient Descent(512/999): loss=0.10329186972862478\n",
      "Stochastic Gradient Descent(513/999): loss=0.1032887292431147\n",
      "Stochastic Gradient Descent(514/999): loss=0.10328559227069058\n",
      "Stochastic Gradient Descent(515/999): loss=0.10328245876794163\n",
      "Stochastic Gradient Descent(516/999): loss=0.10327932869215335\n",
      "Stochastic Gradient Descent(517/999): loss=0.10327620200129599\n",
      "Stochastic Gradient Descent(518/999): loss=0.10327307865401365\n",
      "Stochastic Gradient Descent(519/999): loss=0.10326995860961341\n",
      "Stochastic Gradient Descent(520/999): loss=0.10326684182805457\n",
      "Stochastic Gradient Descent(521/999): loss=0.10326372826993817\n",
      "Stochastic Gradient Descent(522/999): loss=0.10326061789649664\n",
      "Stochastic Gradient Descent(523/999): loss=0.1032575106695836\n",
      "Stochastic Gradient Descent(524/999): loss=0.10325440655166386\n",
      "Stochastic Gradient Descent(525/999): loss=0.10325130550580351\n",
      "Stochastic Gradient Descent(526/999): loss=0.10324820749566027\n",
      "Stochastic Gradient Descent(527/999): loss=0.10324511248547395\n",
      "Stochastic Gradient Descent(528/999): loss=0.10324202044005702\n",
      "Stochastic Gradient Descent(529/999): loss=0.10323893132478537\n",
      "Stochastic Gradient Descent(530/999): loss=0.1032358451055893\n",
      "Stochastic Gradient Descent(531/999): loss=0.10323276174894438\n",
      "Stochastic Gradient Descent(532/999): loss=0.103229681221863\n",
      "Stochastic Gradient Descent(533/999): loss=0.1032266034918853\n",
      "Stochastic Gradient Descent(534/999): loss=0.10322352852707098\n",
      "Stochastic Gradient Descent(535/999): loss=0.10322045629599078\n",
      "Stochastic Gradient Descent(536/999): loss=0.10321738676771813\n",
      "Stochastic Gradient Descent(537/999): loss=0.10321431991182137\n",
      "Stochastic Gradient Descent(538/999): loss=0.10321125569835535\n",
      "Stochastic Gradient Descent(539/999): loss=0.10320819409785392\n",
      "Stochastic Gradient Descent(540/999): loss=0.10320513508132204\n",
      "Stochastic Gradient Descent(541/999): loss=0.10320207862022818\n",
      "Stochastic Gradient Descent(542/999): loss=0.10319902468649692\n",
      "Stochastic Gradient Descent(543/999): loss=0.10319597325250145\n",
      "Stochastic Gradient Descent(544/999): loss=0.10319292429105656\n",
      "Stochastic Gradient Descent(545/999): loss=0.10318987777541136\n",
      "Stochastic Gradient Descent(546/999): loss=0.10318683367924215\n",
      "Stochastic Gradient Descent(547/999): loss=0.10318379197664593\n",
      "Stochastic Gradient Descent(548/999): loss=0.1031807526421331\n",
      "Stochastic Gradient Descent(549/999): loss=0.10317771565062128\n",
      "Stochastic Gradient Descent(550/999): loss=0.10317468097742832\n",
      "Stochastic Gradient Descent(551/999): loss=0.10317164859826619\n",
      "Stochastic Gradient Descent(552/999): loss=0.10316861848923436\n",
      "Stochastic Gradient Descent(553/999): loss=0.10316559062681371\n",
      "Stochastic Gradient Descent(554/999): loss=0.10316256498786038\n",
      "Stochastic Gradient Descent(555/999): loss=0.10315954154959954\n",
      "Stochastic Gradient Descent(556/999): loss=0.10315652028961966\n",
      "Stochastic Gradient Descent(557/999): loss=0.10315350118586646\n",
      "Stochastic Gradient Descent(558/999): loss=0.10315048421663735\n",
      "Stochastic Gradient Descent(559/999): loss=0.10314746936057556\n",
      "Stochastic Gradient Descent(560/999): loss=0.10314445659666464\n",
      "Stochastic Gradient Descent(561/999): loss=0.10314144590422307\n",
      "Stochastic Gradient Descent(562/999): loss=0.10313843726289869\n",
      "Stochastic Gradient Descent(563/999): loss=0.10313543065266342\n",
      "Stochastic Gradient Descent(564/999): loss=0.10313242605380814\n",
      "Stochastic Gradient Descent(565/999): loss=0.10312942344693747\n",
      "Stochastic Gradient Descent(566/999): loss=0.10312642281296473\n",
      "Stochastic Gradient Descent(567/999): loss=0.10312342413310692\n",
      "Stochastic Gradient Descent(568/999): loss=0.10312042738887983\n",
      "Stochastic Gradient Descent(569/999): loss=0.10311743256209328\n",
      "Stochastic Gradient Descent(570/999): loss=0.10311443963484634\n",
      "Stochastic Gradient Descent(571/999): loss=0.10311144858952263\n",
      "Stochastic Gradient Descent(572/999): loss=0.10310845940878578\n",
      "Stochastic Gradient Descent(573/999): loss=0.10310547207557491\n",
      "Stochastic Gradient Descent(574/999): loss=0.10310248657310017\n",
      "Stochastic Gradient Descent(575/999): loss=0.10309950288483836\n",
      "Stochastic Gradient Descent(576/999): loss=0.10309652099452857\n",
      "Stochastic Gradient Descent(577/999): loss=0.10309354088616814\n",
      "Stochastic Gradient Descent(578/999): loss=0.10309056254400836\n",
      "Stochastic Gradient Descent(579/999): loss=0.10308758595255028\n",
      "Stochastic Gradient Descent(580/999): loss=0.10308461109654092\n",
      "Stochastic Gradient Descent(581/999): loss=0.1030816379609691\n",
      "Stochastic Gradient Descent(582/999): loss=0.10307866653106158\n",
      "Stochastic Gradient Descent(583/999): loss=0.10307569679227936\n",
      "Stochastic Gradient Descent(584/999): loss=0.1030727287303137\n",
      "Stochastic Gradient Descent(585/999): loss=0.10306976233108259\n",
      "Stochastic Gradient Descent(586/999): loss=0.10306679758072691\n",
      "Stochastic Gradient Descent(587/999): loss=0.10306383446560702\n",
      "Stochastic Gradient Descent(588/999): loss=0.10306087297229902\n",
      "Stochastic Gradient Descent(589/999): loss=0.10305791308759153\n",
      "Stochastic Gradient Descent(590/999): loss=0.10305495479848202\n",
      "Stochastic Gradient Descent(591/999): loss=0.10305199809217354\n",
      "Stochastic Gradient Descent(592/999): loss=0.10304904295607144\n",
      "Stochastic Gradient Descent(593/999): loss=0.10304608937778002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(594/999): loss=0.10304313734509943\n",
      "Stochastic Gradient Descent(595/999): loss=0.1030401868460224\n",
      "Stochastic Gradient Descent(596/999): loss=0.1030372378687312\n",
      "Stochastic Gradient Descent(597/999): loss=0.10303429040159462\n",
      "Stochastic Gradient Descent(598/999): loss=0.10303134443316475\n",
      "Stochastic Gradient Descent(599/999): loss=0.10302839995217443\n",
      "Stochastic Gradient Descent(600/999): loss=0.10302545694753389\n",
      "Stochastic Gradient Descent(601/999): loss=0.10302251540832821\n",
      "Stochastic Gradient Descent(602/999): loss=0.10301957532381426\n",
      "Stochastic Gradient Descent(603/999): loss=0.10301663668341816\n",
      "Stochastic Gradient Descent(604/999): loss=0.10301369947673243\n",
      "Stochastic Gradient Descent(605/999): loss=0.10301076369351324\n",
      "Stochastic Gradient Descent(606/999): loss=0.1030078293236779\n",
      "Stochastic Gradient Descent(607/999): loss=0.10300489635730221\n",
      "Stochastic Gradient Descent(608/999): loss=0.10300196478461794\n",
      "Stochastic Gradient Descent(609/999): loss=0.10299903459601015\n",
      "Stochastic Gradient Descent(610/999): loss=0.10299610578201505\n",
      "Stochastic Gradient Descent(611/999): loss=0.10299317833331717\n",
      "Stochastic Gradient Descent(612/999): loss=0.10299025224074722\n",
      "Stochastic Gradient Descent(613/999): loss=0.10298732749527972\n",
      "Stochastic Gradient Descent(614/999): loss=0.1029844040880306\n",
      "Stochastic Gradient Descent(615/999): loss=0.10298148201025492\n",
      "Stochastic Gradient Descent(616/999): loss=0.10297856125334474\n",
      "Stochastic Gradient Descent(617/999): loss=0.1029756418088267\n",
      "Stochastic Gradient Descent(618/999): loss=0.1029727236683601\n",
      "Stochastic Gradient Descent(619/999): loss=0.10296980682373462\n",
      "Stochastic Gradient Descent(620/999): loss=0.10296689126686821\n",
      "Stochastic Gradient Descent(621/999): loss=0.10296397698980499\n",
      "Stochastic Gradient Descent(622/999): loss=0.10296106398471343\n",
      "Stochastic Gradient Descent(623/999): loss=0.10295815224388406\n",
      "Stochastic Gradient Descent(624/999): loss=0.10295524175972769\n",
      "Stochastic Gradient Descent(625/999): loss=0.10295233252477341\n",
      "Stochastic Gradient Descent(626/999): loss=0.10294942453166667\n",
      "Stochastic Gradient Descent(627/999): loss=0.10294651777316745\n",
      "Stochastic Gradient Descent(628/999): loss=0.10294361224214836\n",
      "Stochastic Gradient Descent(629/999): loss=0.10294070793159284\n",
      "Stochastic Gradient Descent(630/999): loss=0.10293780483459346\n",
      "Stochastic Gradient Descent(631/999): loss=0.10293490294434991\n",
      "Stochastic Gradient Descent(632/999): loss=0.10293200225416768\n",
      "Stochastic Gradient Descent(633/999): loss=0.10292910275745595\n",
      "Stochastic Gradient Descent(634/999): loss=0.10292620444772618\n",
      "Stochastic Gradient Descent(635/999): loss=0.10292330731859028\n",
      "Stochastic Gradient Descent(636/999): loss=0.10292041136375916\n",
      "Stochastic Gradient Descent(637/999): loss=0.10291751657704103\n",
      "Stochastic Gradient Descent(638/999): loss=0.10291462295233983\n",
      "Stochastic Gradient Descent(639/999): loss=0.10291173048365379\n",
      "Stochastic Gradient Descent(640/999): loss=0.10290883916507378\n",
      "Stochastic Gradient Descent(641/999): loss=0.10290594899078183\n",
      "Stochastic Gradient Descent(642/999): loss=0.10290305995504981\n",
      "Stochastic Gradient Descent(643/999): loss=0.10290017205223777\n",
      "Stochastic Gradient Descent(644/999): loss=0.10289728527679269\n",
      "Stochastic Gradient Descent(645/999): loss=0.102894399623247\n",
      "Stochastic Gradient Descent(646/999): loss=0.10289151508621715\n",
      "Stochastic Gradient Descent(647/999): loss=0.10288863166040245\n",
      "Stochastic Gradient Descent(648/999): loss=0.10288574934058348\n",
      "Stochastic Gradient Descent(649/999): loss=0.10288286812162095\n",
      "Stochastic Gradient Descent(650/999): loss=0.10287998799845435\n",
      "Stochastic Gradient Descent(651/999): loss=0.10287710896610076\n",
      "Stochastic Gradient Descent(652/999): loss=0.10287423101965348\n",
      "Stochastic Gradient Descent(653/999): loss=0.1028713541542808\n",
      "Stochastic Gradient Descent(654/999): loss=0.10286847836522497\n",
      "Stochastic Gradient Descent(655/999): loss=0.10286560364780081\n",
      "Stochastic Gradient Descent(656/999): loss=0.10286272999739457\n",
      "Stochastic Gradient Descent(657/999): loss=0.10285985740946293\n",
      "Stochastic Gradient Descent(658/999): loss=0.10285698587953176\n",
      "Stochastic Gradient Descent(659/999): loss=0.10285411540319489\n",
      "Stochastic Gradient Descent(660/999): loss=0.10285124597611317\n",
      "Stochastic Gradient Descent(661/999): loss=0.10284837759401345\n",
      "Stochastic Gradient Descent(662/999): loss=0.10284551025268733\n",
      "Stochastic Gradient Descent(663/999): loss=0.1028426439479902\n",
      "Stochastic Gradient Descent(664/999): loss=0.10283977867584021\n",
      "Stochastic Gradient Descent(665/999): loss=0.10283691443221733\n",
      "Stochastic Gradient Descent(666/999): loss=0.10283405121316222\n",
      "Stochastic Gradient Descent(667/999): loss=0.1028311890147754\n",
      "Stochastic Gradient Descent(668/999): loss=0.10282832783321612\n",
      "Stochastic Gradient Descent(669/999): loss=0.10282546766470149\n",
      "Stochastic Gradient Descent(670/999): loss=0.10282260850550565\n",
      "Stochastic Gradient Descent(671/999): loss=0.10281975035195874\n",
      "Stochastic Gradient Descent(672/999): loss=0.10281689320044596\n",
      "Stochastic Gradient Descent(673/999): loss=0.10281403704740673\n",
      "Stochastic Gradient Descent(674/999): loss=0.10281118188933398\n",
      "Stochastic Gradient Descent(675/999): loss=0.10280832772277294\n",
      "Stochastic Gradient Descent(676/999): loss=0.10280547454432064\n",
      "Stochastic Gradient Descent(677/999): loss=0.10280262235062476\n",
      "Stochastic Gradient Descent(678/999): loss=0.10279977113838323\n",
      "Stochastic Gradient Descent(679/999): loss=0.10279692090434298\n",
      "Stochastic Gradient Descent(680/999): loss=0.10279407164529937\n",
      "Stochastic Gradient Descent(681/999): loss=0.10279122335809546\n",
      "Stochastic Gradient Descent(682/999): loss=0.10278837603962111\n",
      "Stochastic Gradient Descent(683/999): loss=0.10278552968681232\n",
      "Stochastic Gradient Descent(684/999): loss=0.1027826842966504\n",
      "Stochastic Gradient Descent(685/999): loss=0.10277983986616131\n",
      "Stochastic Gradient Descent(686/999): loss=0.102776996392415\n",
      "Stochastic Gradient Descent(687/999): loss=0.10277415387252449\n",
      "Stochastic Gradient Descent(688/999): loss=0.10277131230364539\n",
      "Stochastic Gradient Descent(689/999): loss=0.10276847168297512\n",
      "Stochastic Gradient Descent(690/999): loss=0.10276563200775232\n",
      "Stochastic Gradient Descent(691/999): loss=0.10276279327525596\n",
      "Stochastic Gradient Descent(692/999): loss=0.10275995548280496\n",
      "Stochastic Gradient Descent(693/999): loss=0.10275711862775745\n",
      "Stochastic Gradient Descent(694/999): loss=0.10275428270751008\n",
      "Stochastic Gradient Descent(695/999): loss=0.10275144771949743\n",
      "Stochastic Gradient Descent(696/999): loss=0.10274861366119145\n",
      "Stochastic Gradient Descent(697/999): loss=0.10274578053010086\n",
      "Stochastic Gradient Descent(698/999): loss=0.1027429483237704\n",
      "Stochastic Gradient Descent(699/999): loss=0.10274011703978046\n",
      "Stochastic Gradient Descent(700/999): loss=0.10273728667574648\n",
      "Stochastic Gradient Descent(701/999): loss=0.10273445722931814\n",
      "Stochastic Gradient Descent(702/999): loss=0.10273162869817914\n",
      "Stochastic Gradient Descent(703/999): loss=0.1027288010800465\n",
      "Stochastic Gradient Descent(704/999): loss=0.10272597437266992\n",
      "Stochastic Gradient Descent(705/999): loss=0.1027231485738315\n",
      "Stochastic Gradient Descent(706/999): loss=0.10272032368134495\n",
      "Stochastic Gradient Descent(707/999): loss=0.10271749969305534\n",
      "Stochastic Gradient Descent(708/999): loss=0.10271467660683842\n",
      "Stochastic Gradient Descent(709/999): loss=0.1027118544206001\n",
      "Stochastic Gradient Descent(710/999): loss=0.10270903313227614\n",
      "Stochastic Gradient Descent(711/999): loss=0.10270621273983155\n",
      "Stochastic Gradient Descent(712/999): loss=0.10270339324126013\n",
      "Stochastic Gradient Descent(713/999): loss=0.10270057463458394\n",
      "Stochastic Gradient Descent(714/999): loss=0.10269775691785307\n",
      "Stochastic Gradient Descent(715/999): loss=0.1026949400891449\n",
      "Stochastic Gradient Descent(716/999): loss=0.10269212414656399\n",
      "Stochastic Gradient Descent(717/999): loss=0.1026893090882412\n",
      "Stochastic Gradient Descent(718/999): loss=0.10268649491233373\n",
      "Stochastic Gradient Descent(719/999): loss=0.10268368161702439\n",
      "Stochastic Gradient Descent(720/999): loss=0.1026808692005213\n",
      "Stochastic Gradient Descent(721/999): loss=0.10267805766105752\n",
      "Stochastic Gradient Descent(722/999): loss=0.10267524699689054\n",
      "Stochastic Gradient Descent(723/999): loss=0.10267243720630195\n",
      "Stochastic Gradient Descent(724/999): loss=0.10266962828759708\n",
      "Stochastic Gradient Descent(725/999): loss=0.10266682023910467\n",
      "Stochastic Gradient Descent(726/999): loss=0.10266401305917619\n",
      "Stochastic Gradient Descent(727/999): loss=0.10266120674618588\n",
      "Stochastic Gradient Descent(728/999): loss=0.10265840129853013\n",
      "Stochastic Gradient Descent(729/999): loss=0.10265559671462723\n",
      "Stochastic Gradient Descent(730/999): loss=0.10265279299291698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(731/999): loss=0.10264999013186028\n",
      "Stochastic Gradient Descent(732/999): loss=0.10264718812993899\n",
      "Stochastic Gradient Descent(733/999): loss=0.10264438698565528\n",
      "Stochastic Gradient Descent(734/999): loss=0.10264158669753169\n",
      "Stochastic Gradient Descent(735/999): loss=0.10263878726411044\n",
      "Stochastic Gradient Descent(736/999): loss=0.10263598868395331\n",
      "Stochastic Gradient Descent(737/999): loss=0.10263319095564136\n",
      "Stochastic Gradient Descent(738/999): loss=0.10263039407777441\n",
      "Stochastic Gradient Descent(739/999): loss=0.10262759804897108\n",
      "Stochastic Gradient Descent(740/999): loss=0.10262480286786807\n",
      "Stochastic Gradient Descent(741/999): loss=0.10262200853312022\n",
      "Stochastic Gradient Descent(742/999): loss=0.10261921504340005\n",
      "Stochastic Gradient Descent(743/999): loss=0.10261642239739753\n",
      "Stochastic Gradient Descent(744/999): loss=0.10261363059381971\n",
      "Stochastic Gradient Descent(745/999): loss=0.10261083963139055\n",
      "Stochastic Gradient Descent(746/999): loss=0.10260804950885068\n",
      "Stochastic Gradient Descent(747/999): loss=0.10260526022495699\n",
      "Stochastic Gradient Descent(748/999): loss=0.10260247177848247\n",
      "Stochastic Gradient Descent(749/999): loss=0.10259968416821591\n",
      "Stochastic Gradient Descent(750/999): loss=0.10259689739296177\n",
      "Stochastic Gradient Descent(751/999): loss=0.10259411145153964\n",
      "Stochastic Gradient Descent(752/999): loss=0.10259132634278427\n",
      "Stochastic Gradient Descent(753/999): loss=0.10258854206554532\n",
      "Stochastic Gradient Descent(754/999): loss=0.1025857586186869\n",
      "Stochastic Gradient Descent(755/999): loss=0.1025829760010876\n",
      "Stochastic Gradient Descent(756/999): loss=0.10258019421164001\n",
      "Stochastic Gradient Descent(757/999): loss=0.10257741324925075\n",
      "Stochastic Gradient Descent(758/999): loss=0.10257463311283997\n",
      "Stochastic Gradient Descent(759/999): loss=0.1025718538013414\n",
      "Stochastic Gradient Descent(760/999): loss=0.102569075313702\n",
      "Stochastic Gradient Descent(761/999): loss=0.10256629764888166\n",
      "Stochastic Gradient Descent(762/999): loss=0.10256352080585322\n",
      "Stochastic Gradient Descent(763/999): loss=0.102560744783602\n",
      "Stochastic Gradient Descent(764/999): loss=0.10255796958112588\n",
      "Stochastic Gradient Descent(765/999): loss=0.10255519519743486\n",
      "Stochastic Gradient Descent(766/999): loss=0.10255242163155094\n",
      "Stochastic Gradient Descent(767/999): loss=0.102549648882508\n",
      "Stochastic Gradient Descent(768/999): loss=0.10254687694935159\n",
      "Stochastic Gradient Descent(769/999): loss=0.10254410583113859\n",
      "Stochastic Gradient Descent(770/999): loss=0.10254133552693726\n",
      "Stochastic Gradient Descent(771/999): loss=0.1025385660358269\n",
      "Stochastic Gradient Descent(772/999): loss=0.10253579735689772\n",
      "Stochastic Gradient Descent(773/999): loss=0.10253302948925064\n",
      "Stochastic Gradient Descent(774/999): loss=0.1025302624319972\n",
      "Stochastic Gradient Descent(775/999): loss=0.10252749618425935\n",
      "Stochastic Gradient Descent(776/999): loss=0.10252473074516916\n",
      "Stochastic Gradient Descent(777/999): loss=0.10252196611386886\n",
      "Stochastic Gradient Descent(778/999): loss=0.1025192022895105\n",
      "Stochastic Gradient Descent(779/999): loss=0.10251643927125599\n",
      "Stochastic Gradient Descent(780/999): loss=0.10251367705827674\n",
      "Stochastic Gradient Descent(781/999): loss=0.10251091564975365\n",
      "Stochastic Gradient Descent(782/999): loss=0.10250815504487683\n",
      "Stochastic Gradient Descent(783/999): loss=0.10250539524284563\n",
      "Stochastic Gradient Descent(784/999): loss=0.10250263624286825\n",
      "Stochastic Gradient Descent(785/999): loss=0.10249987804416191\n",
      "Stochastic Gradient Descent(786/999): loss=0.10249712064595236\n",
      "Stochastic Gradient Descent(787/999): loss=0.10249436404747413\n",
      "Stochastic Gradient Descent(788/999): loss=0.10249160824796992\n",
      "Stochastic Gradient Descent(789/999): loss=0.10248885324669095\n",
      "Stochastic Gradient Descent(790/999): loss=0.10248609904289645\n",
      "Stochastic Gradient Descent(791/999): loss=0.10248334563585382\n",
      "Stochastic Gradient Descent(792/999): loss=0.10248059302483822\n",
      "Stochastic Gradient Descent(793/999): loss=0.10247784120913271\n",
      "Stochastic Gradient Descent(794/999): loss=0.10247509018802792\n",
      "Stochastic Gradient Descent(795/999): loss=0.10247233996082207\n",
      "Stochastic Gradient Descent(796/999): loss=0.10246959052682077\n",
      "Stochastic Gradient Descent(797/999): loss=0.10246684188533693\n",
      "Stochastic Gradient Descent(798/999): loss=0.10246409403569054\n",
      "Stochastic Gradient Descent(799/999): loss=0.10246134697720889\n",
      "Stochastic Gradient Descent(800/999): loss=0.10245860070922597\n",
      "Stochastic Gradient Descent(801/999): loss=0.10245585523108278\n",
      "Stochastic Gradient Descent(802/999): loss=0.10245311054212691\n",
      "Stochastic Gradient Descent(803/999): loss=0.10245036664171277\n",
      "Stochastic Gradient Descent(804/999): loss=0.10244762352920106\n",
      "Stochastic Gradient Descent(805/999): loss=0.10244488120395907\n",
      "Stochastic Gradient Descent(806/999): loss=0.10244213966536031\n",
      "Stochastic Gradient Descent(807/999): loss=0.10243939891278453\n",
      "Stochastic Gradient Descent(808/999): loss=0.10243665894561761\n",
      "Stochastic Gradient Descent(809/999): loss=0.10243391976325142\n",
      "Stochastic Gradient Descent(810/999): loss=0.10243118136508379\n",
      "Stochastic Gradient Descent(811/999): loss=0.10242844375051832\n",
      "Stochastic Gradient Descent(812/999): loss=0.10242570691896438\n",
      "Stochastic Gradient Descent(813/999): loss=0.10242297086983701\n",
      "Stochastic Gradient Descent(814/999): loss=0.10242023560255671\n",
      "Stochastic Gradient Descent(815/999): loss=0.10241750111654956\n",
      "Stochastic Gradient Descent(816/999): loss=0.10241476741124693\n",
      "Stochastic Gradient Descent(817/999): loss=0.10241203448608552\n",
      "Stochastic Gradient Descent(818/999): loss=0.10240930234050726\n",
      "Stochastic Gradient Descent(819/999): loss=0.10240657097395921\n",
      "Stochastic Gradient Descent(820/999): loss=0.10240384038589335\n",
      "Stochastic Gradient Descent(821/999): loss=0.10240111057576683\n",
      "Stochastic Gradient Descent(822/999): loss=0.10239838154304151\n",
      "Stochastic Gradient Descent(823/999): loss=0.1023956532871842\n",
      "Stochastic Gradient Descent(824/999): loss=0.1023929258076663\n",
      "Stochastic Gradient Descent(825/999): loss=0.10239019910396399\n",
      "Stochastic Gradient Descent(826/999): loss=0.10238747317555799\n",
      "Stochastic Gradient Descent(827/999): loss=0.10238474802193347\n",
      "Stochastic Gradient Descent(828/999): loss=0.1023820236425802\n",
      "Stochastic Gradient Descent(829/999): loss=0.10237930003699212\n",
      "Stochastic Gradient Descent(830/999): loss=0.1023765772046676\n",
      "Stochastic Gradient Descent(831/999): loss=0.10237385514510924\n",
      "Stochastic Gradient Descent(832/999): loss=0.10237113385782376\n",
      "Stochastic Gradient Descent(833/999): loss=0.10236841334232202\n",
      "Stochastic Gradient Descent(834/999): loss=0.10236569359811885\n",
      "Stochastic Gradient Descent(835/999): loss=0.10236297462473316\n",
      "Stochastic Gradient Descent(836/999): loss=0.10236025642168765\n",
      "Stochastic Gradient Descent(837/999): loss=0.10235753898850898\n",
      "Stochastic Gradient Descent(838/999): loss=0.10235482232472752\n",
      "Stochastic Gradient Descent(839/999): loss=0.1023521064298774\n",
      "Stochastic Gradient Descent(840/999): loss=0.10234939130349634\n",
      "Stochastic Gradient Descent(841/999): loss=0.10234667694512582\n",
      "Stochastic Gradient Descent(842/999): loss=0.10234396335431076\n",
      "Stochastic Gradient Descent(843/999): loss=0.10234125053059966\n",
      "Stochastic Gradient Descent(844/999): loss=0.10233853847354434\n",
      "Stochastic Gradient Descent(845/999): loss=0.10233582718270012\n",
      "Stochastic Gradient Descent(846/999): loss=0.10233311665762562\n",
      "Stochastic Gradient Descent(847/999): loss=0.10233040689788275\n",
      "Stochastic Gradient Descent(848/999): loss=0.10232769790303664\n",
      "Stochastic Gradient Descent(849/999): loss=0.1023249896726556\n",
      "Stochastic Gradient Descent(850/999): loss=0.10232228220631108\n",
      "Stochastic Gradient Descent(851/999): loss=0.1023195755035776\n",
      "Stochastic Gradient Descent(852/999): loss=0.1023168695640327\n",
      "Stochastic Gradient Descent(853/999): loss=0.10231416438725703\n",
      "Stochastic Gradient Descent(854/999): loss=0.10231145997283395\n",
      "Stochastic Gradient Descent(855/999): loss=0.10230875632034991\n",
      "Stochastic Gradient Descent(856/999): loss=0.10230605342939413\n",
      "Stochastic Gradient Descent(857/999): loss=0.1023033512995586\n",
      "Stochastic Gradient Descent(858/999): loss=0.10230064993043816\n",
      "Stochastic Gradient Descent(859/999): loss=0.10229794932163036\n",
      "Stochastic Gradient Descent(860/999): loss=0.10229524947273529\n",
      "Stochastic Gradient Descent(861/999): loss=0.10229255038335577\n",
      "Stochastic Gradient Descent(862/999): loss=0.10228985205309728\n",
      "Stochastic Gradient Descent(863/999): loss=0.10228715448156771\n",
      "Stochastic Gradient Descent(864/999): loss=0.10228445766837758\n",
      "Stochastic Gradient Descent(865/999): loss=0.10228176161313977\n",
      "Stochastic Gradient Descent(866/999): loss=0.10227906631546972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(867/999): loss=0.10227637177498515\n",
      "Stochastic Gradient Descent(868/999): loss=0.10227367799130621\n",
      "Stochastic Gradient Descent(869/999): loss=0.10227098496405533\n",
      "Stochastic Gradient Descent(870/999): loss=0.10226829269285727\n",
      "Stochastic Gradient Descent(871/999): loss=0.10226560117733896\n",
      "Stochastic Gradient Descent(872/999): loss=0.10226291041712966\n",
      "Stochastic Gradient Descent(873/999): loss=0.10226022041186074\n",
      "Stochastic Gradient Descent(874/999): loss=0.10225753116116572\n",
      "Stochastic Gradient Descent(875/999): loss=0.10225484266468024\n",
      "Stochastic Gradient Descent(876/999): loss=0.10225215492204197\n",
      "Stochastic Gradient Descent(877/999): loss=0.1022494679328908\n",
      "Stochastic Gradient Descent(878/999): loss=0.1022467816968684\n",
      "Stochastic Gradient Descent(879/999): loss=0.10224409621361864\n",
      "Stochastic Gradient Descent(880/999): loss=0.1022414114827872\n",
      "Stochastic Gradient Descent(881/999): loss=0.10223872750402176\n",
      "Stochastic Gradient Descent(882/999): loss=0.10223604427697183\n",
      "Stochastic Gradient Descent(883/999): loss=0.10223336180128884\n",
      "Stochastic Gradient Descent(884/999): loss=0.10223068007662608\n",
      "Stochastic Gradient Descent(885/999): loss=0.10222799910263856\n",
      "Stochastic Gradient Descent(886/999): loss=0.10222531887898319\n",
      "Stochastic Gradient Descent(887/999): loss=0.10222263940531853\n",
      "Stochastic Gradient Descent(888/999): loss=0.1022199606813049\n",
      "Stochastic Gradient Descent(889/999): loss=0.10221728270660436\n",
      "Stochastic Gradient Descent(890/999): loss=0.10221460548088059\n",
      "Stochastic Gradient Descent(891/999): loss=0.10221192900379894\n",
      "Stochastic Gradient Descent(892/999): loss=0.10220925327502642\n",
      "Stochastic Gradient Descent(893/999): loss=0.10220657829423158\n",
      "Stochastic Gradient Descent(894/999): loss=0.10220390406108455\n",
      "Stochastic Gradient Descent(895/999): loss=0.10220123057525708\n",
      "Stochastic Gradient Descent(896/999): loss=0.10219855783642234\n",
      "Stochastic Gradient Descent(897/999): loss=0.10219588584425511\n",
      "Stochastic Gradient Descent(898/999): loss=0.10219321459843157\n",
      "Stochastic Gradient Descent(899/999): loss=0.10219054409862943\n",
      "Stochastic Gradient Descent(900/999): loss=0.10218787434452775\n",
      "Stochastic Gradient Descent(901/999): loss=0.10218520533580706\n",
      "Stochastic Gradient Descent(902/999): loss=0.1021825370721493\n",
      "Stochastic Gradient Descent(903/999): loss=0.10217986955323774\n",
      "Stochastic Gradient Descent(904/999): loss=0.10217720277875703\n",
      "Stochastic Gradient Descent(905/999): loss=0.10217453674839308\n",
      "Stochastic Gradient Descent(906/999): loss=0.10217187146183324\n",
      "Stochastic Gradient Descent(907/999): loss=0.10216920691876605\n",
      "Stochastic Gradient Descent(908/999): loss=0.10216654311888133\n",
      "Stochastic Gradient Descent(909/999): loss=0.10216388006187023\n",
      "Stochastic Gradient Descent(910/999): loss=0.10216121774742501\n",
      "Stochastic Gradient Descent(911/999): loss=0.10215855617523925\n",
      "Stochastic Gradient Descent(912/999): loss=0.10215589534500763\n",
      "Stochastic Gradient Descent(913/999): loss=0.10215323525642617\n",
      "Stochastic Gradient Descent(914/999): loss=0.10215057590919183\n",
      "Stochastic Gradient Descent(915/999): loss=0.1021479173030029\n",
      "Stochastic Gradient Descent(916/999): loss=0.1021452594375587\n",
      "Stochastic Gradient Descent(917/999): loss=0.10214260231255971\n",
      "Stochastic Gradient Descent(918/999): loss=0.1021399459277075\n",
      "Stochastic Gradient Descent(919/999): loss=0.10213729028270456\n",
      "Stochastic Gradient Descent(920/999): loss=0.10213463537725477\n",
      "Stochastic Gradient Descent(921/999): loss=0.10213198121106272\n",
      "Stochastic Gradient Descent(922/999): loss=0.10212932778383424\n",
      "Stochastic Gradient Descent(923/999): loss=0.10212667509527605\n",
      "Stochastic Gradient Descent(924/999): loss=0.10212402314509599\n",
      "Stochastic Gradient Descent(925/999): loss=0.10212137193300277\n",
      "Stochastic Gradient Descent(926/999): loss=0.10211872145870614\n",
      "Stochastic Gradient Descent(927/999): loss=0.10211607172191677\n",
      "Stochastic Gradient Descent(928/999): loss=0.10211342272234633\n",
      "Stochastic Gradient Descent(929/999): loss=0.10211077445970734\n",
      "Stochastic Gradient Descent(930/999): loss=0.10210812693371321\n",
      "Stochastic Gradient Descent(931/999): loss=0.10210548014407843\n",
      "Stochastic Gradient Descent(932/999): loss=0.1021028340905182\n",
      "Stochastic Gradient Descent(933/999): loss=0.1021001887727486\n",
      "Stochastic Gradient Descent(934/999): loss=0.10209754419048672\n",
      "Stochastic Gradient Descent(935/999): loss=0.10209490034345034\n",
      "Stochastic Gradient Descent(936/999): loss=0.10209225723135816\n",
      "Stochastic Gradient Descent(937/999): loss=0.1020896148539297\n",
      "Stochastic Gradient Descent(938/999): loss=0.10208697321088525\n",
      "Stochastic Gradient Descent(939/999): loss=0.10208433230194595\n",
      "Stochastic Gradient Descent(940/999): loss=0.10208169212683366\n",
      "Stochastic Gradient Descent(941/999): loss=0.10207905268527115\n",
      "Stochastic Gradient Descent(942/999): loss=0.1020764139769818\n",
      "Stochastic Gradient Descent(943/999): loss=0.10207377600168983\n",
      "Stochastic Gradient Descent(944/999): loss=0.10207113875912026\n",
      "Stochastic Gradient Descent(945/999): loss=0.10206850224899869\n",
      "Stochastic Gradient Descent(946/999): loss=0.10206586647105162\n",
      "Stochastic Gradient Descent(947/999): loss=0.10206323142500608\n",
      "Stochastic Gradient Descent(948/999): loss=0.10206059711058996\n",
      "Stochastic Gradient Descent(949/999): loss=0.10205796352753177\n",
      "Stochastic Gradient Descent(950/999): loss=0.10205533067556075\n",
      "Stochastic Gradient Descent(951/999): loss=0.10205269855440673\n",
      "Stochastic Gradient Descent(952/999): loss=0.10205006716380026\n",
      "Stochastic Gradient Descent(953/999): loss=0.10204743650347259\n",
      "Stochastic Gradient Descent(954/999): loss=0.10204480657315551\n",
      "Stochastic Gradient Descent(955/999): loss=0.1020421773725815\n",
      "Stochastic Gradient Descent(956/999): loss=0.10203954890148371\n",
      "Stochastic Gradient Descent(957/999): loss=0.10203692115959584\n",
      "Stochastic Gradient Descent(958/999): loss=0.10203429414665226\n",
      "Stochastic Gradient Descent(959/999): loss=0.10203166786238788\n",
      "Stochastic Gradient Descent(960/999): loss=0.10202904230653823\n",
      "Stochastic Gradient Descent(961/999): loss=0.10202641747883948\n",
      "Stochastic Gradient Descent(962/999): loss=0.10202379337902824\n",
      "Stochastic Gradient Descent(963/999): loss=0.10202117000684184\n",
      "Stochastic Gradient Descent(964/999): loss=0.10201854736201811\n",
      "Stochastic Gradient Descent(965/999): loss=0.1020159254442954\n",
      "Stochastic Gradient Descent(966/999): loss=0.1020133042534126\n",
      "Stochastic Gradient Descent(967/999): loss=0.10201068378910928\n",
      "Stochastic Gradient Descent(968/999): loss=0.10200806405112534\n",
      "Stochastic Gradient Descent(969/999): loss=0.10200544503920136\n",
      "Stochastic Gradient Descent(970/999): loss=0.10200282675307831\n",
      "Stochastic Gradient Descent(971/999): loss=0.10200020919249779\n",
      "Stochastic Gradient Descent(972/999): loss=0.10199759235720182\n",
      "Stochastic Gradient Descent(973/999): loss=0.10199497624693298\n",
      "Stochastic Gradient Descent(974/999): loss=0.10199236086143423\n",
      "Stochastic Gradient Descent(975/999): loss=0.10198974620044916\n",
      "Stochastic Gradient Descent(976/999): loss=0.10198713226372176\n",
      "Stochastic Gradient Descent(977/999): loss=0.10198451905099643\n",
      "Stochastic Gradient Descent(978/999): loss=0.10198190656201815\n",
      "Stochastic Gradient Descent(979/999): loss=0.10197929479653227\n",
      "Stochastic Gradient Descent(980/999): loss=0.10197668375428466\n",
      "Stochastic Gradient Descent(981/999): loss=0.10197407343502157\n",
      "Stochastic Gradient Descent(982/999): loss=0.10197146383848973\n",
      "Stochastic Gradient Descent(983/999): loss=0.10196885496443628\n",
      "Stochastic Gradient Descent(984/999): loss=0.10196624681260884\n",
      "Stochastic Gradient Descent(985/999): loss=0.10196363938275539\n",
      "Stochastic Gradient Descent(986/999): loss=0.10196103267462432\n",
      "Stochastic Gradient Descent(987/999): loss=0.10195842668796454\n",
      "Stochastic Gradient Descent(988/999): loss=0.10195582142252521\n",
      "Stochastic Gradient Descent(989/999): loss=0.10195321687805602\n",
      "Stochastic Gradient Descent(990/999): loss=0.10195061305430698\n",
      "Stochastic Gradient Descent(991/999): loss=0.10194800995102854\n",
      "Stochastic Gradient Descent(992/999): loss=0.10194540756797149\n",
      "Stochastic Gradient Descent(993/999): loss=0.10194280590488704\n",
      "Stochastic Gradient Descent(994/999): loss=0.10194020496152673\n",
      "Stochastic Gradient Descent(995/999): loss=0.10193760473764256\n",
      "Stochastic Gradient Descent(996/999): loss=0.10193500523298678\n",
      "Stochastic Gradient Descent(997/999): loss=0.10193240644731212\n",
      "Stochastic Gradient Descent(998/999): loss=0.10192980838037155\n",
      "Stochastic Gradient Descent(999/999): loss=0.10192721103191846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(0/999): loss=0.14738697924684596\n",
      "Stochastic Gradient Descent(1/999): loss=0.14669780540718066\n",
      "Stochastic Gradient Descent(2/999): loss=0.1460196931226375\n",
      "Stochastic Gradient Descent(3/999): loss=0.1453524639961078\n",
      "Stochastic Gradient Descent(4/999): loss=0.1446959425078572\n",
      "Stochastic Gradient Descent(5/999): loss=0.14404995596911616\n",
      "Stochastic Gradient Descent(6/999): loss=0.14341433447641927\n",
      "Stochastic Gradient Descent(7/999): loss=0.14278891086668083\n",
      "Stochastic Gradient Descent(8/999): loss=0.142173520672995\n",
      "Stochastic Gradient Descent(9/999): loss=0.14156800208114903\n",
      "Stochastic Gradient Descent(10/999): loss=0.14097219588683788\n",
      "Stochastic Gradient Descent(11/999): loss=0.14038594545356856\n",
      "Stochastic Gradient Descent(12/999): loss=0.13980909667124386\n",
      "Stochastic Gradient Descent(13/999): loss=0.1392414979154141\n",
      "Stochastic Gradient Descent(14/999): loss=0.13868300000718564\n",
      "Stochastic Gradient Descent(15/999): loss=0.13813345617377662\n",
      "Stochastic Gradient Descent(16/999): loss=0.13759272200970835\n",
      "Stochastic Gradient Descent(17/999): loss=0.1370606554386232\n",
      "Stochastic Gradient Descent(18/999): loss=0.1365371166757178\n",
      "Stochastic Gradient Descent(19/999): loss=0.13602196819078255\n",
      "Stochastic Gradient Descent(20/999): loss=0.13551507467183693\n",
      "Stochastic Gradient Descent(21/999): loss=0.13501630298935202\n",
      "Stochastic Gradient Descent(22/999): loss=0.13452552216104877\n",
      "Stochastic Gradient Descent(23/999): loss=0.1340426033172657\n",
      "Stochastic Gradient Descent(24/999): loss=0.13356741966688376\n",
      "Stochastic Gradient Descent(25/999): loss=0.1330998464638017\n",
      "Stochastic Gradient Descent(26/999): loss=0.13263976097395172\n",
      "Stochastic Gradient Descent(27/999): loss=0.13218704244284718\n",
      "Stochastic Gradient Descent(28/999): loss=0.1317415720636539\n",
      "Stochastic Gradient Descent(29/999): loss=0.13130323294577625\n",
      "Stochastic Gradient Descent(30/999): loss=0.13087191008395027\n",
      "Stochastic Gradient Descent(31/999): loss=0.13044749032783454\n",
      "Stochastic Gradient Descent(32/999): loss=0.13002986235209238\n",
      "Stochastic Gradient Descent(33/999): loss=0.12961891662695624\n",
      "Stochastic Gradient Descent(34/999): loss=0.1292145453892666\n",
      "Stochastic Gradient Descent(35/999): loss=0.12881664261397835\n",
      "Stochastic Gradient Descent(36/999): loss=0.12842510398612678\n",
      "Stochastic Gradient Descent(37/999): loss=0.12803982687324525\n",
      "Stochastic Gradient Descent(38/999): loss=0.12766071029822834\n",
      "Stochastic Gradient Descent(39/999): loss=0.12728765491263214\n",
      "Stochastic Gradient Descent(40/999): loss=0.12692056297040594\n",
      "Stochastic Gradient Descent(41/999): loss=0.12655933830204652\n",
      "Stochastic Gradient Descent(42/999): loss=0.12620388628917031\n",
      "Stochastic Gradient Descent(43/999): loss=0.12585411383949513\n",
      "Stochastic Gradient Descent(44/999): loss=0.12550992936222574\n",
      "Stochastic Gradient Descent(45/999): loss=0.12517124274383595\n",
      "Stochastic Gradient Descent(46/999): loss=0.12483796532424195\n",
      "Stochastic Gradient Descent(47/999): loss=0.12451000987335947\n",
      "Stochastic Gradient Descent(48/999): loss=0.12418729056803936\n",
      "Stochastic Gradient Descent(49/999): loss=0.12386972296937486\n",
      "Stochastic Gradient Descent(50/999): loss=0.12355722400037514\n",
      "Stochastic Gradient Descent(51/999): loss=0.12324971192399892\n",
      "Stochastic Gradient Descent(52/999): loss=0.1229471063215423\n",
      "Stochastic Gradient Descent(53/999): loss=0.12264932807137514\n",
      "Stochastic Gradient Descent(54/999): loss=0.12235629932802056\n",
      "Stochastic Gradient Descent(55/999): loss=0.12206794350157169\n",
      "Stochastic Gradient Descent(56/999): loss=0.12178418523744057\n",
      "Stochastic Gradient Descent(57/999): loss=0.12150495039643362\n",
      "Stochastic Gradient Descent(58/999): loss=0.1212301660351483\n",
      "Stochastic Gradient Descent(59/999): loss=0.12095976038668636\n",
      "Stochastic Gradient Descent(60/999): loss=0.1206936628416777\n",
      "Stochastic Gradient Descent(61/999): loss=0.12043180392961066\n",
      "Stochastic Gradient Descent(62/999): loss=0.12017411530046326\n",
      "Stochastic Gradient Descent(63/999): loss=0.11992052970663071\n",
      "Stochastic Gradient Descent(64/999): loss=0.11967098098514449\n",
      "Stochastic Gradient Descent(65/999): loss=0.11942540404017812\n",
      "Stochastic Gradient Descent(66/999): loss=0.11918373482583525\n",
      "Stochastic Gradient Descent(67/999): loss=0.11894591032921516\n",
      "Stochastic Gradient Descent(68/999): loss=0.11871186855375158\n",
      "Stochastic Gradient Descent(69/999): loss=0.11848154850282012\n",
      "Stochastic Gradient Descent(70/999): loss=0.11825489016361015\n",
      "Stochastic Gradient Descent(71/999): loss=0.11803183449125683\n",
      "Stochastic Gradient Descent(72/999): loss=0.11781232339322895\n",
      "Stochastic Gradient Descent(73/999): loss=0.1175962997139689\n",
      "Stochastic Gradient Descent(74/999): loss=0.1173837072197798\n",
      "Stochastic Gradient Descent(75/999): loss=0.11717449058395712\n",
      "Stochastic Gradient Descent(76/999): loss=0.11696859537215946\n",
      "Stochastic Gradient Descent(77/999): loss=0.11676596802801575\n",
      "Stochastic Gradient Descent(78/999): loss=0.11656655585896432\n",
      "Stochastic Gradient Descent(79/999): loss=0.11637030702232044\n",
      "Stochastic Gradient Descent(80/999): loss=0.11617717051156863\n",
      "Stochastic Gradient Descent(81/999): loss=0.11598709614287606\n",
      "Stochastic Gradient Descent(82/999): loss=0.11580003454182339\n",
      "Stochastic Gradient Descent(83/999): loss=0.11561593713034972\n",
      "Stochastic Gradient Descent(84/999): loss=0.11543475611390805\n",
      "Stochastic Gradient Descent(85/999): loss=0.11525644446882792\n",
      "Stochastic Gradient Descent(86/999): loss=0.11508095592988206\n",
      "Stochastic Gradient Descent(87/999): loss=0.11490824497805299\n",
      "Stochastic Gradient Descent(88/999): loss=0.11473826682849789\n",
      "Stochastic Gradient Descent(89/999): loss=0.11457097741870663\n",
      "Stochastic Gradient Descent(90/999): loss=0.11440633339685136\n",
      "Stochastic Gradient Descent(91/999): loss=0.114244292110324\n",
      "Stochastic Gradient Descent(92/999): loss=0.11408481159445824\n",
      "Stochastic Gradient Descent(93/999): loss=0.1139278505614338\n",
      "Stochastic Gradient Descent(94/999): loss=0.11377336838935956\n",
      "Stochastic Gradient Descent(95/999): loss=0.11362132511153274\n",
      "Stochastic Gradient Descent(96/999): loss=0.11347168140587124\n",
      "Stochastic Gradient Descent(97/999): loss=0.11332439858451664\n",
      "Stochastic Gradient Descent(98/999): loss=0.11317943858360455\n",
      "Stochastic Gradient Descent(99/999): loss=0.11303676395320046\n",
      "Stochastic Gradient Descent(100/999): loss=0.1128963378473974\n",
      "Stochastic Gradient Descent(101/999): loss=0.11275812401457364\n",
      "Stochastic Gradient Descent(102/999): loss=0.11262208678780734\n",
      "Stochastic Gradient Descent(103/999): loss=0.11248819107544598\n",
      "Stochastic Gradient Descent(104/999): loss=0.11235640235182787\n",
      "Stochastic Gradient Descent(105/999): loss=0.11222668664815288\n",
      "Stochastic Gradient Descent(106/999): loss=0.1120990105435013\n",
      "Stochastic Gradient Descent(107/999): loss=0.11197334115599665\n",
      "Stochastic Gradient Descent(108/999): loss=0.11184964613411161\n",
      "Stochastic Gradient Descent(109/999): loss=0.11172789364811386\n",
      "Stochastic Gradient Descent(110/999): loss=0.11160805238165017\n",
      "Stochastic Gradient Descent(111/999): loss=0.11149009152346588\n",
      "Stochastic Gradient Descent(112/999): loss=0.1113739807592583\n",
      "Stochastic Gradient Descent(113/999): loss=0.11125969026366127\n",
      "Stochastic Gradient Descent(114/999): loss=0.11114719069235904\n",
      "Stochastic Gradient Descent(115/999): loss=0.11103645317432743\n",
      "Stochastic Gradient Descent(116/999): loss=0.11092744930420004\n",
      "Stochastic Gradient Descent(117/999): loss=0.1108201511347577\n",
      "Stochastic Gradient Descent(118/999): loss=0.11071453116953886\n",
      "Stochastic Gradient Descent(119/999): loss=0.11061056235556936\n",
      "Stochastic Gradient Descent(120/999): loss=0.11050821807620947\n",
      "Stochastic Gradient Descent(121/999): loss=0.11040747214411606\n",
      "Stochastic Gradient Descent(122/999): loss=0.11030829879431851\n",
      "Stochastic Gradient Descent(123/999): loss=0.11021067267740643\n",
      "Stochastic Gradient Descent(124/999): loss=0.11011456885282665\n",
      "Stochastic Gradient Descent(125/999): loss=0.1100199627822891\n",
      "Stochastic Gradient Descent(126/999): loss=0.10992683032327856\n",
      "Stochastic Gradient Descent(127/999): loss=0.1098351477226712\n",
      "Stochastic Gradient Descent(128/999): loss=0.10974489161045396\n",
      "Stochastic Gradient Descent(129/999): loss=0.10965603899354544\n",
      "Stochastic Gradient Descent(130/999): loss=0.10956856724971635\n",
      "Stochastic Gradient Descent(131/999): loss=0.109482454121608\n",
      "Stochastic Gradient Descent(132/999): loss=0.10939767771084709\n",
      "Stochastic Gradient Descent(133/999): loss=0.10931421647225605\n",
      "Stochastic Gradient Descent(134/999): loss=0.10923204920815573\n",
      "Stochastic Gradient Descent(135/999): loss=0.10915115506276123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(136/999): loss=0.10907151351666683\n",
      "Stochastic Gradient Descent(137/999): loss=0.10899310438142101\n",
      "Stochastic Gradient Descent(138/999): loss=0.10891590779418825\n",
      "Stochastic Gradient Descent(139/999): loss=0.10883990421249705\n",
      "Stochastic Gradient Descent(140/999): loss=0.108765074409073\n",
      "Stochastic Gradient Descent(141/999): loss=0.10869139946675477\n",
      "Stochastic Gradient Descent(142/999): loss=0.10861886077349227\n",
      "Stochastic Gradient Descent(143/999): loss=0.10854744001742568\n",
      "Stochastic Gradient Descent(144/999): loss=0.10847711918204345\n",
      "Stochastic Gradient Descent(145/999): loss=0.10840788054141869\n",
      "Stochastic Gradient Descent(146/999): loss=0.10833970665552234\n",
      "Stochastic Gradient Descent(147/999): loss=0.10827258036561187\n",
      "Stochastic Gradient Descent(148/999): loss=0.10820648478969438\n",
      "Stochastic Gradient Descent(149/999): loss=0.10814140331806295\n",
      "Stochastic Gradient Descent(150/999): loss=0.1080773196089048\n",
      "Stochastic Gradient Descent(151/999): loss=0.10801421758398064\n",
      "Stochastic Gradient Descent(152/999): loss=0.10795208142437314\n",
      "Stochastic Gradient Descent(153/999): loss=0.10789089556630463\n",
      "Stochastic Gradient Descent(154/999): loss=0.10783064469702171\n",
      "Stochastic Gradient Descent(155/999): loss=0.1077713137507467\n",
      "Stochastic Gradient Descent(156/999): loss=0.10771288790469383\n",
      "Stochastic Gradient Descent(157/999): loss=0.10765535257515027\n",
      "Stochastic Gradient Descent(158/999): loss=0.10759869341362008\n",
      "Stochastic Gradient Descent(159/999): loss=0.10754289630303025\n",
      "Stochastic Gradient Descent(160/999): loss=0.10748794735399812\n",
      "Stochastic Gradient Descent(161/999): loss=0.10743383290115913\n",
      "Stochastic Gradient Descent(162/999): loss=0.10738053949955323\n",
      "Stochastic Gradient Descent(163/999): loss=0.10732805392107034\n",
      "Stochastic Gradient Descent(164/999): loss=0.10727636315095264\n",
      "Stochastic Gradient Descent(165/999): loss=0.10722545438435324\n",
      "Stochastic Gradient Descent(166/999): loss=0.10717531502295082\n",
      "Stochastic Gradient Descent(167/999): loss=0.10712593267161821\n",
      "Stochastic Gradient Descent(168/999): loss=0.10707729513514541\n",
      "Stochastic Gradient Descent(169/999): loss=0.10702939041501482\n",
      "Stochastic Gradient Descent(170/999): loss=0.10698220670622897\n",
      "Stochastic Gradient Descent(171/999): loss=0.10693573239418913\n",
      "Stochastic Gradient Descent(172/999): loss=0.10688995605162442\n",
      "Stochastic Gradient Descent(173/999): loss=0.1068448664355703\n",
      "Stochastic Gradient Descent(174/999): loss=0.10680045248439601\n",
      "Stochastic Gradient Descent(175/999): loss=0.10675670331487967\n",
      "Stochastic Gradient Descent(176/999): loss=0.10671360821933078\n",
      "Stochastic Gradient Descent(177/999): loss=0.10667115666275918\n",
      "Stochastic Gradient Descent(178/999): loss=0.10662933828008933\n",
      "Stochastic Gradient Descent(179/999): loss=0.10658814287342003\n",
      "Stochastic Gradient Descent(180/999): loss=0.10654756040932771\n",
      "Stochastic Gradient Descent(181/999): loss=0.10650758101621385\n",
      "Stochastic Gradient Descent(182/999): loss=0.10646819498169481\n",
      "Stochastic Gradient Descent(183/999): loss=0.10642939275003371\n",
      "Stochastic Gradient Descent(184/999): loss=0.10639116491961417\n",
      "Stochastic Gradient Descent(185/999): loss=0.10635350224045409\n",
      "Stochastic Gradient Descent(186/999): loss=0.10631639561176029\n",
      "Stochastic Gradient Descent(187/999): loss=0.10627983607952204\n",
      "Stochastic Gradient Descent(188/999): loss=0.10624381483414362\n",
      "Stochastic Gradient Descent(189/999): loss=0.10620832320811507\n",
      "Stochastic Gradient Descent(190/999): loss=0.10617335267372055\n",
      "Stochastic Gradient Descent(191/999): loss=0.10613889484078344\n",
      "Stochastic Gradient Descent(192/999): loss=0.10610494145444803\n",
      "Stochastic Gradient Descent(193/999): loss=0.10607148439299703\n",
      "Stochastic Gradient Descent(194/999): loss=0.10603851566570399\n",
      "Stochastic Gradient Descent(195/999): loss=0.10600602741072065\n",
      "Stochastic Gradient Descent(196/999): loss=0.10597401189299822\n",
      "Stochastic Gradient Descent(197/999): loss=0.10594246150224226\n",
      "Stochastic Gradient Descent(198/999): loss=0.10591136875090057\n",
      "Stochastic Gradient Descent(199/999): loss=0.10588072627218323\n",
      "Stochastic Gradient Descent(200/999): loss=0.10585052681811505\n",
      "Stochastic Gradient Descent(201/999): loss=0.10582076325761919\n",
      "Stochastic Gradient Descent(202/999): loss=0.10579142857463154\n",
      "Stochastic Gradient Descent(203/999): loss=0.1057625158662459\n",
      "Stochastic Gradient Descent(204/999): loss=0.10573401834088862\n",
      "Stochastic Gradient Descent(205/999): loss=0.10570592931652305\n",
      "Stochastic Gradient Descent(206/999): loss=0.10567824221888278\n",
      "Stochastic Gradient Descent(207/999): loss=0.10565095057973346\n",
      "Stochastic Gradient Descent(208/999): loss=0.10562404803516244\n",
      "Stochastic Gradient Descent(209/999): loss=0.10559752832389632\n",
      "Stochastic Gradient Descent(210/999): loss=0.10557138528564558\n",
      "Stochastic Gradient Descent(211/999): loss=0.10554561285947539\n",
      "Stochastic Gradient Descent(212/999): loss=0.1055202050822035\n",
      "Stochastic Gradient Descent(213/999): loss=0.10549515608682346\n",
      "Stochastic Gradient Descent(214/999): loss=0.10547046010095346\n",
      "Stochastic Gradient Descent(215/999): loss=0.10544611144530995\n",
      "Stochastic Gradient Descent(216/999): loss=0.10542210453220623\n",
      "Stochastic Gradient Descent(217/999): loss=0.10539843386407495\n",
      "Stochastic Gradient Descent(218/999): loss=0.10537509403201475\n",
      "Stochastic Gradient Descent(219/999): loss=0.10535207971435988\n",
      "Stochastic Gradient Descent(220/999): loss=0.10532938567527352\n",
      "Stochastic Gradient Descent(221/999): loss=0.10530700676336309\n",
      "Stochastic Gradient Descent(222/999): loss=0.10528493791031857\n",
      "Stochastic Gradient Descent(223/999): loss=0.10526317412957216\n",
      "Stochastic Gradient Descent(224/999): loss=0.10524171051498007\n",
      "Stochastic Gradient Descent(225/999): loss=0.10522054223952537\n",
      "Stochastic Gradient Descent(226/999): loss=0.10519966455404144\n",
      "Stochastic Gradient Descent(227/999): loss=0.10517907278595677\n",
      "Stochastic Gradient Descent(228/999): loss=0.10515876233805924\n",
      "Stochastic Gradient Descent(229/999): loss=0.10513872868728068\n",
      "Stochastic Gradient Descent(230/999): loss=0.10511896738350106\n",
      "Stochastic Gradient Descent(231/999): loss=0.105099474048372\n",
      "Stochastic Gradient Descent(232/999): loss=0.10508024437415898\n",
      "Stochastic Gradient Descent(233/999): loss=0.10506127412260251\n",
      "Stochastic Gradient Descent(234/999): loss=0.1050425591237975\n",
      "Stochastic Gradient Descent(235/999): loss=0.10502409527509085\n",
      "Stochastic Gradient Descent(236/999): loss=0.10500587853999668\n",
      "Stochastic Gradient Descent(237/999): loss=0.10498790494712919\n",
      "Stochastic Gradient Descent(238/999): loss=0.10497017058915248\n",
      "Stochastic Gradient Descent(239/999): loss=0.10495267162174768\n",
      "Stochastic Gradient Descent(240/999): loss=0.10493540426259655\n",
      "Stochastic Gradient Descent(241/999): loss=0.10491836479038123\n",
      "Stochastic Gradient Descent(242/999): loss=0.10490154954380067\n",
      "Stochastic Gradient Descent(243/999): loss=0.10488495492060242\n",
      "Stochastic Gradient Descent(244/999): loss=0.10486857737663029\n",
      "Stochastic Gradient Descent(245/999): loss=0.10485241342488726\n",
      "Stochastic Gradient Descent(246/999): loss=0.10483645963461367\n",
      "Stochastic Gradient Descent(247/999): loss=0.10482071263038016\n",
      "Stochastic Gradient Descent(248/999): loss=0.10480516909119499\n",
      "Stochastic Gradient Descent(249/999): loss=0.1047898257496264\n",
      "Stochastic Gradient Descent(250/999): loss=0.10477467939093853\n",
      "Stochastic Gradient Descent(251/999): loss=0.10475972685224137\n",
      "Stochastic Gradient Descent(252/999): loss=0.1047449650216548\n",
      "Stochastic Gradient Descent(253/999): loss=0.1047303908374857\n",
      "Stochastic Gradient Descent(254/999): loss=0.10471600128741851\n",
      "Stochastic Gradient Descent(255/999): loss=0.10470179340771875\n",
      "Stochastic Gradient Descent(256/999): loss=0.10468776428244962\n",
      "Stochastic Gradient Descent(257/999): loss=0.10467391104270087\n",
      "Stochastic Gradient Descent(258/999): loss=0.10466023086583047\n",
      "Stochastic Gradient Descent(259/999): loss=0.10464672097471825\n",
      "Stochastic Gradient Descent(260/999): loss=0.10463337863703169\n",
      "Stochastic Gradient Descent(261/999): loss=0.10462020116450364\n",
      "Stochastic Gradient Descent(262/999): loss=0.10460718591222143\n",
      "Stochastic Gradient Descent(263/999): loss=0.10459433027792768\n",
      "Stochastic Gradient Descent(264/999): loss=0.1045816317013324\n",
      "Stochastic Gradient Descent(265/999): loss=0.10456908766343587\n",
      "Stochastic Gradient Descent(266/999): loss=0.10455669568586291\n",
      "Stochastic Gradient Descent(267/999): loss=0.10454445333020755\n",
      "Stochastic Gradient Descent(268/999): loss=0.1045323581973883\n",
      "Stochastic Gradient Descent(269/999): loss=0.10452040792701402\n",
      "Stochastic Gradient Descent(270/999): loss=0.10450860019675964\n",
      "Stochastic Gradient Descent(271/999): loss=0.1044969327217526\n",
      "Stochastic Gradient Descent(272/999): loss=0.10448540325396838\n",
      "Stochastic Gradient Descent(273/999): loss=0.10447400958163636\n",
      "Stochastic Gradient Descent(274/999): loss=0.104462749528655\n",
      "Stochastic Gradient Descent(275/999): loss=0.10445162095401668\n",
      "Stochastic Gradient Descent(276/999): loss=0.10444062175124158\n",
      "Stochastic Gradient Descent(277/999): loss=0.10442974984782068\n",
      "Stochastic Gradient Descent(278/999): loss=0.10441900320466797\n",
      "Stochastic Gradient Descent(279/999): loss=0.10440837981558145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(280/999): loss=0.10439787770671248\n",
      "Stochastic Gradient Descent(281/999): loss=0.10438749493604418\n",
      "Stochastic Gradient Descent(282/999): loss=0.10437722959287797\n",
      "Stochastic Gradient Descent(283/999): loss=0.1043670797973283\n",
      "Stochastic Gradient Descent(284/999): loss=0.10435704369982592\n",
      "Stochastic Gradient Descent(285/999): loss=0.10434711948062861\n",
      "Stochastic Gradient Descent(286/999): loss=0.1043373053493404\n",
      "Stochastic Gradient Descent(287/999): loss=0.10432759954443797\n",
      "Stochastic Gradient Descent(288/999): loss=0.10431800033280515\n",
      "Stochastic Gradient Descent(289/999): loss=0.10430850600927469\n",
      "Stochastic Gradient Descent(290/999): loss=0.10429911489617742\n",
      "Stochastic Gradient Descent(291/999): loss=0.1042898253428988\n",
      "Stochastic Gradient Descent(292/999): loss=0.10428063572544252\n",
      "Stochastic Gradient Descent(293/999): loss=0.10427154444600117\n",
      "Stochastic Gradient Descent(294/999): loss=0.10426254993253393\n",
      "Stochastic Gradient Descent(295/999): loss=0.1042536506383508\n",
      "Stochastic Gradient Descent(296/999): loss=0.1042448450417039\n",
      "Stochastic Gradient Descent(297/999): loss=0.10423613164538502\n",
      "Stochastic Gradient Descent(298/999): loss=0.10422750897632997\n",
      "Stochastic Gradient Descent(299/999): loss=0.10421897558522912\n",
      "Stochastic Gradient Descent(300/999): loss=0.10421053004614408\n",
      "Stochastic Gradient Descent(301/999): loss=0.10420217095613113\n",
      "Stochastic Gradient Descent(302/999): loss=0.10419389693486983\n",
      "Stochastic Gradient Descent(303/999): loss=0.10418570662429867\n",
      "Stochastic Gradient Descent(304/999): loss=0.10417759868825564\n",
      "Stochastic Gradient Descent(305/999): loss=0.10416957181212522\n",
      "Stochastic Gradient Descent(306/999): loss=0.10416162470249077\n",
      "Stochastic Gradient Descent(307/999): loss=0.10415375608679271\n",
      "Stochastic Gradient Descent(308/999): loss=0.10414596471299195\n",
      "Stochastic Gradient Descent(309/999): loss=0.1041382493492391\n",
      "Stochastic Gradient Descent(310/999): loss=0.1041306087835488\n",
      "Stochastic Gradient Descent(311/999): loss=0.1041230418234791\n",
      "Stochastic Gradient Descent(312/999): loss=0.10411554729581667\n",
      "Stochastic Gradient Descent(313/999): loss=0.10410812404626621\n",
      "Stochastic Gradient Descent(314/999): loss=0.10410077093914563\n",
      "Stochastic Gradient Descent(315/999): loss=0.10409348685708573\n",
      "Stochastic Gradient Descent(316/999): loss=0.10408627070073485\n",
      "Stochastic Gradient Descent(317/999): loss=0.10407912138846825\n",
      "Stochastic Gradient Descent(318/999): loss=0.10407203785610221\n",
      "Stochastic Gradient Descent(319/999): loss=0.1040650190566127\n",
      "Stochastic Gradient Descent(320/999): loss=0.1040580639598585\n",
      "Stochastic Gradient Descent(321/999): loss=0.10405117155230938\n",
      "Stochastic Gradient Descent(322/999): loss=0.10404434083677745\n",
      "Stochastic Gradient Descent(323/999): loss=0.10403757083215413\n",
      "Stochastic Gradient Descent(324/999): loss=0.10403086057315064\n",
      "Stochastic Gradient Descent(325/999): loss=0.10402420911004273\n",
      "Stochastic Gradient Descent(326/999): loss=0.10401761550841987\n",
      "Stochastic Gradient Descent(327/999): loss=0.10401107884893805\n",
      "Stochastic Gradient Descent(328/999): loss=0.10400459822707685\n",
      "Stochastic Gradient Descent(329/999): loss=0.10399817275290038\n",
      "Stochastic Gradient Descent(330/999): loss=0.10399180155082202\n",
      "Stochastic Gradient Descent(331/999): loss=0.10398548375937294\n",
      "Stochastic Gradient Descent(332/999): loss=0.1039792185309745\n",
      "Stochastic Gradient Descent(333/999): loss=0.1039730050317141\n",
      "Stochastic Gradient Descent(334/999): loss=0.10396684244112482\n",
      "Stochastic Gradient Descent(335/999): loss=0.10396072995196859\n",
      "Stochastic Gradient Descent(336/999): loss=0.10395466677002267\n",
      "Stochastic Gradient Descent(337/999): loss=0.10394865211387004\n",
      "Stochastic Gradient Descent(338/999): loss=0.10394268521469247\n",
      "Stochastic Gradient Descent(339/999): loss=0.10393676531606755\n",
      "Stochastic Gradient Descent(340/999): loss=0.10393089167376869\n",
      "Stochastic Gradient Descent(341/999): loss=0.10392506355556855\n",
      "Stochastic Gradient Descent(342/999): loss=0.10391928024104526\n",
      "Stochastic Gradient Descent(343/999): loss=0.10391354102139215\n",
      "Stochastic Gradient Descent(344/999): loss=0.1039078451992305\n",
      "Stochastic Gradient Descent(345/999): loss=0.10390219208842497\n",
      "Stochastic Gradient Descent(346/999): loss=0.10389658101390258\n",
      "Stochastic Gradient Descent(347/999): loss=0.10389101131147394\n",
      "Stochastic Gradient Descent(348/999): loss=0.1038854823276579\n",
      "Stochastic Gradient Descent(349/999): loss=0.10387999341950899\n",
      "Stochastic Gradient Descent(350/999): loss=0.10387454395444715\n",
      "Stochastic Gradient Descent(351/999): loss=0.10386913331009075\n",
      "Stochastic Gradient Descent(352/999): loss=0.10386376087409212\n",
      "Stochastic Gradient Descent(353/999): loss=0.10385842604397556\n",
      "Stochastic Gradient Descent(354/999): loss=0.10385312822697836\n",
      "Stochastic Gradient Descent(355/999): loss=0.10384786683989383\n",
      "Stochastic Gradient Descent(356/999): loss=0.10384264130891753\n",
      "Stochastic Gradient Descent(357/999): loss=0.10383745106949521\n",
      "Stochastic Gradient Descent(358/999): loss=0.10383229556617403\n",
      "Stochastic Gradient Descent(359/999): loss=0.1038271742524555\n",
      "Stochastic Gradient Descent(360/999): loss=0.10382208659065105\n",
      "Stochastic Gradient Descent(361/999): loss=0.10381703205174012\n",
      "Stochastic Gradient Descent(362/999): loss=0.10381201011523021\n",
      "Stochastic Gradient Descent(363/999): loss=0.10380702026901932\n",
      "Stochastic Gradient Descent(364/999): loss=0.10380206200926081\n",
      "Stochastic Gradient Descent(365/999): loss=0.10379713484022994\n",
      "Stochastic Gradient Descent(366/999): loss=0.10379223827419315\n",
      "Stochastic Gradient Descent(367/999): loss=0.10378737183127905\n",
      "Stochastic Gradient Descent(368/999): loss=0.10378253503935156\n",
      "Stochastic Gradient Descent(369/999): loss=0.10377772743388529\n",
      "Stochastic Gradient Descent(370/999): loss=0.10377294855784273\n",
      "Stochastic Gradient Descent(371/999): loss=0.10376819796155337\n",
      "Stochastic Gradient Descent(372/999): loss=0.10376347520259509\n",
      "Stochastic Gradient Descent(373/999): loss=0.10375877984567705\n",
      "Stochastic Gradient Descent(374/999): loss=0.10375411146252474\n",
      "Stochastic Gradient Descent(375/999): loss=0.10374946963176691\n",
      "Stochastic Gradient Descent(376/999): loss=0.10374485393882409\n",
      "Stochastic Gradient Descent(377/999): loss=0.10374026397579912\n",
      "Stochastic Gradient Descent(378/999): loss=0.10373569934136934\n",
      "Stochastic Gradient Descent(379/999): loss=0.10373115964068061\n",
      "Stochastic Gradient Descent(380/999): loss=0.10372664448524294\n",
      "Stochastic Gradient Descent(381/999): loss=0.10372215349282776\n",
      "Stochastic Gradient Descent(382/999): loss=0.1037176862873671\n",
      "Stochastic Gradient Descent(383/999): loss=0.10371324249885414\n",
      "Stochastic Gradient Descent(384/999): loss=0.10370882176324539\n",
      "Stochastic Gradient Descent(385/999): loss=0.10370442372236459\n",
      "Stochastic Gradient Descent(386/999): loss=0.10370004802380814\n",
      "Stochastic Gradient Descent(387/999): loss=0.10369569432085175\n",
      "Stochastic Gradient Descent(388/999): loss=0.1036913622723592\n",
      "Stochastic Gradient Descent(389/999): loss=0.10368705154269187\n",
      "Stochastic Gradient Descent(390/999): loss=0.10368276180162025\n",
      "Stochastic Gradient Descent(391/999): loss=0.10367849272423671\n",
      "Stochastic Gradient Descent(392/999): loss=0.10367424399086954\n",
      "Stochastic Gradient Descent(393/999): loss=0.10367001528699862\n",
      "Stochastic Gradient Descent(394/999): loss=0.10366580630317235\n",
      "Stochastic Gradient Descent(395/999): loss=0.10366161673492567\n",
      "Stochastic Gradient Descent(396/999): loss=0.10365744628270004\n",
      "Stochastic Gradient Descent(397/999): loss=0.10365329465176391\n",
      "Stochastic Gradient Descent(398/999): loss=0.10364916155213498\n",
      "Stochastic Gradient Descent(399/999): loss=0.1036450466985038\n",
      "Stochastic Gradient Descent(400/999): loss=0.10364094981015817\n",
      "Stochastic Gradient Descent(401/999): loss=0.10363687061090907\n",
      "Stochastic Gradient Descent(402/999): loss=0.10363280882901772\n",
      "Stochastic Gradient Descent(403/999): loss=0.1036287641971238\n",
      "Stochastic Gradient Descent(404/999): loss=0.1036247364521748\n",
      "Stochastic Gradient Descent(405/999): loss=0.1036207253353566\n",
      "Stochastic Gradient Descent(406/999): loss=0.10361673059202502\n",
      "Stochastic Gradient Descent(407/999): loss=0.10361275197163865\n",
      "Stochastic Gradient Descent(408/999): loss=0.10360878922769269\n",
      "Stochastic Gradient Descent(409/999): loss=0.10360484211765372\n",
      "Stochastic Gradient Descent(410/999): loss=0.10360091040289579\n",
      "Stochastic Gradient Descent(411/999): loss=0.10359699384863731\n",
      "Stochastic Gradient Descent(412/999): loss=0.10359309222387911\n",
      "Stochastic Gradient Descent(413/999): loss=0.10358920530134333\n",
      "Stochastic Gradient Descent(414/999): loss=0.10358533285741346\n",
      "Stochastic Gradient Descent(415/999): loss=0.10358147467207544\n",
      "Stochastic Gradient Descent(416/999): loss=0.10357763052885931\n",
      "Stochastic Gradient Descent(417/999): loss=0.10357380021478206\n",
      "Stochastic Gradient Descent(418/999): loss=0.10356998352029162\n",
      "Stochastic Gradient Descent(419/999): loss=0.1035661802392114\n",
      "Stochastic Gradient Descent(420/999): loss=0.10356239016868571\n",
      "Stochastic Gradient Descent(421/999): loss=0.1035586131091264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(422/999): loss=0.10355484886416011\n",
      "Stochastic Gradient Descent(423/999): loss=0.10355109724057629\n",
      "Stochastic Gradient Descent(424/999): loss=0.1035473580482764\n",
      "Stochastic Gradient Descent(425/999): loss=0.1035436311002236\n",
      "Stochastic Gradient Descent(426/999): loss=0.10353991621239322\n",
      "Stochastic Gradient Descent(427/999): loss=0.1035362132037246\n",
      "Stochastic Gradient Descent(428/999): loss=0.10353252189607279\n",
      "Stochastic Gradient Descent(429/999): loss=0.10352884211416186\n",
      "Stochastic Gradient Descent(430/999): loss=0.10352517368553853\n",
      "Stochastic Gradient Descent(431/999): loss=0.1035215164405266\n",
      "Stochastic Gradient Descent(432/999): loss=0.10351787021218223\n",
      "Stochastic Gradient Descent(433/999): loss=0.10351423483624977\n",
      "Stochastic Gradient Descent(434/999): loss=0.10351061015111855\n",
      "Stochastic Gradient Descent(435/999): loss=0.10350699599778011\n",
      "Stochastic Gradient Descent(436/999): loss=0.1035033922197863\n",
      "Stochastic Gradient Descent(437/999): loss=0.10349979866320787\n",
      "Stochastic Gradient Descent(438/999): loss=0.103496215176594\n",
      "Stochastic Gradient Descent(439/999): loss=0.10349264161093219\n",
      "Stochastic Gradient Descent(440/999): loss=0.10348907781960907\n",
      "Stochastic Gradient Descent(441/999): loss=0.10348552365837153\n",
      "Stochastic Gradient Descent(442/999): loss=0.10348197898528887\n",
      "Stochastic Gradient Descent(443/999): loss=0.10347844366071515\n",
      "Stochastic Gradient Descent(444/999): loss=0.10347491754725235\n",
      "Stochastic Gradient Descent(445/999): loss=0.1034714005097144\n",
      "Stochastic Gradient Descent(446/999): loss=0.10346789241509105\n",
      "Stochastic Gradient Descent(447/999): loss=0.10346439313251314\n",
      "Stochastic Gradient Descent(448/999): loss=0.10346090253321796\n",
      "Stochastic Gradient Descent(449/999): loss=0.1034574204905152\n",
      "Stochastic Gradient Descent(450/999): loss=0.10345394687975351\n",
      "Stochastic Gradient Descent(451/999): loss=0.10345048157828793\n",
      "Stochastic Gradient Descent(452/999): loss=0.10344702446544704\n",
      "Stochastic Gradient Descent(453/999): loss=0.10344357542250156\n",
      "Stochastic Gradient Descent(454/999): loss=0.10344013433263266\n",
      "Stochastic Gradient Descent(455/999): loss=0.10343670108090153\n",
      "Stochastic Gradient Descent(456/999): loss=0.10343327555421881\n",
      "Stochastic Gradient Descent(457/999): loss=0.10342985764131471\n",
      "Stochastic Gradient Descent(458/999): loss=0.10342644723270987\n",
      "Stochastic Gradient Descent(459/999): loss=0.10342304422068642\n",
      "Stochastic Gradient Descent(460/999): loss=0.10341964849925939\n",
      "Stochastic Gradient Descent(461/999): loss=0.10341625996414902\n",
      "Stochastic Gradient Descent(462/999): loss=0.1034128785127531\n",
      "Stochastic Gradient Descent(463/999): loss=0.10340950404412003\n",
      "Stochastic Gradient Descent(464/999): loss=0.103406136458922\n",
      "Stochastic Gradient Descent(465/999): loss=0.103402775659429\n",
      "Stochastic Gradient Descent(466/999): loss=0.10339942154948302\n",
      "Stochastic Gradient Descent(467/999): loss=0.10339607403447258\n",
      "Stochastic Gradient Descent(468/999): loss=0.10339273302130789\n",
      "Stochastic Gradient Descent(469/999): loss=0.10338939841839627\n",
      "Stochastic Gradient Descent(470/999): loss=0.10338607013561804\n",
      "Stochastic Gradient Descent(471/999): loss=0.10338274808430277\n",
      "Stochastic Gradient Descent(472/999): loss=0.10337943217720583\n",
      "Stochastic Gradient Descent(473/999): loss=0.10337612232848538\n",
      "Stochastic Gradient Descent(474/999): loss=0.10337281845367993\n",
      "Stochastic Gradient Descent(475/999): loss=0.1033695204696859\n",
      "Stochastic Gradient Descent(476/999): loss=0.10336622829473573\n",
      "Stochastic Gradient Descent(477/999): loss=0.10336294184837645\n",
      "Stochastic Gradient Descent(478/999): loss=0.10335966105144838\n",
      "Stochastic Gradient Descent(479/999): loss=0.10335638582606428\n",
      "Stochastic Gradient Descent(480/999): loss=0.10335311609558889\n",
      "Stochastic Gradient Descent(481/999): loss=0.10334985178461865\n",
      "Stochastic Gradient Descent(482/999): loss=0.10334659281896189\n",
      "Stochastic Gradient Descent(483/999): loss=0.10334333912561927\n",
      "Stochastic Gradient Descent(484/999): loss=0.10334009063276459\n",
      "Stochastic Gradient Descent(485/999): loss=0.10333684726972582\n",
      "Stochastic Gradient Descent(486/999): loss=0.10333360896696647\n",
      "Stochastic Gradient Descent(487/999): loss=0.10333037565606733\n",
      "Stochastic Gradient Descent(488/999): loss=0.10332714726970846\n",
      "Stochastic Gradient Descent(489/999): loss=0.1033239237416514\n",
      "Stochastic Gradient Descent(490/999): loss=0.10332070500672172\n",
      "Stochastic Gradient Descent(491/999): loss=0.10331749100079199\n",
      "Stochastic Gradient Descent(492/999): loss=0.1033142816607647\n",
      "Stochastic Gradient Descent(493/999): loss=0.10331107692455588\n",
      "Stochastic Gradient Descent(494/999): loss=0.1033078767310785\n",
      "Stochastic Gradient Descent(495/999): loss=0.10330468102022657\n",
      "Stochastic Gradient Descent(496/999): loss=0.1033014897328593\n",
      "Stochastic Gradient Descent(497/999): loss=0.10329830281078542\n",
      "Stochastic Gradient Descent(498/999): loss=0.10329512019674811\n",
      "Stochastic Gradient Descent(499/999): loss=0.10329194183440954\n",
      "Stochastic Gradient Descent(500/999): loss=0.10328876766833646\n",
      "Stochastic Gradient Descent(501/999): loss=0.1032855976439853\n",
      "Stochastic Gradient Descent(502/999): loss=0.10328243170768812\n",
      "Stochastic Gradient Descent(503/999): loss=0.10327926980663812\n",
      "Stochastic Gradient Descent(504/999): loss=0.1032761118888761\n",
      "Stochastic Gradient Descent(505/999): loss=0.10327295790327654\n",
      "Stochastic Gradient Descent(506/999): loss=0.10326980779953436\n",
      "Stochastic Gradient Descent(507/999): loss=0.10326666152815149\n",
      "Stochastic Gradient Descent(508/999): loss=0.10326351904042401\n",
      "Stochastic Gradient Descent(509/999): loss=0.10326038028842925\n",
      "Stochastic Gradient Descent(510/999): loss=0.10325724522501327\n",
      "Stochastic Gradient Descent(511/999): loss=0.1032541138037784\n",
      "Stochastic Gradient Descent(512/999): loss=0.10325098597907097\n",
      "Stochastic Gradient Descent(513/999): loss=0.10324786170596954\n",
      "Stochastic Gradient Descent(514/999): loss=0.1032447409402729\n",
      "Stochastic Gradient Descent(515/999): loss=0.10324162363848849\n",
      "Stochastic Gradient Descent(516/999): loss=0.10323850975782103\n",
      "Stochastic Gradient Descent(517/999): loss=0.10323539925616113\n",
      "Stochastic Gradient Descent(518/999): loss=0.10323229209207453\n",
      "Stochastic Gradient Descent(519/999): loss=0.10322918822479084\n",
      "Stochastic Gradient Descent(520/999): loss=0.10322608761419311\n",
      "Stochastic Gradient Descent(521/999): loss=0.10322299022080711\n",
      "Stochastic Gradient Descent(522/999): loss=0.10321989600579111\n",
      "Stochastic Gradient Descent(523/999): loss=0.10321680493092565\n",
      "Stochastic Gradient Descent(524/999): loss=0.10321371695860339\n",
      "Stochastic Gradient Descent(525/999): loss=0.10321063205181934\n",
      "Stochastic Gradient Descent(526/999): loss=0.10320755017416124\n",
      "Stochastic Gradient Descent(527/999): loss=0.10320447128979968\n",
      "Stochastic Gradient Descent(528/999): loss=0.10320139536347911\n",
      "Stochastic Gradient Descent(529/999): loss=0.10319832236050826\n",
      "Stochastic Gradient Descent(530/999): loss=0.10319525224675127\n",
      "Stochastic Gradient Descent(531/999): loss=0.10319218498861853\n",
      "Stochastic Gradient Descent(532/999): loss=0.10318912055305803\n",
      "Stochastic Gradient Descent(533/999): loss=0.10318605890754662\n",
      "Stochastic Gradient Descent(534/999): loss=0.10318300002008148\n",
      "Stochastic Gradient Descent(535/999): loss=0.10317994385917177\n",
      "Stochastic Gradient Descent(536/999): loss=0.1031768903938303\n",
      "Stochastic Gradient Descent(537/999): loss=0.10317383959356546\n",
      "Stochastic Gradient Descent(538/999): loss=0.10317079142837333\n",
      "Stochastic Gradient Descent(539/999): loss=0.10316774586872958\n",
      "Stochastic Gradient Descent(540/999): loss=0.10316470288558188\n",
      "Stochastic Gradient Descent(541/999): loss=0.10316166245034235\n",
      "Stochastic Gradient Descent(542/999): loss=0.1031586245348799\n",
      "Stochastic Gradient Descent(543/999): loss=0.10315558911151305\n",
      "Stochastic Gradient Descent(544/999): loss=0.10315255615300253\n",
      "Stochastic Gradient Descent(545/999): loss=0.10314952563254419\n",
      "Stochastic Gradient Descent(546/999): loss=0.10314649752376204\n",
      "Stochastic Gradient Descent(547/999): loss=0.10314347180070131\n",
      "Stochastic Gradient Descent(548/999): loss=0.1031404484378216\n",
      "Stochastic Gradient Descent(549/999): loss=0.10313742740999036\n",
      "Stochastic Gradient Descent(550/999): loss=0.1031344086924761\n",
      "Stochastic Gradient Descent(551/999): loss=0.10313139226094203\n",
      "Stochastic Gradient Descent(552/999): loss=0.10312837809143978\n",
      "Stochastic Gradient Descent(553/999): loss=0.10312536616040296\n",
      "Stochastic Gradient Descent(554/999): loss=0.10312235644464114\n",
      "Stochastic Gradient Descent(555/999): loss=0.10311934892133374\n",
      "Stochastic Gradient Descent(556/999): loss=0.10311634356802404\n",
      "Stochastic Gradient Descent(557/999): loss=0.10311334036261334\n",
      "Stochastic Gradient Descent(558/999): loss=0.10311033928335522\n",
      "Stochastic Gradient Descent(559/999): loss=0.10310734030884978\n",
      "Stochastic Gradient Descent(560/999): loss=0.10310434341803812\n",
      "Stochastic Gradient Descent(561/999): loss=0.10310134859019676\n",
      "Stochastic Gradient Descent(562/999): loss=0.10309835580493246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(563/999): loss=0.10309536504217655\n",
      "Stochastic Gradient Descent(564/999): loss=0.10309237628217989\n",
      "Stochastic Gradient Descent(565/999): loss=0.10308938950550778\n",
      "Stochastic Gradient Descent(566/999): loss=0.10308640469303476\n",
      "Stochastic Gradient Descent(567/999): loss=0.1030834218259397\n",
      "Stochastic Gradient Descent(568/999): loss=0.10308044088570083\n",
      "Stochastic Gradient Descent(569/999): loss=0.10307746185409099\n",
      "Stochastic Gradient Descent(570/999): loss=0.10307448471317276\n",
      "Stochastic Gradient Descent(571/999): loss=0.10307150944529404\n",
      "Stochastic Gradient Descent(572/999): loss=0.10306853603308304\n",
      "Stochastic Gradient Descent(573/999): loss=0.10306556445944415\n",
      "Stochastic Gradient Descent(574/999): loss=0.10306259470755331\n",
      "Stochastic Gradient Descent(575/999): loss=0.10305962676085374\n",
      "Stochastic Gradient Descent(576/999): loss=0.10305666060305135\n",
      "Stochastic Gradient Descent(577/999): loss=0.10305369621811084\n",
      "Stochastic Gradient Descent(578/999): loss=0.10305073359025135\n",
      "Stochastic Gradient Descent(579/999): loss=0.10304777270394244\n",
      "Stochastic Gradient Descent(580/999): loss=0.10304481354389991\n",
      "Stochastic Gradient Descent(581/999): loss=0.10304185609508205\n",
      "Stochastic Gradient Descent(582/999): loss=0.10303890034268556\n",
      "Stochastic Gradient Descent(583/999): loss=0.10303594627214166\n",
      "Stochastic Gradient Descent(584/999): loss=0.10303299386911265\n",
      "Stochastic Gradient Descent(585/999): loss=0.10303004311948763\n",
      "Stochastic Gradient Descent(586/999): loss=0.10302709400937939\n",
      "Stochastic Gradient Descent(587/999): loss=0.10302414652512046\n",
      "Stochastic Gradient Descent(588/999): loss=0.1030212006532597\n",
      "Stochastic Gradient Descent(589/999): loss=0.10301825638055877\n",
      "Stochastic Gradient Descent(590/999): loss=0.10301531369398877\n",
      "Stochastic Gradient Descent(591/999): loss=0.1030123725807267\n",
      "Stochastic Gradient Descent(592/999): loss=0.10300943302815235\n",
      "Stochastic Gradient Descent(593/999): loss=0.10300649502384489\n",
      "Stochastic Gradient Descent(594/999): loss=0.10300355855557973\n",
      "Stochastic Gradient Descent(595/999): loss=0.10300062361132524\n",
      "Stochastic Gradient Descent(596/999): loss=0.10299769017923982\n",
      "Stochastic Gradient Descent(597/999): loss=0.1029947582476687\n",
      "Stochastic Gradient Descent(598/999): loss=0.10299182780514089\n",
      "Stochastic Gradient Descent(599/999): loss=0.10298889884036645\n",
      "Stochastic Gradient Descent(600/999): loss=0.10298597134223328\n",
      "Stochastic Gradient Descent(601/999): loss=0.10298304529980448\n",
      "Stochastic Gradient Descent(602/999): loss=0.10298012070231531\n",
      "Stochastic Gradient Descent(603/999): loss=0.10297719753917069\n",
      "Stochastic Gradient Descent(604/999): loss=0.1029742757999422\n",
      "Stochastic Gradient Descent(605/999): loss=0.10297135547436556\n",
      "Stochastic Gradient Descent(606/999): loss=0.10296843655233795\n",
      "Stochastic Gradient Descent(607/999): loss=0.10296551902391533\n",
      "Stochastic Gradient Descent(608/999): loss=0.10296260287930999\n",
      "Stochastic Gradient Descent(609/999): loss=0.10295968810888792\n",
      "Stochastic Gradient Descent(610/999): loss=0.10295677470316648\n",
      "Stochastic Gradient Descent(611/999): loss=0.10295386265281178\n",
      "Stochastic Gradient Descent(612/999): loss=0.10295095194863643\n",
      "Stochastic Gradient Descent(613/999): loss=0.10294804258159715\n",
      "Stochastic Gradient Descent(614/999): loss=0.10294513454279228\n",
      "Stochastic Gradient Descent(615/999): loss=0.1029422278234598\n",
      "Stochastic Gradient Descent(616/999): loss=0.10293932241497487\n",
      "Stochastic Gradient Descent(617/999): loss=0.10293641830884759\n",
      "Stochastic Gradient Descent(618/999): loss=0.10293351549672101\n",
      "Stochastic Gradient Descent(619/999): loss=0.1029306139703688\n",
      "Stochastic Gradient Descent(620/999): loss=0.10292771372169325\n",
      "Stochastic Gradient Descent(621/999): loss=0.10292481474272318\n",
      "Stochastic Gradient Descent(622/999): loss=0.10292191702561186\n",
      "Stochastic Gradient Descent(623/999): loss=0.10291902056263505\n",
      "Stochastic Gradient Descent(624/999): loss=0.10291612534618902\n",
      "Stochastic Gradient Descent(625/999): loss=0.10291323136878862\n",
      "Stochastic Gradient Descent(626/999): loss=0.10291033862306528\n",
      "Stochastic Gradient Descent(627/999): loss=0.10290744710176519\n",
      "Stochastic Gradient Descent(628/999): loss=0.10290455679774764\n",
      "Stochastic Gradient Descent(629/999): loss=0.10290166770398276\n",
      "Stochastic Gradient Descent(630/999): loss=0.10289877981355017\n",
      "Stochastic Gradient Descent(631/999): loss=0.10289589311963693\n",
      "Stochastic Gradient Descent(632/999): loss=0.102893007615536\n",
      "Stochastic Gradient Descent(633/999): loss=0.10289012329464442\n",
      "Stochastic Gradient Descent(634/999): loss=0.10288724015046155\n",
      "Stochastic Gradient Descent(635/999): loss=0.10288435817658777\n",
      "Stochastic Gradient Descent(636/999): loss=0.10288147736672235\n",
      "Stochastic Gradient Descent(637/999): loss=0.10287859771466233\n",
      "Stochastic Gradient Descent(638/999): loss=0.10287571921430062\n",
      "Stochastic Gradient Descent(639/999): loss=0.10287284185962457\n",
      "Stochastic Gradient Descent(640/999): loss=0.10286996564471448\n",
      "Stochastic Gradient Descent(641/999): loss=0.10286709056374212\n",
      "Stochastic Gradient Descent(642/999): loss=0.1028642166109691\n",
      "Stochastic Gradient Descent(643/999): loss=0.10286134378074561\n",
      "Stochastic Gradient Descent(644/999): loss=0.10285847206750887\n",
      "Stochastic Gradient Descent(645/999): loss=0.10285560146578178\n",
      "Stochastic Gradient Descent(646/999): loss=0.10285273197017156\n",
      "Stochastic Gradient Descent(647/999): loss=0.10284986357536828\n",
      "Stochastic Gradient Descent(648/999): loss=0.10284699627614372\n",
      "Stochastic Gradient Descent(649/999): loss=0.10284413006734985\n",
      "Stochastic Gradient Descent(650/999): loss=0.10284126494391767\n",
      "Stochastic Gradient Descent(651/999): loss=0.10283840090085587\n",
      "Stochastic Gradient Descent(652/999): loss=0.10283553793324961\n",
      "Stochastic Gradient Descent(653/999): loss=0.1028326760362593\n",
      "Stochastic Gradient Descent(654/999): loss=0.10282981520511936\n",
      "Stochastic Gradient Descent(655/999): loss=0.102826955435137\n",
      "Stochastic Gradient Descent(656/999): loss=0.10282409672169117\n",
      "Stochastic Gradient Descent(657/999): loss=0.10282123906023122\n",
      "Stochastic Gradient Descent(658/999): loss=0.10281838244627595\n",
      "Stochastic Gradient Descent(659/999): loss=0.10281552687541232\n",
      "Stochastic Gradient Descent(660/999): loss=0.1028126723432945\n",
      "Stochastic Gradient Descent(661/999): loss=0.1028098188456427\n",
      "Stochastic Gradient Descent(662/999): loss=0.10280696637824215\n",
      "Stochastic Gradient Descent(663/999): loss=0.10280411493694205\n",
      "Stochastic Gradient Descent(664/999): loss=0.10280126451765448\n",
      "Stochastic Gradient Descent(665/999): loss=0.10279841511635349\n",
      "Stochastic Gradient Descent(666/999): loss=0.10279556672907389\n",
      "Stochastic Gradient Descent(667/999): loss=0.10279271935191063\n",
      "Stochastic Gradient Descent(668/999): loss=0.10278987298101752\n",
      "Stochastic Gradient Descent(669/999): loss=0.10278702761260644\n",
      "Stochastic Gradient Descent(670/999): loss=0.10278418324294625\n",
      "Stochastic Gradient Descent(671/999): loss=0.1027813398683621\n",
      "Stochastic Gradient Descent(672/999): loss=0.10277849748523431\n",
      "Stochastic Gradient Descent(673/999): loss=0.10277565608999767\n",
      "Stochastic Gradient Descent(674/999): loss=0.10277281567914029\n",
      "Stochastic Gradient Descent(675/999): loss=0.1027699762492031\n",
      "Stochastic Gradient Descent(676/999): loss=0.10276713779677873\n",
      "Stochastic Gradient Descent(677/999): loss=0.10276430031851078\n",
      "Stochastic Gradient Descent(678/999): loss=0.10276146381109295\n",
      "Stochastic Gradient Descent(679/999): loss=0.10275862827126835\n",
      "Stochastic Gradient Descent(680/999): loss=0.10275579369582852\n",
      "Stochastic Gradient Descent(681/999): loss=0.10275296008161276\n",
      "Stochastic Gradient Descent(682/999): loss=0.10275012742550742\n",
      "Stochastic Gradient Descent(683/999): loss=0.10274729572444505\n",
      "Stochastic Gradient Descent(684/999): loss=0.10274446497540367\n",
      "Stochastic Gradient Descent(685/999): loss=0.10274163517540601\n",
      "Stochastic Gradient Descent(686/999): loss=0.10273880632151891\n",
      "Stochastic Gradient Descent(687/999): loss=0.10273597841085243\n",
      "Stochastic Gradient Descent(688/999): loss=0.10273315144055938\n",
      "Stochastic Gradient Descent(689/999): loss=0.10273032540783442\n",
      "Stochastic Gradient Descent(690/999): loss=0.10272750030991344\n",
      "Stochastic Gradient Descent(691/999): loss=0.10272467614407298\n",
      "Stochastic Gradient Descent(692/999): loss=0.10272185290762958\n",
      "Stochastic Gradient Descent(693/999): loss=0.10271903059793898\n",
      "Stochastic Gradient Descent(694/999): loss=0.10271620921239562\n",
      "Stochastic Gradient Descent(695/999): loss=0.10271338874843199\n",
      "Stochastic Gradient Descent(696/999): loss=0.10271056920351807\n",
      "Stochastic Gradient Descent(697/999): loss=0.10270775057516057\n",
      "Stochastic Gradient Descent(698/999): loss=0.10270493286090253\n",
      "Stochastic Gradient Descent(699/999): loss=0.10270211605832259\n",
      "Stochastic Gradient Descent(700/999): loss=0.1026993001650345\n",
      "Stochastic Gradient Descent(701/999): loss=0.10269648517868649\n",
      "Stochastic Gradient Descent(702/999): loss=0.10269367109696087\n",
      "Stochastic Gradient Descent(703/999): loss=0.10269085791757318\n",
      "Stochastic Gradient Descent(704/999): loss=0.10268804563827197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(705/999): loss=0.10268523425683816\n",
      "Stochastic Gradient Descent(706/999): loss=0.10268242377108441\n",
      "Stochastic Gradient Descent(707/999): loss=0.10267961417885473\n",
      "Stochastic Gradient Descent(708/999): loss=0.102676805478024\n",
      "Stochastic Gradient Descent(709/999): loss=0.10267399766649729\n",
      "Stochastic Gradient Descent(710/999): loss=0.10267119074220972\n",
      "Stochastic Gradient Descent(711/999): loss=0.10266838470312549\n",
      "Stochastic Gradient Descent(712/999): loss=0.1026655795472379\n",
      "Stochastic Gradient Descent(713/999): loss=0.10266277527256858\n",
      "Stochastic Gradient Descent(714/999): loss=0.10265997187716708\n",
      "Stochastic Gradient Descent(715/999): loss=0.10265716935911051\n",
      "Stochastic Gradient Descent(716/999): loss=0.10265436771650306\n",
      "Stochastic Gradient Descent(717/999): loss=0.10265156694747554\n",
      "Stochastic Gradient Descent(718/999): loss=0.10264876705018491\n",
      "Stochastic Gradient Descent(719/999): loss=0.10264596802281398\n",
      "Stochastic Gradient Descent(720/999): loss=0.1026431698635709\n",
      "Stochastic Gradient Descent(721/999): loss=0.10264037257068884\n",
      "Stochastic Gradient Descent(722/999): loss=0.10263757614242536\n",
      "Stochastic Gradient Descent(723/999): loss=0.10263478057706239\n",
      "Stochastic Gradient Descent(724/999): loss=0.10263198587290553\n",
      "Stochastic Gradient Descent(725/999): loss=0.10262919202828374\n",
      "Stochastic Gradient Descent(726/999): loss=0.10262639904154915\n",
      "Stochastic Gradient Descent(727/999): loss=0.10262360691107629\n",
      "Stochastic Gradient Descent(728/999): loss=0.10262081563526224\n",
      "Stochastic Gradient Descent(729/999): loss=0.10261802521252582\n",
      "Stochastic Gradient Descent(730/999): loss=0.10261523564130756\n",
      "Stochastic Gradient Descent(731/999): loss=0.10261244692006911\n",
      "Stochastic Gradient Descent(732/999): loss=0.10260965904729304\n",
      "Stochastic Gradient Descent(733/999): loss=0.10260687202148247\n",
      "Stochastic Gradient Descent(734/999): loss=0.10260408584116079\n",
      "Stochastic Gradient Descent(735/999): loss=0.10260130050487122\n",
      "Stochastic Gradient Descent(736/999): loss=0.10259851601117657\n",
      "Stochastic Gradient Descent(737/999): loss=0.102595732358659\n",
      "Stochastic Gradient Descent(738/999): loss=0.10259294954591949\n",
      "Stochastic Gradient Descent(739/999): loss=0.1025901675715777\n",
      "Stochastic Gradient Descent(740/999): loss=0.10258738643427176\n",
      "Stochastic Gradient Descent(741/999): loss=0.10258460613265771\n",
      "Stochastic Gradient Descent(742/999): loss=0.10258182666540944\n",
      "Stochastic Gradient Descent(743/999): loss=0.10257904803121827\n",
      "Stochastic Gradient Descent(744/999): loss=0.10257627022879273\n",
      "Stochastic Gradient Descent(745/999): loss=0.10257349325685834\n",
      "Stochastic Gradient Descent(746/999): loss=0.10257071711415718\n",
      "Stochastic Gradient Descent(747/999): loss=0.10256794179944777\n",
      "Stochastic Gradient Descent(748/999): loss=0.10256516731150475\n",
      "Stochastic Gradient Descent(749/999): loss=0.10256239364911858\n",
      "Stochastic Gradient Descent(750/999): loss=0.1025596208110954\n",
      "Stochastic Gradient Descent(751/999): loss=0.10255684879625669\n",
      "Stochastic Gradient Descent(752/999): loss=0.10255407760343895\n",
      "Stochastic Gradient Descent(753/999): loss=0.10255130723149368\n",
      "Stochastic Gradient Descent(754/999): loss=0.10254853767928691\n",
      "Stochastic Gradient Descent(755/999): loss=0.10254576894569921\n",
      "Stochastic Gradient Descent(756/999): loss=0.10254300102962513\n",
      "Stochastic Gradient Descent(757/999): loss=0.10254023392997333\n",
      "Stochastic Gradient Descent(758/999): loss=0.10253746764566603\n",
      "Stochastic Gradient Descent(759/999): loss=0.10253470217563908\n",
      "Stochastic Gradient Descent(760/999): loss=0.1025319375188415\n",
      "Stochastic Gradient Descent(761/999): loss=0.10252917367423549\n",
      "Stochastic Gradient Descent(762/999): loss=0.10252641064079604\n",
      "Stochastic Gradient Descent(763/999): loss=0.10252364841751078\n",
      "Stochastic Gradient Descent(764/999): loss=0.10252088700337988\n",
      "Stochastic Gradient Descent(765/999): loss=0.10251812639741557\n",
      "Stochastic Gradient Descent(766/999): loss=0.10251536659864237\n",
      "Stochastic Gradient Descent(767/999): loss=0.10251260760609653\n",
      "Stochastic Gradient Descent(768/999): loss=0.10250984941882593\n",
      "Stochastic Gradient Descent(769/999): loss=0.10250709203589005\n",
      "Stochastic Gradient Descent(770/999): loss=0.10250433545635956\n",
      "Stochastic Gradient Descent(771/999): loss=0.10250157967931639\n",
      "Stochastic Gradient Descent(772/999): loss=0.10249882470385326\n",
      "Stochastic Gradient Descent(773/999): loss=0.10249607052907374\n",
      "Stochastic Gradient Descent(774/999): loss=0.10249331715409193\n",
      "Stochastic Gradient Descent(775/999): loss=0.10249056457803252\n",
      "Stochastic Gradient Descent(776/999): loss=0.1024878128000302\n",
      "Stochastic Gradient Descent(777/999): loss=0.10248506181923\n",
      "Stochastic Gradient Descent(778/999): loss=0.10248231163478672\n",
      "Stochastic Gradient Descent(779/999): loss=0.10247956224586503\n",
      "Stochastic Gradient Descent(780/999): loss=0.10247681365163912\n",
      "Stochastic Gradient Descent(781/999): loss=0.10247406585129275\n",
      "Stochastic Gradient Descent(782/999): loss=0.10247131884401893\n",
      "Stochastic Gradient Descent(783/999): loss=0.10246857262901986\n",
      "Stochastic Gradient Descent(784/999): loss=0.10246582720550676\n",
      "Stochastic Gradient Descent(785/999): loss=0.10246308257269966\n",
      "Stochastic Gradient Descent(786/999): loss=0.10246033872982742\n",
      "Stochastic Gradient Descent(787/999): loss=0.10245759567612742\n",
      "Stochastic Gradient Descent(788/999): loss=0.10245485341084547\n",
      "Stochastic Gradient Descent(789/999): loss=0.10245211193323588\n",
      "Stochastic Gradient Descent(790/999): loss=0.1024493712425609\n",
      "Stochastic Gradient Descent(791/999): loss=0.10244663133809102\n",
      "Stochastic Gradient Descent(792/999): loss=0.10244389221910453\n",
      "Stochastic Gradient Descent(793/999): loss=0.10244115388488768\n",
      "Stochastic Gradient Descent(794/999): loss=0.10243841633473419\n",
      "Stochastic Gradient Descent(795/999): loss=0.10243567956794551\n",
      "Stochastic Gradient Descent(796/999): loss=0.1024329435838305\n",
      "Stochastic Gradient Descent(797/999): loss=0.10243020838170516\n",
      "Stochastic Gradient Descent(798/999): loss=0.10242747396089297\n",
      "Stochastic Gradient Descent(799/999): loss=0.10242474032072427\n",
      "Stochastic Gradient Descent(800/999): loss=0.10242200746053651\n",
      "Stochastic Gradient Descent(801/999): loss=0.10241927537967384\n",
      "Stochastic Gradient Descent(802/999): loss=0.1024165440774874\n",
      "Stochastic Gradient Descent(803/999): loss=0.1024138135533347\n",
      "Stochastic Gradient Descent(804/999): loss=0.10241108380658004\n",
      "Stochastic Gradient Descent(805/999): loss=0.10240835483659401\n",
      "Stochastic Gradient Descent(806/999): loss=0.1024056266427535\n",
      "Stochastic Gradient Descent(807/999): loss=0.10240289922444178\n",
      "Stochastic Gradient Descent(808/999): loss=0.10240017258104808\n",
      "Stochastic Gradient Descent(809/999): loss=0.10239744671196785\n",
      "Stochastic Gradient Descent(810/999): loss=0.1023947216166023\n",
      "Stochastic Gradient Descent(811/999): loss=0.10239199729435858\n",
      "Stochastic Gradient Descent(812/999): loss=0.10238927374464953\n",
      "Stochastic Gradient Descent(813/999): loss=0.10238655096689381\n",
      "Stochastic Gradient Descent(814/999): loss=0.10238382896051543\n",
      "Stochastic Gradient Descent(815/999): loss=0.102381107724944\n",
      "Stochastic Gradient Descent(816/999): loss=0.10237838725961451\n",
      "Stochastic Gradient Descent(817/999): loss=0.10237566756396732\n",
      "Stochastic Gradient Descent(818/999): loss=0.10237294863744788\n",
      "Stochastic Gradient Descent(819/999): loss=0.10237023047950684\n",
      "Stochastic Gradient Descent(820/999): loss=0.10236751308959997\n",
      "Stochastic Gradient Descent(821/999): loss=0.10236479646718796\n",
      "Stochastic Gradient Descent(822/999): loss=0.10236208061173642\n",
      "Stochastic Gradient Descent(823/999): loss=0.10235936552271578\n",
      "Stochastic Gradient Descent(824/999): loss=0.10235665119960118\n",
      "Stochastic Gradient Descent(825/999): loss=0.10235393764187249\n",
      "Stochastic Gradient Descent(826/999): loss=0.10235122484901416\n",
      "Stochastic Gradient Descent(827/999): loss=0.1023485128205151\n",
      "Stochastic Gradient Descent(828/999): loss=0.10234580155586884\n",
      "Stochastic Gradient Descent(829/999): loss=0.10234309105457311\n",
      "Stochastic Gradient Descent(830/999): loss=0.10234038131613003\n",
      "Stochastic Gradient Descent(831/999): loss=0.10233767234004595\n",
      "Stochastic Gradient Descent(832/999): loss=0.10233496412583148\n",
      "Stochastic Gradient Descent(833/999): loss=0.10233225667300123\n",
      "Stochastic Gradient Descent(834/999): loss=0.10232954998107387\n",
      "Stochastic Gradient Descent(835/999): loss=0.10232684404957214\n",
      "Stochastic Gradient Descent(836/999): loss=0.10232413887802258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(837/999): loss=0.10232143446595574\n",
      "Stochastic Gradient Descent(838/999): loss=0.1023187308129058\n",
      "Stochastic Gradient Descent(839/999): loss=0.10231602791841077\n",
      "Stochastic Gradient Descent(840/999): loss=0.10231332578201237\n",
      "Stochastic Gradient Descent(841/999): loss=0.10231062440325586\n",
      "Stochastic Gradient Descent(842/999): loss=0.10230792378169014\n",
      "Stochastic Gradient Descent(843/999): loss=0.10230522391686755\n",
      "Stochastic Gradient Descent(844/999): loss=0.10230252480834391\n",
      "Stochastic Gradient Descent(845/999): loss=0.10229982645567844\n",
      "Stochastic Gradient Descent(846/999): loss=0.10229712885843376\n",
      "Stochastic Gradient Descent(847/999): loss=0.10229443201617568\n",
      "Stochastic Gradient Descent(848/999): loss=0.10229173592847327\n",
      "Stochastic Gradient Descent(849/999): loss=0.10228904059489886\n",
      "Stochastic Gradient Descent(850/999): loss=0.10228634601502787\n",
      "Stochastic Gradient Descent(851/999): loss=0.10228365218843885\n",
      "Stochastic Gradient Descent(852/999): loss=0.10228095911471327\n",
      "Stochastic Gradient Descent(853/999): loss=0.10227826679343582\n",
      "Stochastic Gradient Descent(854/999): loss=0.10227557522419385\n",
      "Stochastic Gradient Descent(855/999): loss=0.10227288440657786\n",
      "Stochastic Gradient Descent(856/999): loss=0.10227019434018109\n",
      "Stochastic Gradient Descent(857/999): loss=0.1022675050245996\n",
      "Stochastic Gradient Descent(858/999): loss=0.10226481645943221\n",
      "Stochastic Gradient Descent(859/999): loss=0.10226212864428048\n",
      "Stochastic Gradient Descent(860/999): loss=0.10225944157874872\n",
      "Stochastic Gradient Descent(861/999): loss=0.10225675526244371\n",
      "Stochastic Gradient Descent(862/999): loss=0.10225406969497497\n",
      "Stochastic Gradient Descent(863/999): loss=0.10225138487595455\n",
      "Stochastic Gradient Descent(864/999): loss=0.10224870080499696\n",
      "Stochastic Gradient Descent(865/999): loss=0.10224601748171924\n",
      "Stochastic Gradient Descent(866/999): loss=0.10224333490574093\n",
      "Stochastic Gradient Descent(867/999): loss=0.1022406530766838\n",
      "Stochastic Gradient Descent(868/999): loss=0.10223797199417214\n",
      "Stochastic Gradient Descent(869/999): loss=0.10223529165783249\n",
      "Stochastic Gradient Descent(870/999): loss=0.10223261206729371\n",
      "Stochastic Gradient Descent(871/999): loss=0.10222993322218697\n",
      "Stochastic Gradient Descent(872/999): loss=0.10222725512214549\n",
      "Stochastic Gradient Descent(873/999): loss=0.10222457776680484\n",
      "Stochastic Gradient Descent(874/999): loss=0.10222190115580271\n",
      "Stochastic Gradient Descent(875/999): loss=0.10221922528877886\n",
      "Stochastic Gradient Descent(876/999): loss=0.10221655016537518\n",
      "Stochastic Gradient Descent(877/999): loss=0.10221387578523554\n",
      "Stochastic Gradient Descent(878/999): loss=0.10221120214800598\n",
      "Stochastic Gradient Descent(879/999): loss=0.10220852925333433\n",
      "Stochastic Gradient Descent(880/999): loss=0.10220585710087063\n",
      "Stochastic Gradient Descent(881/999): loss=0.1022031856902666\n",
      "Stochastic Gradient Descent(882/999): loss=0.10220051502117598\n",
      "Stochastic Gradient Descent(883/999): loss=0.10219784509325441\n",
      "Stochastic Gradient Descent(884/999): loss=0.10219517590615929\n",
      "Stochastic Gradient Descent(885/999): loss=0.1021925074595499\n",
      "Stochastic Gradient Descent(886/999): loss=0.10218983975308728\n",
      "Stochastic Gradient Descent(887/999): loss=0.1021871727864342\n",
      "Stochastic Gradient Descent(888/999): loss=0.10218450655925525\n",
      "Stochastic Gradient Descent(889/999): loss=0.10218184107121663\n",
      "Stochastic Gradient Descent(890/999): loss=0.10217917632198618\n",
      "Stochastic Gradient Descent(891/999): loss=0.10217651231123356\n",
      "Stochastic Gradient Descent(892/999): loss=0.10217384903862994\n",
      "Stochastic Gradient Descent(893/999): loss=0.10217118650384809\n",
      "Stochastic Gradient Descent(894/999): loss=0.10216852470656239\n",
      "Stochastic Gradient Descent(895/999): loss=0.10216586364644878\n",
      "Stochastic Gradient Descent(896/999): loss=0.1021632033231847\n",
      "Stochastic Gradient Descent(897/999): loss=0.10216054373644906\n",
      "Stochastic Gradient Descent(898/999): loss=0.10215788488592242\n",
      "Stochastic Gradient Descent(899/999): loss=0.1021552267712866\n",
      "Stochastic Gradient Descent(900/999): loss=0.10215256939222496\n",
      "Stochastic Gradient Descent(901/999): loss=0.10214991274842225\n",
      "Stochastic Gradient Descent(902/999): loss=0.1021472568395647\n",
      "Stochastic Gradient Descent(903/999): loss=0.10214460166533976\n",
      "Stochastic Gradient Descent(904/999): loss=0.10214194722543633\n",
      "Stochastic Gradient Descent(905/999): loss=0.10213929351954461\n",
      "Stochastic Gradient Descent(906/999): loss=0.1021366405473562\n",
      "Stochastic Gradient Descent(907/999): loss=0.10213398830856386\n",
      "Stochastic Gradient Descent(908/999): loss=0.10213133680286172\n",
      "Stochastic Gradient Descent(909/999): loss=0.1021286860299451\n",
      "Stochastic Gradient Descent(910/999): loss=0.10212603598951057\n",
      "Stochastic Gradient Descent(911/999): loss=0.102123386681256\n",
      "Stochastic Gradient Descent(912/999): loss=0.10212073810488033\n",
      "Stochastic Gradient Descent(913/999): loss=0.10211809026008377\n",
      "Stochastic Gradient Descent(914/999): loss=0.10211544314656766\n",
      "Stochastic Gradient Descent(915/999): loss=0.10211279676403451\n",
      "Stochastic Gradient Descent(916/999): loss=0.10211015111218782\n",
      "Stochastic Gradient Descent(917/999): loss=0.10210750619073249\n",
      "Stochastic Gradient Descent(918/999): loss=0.1021048619993742\n",
      "Stochastic Gradient Descent(919/999): loss=0.10210221853781994\n",
      "Stochastic Gradient Descent(920/999): loss=0.10209957580577764\n",
      "Stochastic Gradient Descent(921/999): loss=0.10209693380295627\n",
      "Stochastic Gradient Descent(922/999): loss=0.10209429252906592\n",
      "Stochastic Gradient Descent(923/999): loss=0.10209165198381762\n",
      "Stochastic Gradient Descent(924/999): loss=0.10208901216692345\n",
      "Stochastic Gradient Descent(925/999): loss=0.1020863730780964\n",
      "Stochastic Gradient Descent(926/999): loss=0.10208373471705054\n",
      "Stochastic Gradient Descent(927/999): loss=0.10208109708350081\n",
      "Stochastic Gradient Descent(928/999): loss=0.10207846017716311\n",
      "Stochastic Gradient Descent(929/999): loss=0.10207582399775429\n",
      "Stochastic Gradient Descent(930/999): loss=0.10207318854499214\n",
      "Stochastic Gradient Descent(931/999): loss=0.1020705538185952\n",
      "Stochastic Gradient Descent(932/999): loss=0.10206791981828316\n",
      "Stochastic Gradient Descent(933/999): loss=0.10206528654377635\n",
      "Stochastic Gradient Descent(934/999): loss=0.10206265399479605\n",
      "Stochastic Gradient Descent(935/999): loss=0.10206002217106441\n",
      "Stochastic Gradient Descent(936/999): loss=0.10205739107230442\n",
      "Stochastic Gradient Descent(937/999): loss=0.10205476069823986\n",
      "Stochastic Gradient Descent(938/999): loss=0.10205213104859533\n",
      "Stochastic Gradient Descent(939/999): loss=0.10204950212309621\n",
      "Stochastic Gradient Descent(940/999): loss=0.10204687392146873\n",
      "Stochastic Gradient Descent(941/999): loss=0.10204424644343989\n",
      "Stochastic Gradient Descent(942/999): loss=0.10204161968873741\n",
      "Stochastic Gradient Descent(943/999): loss=0.10203899365708974\n",
      "Stochastic Gradient Descent(944/999): loss=0.10203636834822621\n",
      "Stochastic Gradient Descent(945/999): loss=0.10203374376187671\n",
      "Stochastic Gradient Descent(946/999): loss=0.10203111989777204\n",
      "Stochastic Gradient Descent(947/999): loss=0.10202849675564354\n",
      "Stochastic Gradient Descent(948/999): loss=0.10202587433522337\n",
      "Stochastic Gradient Descent(949/999): loss=0.10202325263624432\n",
      "Stochastic Gradient Descent(950/999): loss=0.1020206316584399\n",
      "Stochastic Gradient Descent(951/999): loss=0.10201801140154425\n",
      "Stochastic Gradient Descent(952/999): loss=0.10201539186529228\n",
      "Stochastic Gradient Descent(953/999): loss=0.10201277304941939\n",
      "Stochastic Gradient Descent(954/999): loss=0.10201015495366178\n",
      "Stochastic Gradient Descent(955/999): loss=0.10200753757775617\n",
      "Stochastic Gradient Descent(956/999): loss=0.10200492092143998\n",
      "Stochastic Gradient Descent(957/999): loss=0.10200230498445124\n",
      "Stochastic Gradient Descent(958/999): loss=0.10199968976652857\n",
      "Stochastic Gradient Descent(959/999): loss=0.10199707526741117\n",
      "Stochastic Gradient Descent(960/999): loss=0.10199446148683888\n",
      "Stochastic Gradient Descent(961/999): loss=0.1019918484245521\n",
      "Stochastic Gradient Descent(962/999): loss=0.10198923608029183\n",
      "Stochastic Gradient Descent(963/999): loss=0.10198662445379961\n",
      "Stochastic Gradient Descent(964/999): loss=0.10198401354481752\n",
      "Stochastic Gradient Descent(965/999): loss=0.10198140335308827\n",
      "Stochastic Gradient Descent(966/999): loss=0.10197879387835504\n",
      "Stochastic Gradient Descent(967/999): loss=0.10197618512036156\n",
      "Stochastic Gradient Descent(968/999): loss=0.10197357707885217\n",
      "Stochastic Gradient Descent(969/999): loss=0.10197096975357157\n",
      "Stochastic Gradient Descent(970/999): loss=0.10196836314426519\n",
      "Stochastic Gradient Descent(971/999): loss=0.10196575725067879\n",
      "Stochastic Gradient Descent(972/999): loss=0.10196315207255868\n",
      "Stochastic Gradient Descent(973/999): loss=0.10196054760965172\n",
      "Stochastic Gradient Descent(974/999): loss=0.1019579438617052\n",
      "Stochastic Gradient Descent(975/999): loss=0.1019553408284669\n",
      "Stochastic Gradient Descent(976/999): loss=0.10195273850968511\n",
      "Stochastic Gradient Descent(977/999): loss=0.10195013690510857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(978/999): loss=0.10194753601448645\n",
      "Stochastic Gradient Descent(979/999): loss=0.10194493583756842\n",
      "Stochastic Gradient Descent(980/999): loss=0.1019423363741046\n",
      "Stochastic Gradient Descent(981/999): loss=0.10193973762384552\n",
      "Stochastic Gradient Descent(982/999): loss=0.10193713958654217\n",
      "Stochastic Gradient Descent(983/999): loss=0.10193454226194594\n",
      "Stochastic Gradient Descent(984/999): loss=0.10193194564980877\n",
      "Stochastic Gradient Descent(985/999): loss=0.10192934974988285\n",
      "Stochastic Gradient Descent(986/999): loss=0.10192675456192087\n",
      "Stochastic Gradient Descent(987/999): loss=0.10192416008567595\n",
      "Stochastic Gradient Descent(988/999): loss=0.1019215663209016\n",
      "Stochastic Gradient Descent(989/999): loss=0.1019189732673517\n",
      "Stochastic Gradient Descent(990/999): loss=0.10191638092478054\n",
      "Stochastic Gradient Descent(991/999): loss=0.10191378929294284\n",
      "Stochastic Gradient Descent(992/999): loss=0.1019111983715936\n",
      "Stochastic Gradient Descent(993/999): loss=0.10190860816048836\n",
      "Stochastic Gradient Descent(994/999): loss=0.10190601865938287\n",
      "Stochastic Gradient Descent(995/999): loss=0.10190342986803334\n",
      "Stochastic Gradient Descent(996/999): loss=0.10190084178619639\n",
      "Stochastic Gradient Descent(997/999): loss=0.10189825441362883\n",
      "Stochastic Gradient Descent(998/999): loss=0.10189566775008804\n",
      "Stochastic Gradient Descent(999/999): loss=0.1018930817953316\n",
      "Stochastic Gradient Descent(0/999): loss=0.1484316010937374\n",
      "Stochastic Gradient Descent(1/999): loss=0.14773301101196282\n",
      "Stochastic Gradient Descent(2/999): loss=0.14704562192738327\n",
      "Stochastic Gradient Descent(3/999): loss=0.14636925338179332\n",
      "Stochastic Gradient Descent(4/999): loss=0.14570372782458213\n",
      "Stochastic Gradient Descent(5/999): loss=0.14504887056588542\n",
      "Stochastic Gradient Descent(6/999): loss=0.14440450973049243\n",
      "Stochastic Gradient Descent(7/999): loss=0.14377047621249503\n",
      "Stochastic Gradient Descent(8/999): loss=0.14314660363066822\n",
      "Stochastic Gradient Descent(9/999): loss=0.14253272828456867\n",
      "Stochastic Gradient Descent(10/999): loss=0.1419286891113416\n",
      "Stochastic Gradient Descent(11/999): loss=0.14133432764322276\n",
      "Stochastic Gradient Descent(12/999): loss=0.14074948796572584\n",
      "Stochastic Gradient Descent(13/999): loss=0.14017401667650284\n",
      "Stochastic Gradient Descent(14/999): loss=0.13960776284486814\n",
      "Stochastic Gradient Descent(15/999): loss=0.13905057797197384\n",
      "Stochastic Gradient Descent(16/999): loss=0.1385023159516278\n",
      "Stochastic Gradient Descent(17/999): loss=0.1379628330317417\n",
      "Stochastic Gradient Descent(18/999): loss=0.13743198777640153\n",
      "Stochastic Gradient Descent(19/999): loss=0.13690964102854858\n",
      "Stochastic Gradient Descent(20/999): loss=0.1363956558732615\n",
      "Stochastic Gradient Descent(21/999): loss=0.13588989760163048\n",
      "Stochastic Gradient Descent(22/999): loss=0.13539223367521344\n",
      "Stochastic Gradient Descent(23/999): loss=0.13490253369106445\n",
      "Stochastic Gradient Descent(24/999): loss=0.1344206693473262\n",
      "Stochastic Gradient Descent(25/999): loss=0.1339465144093764\n",
      "Stochastic Gradient Descent(26/999): loss=0.13347994467652002\n",
      "Stochastic Gradient Descent(27/999): loss=0.13302083794921774\n",
      "Stochastic Gradient Descent(28/999): loss=0.13256907399684317\n",
      "Stochastic Gradient Descent(29/999): loss=0.13212453452595851\n",
      "Stochastic Gradient Descent(30/999): loss=0.13168710314910242\n",
      "Stochastic Gradient Descent(31/999): loss=0.1312566653540801\n",
      "Stochastic Gradient Descent(32/999): loss=0.1308331084737484\n",
      "Stochastic Gradient Descent(33/999): loss=0.130416321656288\n",
      "Stochastic Gradient Descent(34/999): loss=0.1300061958359539\n",
      "Stochastic Gradient Descent(35/999): loss=0.12960262370429804\n",
      "Stochastic Gradient Descent(36/999): loss=0.129205499681855\n",
      "Stochastic Gradient Descent(37/999): loss=0.12881471989028412\n",
      "Stochastic Gradient Descent(38/999): loss=0.1284301821249603\n",
      "Stochastic Gradient Descent(39/999): loss=0.12805178582800686\n",
      "Stochastic Gradient Descent(40/999): loss=0.1276794320617623\n",
      "Stochastic Gradient Descent(41/999): loss=0.12731302348267504\n",
      "Stochastic Gradient Descent(42/999): loss=0.12695246431561866\n",
      "Stochastic Gradient Descent(43/999): loss=0.12659766032862074\n",
      "Stochastic Gradient Descent(44/999): loss=0.1262485188079994\n",
      "Stochastic Gradient Descent(45/999): loss=0.12590494853389975\n",
      "Stochastic Gradient Descent(46/999): loss=0.1255668597562251\n",
      "Stochastic Gradient Descent(47/999): loss=0.12523416417095573\n",
      "Stochastic Gradient Descent(48/999): loss=0.12490677489684922\n",
      "Stochastic Gradient Descent(49/999): loss=0.12458460645251616\n",
      "Stochastic Gradient Descent(50/999): loss=0.12426757473386536\n",
      "Stochastic Gradient Descent(51/999): loss=0.12395559699191233\n",
      "Stochastic Gradient Descent(52/999): loss=0.12364859181094547\n",
      "Stochastic Gradient Descent(53/999): loss=0.12334647908704376\n",
      "Stochastic Gradient Descent(54/999): loss=0.1230491800069409\n",
      "Stochastic Gradient Descent(55/999): loss=0.12275661702722998\n",
      "Stochastic Gradient Descent(56/999): loss=0.12246871385390286\n",
      "Stochastic Gradient Descent(57/999): loss=0.12218539542221969\n",
      "Stochastic Gradient Descent(58/999): loss=0.1219065878769024\n",
      "Stochastic Gradient Descent(59/999): loss=0.12163221855264748\n",
      "Stochastic Gradient Descent(60/999): loss=0.12136221595495292\n",
      "Stochastic Gradient Descent(61/999): loss=0.12109650974125358\n",
      "Stochastic Gradient Descent(62/999): loss=0.12083503070236105\n",
      "Stochastic Gradient Descent(63/999): loss=0.1205777107442023\n",
      "Stochastic Gradient Descent(64/999): loss=0.12032448286985248\n",
      "Stochastic Gradient Descent(65/999): loss=0.12007528116185745\n",
      "Stochastic Gradient Descent(66/999): loss=0.11983004076484087\n",
      "Stochastic Gradient Descent(67/999): loss=0.11958869786839181\n",
      "Stochastic Gradient Descent(68/999): loss=0.11935118969022783\n",
      "Stochastic Gradient Descent(69/999): loss=0.1191174544596295\n",
      "Stochastic Gradient Descent(70/999): loss=0.11888743140114176\n",
      "Stochastic Gradient Descent(71/999): loss=0.11866106071853795\n",
      "Stochastic Gradient Descent(72/999): loss=0.11843828357904208\n",
      "Stochastic Gradient Descent(73/999): loss=0.1182190420978055\n",
      "Stochastic Gradient Descent(74/999): loss=0.1180032793226336\n",
      "Stochastic Gradient Descent(75/999): loss=0.11779093921895843\n",
      "Stochastic Gradient Descent(76/999): loss=0.11758196665505385\n",
      "Stochastic Gradient Descent(77/999): loss=0.11737630738748848\n",
      "Stochastic Gradient Descent(78/999): loss=0.11717390804681335\n",
      "Stochastic Gradient Descent(79/999): loss=0.11697471612347983\n",
      "Stochastic Gradient Descent(80/999): loss=0.11677867995398486\n",
      "Stochastic Gradient Descent(81/999): loss=0.11658574870723905\n",
      "Stochastic Gradient Descent(82/999): loss=0.11639587237115466\n",
      "Stochastic Gradient Descent(83/999): loss=0.11620900173944947\n",
      "Stochastic Gradient Descent(84/999): loss=0.11602508839866349\n",
      "Stochastic Gradient Descent(85/999): loss=0.1158440847153848\n",
      "Stochastic Gradient Descent(86/999): loss=0.11566594382368103\n",
      "Stochastic Gradient Descent(87/999): loss=0.11549061961273352\n",
      "Stochastic Gradient Descent(88/999): loss=0.1153180667146708\n",
      "Stochastic Gradient Descent(89/999): loss=0.1151482404925979\n",
      "Stochastic Gradient Descent(90/999): loss=0.11498109702881862\n",
      "Stochastic Gradient Descent(91/999): loss=0.11481659311324756\n",
      "Stochastic Gradient Descent(92/999): loss=0.11465468623200903\n",
      "Stochastic Gradient Descent(93/999): loss=0.11449533455621932\n",
      "Stochastic Gradient Descent(94/999): loss=0.11433849693094986\n",
      "Stochastic Gradient Descent(95/999): loss=0.11418413286436847\n",
      "Stochastic Gradient Descent(96/999): loss=0.11403220251705513\n",
      "Stochastic Gradient Descent(97/999): loss=0.11388266669149018\n",
      "Stochastic Gradient Descent(98/999): loss=0.11373548682171189\n",
      "Stochastic Gradient Descent(99/999): loss=0.11359062496314044\n",
      "Stochastic Gradient Descent(100/999): loss=0.11344804378256625\n",
      "Stochastic Gradient Descent(101/999): loss=0.11330770654829916\n",
      "Stochastic Gradient Descent(102/999): loss=0.11316957712047664\n",
      "Stochastic Gradient Descent(103/999): loss=0.11303361994152811\n",
      "Stochastic Gradient Descent(104/999): loss=0.11289980002679291\n",
      "Stochastic Gradient Descent(105/999): loss=0.11276808295528928\n",
      "Stochastic Gradient Descent(106/999): loss=0.11263843486063228\n",
      "Stochastic Gradient Descent(107/999): loss=0.11251082242209785\n",
      "Stochastic Gradient Descent(108/999): loss=0.11238521285583108\n",
      "Stochastic Gradient Descent(109/999): loss=0.11226157390619594\n",
      "Stochastic Gradient Descent(110/999): loss=0.11213987383726445\n",
      "Stochastic Gradient Descent(111/999): loss=0.11202008142444295\n",
      "Stochastic Gradient Descent(112/999): loss=0.11190216594623342\n",
      "Stochastic Gradient Descent(113/999): loss=0.11178609717612728\n",
      "Stochastic Gradient Descent(114/999): loss=0.1116718453746302\n",
      "Stochastic Gradient Descent(115/999): loss=0.11155938128141493\n",
      "Stochastic Gradient Descent(116/999): loss=0.11144867610760098\n",
      "Stochastic Gradient Descent(117/999): loss=0.11133970152815842\n",
      "Stochastic Gradient Descent(118/999): loss=0.11123242967443424\n",
      "Stochastic Gradient Descent(119/999): loss=0.1111268331267991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(120/999): loss=0.11102288490741224\n",
      "Stochastic Gradient Descent(121/999): loss=0.11092055847310377\n",
      "Stochastic Gradient Descent(122/999): loss=0.11081982770837058\n",
      "Stochastic Gradient Descent(123/999): loss=0.11072066691848596\n",
      "Stochastic Gradient Descent(124/999): loss=0.11062305082271977\n",
      "Stochastic Gradient Descent(125/999): loss=0.11052695454766813\n",
      "Stochastic Gradient Descent(126/999): loss=0.11043235362069029\n",
      "Stochastic Gradient Descent(127/999): loss=0.11033922396345154\n",
      "Stochastic Gradient Descent(128/999): loss=0.11024754188557004\n",
      "Stochastic Gradient Descent(129/999): loss=0.11015728407836581\n",
      "Stochastic Gradient Descent(130/999): loss=0.1100684276087109\n",
      "Stochastic Gradient Descent(131/999): loss=0.1099809499129781\n",
      "Stochastic Gradient Descent(132/999): loss=0.10989482879108774\n",
      "Stochastic Gradient Descent(133/999): loss=0.1098100424006498\n",
      "Stochastic Gradient Descent(134/999): loss=0.10972656925120089\n",
      "Stochastic Gradient Descent(135/999): loss=0.10964438819853373\n",
      "Stochastic Gradient Descent(136/999): loss=0.1095634784391185\n",
      "Stochastic Gradient Descent(137/999): loss=0.1094838195046133\n",
      "Stochastic Gradient Descent(138/999): loss=0.10940539125646381\n",
      "Stochastic Gradient Descent(139/999): loss=0.1093281738805895\n",
      "Stochastic Gradient Descent(140/999): loss=0.10925214788215575\n",
      "Stochastic Gradient Descent(141/999): loss=0.10917729408042981\n",
      "Stochastic Gradient Descent(142/999): loss=0.10910359360372018\n",
      "Stochastic Gradient Descent(143/999): loss=0.10903102788439706\n",
      "Stochastic Gradient Descent(144/999): loss=0.10895957865399333\n",
      "Stochastic Gradient Descent(145/999): loss=0.10888922793838415\n",
      "Stochastic Gradient Descent(146/999): loss=0.10881995805304463\n",
      "Stochastic Gradient Descent(147/999): loss=0.10875175159838373\n",
      "Stochastic Gradient Descent(148/999): loss=0.10868459145515297\n",
      "Stochastic Gradient Descent(149/999): loss=0.10861846077993001\n",
      "Stochastic Gradient Descent(150/999): loss=0.10855334300067389\n",
      "Stochastic Gradient Descent(151/999): loss=0.10848922181235289\n",
      "Stochastic Gradient Descent(152/999): loss=0.10842608117264223\n",
      "Stochastic Gradient Descent(153/999): loss=0.10836390529769103\n",
      "Stochastic Gradient Descent(154/999): loss=0.10830267865795819\n",
      "Stochastic Gradient Descent(155/999): loss=0.10824238597411441\n",
      "Stochastic Gradient Descent(156/999): loss=0.10818301221301098\n",
      "Stochastic Gradient Descent(157/999): loss=0.10812454258371301\n",
      "Stochastic Gradient Descent(158/999): loss=0.1080669625335971\n",
      "Stochastic Gradient Descent(159/999): loss=0.10801025774451134\n",
      "Stochastic Gradient Descent(160/999): loss=0.10795441412899755\n",
      "Stochastic Gradient Descent(161/999): loss=0.10789941782657429\n",
      "Stochastic Gradient Descent(162/999): loss=0.1078452552000796\n",
      "Stochastic Gradient Descent(163/999): loss=0.10779191283207298\n",
      "Stochastic Gradient Descent(164/999): loss=0.10773937752129505\n",
      "Stochastic Gradient Descent(165/999): loss=0.10768763627918446\n",
      "Stochastic Gradient Descent(166/999): loss=0.10763667632645076\n",
      "Stochastic Gradient Descent(167/999): loss=0.10758648508970259\n",
      "Stochastic Gradient Descent(168/999): loss=0.10753705019813019\n",
      "Stochastic Gradient Descent(169/999): loss=0.10748835948024134\n",
      "Stochastic Gradient Descent(170/999): loss=0.10744040096064984\n",
      "Stochastic Gradient Descent(171/999): loss=0.10739316285691587\n",
      "Stochastic Gradient Descent(172/999): loss=0.10734663357643703\n",
      "Stochastic Gradient Descent(173/999): loss=0.10730080171338999\n",
      "Stochastic Gradient Descent(174/999): loss=0.10725565604572043\n",
      "Stochastic Gradient Descent(175/999): loss=0.10721118553218278\n",
      "Stochastic Gradient Descent(176/999): loss=0.1071673793094264\n",
      "Stochastic Gradient Descent(177/999): loss=0.1071242266891297\n",
      "Stochastic Gradient Descent(178/999): loss=0.10708171715517976\n",
      "Stochastic Gradient Descent(179/999): loss=0.10703984036089777\n",
      "Stochastic Gradient Descent(180/999): loss=0.10699858612630903\n",
      "Stochastic Gradient Descent(181/999): loss=0.10695794443545689\n",
      "Stochastic Gradient Descent(182/999): loss=0.10691790543376029\n",
      "Stochastic Gradient Descent(183/999): loss=0.10687845942541316\n",
      "Stochastic Gradient Descent(184/999): loss=0.10683959687082674\n",
      "Stochastic Gradient Descent(185/999): loss=0.10680130838411216\n",
      "Stochastic Gradient Descent(186/999): loss=0.10676358473060411\n",
      "Stochastic Gradient Descent(187/999): loss=0.10672641682442435\n",
      "Stochastic Gradient Descent(188/999): loss=0.10668979572608427\n",
      "Stochastic Gradient Descent(189/999): loss=0.10665371264012631\n",
      "Stochastic Gradient Descent(190/999): loss=0.10661815891280335\n",
      "Stochastic Gradient Descent(191/999): loss=0.10658312602979514\n",
      "Stochastic Gradient Descent(192/999): loss=0.10654860561396222\n",
      "Stochastic Gradient Descent(193/999): loss=0.1065145894231353\n",
      "Stochastic Gradient Descent(194/999): loss=0.10648106934794067\n",
      "Stochastic Gradient Descent(195/999): loss=0.10644803740966076\n",
      "Stochastic Gradient Descent(196/999): loss=0.10641548575812855\n",
      "Stochastic Gradient Descent(197/999): loss=0.10638340666965669\n",
      "Stochastic Gradient Descent(198/999): loss=0.10635179254499952\n",
      "Stochastic Gradient Descent(199/999): loss=0.10632063590734794\n",
      "Stochastic Gradient Descent(200/999): loss=0.10628992940035677\n",
      "Stochastic Gradient Descent(201/999): loss=0.10625966578620377\n",
      "Stochastic Gradient Descent(202/999): loss=0.10622983794367995\n",
      "Stochastic Gradient Descent(203/999): loss=0.10620043886631066\n",
      "Stochastic Gradient Descent(204/999): loss=0.10617146166050696\n",
      "Stochastic Gradient Descent(205/999): loss=0.10614289954374681\n",
      "Stochastic Gradient Descent(206/999): loss=0.10611474584278548\n",
      "Stochastic Gradient Descent(207/999): loss=0.10608699399189492\n",
      "Stochastic Gradient Descent(208/999): loss=0.10605963753113117\n",
      "Stochastic Gradient Descent(209/999): loss=0.10603267010463031\n",
      "Stochastic Gradient Descent(210/999): loss=0.10600608545893125\n",
      "Stochastic Gradient Descent(211/999): loss=0.10597987744132578\n",
      "Stochastic Gradient Descent(212/999): loss=0.10595403999823533\n",
      "Stochastic Gradient Descent(213/999): loss=0.10592856717361364\n",
      "Stochastic Gradient Descent(214/999): loss=0.10590345310737541\n",
      "Stochastic Gradient Descent(215/999): loss=0.10587869203385003\n",
      "Stochastic Gradient Descent(216/999): loss=0.10585427828026045\n",
      "Stochastic Gradient Descent(217/999): loss=0.10583020626522638\n",
      "Stochastic Gradient Descent(218/999): loss=0.10580647049729168\n",
      "Stochastic Gradient Descent(219/999): loss=0.10578306557347547\n",
      "Stochastic Gradient Descent(220/999): loss=0.10575998617784675\n",
      "Stochastic Gradient Descent(221/999): loss=0.10573722708012154\n",
      "Stochastic Gradient Descent(222/999): loss=0.10571478313428308\n",
      "Stochastic Gradient Descent(223/999): loss=0.10569264927722416\n",
      "Stochastic Gradient Descent(224/999): loss=0.10567082052741107\n",
      "Stochastic Gradient Descent(225/999): loss=0.10564929198356948\n",
      "Stochastic Gradient Descent(226/999): loss=0.10562805882339098\n",
      "Stochastic Gradient Descent(227/999): loss=0.10560711630226113\n",
      "Stochastic Gradient Descent(228/999): loss=0.10558645975200731\n",
      "Stochastic Gradient Descent(229/999): loss=0.10556608457966725\n",
      "Stochastic Gradient Descent(230/999): loss=0.10554598626627713\n",
      "Stochastic Gradient Descent(231/999): loss=0.10552616036567938\n",
      "Stochastic Gradient Descent(232/999): loss=0.10550660250334956\n",
      "Stochastic Gradient Descent(233/999): loss=0.10548730837524205\n",
      "Stochastic Gradient Descent(234/999): loss=0.10546827374665475\n",
      "Stochastic Gradient Descent(235/999): loss=0.10544949445111163\n",
      "Stochastic Gradient Descent(236/999): loss=0.1054309663892634\n",
      "Stochastic Gradient Descent(237/999): loss=0.10541268552780605\n",
      "Stochastic Gradient Descent(238/999): loss=0.10539464789841677\n",
      "Stochastic Gradient Descent(239/999): loss=0.10537684959670668\n",
      "Stochastic Gradient Descent(240/999): loss=0.10535928678119104\n",
      "Stochastic Gradient Descent(241/999): loss=0.10534195567227547\n",
      "Stochastic Gradient Descent(242/999): loss=0.10532485255125898\n",
      "Stochastic Gradient Descent(243/999): loss=0.10530797375935257\n",
      "Stochastic Gradient Descent(244/999): loss=0.10529131569671413\n",
      "Stochastic Gradient Descent(245/999): loss=0.10527487482149854\n",
      "Stochastic Gradient Descent(246/999): loss=0.10525864764892337\n",
      "Stochastic Gradient Descent(247/999): loss=0.10524263075034919\n",
      "Stochastic Gradient Descent(248/999): loss=0.1052268207523753\n",
      "Stochastic Gradient Descent(249/999): loss=0.10521121433594947\n",
      "Stochastic Gradient Descent(250/999): loss=0.10519580823549239\n",
      "Stochastic Gradient Descent(251/999): loss=0.105180599238036\n",
      "Stochastic Gradient Descent(252/999): loss=0.10516558418237587\n",
      "Stochastic Gradient Descent(253/999): loss=0.10515075995823728\n",
      "Stochastic Gradient Descent(254/999): loss=0.10513612350545429\n",
      "Stochastic Gradient Descent(255/999): loss=0.10512167181316273\n",
      "Stochastic Gradient Descent(256/999): loss=0.10510740191900568\n",
      "Stochastic Gradient Descent(257/999): loss=0.10509331090835188\n",
      "Stochastic Gradient Descent(258/999): loss=0.10507939591352677\n",
      "Stochastic Gradient Descent(259/999): loss=0.10506565411305609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(260/999): loss=0.10505208273092115\n",
      "Stochastic Gradient Descent(261/999): loss=0.10503867903582674\n",
      "Stochastic Gradient Descent(262/999): loss=0.10502544034048042\n",
      "Stochastic Gradient Descent(263/999): loss=0.10501236400088343\n",
      "Stochastic Gradient Descent(264/999): loss=0.10499944741563338\n",
      "Stochastic Gradient Descent(265/999): loss=0.10498668802523767\n",
      "Stochastic Gradient Descent(266/999): loss=0.1049740833114384\n",
      "Stochastic Gradient Descent(267/999): loss=0.10496163079654788\n",
      "Stochastic Gradient Descent(268/999): loss=0.104949328042795\n",
      "Stochastic Gradient Descent(269/999): loss=0.10493717265168209\n",
      "Stochastic Gradient Descent(270/999): loss=0.10492516226335216\n",
      "Stochastic Gradient Descent(271/999): loss=0.10491329455596611\n",
      "Stochastic Gradient Descent(272/999): loss=0.10490156724509063\n",
      "Stochastic Gradient Descent(273/999): loss=0.10488997808309491\n",
      "Stochastic Gradient Descent(274/999): loss=0.10487852485855821\n",
      "Stochastic Gradient Descent(275/999): loss=0.10486720539568603\n",
      "Stochastic Gradient Descent(276/999): loss=0.10485601755373636\n",
      "Stochastic Gradient Descent(277/999): loss=0.10484495922645475\n",
      "Stochastic Gradient Descent(278/999): loss=0.10483402834151873\n",
      "Stochastic Gradient Descent(279/999): loss=0.10482322285999102\n",
      "Stochastic Gradient Descent(280/999): loss=0.10481254077578166\n",
      "Stochastic Gradient Descent(281/999): loss=0.10480198011511874\n",
      "Stochastic Gradient Descent(282/999): loss=0.10479153893602774\n",
      "Stochastic Gradient Descent(283/999): loss=0.10478121532781909\n",
      "Stochastic Gradient Descent(284/999): loss=0.10477100741058425\n",
      "Stochastic Gradient Descent(285/999): loss=0.10476091333469954\n",
      "Stochastic Gradient Descent(286/999): loss=0.10475093128033852\n",
      "Stochastic Gradient Descent(287/999): loss=0.10474105945699148\n",
      "Stochastic Gradient Descent(288/999): loss=0.10473129610299341\n",
      "Stochastic Gradient Descent(289/999): loss=0.10472163948505912\n",
      "Stochastic Gradient Descent(290/999): loss=0.10471208789782593\n",
      "Stochastic Gradient Descent(291/999): loss=0.10470263966340387\n",
      "Stochastic Gradient Descent(292/999): loss=0.10469329313093292\n",
      "Stochastic Gradient Descent(293/999): loss=0.10468404667614774\n",
      "Stochastic Gradient Descent(294/999): loss=0.1046748987009487\n",
      "Stochastic Gradient Descent(295/999): loss=0.1046658476329809\n",
      "Stochastic Gradient Descent(296/999): loss=0.10465689192521885\n",
      "Stochastic Gradient Descent(297/999): loss=0.10464803005555864\n",
      "Stochastic Gradient Descent(298/999): loss=0.10463926052641624\n",
      "Stochastic Gradient Descent(299/999): loss=0.10463058186433266\n",
      "Stochastic Gradient Descent(300/999): loss=0.10462199261958492\n",
      "Stochastic Gradient Descent(301/999): loss=0.10461349136580388\n",
      "Stochastic Gradient Descent(302/999): loss=0.10460507669959779\n",
      "Stochastic Gradient Descent(303/999): loss=0.1045967472401821\n",
      "Stochastic Gradient Descent(304/999): loss=0.10458850162901537\n",
      "Stochastic Gradient Descent(305/999): loss=0.10458033852944072\n",
      "Stochastic Gradient Descent(306/999): loss=0.10457225662633313\n",
      "Stochastic Gradient Descent(307/999): loss=0.10456425462575283\n",
      "Stochastic Gradient Descent(308/999): loss=0.10455633125460359\n",
      "Stochastic Gradient Descent(309/999): loss=0.10454848526029721\n",
      "Stochastic Gradient Descent(310/999): loss=0.10454071541042284\n",
      "Stochastic Gradient Descent(311/999): loss=0.10453302049242205\n",
      "Stochastic Gradient Descent(312/999): loss=0.10452539931326885\n",
      "Stochastic Gradient Descent(313/999): loss=0.10451785069915501\n",
      "Stochastic Gradient Descent(314/999): loss=0.10451037349518048\n",
      "Stochastic Gradient Descent(315/999): loss=0.10450296656504868\n",
      "Stochastic Gradient Descent(316/999): loss=0.10449562879076671\n",
      "Stochastic Gradient Descent(317/999): loss=0.10448835907235053\n",
      "Stochastic Gradient Descent(318/999): loss=0.10448115632753478\n",
      "Stochastic Gradient Descent(319/999): loss=0.1044740194914873\n",
      "Stochastic Gradient Descent(320/999): loss=0.10446694751652803\n",
      "Stochastic Gradient Descent(321/999): loss=0.10445993937185312\n",
      "Stochastic Gradient Descent(322/999): loss=0.10445299404326247\n",
      "Stochastic Gradient Descent(323/999): loss=0.10444611053289263\n",
      "Stochastic Gradient Descent(324/999): loss=0.10443928785895336\n",
      "Stochastic Gradient Descent(325/999): loss=0.10443252505546875\n",
      "Stochastic Gradient Descent(326/999): loss=0.10442582117202237\n",
      "Stochastic Gradient Descent(327/999): loss=0.10441917527350648\n",
      "Stochastic Gradient Descent(328/999): loss=0.10441258643987565\n",
      "Stochastic Gradient Descent(329/999): loss=0.10440605376590378\n",
      "Stochastic Gradient Descent(330/999): loss=0.1043995763609455\n",
      "Stochastic Gradient Descent(331/999): loss=0.10439315334870117\n",
      "Stochastic Gradient Descent(332/999): loss=0.10438678386698574\n",
      "Stochastic Gradient Descent(333/999): loss=0.10438046706750136\n",
      "Stochastic Gradient Descent(334/999): loss=0.10437420211561367\n",
      "Stochastic Gradient Descent(335/999): loss=0.10436798819013154\n",
      "Stochastic Gradient Descent(336/999): loss=0.10436182448309053\n",
      "Stochastic Gradient Descent(337/999): loss=0.10435571019953979\n",
      "Stochastic Gradient Descent(338/999): loss=0.10434964455733242\n",
      "Stochastic Gradient Descent(339/999): loss=0.10434362678691902\n",
      "Stochastic Gradient Descent(340/999): loss=0.10433765613114487\n",
      "Stochastic Gradient Descent(341/999): loss=0.10433173184505024\n",
      "Stochastic Gradient Descent(342/999): loss=0.10432585319567372\n",
      "Stochastic Gradient Descent(343/999): loss=0.10432001946185902\n",
      "Stochastic Gradient Descent(344/999): loss=0.10431422993406488\n",
      "Stochastic Gradient Descent(345/999): loss=0.10430848391417767\n",
      "Stochastic Gradient Descent(346/999): loss=0.10430278071532752\n",
      "Stochastic Gradient Descent(347/999): loss=0.10429711966170692\n",
      "Stochastic Gradient Descent(348/999): loss=0.10429150008839268\n",
      "Stochastic Gradient Descent(349/999): loss=0.10428592134117048\n",
      "Stochastic Gradient Descent(350/999): loss=0.10428038277636226\n",
      "Stochastic Gradient Descent(351/999): loss=0.10427488376065662\n",
      "Stochastic Gradient Descent(352/999): loss=0.10426942367094165\n",
      "Stochastic Gradient Descent(353/999): loss=0.10426400189414063\n",
      "Stochastic Gradient Descent(354/999): loss=0.10425861782705037\n",
      "Stochastic Gradient Descent(355/999): loss=0.10425327087618212\n",
      "Stochastic Gradient Descent(356/999): loss=0.10424796045760501\n",
      "Stochastic Gradient Descent(357/999): loss=0.10424268599679211\n",
      "Stochastic Gradient Descent(358/999): loss=0.10423744692846884\n",
      "Stochastic Gradient Descent(359/999): loss=0.104232242696464\n",
      "Stochastic Gradient Descent(360/999): loss=0.10422707275356301\n",
      "Stochastic Gradient Descent(361/999): loss=0.10422193656136355\n",
      "Stochastic Gradient Descent(362/999): loss=0.10421683359013376\n",
      "Stochastic Gradient Descent(363/999): loss=0.10421176331867237\n",
      "Stochastic Gradient Descent(364/999): loss=0.10420672523417131\n",
      "Stochastic Gradient Descent(365/999): loss=0.1042017188320805\n",
      "Stochastic Gradient Descent(366/999): loss=0.10419674361597478\n",
      "Stochastic Gradient Descent(367/999): loss=0.10419179909742295\n",
      "Stochastic Gradient Descent(368/999): loss=0.10418688479585908\n",
      "Stochastic Gradient Descent(369/999): loss=0.10418200023845571\n",
      "Stochastic Gradient Descent(370/999): loss=0.10417714495999912\n",
      "Stochastic Gradient Descent(371/999): loss=0.10417231850276679\n",
      "Stochastic Gradient Descent(372/999): loss=0.10416752041640658\n",
      "Stochastic Gradient Descent(373/999): loss=0.10416275025781803\n",
      "Stochastic Gradient Descent(374/999): loss=0.10415800759103554\n",
      "Stochastic Gradient Descent(375/999): loss=0.10415329198711337\n",
      "Stochastic Gradient Descent(376/999): loss=0.10414860302401252\n",
      "Stochastic Gradient Descent(377/999): loss=0.10414394028648952\n",
      "Stochastic Gradient Descent(378/999): loss=0.10413930336598692\n",
      "Stochastic Gradient Descent(379/999): loss=0.10413469186052543\n",
      "Stochastic Gradient Descent(380/999): loss=0.10413010537459821\n",
      "Stochastic Gradient Descent(381/999): loss=0.10412554351906635\n",
      "Stochastic Gradient Descent(382/999): loss=0.10412100591105627\n",
      "Stochastic Gradient Descent(383/999): loss=0.10411649217385895\n",
      "Stochastic Gradient Descent(384/999): loss=0.1041120019368304\n",
      "Stochastic Gradient Descent(385/999): loss=0.10410753483529414\n",
      "Stochastic Gradient Descent(386/999): loss=0.10410309051044493\n",
      "Stochastic Gradient Descent(387/999): loss=0.1040986686092541\n",
      "Stochastic Gradient Descent(388/999): loss=0.1040942687843767\n",
      "Stochastic Gradient Descent(389/999): loss=0.10408989069405968\n",
      "Stochastic Gradient Descent(390/999): loss=0.10408553400205203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(391/999): loss=0.10408119837751593\n",
      "Stochastic Gradient Descent(392/999): loss=0.10407688349493957\n",
      "Stochastic Gradient Descent(393/999): loss=0.1040725890340515\n",
      "Stochastic Gradient Descent(394/999): loss=0.10406831467973597\n",
      "Stochastic Gradient Descent(395/999): loss=0.10406406012194999\n",
      "Stochastic Gradient Descent(396/999): loss=0.10405982505564162\n",
      "Stochastic Gradient Descent(397/999): loss=0.1040556091806695\n",
      "Stochastic Gradient Descent(398/999): loss=0.10405141220172368\n",
      "Stochastic Gradient Descent(399/999): loss=0.10404723382824793\n",
      "Stochastic Gradient Descent(400/999): loss=0.10404307377436295\n",
      "Stochastic Gradient Descent(401/999): loss=0.10403893175879117\n",
      "Stochastic Gradient Descent(402/999): loss=0.10403480750478258\n",
      "Stochastic Gradient Descent(403/999): loss=0.1040307007400418\n",
      "Stochastic Gradient Descent(404/999): loss=0.10402661119665611\n",
      "Stochastic Gradient Descent(405/999): loss=0.10402253861102508\n",
      "Stochastic Gradient Descent(406/999): loss=0.1040184827237911\n",
      "Stochastic Gradient Descent(407/999): loss=0.10401444327977079\n",
      "Stochastic Gradient Descent(408/999): loss=0.10401042002788795\n",
      "Stochastic Gradient Descent(409/999): loss=0.10400641272110722\n",
      "Stochastic Gradient Descent(410/999): loss=0.1040024211163692\n",
      "Stochastic Gradient Descent(411/999): loss=0.10399844497452618\n",
      "Stochastic Gradient Descent(412/999): loss=0.1039944840602792\n",
      "Stochastic Gradient Descent(413/999): loss=0.10399053814211608\n",
      "Stochastic Gradient Descent(414/999): loss=0.10398660699225024\n",
      "Stochastic Gradient Descent(415/999): loss=0.10398269038656094\n",
      "Stochastic Gradient Descent(416/999): loss=0.10397878810453401\n",
      "Stochastic Gradient Descent(417/999): loss=0.10397489992920378\n",
      "Stochastic Gradient Descent(418/999): loss=0.10397102564709593\n",
      "Stochastic Gradient Descent(419/999): loss=0.1039671650481713\n",
      "Stochastic Gradient Descent(420/999): loss=0.10396331792577036\n",
      "Stochastic Gradient Descent(421/999): loss=0.10395948407655906\n",
      "Stochastic Gradient Descent(422/999): loss=0.10395566330047483\n",
      "Stochastic Gradient Descent(423/999): loss=0.10395185540067434\n",
      "Stochastic Gradient Descent(424/999): loss=0.10394806018348125\n",
      "Stochastic Gradient Descent(425/999): loss=0.10394427745833544\n",
      "Stochastic Gradient Descent(426/999): loss=0.10394050703774255\n",
      "Stochastic Gradient Descent(427/999): loss=0.10393674873722489\n",
      "Stochastic Gradient Descent(428/999): loss=0.10393300237527257\n",
      "Stochastic Gradient Descent(429/999): loss=0.10392926777329586\n",
      "Stochastic Gradient Descent(430/999): loss=0.10392554475557807\n",
      "Stochastic Gradient Descent(431/999): loss=0.10392183314922916\n",
      "Stochastic Gradient Descent(432/999): loss=0.10391813278414046\n",
      "Stochastic Gradient Descent(433/999): loss=0.10391444349293956\n",
      "Stochastic Gradient Descent(434/999): loss=0.10391076511094643\n",
      "Stochastic Gradient Descent(435/999): loss=0.10390709747613\n",
      "Stochastic Gradient Descent(436/999): loss=0.10390344042906534\n",
      "Stochastic Gradient Descent(437/999): loss=0.10389979381289188\n",
      "Stochastic Gradient Descent(438/999): loss=0.10389615747327205\n",
      "Stochastic Gradient Descent(439/999): loss=0.1038925312583504\n",
      "Stochastic Gradient Descent(440/999): loss=0.10388891501871397\n",
      "Stochastic Gradient Descent(441/999): loss=0.10388530860735264\n",
      "Stochastic Gradient Descent(442/999): loss=0.10388171187962059\n",
      "Stochastic Gradient Descent(443/999): loss=0.10387812469319807\n",
      "Stochastic Gradient Descent(444/999): loss=0.1038745469080541\n",
      "Stochastic Gradient Descent(445/999): loss=0.10387097838640941\n",
      "Stochastic Gradient Descent(446/999): loss=0.10386741899270036\n",
      "Stochastic Gradient Descent(447/999): loss=0.10386386859354296\n",
      "Stochastic Gradient Descent(448/999): loss=0.1038603270576981\n",
      "Stochastic Gradient Descent(449/999): loss=0.10385679425603672\n",
      "Stochastic Gradient Descent(450/999): loss=0.10385327006150588\n",
      "Stochastic Gradient Descent(451/999): loss=0.1038497543490954\n",
      "Stochastic Gradient Descent(452/999): loss=0.10384624699580482\n",
      "Stochastic Gradient Descent(453/999): loss=0.10384274788061122\n",
      "Stochastic Gradient Descent(454/999): loss=0.10383925688443708\n",
      "Stochastic Gradient Descent(455/999): loss=0.10383577389011915\n",
      "Stochastic Gradient Descent(456/999): loss=0.10383229878237753\n",
      "Stochastic Gradient Descent(457/999): loss=0.1038288314477853\n",
      "Stochastic Gradient Descent(458/999): loss=0.10382537177473883\n",
      "Stochastic Gradient Descent(459/999): loss=0.10382191965342807\n",
      "Stochastic Gradient Descent(460/999): loss=0.10381847497580807\n",
      "Stochastic Gradient Descent(461/999): loss=0.10381503763557015\n",
      "Stochastic Gradient Descent(462/999): loss=0.10381160752811425\n",
      "Stochastic Gradient Descent(463/999): loss=0.1038081845505212\n",
      "Stochastic Gradient Descent(464/999): loss=0.10380476860152568\n",
      "Stochastic Gradient Descent(465/999): loss=0.10380135958148963\n",
      "Stochastic Gradient Descent(466/999): loss=0.10379795739237596\n",
      "Stochastic Gradient Descent(467/999): loss=0.10379456193772281\n",
      "Stochastic Gradient Descent(468/999): loss=0.10379117312261814\n",
      "Stochastic Gradient Descent(469/999): loss=0.10378779085367479\n",
      "Stochastic Gradient Descent(470/999): loss=0.10378441503900593\n",
      "Stochastic Gradient Descent(471/999): loss=0.10378104558820073\n",
      "Stochastic Gradient Descent(472/999): loss=0.10377768241230079\n",
      "Stochastic Gradient Descent(473/999): loss=0.10377432542377668\n",
      "Stochastic Gradient Descent(474/999): loss=0.10377097453650479\n",
      "Stochastic Gradient Descent(475/999): loss=0.1037676296657449\n",
      "Stochastic Gradient Descent(476/999): loss=0.1037642907281177\n",
      "Stochastic Gradient Descent(477/999): loss=0.10376095764158291\n",
      "Stochastic Gradient Descent(478/999): loss=0.10375763032541777\n",
      "Stochastic Gradient Descent(479/999): loss=0.1037543087001958\n",
      "Stochastic Gradient Descent(480/999): loss=0.10375099268776582\n",
      "Stochastic Gradient Descent(481/999): loss=0.10374768221123155\n",
      "Stochastic Gradient Descent(482/999): loss=0.10374437719493126\n",
      "Stochastic Gradient Descent(483/999): loss=0.10374107756441804\n",
      "Stochastic Gradient Descent(484/999): loss=0.10373778324643995\n",
      "Stochastic Gradient Descent(485/999): loss=0.10373449416892105\n",
      "Stochastic Gradient Descent(486/999): loss=0.10373121026094234\n",
      "Stochastic Gradient Descent(487/999): loss=0.10372793145272312\n",
      "Stochastic Gradient Descent(488/999): loss=0.10372465767560266\n",
      "Stochastic Gradient Descent(489/999): loss=0.10372138886202216\n",
      "Stochastic Gradient Descent(490/999): loss=0.10371812494550696\n",
      "Stochastic Gradient Descent(491/999): loss=0.10371486586064915\n",
      "Stochastic Gradient Descent(492/999): loss=0.10371161154309028\n",
      "Stochastic Gradient Descent(493/999): loss=0.10370836192950456\n",
      "Stochastic Gradient Descent(494/999): loss=0.10370511695758214\n",
      "Stochastic Gradient Descent(495/999): loss=0.1037018765660128\n",
      "Stochastic Gradient Descent(496/999): loss=0.10369864069446982\n",
      "Stochastic Gradient Descent(497/999): loss=0.1036954092835941\n",
      "Stochastic Gradient Descent(498/999): loss=0.10369218227497866\n",
      "Stochastic Gradient Descent(499/999): loss=0.10368895961115315\n",
      "Stochastic Gradient Descent(500/999): loss=0.10368574123556902\n",
      "Stochastic Gradient Descent(501/999): loss=0.10368252709258434\n",
      "Stochastic Gradient Descent(502/999): loss=0.1036793171274495\n",
      "Stochastic Gradient Descent(503/999): loss=0.10367611128629256\n",
      "Stochastic Gradient Descent(504/999): loss=0.10367290951610533\n",
      "Stochastic Gradient Descent(505/999): loss=0.10366971176472937\n",
      "Stochastic Gradient Descent(506/999): loss=0.10366651798084228\n",
      "Stochastic Gradient Descent(507/999): loss=0.10366332811394426\n",
      "Stochastic Gradient Descent(508/999): loss=0.10366014211434481\n",
      "Stochastic Gradient Descent(509/999): loss=0.10365695993314976\n",
      "Stochastic Gradient Descent(510/999): loss=0.10365378152224843\n",
      "Stochastic Gradient Descent(511/999): loss=0.10365060683430094\n",
      "Stochastic Gradient Descent(512/999): loss=0.10364743582272591\n",
      "Stochastic Gradient Descent(513/999): loss=0.1036442684416881\n",
      "Stochastic Gradient Descent(514/999): loss=0.10364110464608647\n",
      "Stochastic Gradient Descent(515/999): loss=0.10363794439154239\n",
      "Stochastic Gradient Descent(516/999): loss=0.10363478763438794\n",
      "Stochastic Gradient Descent(517/999): loss=0.10363163433165443\n",
      "Stochastic Gradient Descent(518/999): loss=0.10362848444106121\n",
      "Stochastic Gradient Descent(519/999): loss=0.10362533792100452\n",
      "Stochastic Gradient Descent(520/999): loss=0.10362219473054674\n",
      "Stochastic Gradient Descent(521/999): loss=0.10361905482940538\n",
      "Stochastic Gradient Descent(522/999): loss=0.1036159181779428\n",
      "Stochastic Gradient Descent(523/999): loss=0.10361278473715563\n",
      "Stochastic Gradient Descent(524/999): loss=0.10360965446866466\n",
      "Stochastic Gradient Descent(525/999): loss=0.10360652733470478\n",
      "Stochastic Gradient Descent(526/999): loss=0.103603403298115\n",
      "Stochastic Gradient Descent(527/999): loss=0.10360028232232885\n",
      "Stochastic Gradient Descent(528/999): loss=0.1035971643713647\n",
      "Stochastic Gradient Descent(529/999): loss=0.10359404940981627\n",
      "Stochastic Gradient Descent(530/999): loss=0.10359093740284363\n",
      "Stochastic Gradient Descent(531/999): loss=0.10358782831616385\n",
      "Stochastic Gradient Descent(532/999): loss=0.10358472211604199\n",
      "Stochastic Gradient Descent(533/999): loss=0.10358161876928247\n",
      "Stochastic Gradient Descent(534/999): loss=0.10357851824322019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(535/999): loss=0.10357542050571221\n",
      "Stochastic Gradient Descent(536/999): loss=0.10357232552512903\n",
      "Stochastic Gradient Descent(537/999): loss=0.10356923327034662\n",
      "Stochastic Gradient Descent(538/999): loss=0.10356614371073804\n",
      "Stochastic Gradient Descent(539/999): loss=0.10356305681616562\n",
      "Stochastic Gradient Descent(540/999): loss=0.10355997255697294\n",
      "Stochastic Gradient Descent(541/999): loss=0.10355689090397723\n",
      "Stochastic Gradient Descent(542/999): loss=0.1035538118284615\n",
      "Stochastic Gradient Descent(543/999): loss=0.10355073530216724\n",
      "Stochastic Gradient Descent(544/999): loss=0.10354766129728701\n",
      "Stochastic Gradient Descent(545/999): loss=0.10354458978645709\n",
      "Stochastic Gradient Descent(546/999): loss=0.10354152074275043\n",
      "Stochastic Gradient Descent(547/999): loss=0.10353845413966953\n",
      "Stochastic Gradient Descent(548/999): loss=0.1035353899511395\n",
      "Stochastic Gradient Descent(549/999): loss=0.10353232815150147\n",
      "Stochastic Gradient Descent(550/999): loss=0.10352926871550554\n",
      "Stochastic Gradient Descent(551/999): loss=0.10352621161830447\n",
      "Stochastic Gradient Descent(552/999): loss=0.10352315683544715\n",
      "Stochastic Gradient Descent(553/999): loss=0.10352010434287207\n",
      "Stochastic Gradient Descent(554/999): loss=0.1035170541169012\n",
      "Stochastic Gradient Descent(555/999): loss=0.10351400613423373\n",
      "Stochastic Gradient Descent(556/999): loss=0.10351096037194009\n",
      "Stochastic Gradient Descent(557/999): loss=0.10350791680745583\n",
      "Stochastic Gradient Descent(558/999): loss=0.1035048754185758\n",
      "Stochastic Gradient Descent(559/999): loss=0.10350183618344846\n",
      "Stochastic Gradient Descent(560/999): loss=0.10349879908057001\n",
      "Stochastic Gradient Descent(561/999): loss=0.10349576408877888\n",
      "Stochastic Gradient Descent(562/999): loss=0.10349273118725028\n",
      "Stochastic Gradient Descent(563/999): loss=0.1034897003554906\n",
      "Stochastic Gradient Descent(564/999): loss=0.1034866715733324\n",
      "Stochastic Gradient Descent(565/999): loss=0.10348364482092864\n",
      "Stochastic Gradient Descent(566/999): loss=0.10348062007874805\n",
      "Stochastic Gradient Descent(567/999): loss=0.10347759732756967\n",
      "Stochastic Gradient Descent(568/999): loss=0.1034745765484781\n",
      "Stochastic Gradient Descent(569/999): loss=0.10347155772285838\n",
      "Stochastic Gradient Descent(570/999): loss=0.10346854083239133\n",
      "Stochastic Gradient Descent(571/999): loss=0.1034655258590486\n",
      "Stochastic Gradient Descent(572/999): loss=0.1034625127850882\n",
      "Stochastic Gradient Descent(573/999): loss=0.1034595015930497\n",
      "Stochastic Gradient Descent(574/999): loss=0.10345649226574971\n",
      "Stochastic Gradient Descent(575/999): loss=0.10345348478627764\n",
      "Stochastic Gradient Descent(576/999): loss=0.103450479137991\n",
      "Stochastic Gradient Descent(577/999): loss=0.10344747530451123\n",
      "Stochastic Gradient Descent(578/999): loss=0.10344447326971948\n",
      "Stochastic Gradient Descent(579/999): loss=0.10344147301775243\n",
      "Stochastic Gradient Descent(580/999): loss=0.10343847453299806\n",
      "Stochastic Gradient Descent(581/999): loss=0.10343547780009163\n",
      "Stochastic Gradient Descent(582/999): loss=0.10343248280391197\n",
      "Stochastic Gradient Descent(583/999): loss=0.10342948952957698\n",
      "Stochastic Gradient Descent(584/999): loss=0.10342649796244044\n",
      "Stochastic Gradient Descent(585/999): loss=0.10342350808808776\n",
      "Stochastic Gradient Descent(586/999): loss=0.10342051989233236\n",
      "Stochastic Gradient Descent(587/999): loss=0.10341753336121211\n",
      "Stochastic Gradient Descent(588/999): loss=0.10341454848098554\n",
      "Stochastic Gradient Descent(589/999): loss=0.1034115652381285\n",
      "Stochastic Gradient Descent(590/999): loss=0.10340858361933045\n",
      "Stochastic Gradient Descent(591/999): loss=0.10340560361149106\n",
      "Stochastic Gradient Descent(592/999): loss=0.103402625201717\n",
      "Stochastic Gradient Descent(593/999): loss=0.10339964837731841\n",
      "Stochastic Gradient Descent(594/999): loss=0.10339667312580567\n",
      "Stochastic Gradient Descent(595/999): loss=0.1033936994348862\n",
      "Stochastic Gradient Descent(596/999): loss=0.10339072729246139\n",
      "Stochastic Gradient Descent(597/999): loss=0.10338775668662314\n",
      "Stochastic Gradient Descent(598/999): loss=0.10338478760565135\n",
      "Stochastic Gradient Descent(599/999): loss=0.10338182003801022\n",
      "Stochastic Gradient Descent(600/999): loss=0.10337885397234582\n",
      "Stochastic Gradient Descent(601/999): loss=0.10337588939748293\n",
      "Stochastic Gradient Descent(602/999): loss=0.10337292630242226\n",
      "Stochastic Gradient Descent(603/999): loss=0.1033699646763374\n",
      "Stochastic Gradient Descent(604/999): loss=0.1033670045085723\n",
      "Stochastic Gradient Descent(605/999): loss=0.10336404578863843\n",
      "Stochastic Gradient Descent(606/999): loss=0.10336108850621203\n",
      "Stochastic Gradient Descent(607/999): loss=0.1033581326511315\n",
      "Stochastic Gradient Descent(608/999): loss=0.1033551782133948\n",
      "Stochastic Gradient Descent(609/999): loss=0.10335222518315682\n",
      "Stochastic Gradient Descent(610/999): loss=0.10334927355072702\n",
      "Stochastic Gradient Descent(611/999): loss=0.10334632330656664\n",
      "Stochastic Gradient Descent(612/999): loss=0.10334337444128658\n",
      "Stochastic Gradient Descent(613/999): loss=0.10334042694564478\n",
      "Stochastic Gradient Descent(614/999): loss=0.10333748081054395\n",
      "Stochastic Gradient Descent(615/999): loss=0.1033345360270292\n",
      "Stochastic Gradient Descent(616/999): loss=0.10333159258628571\n",
      "Stochastic Gradient Descent(617/999): loss=0.10332865047963659\n",
      "Stochastic Gradient Descent(618/999): loss=0.10332570969854055\n",
      "Stochastic Gradient Descent(619/999): loss=0.10332277023458976\n",
      "Stochastic Gradient Descent(620/999): loss=0.10331983207950765\n",
      "Stochastic Gradient Descent(621/999): loss=0.10331689522514695\n",
      "Stochastic Gradient Descent(622/999): loss=0.10331395966348737\n",
      "Stochastic Gradient Descent(623/999): loss=0.1033110253866338\n",
      "Stochastic Gradient Descent(624/999): loss=0.1033080923868141\n",
      "Stochastic Gradient Descent(625/999): loss=0.10330516065637728\n",
      "Stochastic Gradient Descent(626/999): loss=0.10330223018779136\n",
      "Stochastic Gradient Descent(627/999): loss=0.10329930097364165\n",
      "Stochastic Gradient Descent(628/999): loss=0.10329637300662878\n",
      "Stochastic Gradient Descent(629/999): loss=0.10329344627956674\n",
      "Stochastic Gradient Descent(630/999): loss=0.10329052078538126\n",
      "Stochastic Gradient Descent(631/999): loss=0.10328759651710792\n",
      "Stochastic Gradient Descent(632/999): loss=0.1032846734678902\n",
      "Stochastic Gradient Descent(633/999): loss=0.10328175163097815\n",
      "Stochastic Gradient Descent(634/999): loss=0.10327883099972622\n",
      "Stochastic Gradient Descent(635/999): loss=0.1032759115675919\n",
      "Stochastic Gradient Descent(636/999): loss=0.10327299332813392\n",
      "Stochastic Gradient Descent(637/999): loss=0.10327007627501067\n",
      "Stochastic Gradient Descent(638/999): loss=0.10326716040197863\n",
      "Stochastic Gradient Descent(639/999): loss=0.10326424570289061\n",
      "Stochastic Gradient Descent(640/999): loss=0.10326133217169448\n",
      "Stochastic Gradient Descent(641/999): loss=0.10325841980243136\n",
      "Stochastic Gradient Descent(642/999): loss=0.10325550858923435\n",
      "Stochastic Gradient Descent(643/999): loss=0.10325259852632689\n",
      "Stochastic Gradient Descent(644/999): loss=0.10324968960802135\n",
      "Stochastic Gradient Descent(645/999): loss=0.10324678182871767\n",
      "Stochastic Gradient Descent(646/999): loss=0.10324387518290186\n",
      "Stochastic Gradient Descent(647/999): loss=0.10324096966514473\n",
      "Stochastic Gradient Descent(648/999): loss=0.10323806527010027\n",
      "Stochastic Gradient Descent(649/999): loss=0.10323516199250461\n",
      "Stochastic Gradient Descent(650/999): loss=0.10323225982717456\n",
      "Stochastic Gradient Descent(651/999): loss=0.10322935876900628\n",
      "Stochastic Gradient Descent(652/999): loss=0.1032264588129741\n",
      "Stochastic Gradient Descent(653/999): loss=0.10322355995412917\n",
      "Stochastic Gradient Descent(654/999): loss=0.1032206621875983\n",
      "Stochastic Gradient Descent(655/999): loss=0.10321776550858268\n",
      "Stochastic Gradient Descent(656/999): loss=0.10321486991235669\n",
      "Stochastic Gradient Descent(657/999): loss=0.10321197539426677\n",
      "Stochastic Gradient Descent(658/999): loss=0.1032090819497302\n",
      "Stochastic Gradient Descent(659/999): loss=0.10320618957423403\n",
      "Stochastic Gradient Descent(660/999): loss=0.1032032982633339\n",
      "Stochastic Gradient Descent(661/999): loss=0.10320040801265294\n",
      "Stochastic Gradient Descent(662/999): loss=0.10319751881788071\n",
      "Stochastic Gradient Descent(663/999): loss=0.10319463067477215\n",
      "Stochastic Gradient Descent(664/999): loss=0.10319174357914641\n",
      "Stochastic Gradient Descent(665/999): loss=0.103188857526886\n",
      "Stochastic Gradient Descent(666/999): loss=0.1031859725139356\n",
      "Stochastic Gradient Descent(667/999): loss=0.10318308853630122\n",
      "Stochastic Gradient Descent(668/999): loss=0.10318020559004905\n",
      "Stochastic Gradient Descent(669/999): loss=0.10317732367130458\n",
      "Stochastic Gradient Descent(670/999): loss=0.10317444277625167\n",
      "Stochastic Gradient Descent(671/999): loss=0.10317156290113152\n",
      "Stochastic Gradient Descent(672/999): loss=0.10316868404224187\n",
      "Stochastic Gradient Descent(673/999): loss=0.10316580619593599\n",
      "Stochastic Gradient Descent(674/999): loss=0.10316292935862184\n",
      "Stochastic Gradient Descent(675/999): loss=0.10316005352676114\n",
      "Stochastic Gradient Descent(676/999): loss=0.10315717869686851\n",
      "Stochastic Gradient Descent(677/999): loss=0.10315430486551075\n",
      "Stochastic Gradient Descent(678/999): loss=0.10315143202930582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(679/999): loss=0.1031485601849221\n",
      "Stochastic Gradient Descent(680/999): loss=0.10314568932907754\n",
      "Stochastic Gradient Descent(681/999): loss=0.10314281945853901\n",
      "Stochastic Gradient Descent(682/999): loss=0.10313995057012125\n",
      "Stochastic Gradient Descent(683/999): loss=0.10313708266068634\n",
      "Stochastic Gradient Descent(684/999): loss=0.10313421572714286\n",
      "Stochastic Gradient Descent(685/999): loss=0.10313134976644507\n",
      "Stochastic Gradient Descent(686/999): loss=0.10312848477559224\n",
      "Stochastic Gradient Descent(687/999): loss=0.10312562075162794\n",
      "Stochastic Gradient Descent(688/999): loss=0.10312275769163935\n",
      "Stochastic Gradient Descent(689/999): loss=0.10311989559275649\n",
      "Stochastic Gradient Descent(690/999): loss=0.10311703445215147\n",
      "Stochastic Gradient Descent(691/999): loss=0.103114174267038\n",
      "Stochastic Gradient Descent(692/999): loss=0.10311131503467062\n",
      "Stochastic Gradient Descent(693/999): loss=0.10310845675234404\n",
      "Stochastic Gradient Descent(694/999): loss=0.10310559941739245\n",
      "Stochastic Gradient Descent(695/999): loss=0.103102743027189\n",
      "Stochastic Gradient Descent(696/999): loss=0.103099887579145\n",
      "Stochastic Gradient Descent(697/999): loss=0.10309703307070958\n",
      "Stochastic Gradient Descent(698/999): loss=0.10309417949936883\n",
      "Stochastic Gradient Descent(699/999): loss=0.10309132686264526\n",
      "Stochastic Gradient Descent(700/999): loss=0.10308847515809727\n",
      "Stochastic Gradient Descent(701/999): loss=0.10308562438331861\n",
      "Stochastic Gradient Descent(702/999): loss=0.10308277453593774\n",
      "Stochastic Gradient Descent(703/999): loss=0.10307992561361717\n",
      "Stochastic Gradient Descent(704/999): loss=0.10307707761405316\n",
      "Stochastic Gradient Descent(705/999): loss=0.10307423053497498\n",
      "Stochastic Gradient Descent(706/999): loss=0.1030713843741445\n",
      "Stochastic Gradient Descent(707/999): loss=0.10306853912935557\n",
      "Stochastic Gradient Descent(708/999): loss=0.10306569479843353\n",
      "Stochastic Gradient Descent(709/999): loss=0.10306285137923474\n",
      "Stochastic Gradient Descent(710/999): loss=0.10306000886964611\n",
      "Stochastic Gradient Descent(711/999): loss=0.10305716726758449\n",
      "Stochastic Gradient Descent(712/999): loss=0.10305432657099631\n",
      "Stochastic Gradient Descent(713/999): loss=0.10305148677785703\n",
      "Stochastic Gradient Descent(714/999): loss=0.10304864788617071\n",
      "Stochastic Gradient Descent(715/999): loss=0.10304580989396954\n",
      "Stochastic Gradient Descent(716/999): loss=0.10304297279931339\n",
      "Stochastic Gradient Descent(717/999): loss=0.10304013660028935\n",
      "Stochastic Gradient Descent(718/999): loss=0.10303730129501132\n",
      "Stochastic Gradient Descent(719/999): loss=0.10303446688161962\n",
      "Stochastic Gradient Descent(720/999): loss=0.10303163335828042\n",
      "Stochastic Gradient Descent(721/999): loss=0.10302880072318543\n",
      "Stochastic Gradient Descent(722/999): loss=0.10302596897455152\n",
      "Stochastic Gradient Descent(723/999): loss=0.10302313811062029\n",
      "Stochastic Gradient Descent(724/999): loss=0.10302030812965755\n",
      "Stochastic Gradient Descent(725/999): loss=0.10301747902995317\n",
      "Stochastic Gradient Descent(726/999): loss=0.10301465080982049\n",
      "Stochastic Gradient Descent(727/999): loss=0.10301182346759598\n",
      "Stochastic Gradient Descent(728/999): loss=0.10300899700163896\n",
      "Stochastic Gradient Descent(729/999): loss=0.10300617141033121\n",
      "Stochastic Gradient Descent(730/999): loss=0.1030033466920765\n",
      "Stochastic Gradient Descent(731/999): loss=0.1030005228453003\n",
      "Stochastic Gradient Descent(732/999): loss=0.10299769986844952\n",
      "Stochastic Gradient Descent(733/999): loss=0.10299487775999201\n",
      "Stochastic Gradient Descent(734/999): loss=0.10299205651841645\n",
      "Stochastic Gradient Descent(735/999): loss=0.10298923614223167\n",
      "Stochastic Gradient Descent(736/999): loss=0.10298641662996666\n",
      "Stochastic Gradient Descent(737/999): loss=0.10298359798017004\n",
      "Stochastic Gradient Descent(738/999): loss=0.10298078019140992\n",
      "Stochastic Gradient Descent(739/999): loss=0.10297796326227339\n",
      "Stochastic Gradient Descent(740/999): loss=0.10297514719136636\n",
      "Stochastic Gradient Descent(741/999): loss=0.10297233197731319\n",
      "Stochastic Gradient Descent(742/999): loss=0.10296951761875635\n",
      "Stochastic Gradient Descent(743/999): loss=0.10296670411435636\n",
      "Stochastic Gradient Descent(744/999): loss=0.10296389146279118\n",
      "Stochastic Gradient Descent(745/999): loss=0.10296107966275614\n",
      "Stochastic Gradient Descent(746/999): loss=0.1029582687129636\n",
      "Stochastic Gradient Descent(747/999): loss=0.1029554586121427\n",
      "Stochastic Gradient Descent(748/999): loss=0.10295264935903908\n",
      "Stochastic Gradient Descent(749/999): loss=0.10294984095241451\n",
      "Stochastic Gradient Descent(750/999): loss=0.10294703339104687\n",
      "Stochastic Gradient Descent(751/999): loss=0.10294422667372974\n",
      "Stochastic Gradient Descent(752/999): loss=0.10294142079927197\n",
      "Stochastic Gradient Descent(753/999): loss=0.1029386157664979\n",
      "Stochastic Gradient Descent(754/999): loss=0.10293581157424664\n",
      "Stochastic Gradient Descent(755/999): loss=0.10293300822137211\n",
      "Stochastic Gradient Descent(756/999): loss=0.10293020570674272\n",
      "Stochastic Gradient Descent(757/999): loss=0.10292740402924115\n",
      "Stochastic Gradient Descent(758/999): loss=0.10292460318776403\n",
      "Stochastic Gradient Descent(759/999): loss=0.10292180318122195\n",
      "Stochastic Gradient Descent(760/999): loss=0.10291900400853898\n",
      "Stochastic Gradient Descent(761/999): loss=0.10291620566865256\n",
      "Stochastic Gradient Descent(762/999): loss=0.10291340816051331\n",
      "Stochastic Gradient Descent(763/999): loss=0.10291061148308482\n",
      "Stochastic Gradient Descent(764/999): loss=0.1029078156353434\n",
      "Stochastic Gradient Descent(765/999): loss=0.10290502061627782\n",
      "Stochastic Gradient Descent(766/999): loss=0.10290222642488939\n",
      "Stochastic Gradient Descent(767/999): loss=0.10289943306019131\n",
      "Stochastic Gradient Descent(768/999): loss=0.10289664052120884\n",
      "Stochastic Gradient Descent(769/999): loss=0.102893848806979\n",
      "Stochastic Gradient Descent(770/999): loss=0.10289105791655037\n",
      "Stochastic Gradient Descent(771/999): loss=0.1028882678489829\n",
      "Stochastic Gradient Descent(772/999): loss=0.10288547860334772\n",
      "Stochastic Gradient Descent(773/999): loss=0.10288269017872696\n",
      "Stochastic Gradient Descent(774/999): loss=0.10287990257421369\n",
      "Stochastic Gradient Descent(775/999): loss=0.1028771157889115\n",
      "Stochastic Gradient Descent(776/999): loss=0.10287432982193467\n",
      "Stochastic Gradient Descent(777/999): loss=0.10287154467240756\n",
      "Stochastic Gradient Descent(778/999): loss=0.10286876033946489\n",
      "Stochastic Gradient Descent(779/999): loss=0.10286597682225135\n",
      "Stochastic Gradient Descent(780/999): loss=0.10286319411992143\n",
      "Stochastic Gradient Descent(781/999): loss=0.10286041223163928\n",
      "Stochastic Gradient Descent(782/999): loss=0.10285763115657867\n",
      "Stochastic Gradient Descent(783/999): loss=0.10285485089392268\n",
      "Stochastic Gradient Descent(784/999): loss=0.10285207144286365\n",
      "Stochastic Gradient Descent(785/999): loss=0.10284929280260295\n",
      "Stochastic Gradient Descent(786/999): loss=0.10284651497235098\n",
      "Stochastic Gradient Descent(787/999): loss=0.10284373795132686\n",
      "Stochastic Gradient Descent(788/999): loss=0.10284096173875837\n",
      "Stochastic Gradient Descent(789/999): loss=0.10283818633388181\n",
      "Stochastic Gradient Descent(790/999): loss=0.10283541173594186\n",
      "Stochastic Gradient Descent(791/999): loss=0.1028326379441915\n",
      "Stochastic Gradient Descent(792/999): loss=0.10282986495789173\n",
      "Stochastic Gradient Descent(793/999): loss=0.10282709277631161\n",
      "Stochastic Gradient Descent(794/999): loss=0.10282432139872803\n",
      "Stochastic Gradient Descent(795/999): loss=0.10282155082442555\n",
      "Stochastic Gradient Descent(796/999): loss=0.1028187810526965\n",
      "Stochastic Gradient Descent(797/999): loss=0.10281601208284051\n",
      "Stochastic Gradient Descent(798/999): loss=0.10281324391416471\n",
      "Stochastic Gradient Descent(799/999): loss=0.10281047654598345\n",
      "Stochastic Gradient Descent(800/999): loss=0.10280770997761815\n",
      "Stochastic Gradient Descent(801/999): loss=0.1028049442083974\n",
      "Stochastic Gradient Descent(802/999): loss=0.1028021792376565\n",
      "Stochastic Gradient Descent(803/999): loss=0.10279941506473769\n",
      "Stochastic Gradient Descent(804/999): loss=0.10279665168898992\n",
      "Stochastic Gradient Descent(805/999): loss=0.10279388910976857\n",
      "Stochastic Gradient Descent(806/999): loss=0.10279112732643572\n",
      "Stochastic Gradient Descent(807/999): loss=0.10278836633835964\n",
      "Stochastic Gradient Descent(808/999): loss=0.10278560614491496\n",
      "Stochastic Gradient Descent(809/999): loss=0.10278284674548249\n",
      "Stochastic Gradient Descent(810/999): loss=0.10278008813944906\n",
      "Stochastic Gradient Descent(811/999): loss=0.1027773303262076\n",
      "Stochastic Gradient Descent(812/999): loss=0.1027745733051568\n",
      "Stochastic Gradient Descent(813/999): loss=0.10277181707570118\n",
      "Stochastic Gradient Descent(814/999): loss=0.10276906163725101\n",
      "Stochastic Gradient Descent(815/999): loss=0.10276630698922211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(816/999): loss=0.10276355313103587\n",
      "Stochastic Gradient Descent(817/999): loss=0.1027608000621191\n",
      "Stochastic Gradient Descent(818/999): loss=0.10275804778190392\n",
      "Stochastic Gradient Descent(819/999): loss=0.10275529628982775\n",
      "Stochastic Gradient Descent(820/999): loss=0.10275254558533321\n",
      "Stochastic Gradient Descent(821/999): loss=0.10274979566786797\n",
      "Stochastic Gradient Descent(822/999): loss=0.10274704653688473\n",
      "Stochastic Gradient Descent(823/999): loss=0.10274429819184114\n",
      "Stochastic Gradient Descent(824/999): loss=0.10274155063219968\n",
      "Stochastic Gradient Descent(825/999): loss=0.10273880385742767\n",
      "Stochastic Gradient Descent(826/999): loss=0.10273605786699706\n",
      "Stochastic Gradient Descent(827/999): loss=0.10273331266038455\n",
      "Stochastic Gradient Descent(828/999): loss=0.1027305682370712\n",
      "Stochastic Gradient Descent(829/999): loss=0.10272782459654278\n",
      "Stochastic Gradient Descent(830/999): loss=0.10272508173828937\n",
      "Stochastic Gradient Descent(831/999): loss=0.10272233966180531\n",
      "Stochastic Gradient Descent(832/999): loss=0.10271959836658938\n",
      "Stochastic Gradient Descent(833/999): loss=0.10271685785214447\n",
      "Stochastic Gradient Descent(834/999): loss=0.10271411811797766\n",
      "Stochastic Gradient Descent(835/999): loss=0.10271137916360003\n",
      "Stochastic Gradient Descent(836/999): loss=0.10270864098852678\n",
      "Stochastic Gradient Descent(837/999): loss=0.10270590359227698\n",
      "Stochastic Gradient Descent(838/999): loss=0.1027031669743736\n",
      "Stochastic Gradient Descent(839/999): loss=0.10270043113434349\n",
      "Stochastic Gradient Descent(840/999): loss=0.1026976960717172\n",
      "Stochastic Gradient Descent(841/999): loss=0.10269496178602902\n",
      "Stochastic Gradient Descent(842/999): loss=0.10269222827681691\n",
      "Stochastic Gradient Descent(843/999): loss=0.10268949554362233\n",
      "Stochastic Gradient Descent(844/999): loss=0.10268676358599037\n",
      "Stochastic Gradient Descent(845/999): loss=0.10268403240346959\n",
      "Stochastic Gradient Descent(846/999): loss=0.10268130199561187\n",
      "Stochastic Gradient Descent(847/999): loss=0.10267857236197261\n",
      "Stochastic Gradient Descent(848/999): loss=0.10267584350211041\n",
      "Stochastic Gradient Descent(849/999): loss=0.10267311541558719\n",
      "Stochastic Gradient Descent(850/999): loss=0.1026703881019681\n",
      "Stochastic Gradient Descent(851/999): loss=0.10266766156082142\n",
      "Stochastic Gradient Descent(852/999): loss=0.10266493579171851\n",
      "Stochastic Gradient Descent(853/999): loss=0.10266221079423385\n",
      "Stochastic Gradient Descent(854/999): loss=0.10265948656794494\n",
      "Stochastic Gradient Descent(855/999): loss=0.10265676311243219\n",
      "Stochastic Gradient Descent(856/999): loss=0.102654040427279\n",
      "Stochastic Gradient Descent(857/999): loss=0.10265131851207163\n",
      "Stochastic Gradient Descent(858/999): loss=0.10264859736639914\n",
      "Stochastic Gradient Descent(859/999): loss=0.10264587698985339\n",
      "Stochastic Gradient Descent(860/999): loss=0.10264315738202905\n",
      "Stochastic Gradient Descent(861/999): loss=0.10264043854252332\n",
      "Stochastic Gradient Descent(862/999): loss=0.10263772047093626\n",
      "Stochastic Gradient Descent(863/999): loss=0.10263500316687046\n",
      "Stochastic Gradient Descent(864/999): loss=0.10263228662993104\n",
      "Stochastic Gradient Descent(865/999): loss=0.10262957085972571\n",
      "Stochastic Gradient Descent(866/999): loss=0.10262685585586472\n",
      "Stochastic Gradient Descent(867/999): loss=0.10262414161796068\n",
      "Stochastic Gradient Descent(868/999): loss=0.10262142814562865\n",
      "Stochastic Gradient Descent(869/999): loss=0.10261871543848618\n",
      "Stochastic Gradient Descent(870/999): loss=0.10261600349615295\n",
      "Stochastic Gradient Descent(871/999): loss=0.10261329231825114\n",
      "Stochastic Gradient Descent(872/999): loss=0.10261058190440511\n",
      "Stochastic Gradient Descent(873/999): loss=0.10260787225424152\n",
      "Stochastic Gradient Descent(874/999): loss=0.10260516336738915\n",
      "Stochastic Gradient Descent(875/999): loss=0.10260245524347902\n",
      "Stochastic Gradient Descent(876/999): loss=0.10259974788214424\n",
      "Stochastic Gradient Descent(877/999): loss=0.10259704128302005\n",
      "Stochastic Gradient Descent(878/999): loss=0.10259433544574369\n",
      "Stochastic Gradient Descent(879/999): loss=0.10259163036995457\n",
      "Stochastic Gradient Descent(880/999): loss=0.10258892605529399\n",
      "Stochastic Gradient Descent(881/999): loss=0.10258622250140531\n",
      "Stochastic Gradient Descent(882/999): loss=0.10258351970793371\n",
      "Stochastic Gradient Descent(883/999): loss=0.10258081767452643\n",
      "Stochastic Gradient Descent(884/999): loss=0.10257811640083249\n",
      "Stochastic Gradient Descent(885/999): loss=0.10257541588650278\n",
      "Stochastic Gradient Descent(886/999): loss=0.10257271613119008\n",
      "Stochastic Gradient Descent(887/999): loss=0.10257001713454891\n",
      "Stochastic Gradient Descent(888/999): loss=0.10256731889623558\n",
      "Stochastic Gradient Descent(889/999): loss=0.10256462141590816\n",
      "Stochastic Gradient Descent(890/999): loss=0.10256192469322635\n",
      "Stochastic Gradient Descent(891/999): loss=0.10255922872785171\n",
      "Stochastic Gradient Descent(892/999): loss=0.10255653351944724\n",
      "Stochastic Gradient Descent(893/999): loss=0.10255383906767782\n",
      "Stochastic Gradient Descent(894/999): loss=0.10255114537220976\n",
      "Stochastic Gradient Descent(895/999): loss=0.10254845243271103\n",
      "Stochastic Gradient Descent(896/999): loss=0.1025457602488512\n",
      "Stochastic Gradient Descent(897/999): loss=0.10254306882030126\n",
      "Stochastic Gradient Descent(898/999): loss=0.10254037814673388\n",
      "Stochastic Gradient Descent(899/999): loss=0.1025376882278231\n",
      "Stochastic Gradient Descent(900/999): loss=0.10253499906324451\n",
      "Stochastic Gradient Descent(901/999): loss=0.10253231065267504\n",
      "Stochastic Gradient Descent(902/999): loss=0.10252962299579323\n",
      "Stochastic Gradient Descent(903/999): loss=0.10252693609227881\n",
      "Stochastic Gradient Descent(904/999): loss=0.10252424994181304\n",
      "Stochastic Gradient Descent(905/999): loss=0.10252156454407856\n",
      "Stochastic Gradient Descent(906/999): loss=0.10251887989875921\n",
      "Stochastic Gradient Descent(907/999): loss=0.10251619600554025\n",
      "Stochastic Gradient Descent(908/999): loss=0.1025135128641083\n",
      "Stochastic Gradient Descent(909/999): loss=0.10251083047415113\n",
      "Stochastic Gradient Descent(910/999): loss=0.10250814883535783\n",
      "Stochastic Gradient Descent(911/999): loss=0.10250546794741877\n",
      "Stochastic Gradient Descent(912/999): loss=0.10250278781002549\n",
      "Stochastic Gradient Descent(913/999): loss=0.1025001084228708\n",
      "Stochastic Gradient Descent(914/999): loss=0.10249742978564862\n",
      "Stochastic Gradient Descent(915/999): loss=0.10249475189805407\n",
      "Stochastic Gradient Descent(916/999): loss=0.10249207475978347\n",
      "Stochastic Gradient Descent(917/999): loss=0.10248939837053421\n",
      "Stochastic Gradient Descent(918/999): loss=0.10248672273000485\n",
      "Stochastic Gradient Descent(919/999): loss=0.10248404783789501\n",
      "Stochastic Gradient Descent(920/999): loss=0.10248137369390543\n",
      "Stochastic Gradient Descent(921/999): loss=0.10247870029773791\n",
      "Stochastic Gradient Descent(922/999): loss=0.10247602764909529\n",
      "Stochastic Gradient Descent(923/999): loss=0.10247335574768143\n",
      "Stochastic Gradient Descent(924/999): loss=0.1024706845932013\n",
      "Stochastic Gradient Descent(925/999): loss=0.10246801418536075\n",
      "Stochastic Gradient Descent(926/999): loss=0.10246534452386669\n",
      "Stochastic Gradient Descent(927/999): loss=0.10246267560842705\n",
      "Stochastic Gradient Descent(928/999): loss=0.10246000743875068\n",
      "Stochastic Gradient Descent(929/999): loss=0.10245734001454733\n",
      "Stochastic Gradient Descent(930/999): loss=0.1024546733355278\n",
      "Stochastic Gradient Descent(931/999): loss=0.10245200740140367\n",
      "Stochastic Gradient Descent(932/999): loss=0.1024493422118875\n",
      "Stochastic Gradient Descent(933/999): loss=0.10244667776669285\n",
      "Stochastic Gradient Descent(934/999): loss=0.1024440140655339\n",
      "Stochastic Gradient Descent(935/999): loss=0.10244135110812594\n",
      "Stochastic Gradient Descent(936/999): loss=0.10243868889418506\n",
      "Stochastic Gradient Descent(937/999): loss=0.10243602742342811\n",
      "Stochastic Gradient Descent(938/999): loss=0.10243336669557282\n",
      "Stochastic Gradient Descent(939/999): loss=0.10243070671033776\n",
      "Stochastic Gradient Descent(940/999): loss=0.10242804746744227\n",
      "Stochastic Gradient Descent(941/999): loss=0.10242538896660652\n",
      "Stochastic Gradient Descent(942/999): loss=0.10242273120755144\n",
      "Stochastic Gradient Descent(943/999): loss=0.10242007418999875\n",
      "Stochastic Gradient Descent(944/999): loss=0.10241741791367089\n",
      "Stochastic Gradient Descent(945/999): loss=0.1024147623782911\n",
      "Stochastic Gradient Descent(946/999): loss=0.10241210758358331\n",
      "Stochastic Gradient Descent(947/999): loss=0.10240945352927223\n",
      "Stochastic Gradient Descent(948/999): loss=0.10240680021508326\n",
      "Stochastic Gradient Descent(949/999): loss=0.10240414764074249\n",
      "Stochastic Gradient Descent(950/999): loss=0.10240149580597674\n",
      "Stochastic Gradient Descent(951/999): loss=0.10239884471051348\n",
      "Stochastic Gradient Descent(952/999): loss=0.10239619435408095\n",
      "Stochastic Gradient Descent(953/999): loss=0.10239354473640792\n",
      "Stochastic Gradient Descent(954/999): loss=0.10239089585722393\n",
      "Stochastic Gradient Descent(955/999): loss=0.10238824771625912\n",
      "Stochastic Gradient Descent(956/999): loss=0.10238560031324428\n",
      "Stochastic Gradient Descent(957/999): loss=0.10238295364791083\n",
      "Stochastic Gradient Descent(958/999): loss=0.10238030771999077\n",
      "Stochastic Gradient Descent(959/999): loss=0.10237766252921686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(960/999): loss=0.10237501807532223\n",
      "Stochastic Gradient Descent(961/999): loss=0.1023723743580408\n",
      "Stochastic Gradient Descent(962/999): loss=0.10236973137710705\n",
      "Stochastic Gradient Descent(963/999): loss=0.10236708913225595\n",
      "Stochastic Gradient Descent(964/999): loss=0.10236444762322307\n",
      "Stochastic Gradient Descent(965/999): loss=0.10236180684974464\n",
      "Stochastic Gradient Descent(966/999): loss=0.10235916681155734\n",
      "Stochastic Gradient Descent(967/999): loss=0.10235652750839842\n",
      "Stochastic Gradient Descent(968/999): loss=0.10235388894000569\n",
      "Stochastic Gradient Descent(969/999): loss=0.10235125110611752\n",
      "Stochastic Gradient Descent(970/999): loss=0.10234861400647272\n",
      "Stochastic Gradient Descent(971/999): loss=0.10234597764081067\n",
      "Stochastic Gradient Descent(972/999): loss=0.10234334200887132\n",
      "Stochastic Gradient Descent(973/999): loss=0.10234070711039503\n",
      "Stochastic Gradient Descent(974/999): loss=0.10233807294512269\n",
      "Stochastic Gradient Descent(975/999): loss=0.1023354395127957\n",
      "Stochastic Gradient Descent(976/999): loss=0.10233280681315587\n",
      "Stochastic Gradient Descent(977/999): loss=0.10233017484594563\n",
      "Stochastic Gradient Descent(978/999): loss=0.10232754361090779\n",
      "Stochastic Gradient Descent(979/999): loss=0.10232491310778558\n",
      "Stochastic Gradient Descent(980/999): loss=0.10232228333632275\n",
      "Stochastic Gradient Descent(981/999): loss=0.10231965429626351\n",
      "Stochastic Gradient Descent(982/999): loss=0.10231702598735253\n",
      "Stochastic Gradient Descent(983/999): loss=0.10231439840933487\n",
      "Stochastic Gradient Descent(984/999): loss=0.10231177156195598\n",
      "Stochastic Gradient Descent(985/999): loss=0.1023091454449619\n",
      "Stochastic Gradient Descent(986/999): loss=0.10230652005809895\n",
      "Stochastic Gradient Descent(987/999): loss=0.10230389540111393\n",
      "Stochastic Gradient Descent(988/999): loss=0.10230127147375404\n",
      "Stochastic Gradient Descent(989/999): loss=0.10229864827576685\n",
      "Stochastic Gradient Descent(990/999): loss=0.10229602580690039\n",
      "Stochastic Gradient Descent(991/999): loss=0.10229340406690313\n",
      "Stochastic Gradient Descent(992/999): loss=0.10229078305552372\n",
      "Stochastic Gradient Descent(993/999): loss=0.10228816277251145\n",
      "Stochastic Gradient Descent(994/999): loss=0.10228554321761582\n",
      "Stochastic Gradient Descent(995/999): loss=0.10228292439058681\n",
      "Stochastic Gradient Descent(996/999): loss=0.10228030629117475\n",
      "Stochastic Gradient Descent(997/999): loss=0.10227768891913026\n",
      "Stochastic Gradient Descent(998/999): loss=0.10227507227420438\n",
      "Stochastic Gradient Descent(999/999): loss=0.10227245635614855\n"
     ]
    }
   ],
   "source": [
    "meanacc_dtest = []\n",
    "meanacc_dtrain = []\n",
    "\n",
    "# Execute for data originally with no NaNs\n",
    "dtmp_tr,dtmp_te=cross_validation(Y_no_nans,X_no_nans,k_fold=4,seed=1, function_name='sgd', gamma=0.0000001, max_iters=1000)\n",
    "meanacc_dtest.append(dtmp_te)\n",
    "meanacc_dtrain.append(dtmp_tr)\n",
    "\n",
    "# Execute for data originally with NaNs\n",
    "dtmp_tr,dtmp_te=cross_validation(Y_cleaned,X_cleaned,k_fold=4,seed=1, function_name='sgd', gamma=0.0000001, max_iters=1000)\n",
    "meanacc_dtest.append(dtmp_te)\n",
    "meanacc_dtrain.append(dtmp_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No NaNs error:\n",
      "Test: 0.6257487667371389 Data: 0.6257732362383526\n",
      "\n",
      "Cleaned NaNs error:\n",
      "Test: 0.6936179103164655 Data: 0.6937260378409683\n"
     ]
    }
   ],
   "source": [
    "print(\"No NaNs error:\")\n",
    "print(\"Test:\",meanacc_dtest[0],\"Data:\", meanacc_dtrain[0])\n",
    "print(\"\\nCleaned NaNs error:\", )\n",
    "print(\"Test:\",meanacc_dtest[1],\"Data:\", meanacc_dtrain[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Comment: </b>same result we had with GD. Apparently is not even faster. Might be worth measuring with time()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanacc_dtest = []\n",
    "meanacc_dtrain = []\n",
    "\n",
    "# Execute for data originally with no NaNs\n",
    "dtmp_tr,dtmp_te=cross_validation(Y_no_nans,X_no_nans,k_fold=4,seed=1, function_name='least_squares')\n",
    "meanacc_dtest.append(dtmp_te)\n",
    "meanacc_dtrain.append(dtmp_tr)\n",
    "\n",
    "# Execute for data originally with NaNs\n",
    "dtmp_tr,dtmp_te=cross_validation(Y_cleaned,X_cleaned,k_fold=4,seed=1, function_name='least_squares')\n",
    "meanacc_dtest.append(dtmp_te)\n",
    "meanacc_dtrain.append(dtmp_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No NaNs error:\n",
      "Test: 0.7254668780831572 Data: 0.7259317986062173\n",
      "\n",
      "Cleaned NaNs error:\n",
      "Test: 0.7648281322161378 Data: 0.7650223952262614\n"
     ]
    }
   ],
   "source": [
    "print(\"No NaNs error:\")\n",
    "print(\"Test:\",meanacc_dtest[0],\"Data:\", meanacc_dtrain[0])\n",
    "print(\"\\nCleaned NaNs error:\", )\n",
    "print(\"Test:\",meanacc_dtest[1],\"Data:\", meanacc_dtrain[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<b>Comment "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
