{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build prediction for AICROWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from utils_predictions_manipulation import*\n",
    "from utils_nans_manipulation import*\n",
    "from utils_data_loading import*\n",
    "from utils_features_manipulation import*\n",
    "from logistic_regression import*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata,_ = load_data('Data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata,_ = load_data('Data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = structure_data(traindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test,_ = structure_data(testdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace -999 value with np.nan\n",
    "X_nans = replace_bad_data_with_nans(X_train, -999)\n",
    "X_nans_test = replace_bad_data_with_nans(X_test, -999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove columns with more than 50% nans\n",
    "X_clean,del_indexes = replace_nans_with_median(X_nans, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_del = delete_nans_indexes(X_nans_test, del_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace remaining nans with corresponding feature median\n",
    "X_test_clean = replace_test_nans_with_median(X_test_del, X_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data by Jet_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define spliting thresholds for [\"Feature\", \"Value\"]\n",
    "thresh = [[19,0.5],\n",
    "          [19,1.5],\n",
    "          [19,2.5]]\n",
    "\n",
    "#Returns set of all possible combinations of splits definded in thresh\n",
    "#Last set in X_sets, Y_sets contains data split using all thresholds\n",
    "X_sets, Y_sets, thresholds = split_data_set(X_clean, Y_train, thresh)\n",
    "_, Test_ind, _ = split_data_set(X_clean, np.array(range(len(Y_train))), thresh)\n",
    "print(\"Train split sizes:\", [sets.shape[0] for sets in X_sets[-1]])\n",
    "\n",
    "#Returns split data for test. Array containign indices is passed (Train_ind) and later used to re-merge prediction.\n",
    "X_sets_t, Train_ind, thresholds = split_data_set(X_test_clean, np.array(range(X_test_clean.shape[0])), thresh)\n",
    "print(\"Test split sizes:\", [sets.shape[0] for sets in X_sets_t[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating predictions for each split group\n",
    "Data is split by Jet_num and processed. All four groups are processed identically, except for group4 where column 22 is removed (all nans).\n",
    "\n",
    "Each group has a unique polynomial feature expansion. These specific expansions are obtained using test_for_gamma_all/stop functions. Where for each group, and increasing number of polynomials are tested (accounting for adjustments to gamma). See \"Models_testing_split_data_logit.ipnyb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting expansions applied to each feature for each group\n",
    "degree_test = [1, 1/2, 2, 1/3, 3, 1/4, 4, 1/5, 5, 1/6, 6, 1/7]\n",
    "\n",
    "\n",
    "#Transforming Train Data\n",
    "X_pass = X_sets[-1][0] #Select group\n",
    "X_pass = np.delete(X_pass,19,axis=1) #Delete jet_nums column\n",
    "\n",
    "deg_v = [degree_test for i in range(X_pass.shape[1])] #Define polynomials to use\n",
    "ind_v = list(range(1,X_pass.shape[1])) #Define columns to expand\n",
    "\n",
    "X_pass = build_poly_index(X_pass, ind_v, deg_v) #Build expanded array\n",
    "X_pass, mean, std = standardize(X_pass) #Standardize\n",
    "\n",
    "Y_pass = Y_sets[-1][0] #Select output vector\n",
    "\n",
    "#Building model\n",
    "initial_w = np.ones((X_pass.shape[1]))*(-0.01)\n",
    "ws,loss = logistic_regression(Y_pass, X_pass, initial_w, max_iters=10000, gamma=0.2, print_=True)\n",
    "\n",
    "#Transforming Test Data\n",
    "Xt_pass = X_sets_t[-1][0]\n",
    "Xt_pass = np.delete(Xt_pass,19,axis=1)\n",
    "Xt_pass = build_poly_index(Xt_pass, ind_v, deg_v)\n",
    "Xt_pass,_,_ = standardize_test(Xt_pass, mean, std)\n",
    "\n",
    "#Building predictions\n",
    "Y_test = sigmoid(Xt_pass.dot(ws))\n",
    "Y_pred_G1 = probability_to_prediction(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_test = [1, 1/2, 2, 1/3]\n",
    "\n",
    "#Transforming Train Data\n",
    "X_pass = X_sets[-1][1]\n",
    "X_pass = np.delete(X_pass,19,axis=1)\n",
    "\n",
    "deg_v = [degree_test for i in range(X_pass.shape[1])]\n",
    "ind_v = list(range(1,X_pass.shape[1]))\n",
    "\n",
    "X_pass = build_poly_index(X_pass, ind_v, deg_v)\n",
    "X_pass,mean,std = standardize(X_pass)\n",
    "\n",
    "Y_pass = Y_sets[-1][1]\n",
    "\n",
    "#Building model\n",
    "initial_w = np.ones((X_pass.shape[1]))*(-0.01)\n",
    "ws,loss = logistic_regression(Y_pass, X_pass, initial_w, max_iters=10000, gamma=0.5, print_=True)\n",
    "\n",
    "#Transforming Test Data\n",
    "Xt_pass = X_sets_t[-1][1]\n",
    "Xt_pass = np.delete(Xt_pass,19,axis=1)\n",
    "Xt_pass = build_poly_index(Xt_pass, ind_v, deg_v)\n",
    "Xt_pass,_,_ = standardize_test(Xt_pass, mean, std)\n",
    "\n",
    "#Building predictions\n",
    "Y_test = sigmoid(Xt_pass.dot(ws))\n",
    "Y_pred_G2 = probability_to_prediction(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_test = [1, 2, 3, 4, 5, 6, 7, 1/2, 1/3, 1/4]\n",
    "\n",
    "#Transforming Train Data\n",
    "X_pass = X_sets[-1][2]\n",
    "X_pass = np.delete(X_pass,19,axis=1)\n",
    "\n",
    "deg_v = [degree_test for i in range(X_pass.shape[1])]\n",
    "ind_v = list(range(1,X_pass.shape[1]))\n",
    "\n",
    "X_pass = build_poly_index(X_pass, ind_v, deg_v)\n",
    "X_pass,mean,std = standardize(X_pass)\n",
    "\n",
    "Y_pass = Y_sets[-1][2]\n",
    "\n",
    "#Building model\n",
    "initial_w = np.ones((X_pass.shape[1]))*(-0.01)\n",
    "ws,loss = logistic_regression(Y_pass, X_pass, initial_w, max_iters=10000, gamma=0.5, print_=True)\n",
    "\n",
    "#Transforming Test Data\n",
    "Xt_pass = X_sets_t[-1][2]\n",
    "Xt_pass = np.delete(Xt_pass,19,axis=1)\n",
    "Xt_pass = build_poly_index(Xt_pass, ind_v, deg_v)\n",
    "Xt_pass,_,_ = standardize_test(Xt_pass, mean, std)\n",
    "\n",
    "#Building predictions\n",
    "Y_test = sigmoid(Xt_pass.dot(ws))\n",
    "Y_pred_G3 = probability_to_prediction(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_test = [1, 1/2, 2, 1/3, 3, 1/4, 4]\n",
    "\n",
    "#Transforming Train Data\n",
    "X_pass = X_sets[-1][3]\n",
    "X_pass = np.delete(X_pass,19,axis=1)\n",
    "X_pass = np.delete(X_pass,22,axis=1) #Column is deleted because it contains only nans\n",
    "\n",
    "deg_v = [degree_test for i in range(X_pass.shape[1])]\n",
    "ind_v = list(range(1,X_pass.shape[1]))\n",
    "\n",
    "X_pass = build_poly_index(X_pass, ind_v, deg_v)\n",
    "X_pass, mean, std = standardize(X_pass)\n",
    "\n",
    "Y_pass = Y_sets[-1][3]\n",
    "\n",
    "#Building model\n",
    "initial_w = np.ones((X_pass.shape[1]))*(-0.01)\n",
    "ws,loss = logistic_regression(Y_pass, X_pass, initial_w, max_iters=10000, gamma=1, print_=True)\n",
    "\n",
    "#Transforming Test Data\n",
    "Xt_pass = X_sets_t[-1][3]\n",
    "Xt_pass = np.delete(Xt_pass,19,axis=1)\n",
    "Xt_pass = np.delete(Xt_pass,22,axis=1)\n",
    "Xt_pass = build_poly_index(Xt_pass, ind_v, deg_v)\n",
    "Xt_pass,_,_ = standardize_test(Xt_pass, mean, std)\n",
    "\n",
    "#Building predictions\n",
    "Y_test = sigmoid(Xt_pass.dot(ws))\n",
    "Y_pred_G4 = probability_to_prediction(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifying sizes are correct\n",
    "print(\"Test split sizes:\", [sets.shape[0] for sets in X_sets_t[-1]])\n",
    "print(\"Test pred. sizes:\", [len(Y_pred_G1), len(Y_pred_G2), len(Y_pred_G3), len(Y_pred_G4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used stored indices to re-merge predictiosn in correct order\n",
    "Y_pred = np.zeros(X_test.shape[0])\n",
    "\n",
    "for i,j in zip(Train_ind[-1][0], Y_pred_G1):\n",
    "    Y_pred[i]= j\n",
    "for i,j in zip(Train_ind[-1][1], Y_pred_G2):\n",
    "    Y_pred[i]= j\n",
    "for i,j in zip(Train_ind[-1][2], Y_pred_G3):\n",
    "    Y_pred[i]= j\n",
    "for i,j in zip(Train_ind[-1][3], Y_pred_G4):\n",
    "    Y_pred[i]= j\n",
    "\n",
    "#Verify all predictions are defined\n",
    "any(Y_pred == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = testdata[['Id']]\n",
    "\n",
    "Y_final = np.c_[np.array(ids, dtype=np.int64), Y_pred]\n",
    "\n",
    "np.savetxt(\"submission.csv\", Y_final, delimiter=',', header=\"Id,Prediction\", comments=\"\", fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
