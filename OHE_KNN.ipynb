{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN\n",
    "Using two possible distance measures and rangeof k from 1 to 20 ( maybe can check overfitting with decreasing K)\n",
    "try without standardization and try with mi-max (maybe try with Emmanuel's standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils_data_loading import*\n",
    "from one_hot_encoding import*\n",
    "from utils_features_manipulation import*\n",
    "from utils_nans_manipulation import*\n",
    "\n",
    "trainpath = 'Data/train.csv'\n",
    "testpath = 'Data/test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing train data and organizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata,header = load_data(trainpath)\n",
    "\n",
    "X_total, Y_total = structure_data(traindata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OHE\n",
    "Before comparing distances it doesn't make sense to keep categorical variabes as integers instead OHE them. From EDA 3 variabes could be split and considered as categorical. PRI_jet_num definitely since there are only 4 unique values. And the centrality look like they could be split into two distributions with the threshold below.\n",
    "\n",
    "DER_lep_eta_cenrality: nans, 0, 1 threshold 0.5 for 0 vs 1\n",
    "DER_met_phi_centrality: threshold above or below 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nans = replace_bad_data_with_nans(X_total, -999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cleaned, nans_indices = replace_nans_with_median(X_nans, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dict for indices\n",
    "original_indices = {\n",
    "    'jet' : header.index('PRI_jet_num')-1,\n",
    "    'phi' : header.index('DER_met_phi_centrality')-1,\n",
    "    'eta' : header.index('DER_lep_eta_centrality')-1\n",
    "}\n",
    "\n",
    "cat_indices = {}\n",
    "\n",
    "for name,index in original_indices.items():\n",
    "    if index not in nans_indices:\n",
    "        skip = len([i for i in nans_indices if i < index])\n",
    "        cat_indices[name] = index - skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing OHE for jets\n",
    "jets = X_cleaned[:, cat_indices['jet']].astype(np.int8)\n",
    "jets_OHE = get_jet_OHE(jets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing OHE for phi_centrality\n",
    "phi_centrality = X_cleaned[:, cat_indices['phi']]\n",
    "phi_centrality_OHE = get_centrality_split_OHE(phi_centrality, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new matrix\n",
    "X_OHE = np.c_[np.delete(X_cleaned, list(cat_indices.values()), axis=1), jets_OHE, phi_centrality_OHE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Scaling non-categorical feature \n",
    "so that distances are comparable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init vars\n",
    "tot_features = 31\n",
    "cat_features = 2\n",
    "\n",
    "X_scaled = X_OHE.copy()\n",
    "\n",
    "last_non_cat = tot_features - cat_features # All OHE features are at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale non categorical features\n",
    "scalable_features = X_scaled[:,1:last_non_cat]  # Only scale non categorical feature\n",
    "\n",
    "X_scaled[:,1:last_non_cat] = min_max_scale(scalable_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to a .py file I guess?\n",
    "\n",
    "#pull maybe 1000 random samples\n",
    "# Euclidean distance between two vectors\n",
    "def dist_euclidean(row1, row2):\n",
    "    r1=row1[1:] #takes features after column of 1\n",
    "    r2=row2[1:]\n",
    "    dist=np.linalg.norm(r1-rb)\n",
    "    return dist\n",
    "\n",
    "# calculate the manhattan distance between two vectors\n",
    "def dist_manhattan(row1, row2):\n",
    "    r1=row1[1:] #takes features after column of 1\n",
    "    r2=row2[1:]\n",
    "    dist=np.sum(np.abs(r1-r2))\n",
    "    return dist\n",
    "\n",
    "def dist_manhattan_broadcast(matrix, row):\n",
    "    matrix = matrix[:,1:]\n",
    "    row = row[1:]\n",
    "    \n",
    "    dist=np.sum(np.abs(matrix-row), axis=1)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looked at everything before this, cleaned the code a bit and made it more elegant, but it was actually already pretty good/fast (just some copy-paste errors). \n",
    "\n",
    "TODO: check KNN model func?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: delete\n",
    "\n",
    "def split_data_tmp(x,y, ratio, seed=1):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # split the data based on the given ratio: TODO    \n",
    "    p = np.random.permutation(len(y))\n",
    "    y = y[p]\n",
    "    x = x[p]\n",
    "    \n",
    "    limit = int(len(y)*ratio)\n",
    "        \n",
    "    return x[:limit],y[:limit],x[limit:],y[limit:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "def KNNmodel(Xtest,Xtrain,Ytrain,dist_func, k_n):\n",
    "    \"\"\" input are extended X matrices, returns ypredictions array for Xtest using k closest distances\n",
    "    calculated with given dist_func Given X's have been OHE and scales and y predictions are given as 0/1\"\"\"\n",
    "    ypredarray=np.zeros(Xtest.shape[0])\n",
    "    \n",
    "    ret = np.empty((X_test.shape[0],k_n))\n",
    "    np.r_[ret, indexn]\n",
    "    \n",
    "    for j in range(Xtest.shape[0]): #prediction for j'th row of Xtest\n",
    "        rowdistances=np.zeros(Xtrain.shape[0])\n",
    "        #for i in range(Xtrain.shape[0]):   #i denotes index of row of Xtrain\n",
    "        \n",
    "        rowdistances=dist_manhattan_broadcast(Xtrain, Xtest[j])  #computing distance between test point and each Xtrain row\n",
    "        \n",
    "        sortindex=np.argsort(rowdistances) #orders neighbor rows for closest distances \n",
    "        indexn=sortindex[:k_n]\n",
    "        yneighbours=Ytrain[indexn]\n",
    "        ypredclass=(yneighbours.mean()>0.5).astype(int)  #if average above 0.5 majority 1's classified as 1 otherwise classified as 0\n",
    "        ypredarray[j]=ypredclass\n",
    "    \n",
    "    return ypredarray\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNNmodel (...) -> (X_test, k)\n",
    "\n",
    "KNN = KNNmodel(pessimistic_k)\n",
    "for k in range(pessimistic_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = split_data_tmp(X_scaled, Y_total, 0.8)\n",
    "\n",
    "KNNmodel(X_test, X_train, Y_train, dist_manhattan, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE\n",
    "would it be possible to also (instead of splitting NAN - Non NANS and cleaning NAN's) Directly take X_total and clean NANs: remove features if more than 50% are NANs otherwise replace by median. \n",
    "\n",
    "^\n",
    "To do this, just call: 'replace_nans_with_median(X_total, 0.5)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#column indices. The headers as two extra coumn 'id' and 'predictions'\n",
    "#X matrices have only 1 extra column of ones so indices should be 1 less than in the headers \n",
    "indice_OHE=[]\n",
    "indice_OHE.append(header.index('DER_prodeta_jet_jet')-1)\n",
    "indice_OHE.append(header.index('DER_prodeta_jet_jet')-1)   # Are these two names wrong?\n",
    "indice_OHE.append(header.index('PRI_jet_num')-1)  # Shouldn't this be the first one?\n",
    "cat=[2,2,4]   # What is this used for?\n",
    "\n",
    "\n",
    "X_OHE=np.copy(X_total)  #X is the extanded \"30+1 column array\"\n",
    "\n",
    "#JetNumOHE\n",
    "colbefore=X_total[:,indice_OHE[0]] #extract column from Xtotal\n",
    "classes=[0,1,2,3]\n",
    "#initialize ohe sub-matrix for feature col\n",
    "submatrix=np.zeros((X_total.shape[0], 4))\n",
    "for i in range(4):\n",
    "    coltmp=(colbefore==classes[i]).astype(int)\n",
    "    submatrix[:,i]=coltmp\n",
    "    \n",
    "X_OHE=np.append(X_OHE, submatrix, axis=1)\n",
    "    #remove column from XOHE\n",
    "    #append new columns to XOHE\n",
    "\n",
    "#NOTE: ONLY for X_Cleaned when no NAN values for the following two features. \n",
    "#Are these two features retained with the threshold \n",
    "#LepEtaCentrality OHE\n",
    "colbefore=X_total[:,indice_OHE[1]]\n",
    "submatrix=np.zeros((X_total.shape[0], 2))\n",
    "coltmp0=(colbefore < 0.5).astype(int)\n",
    "coltmp1=(colbefore >= 0.5).astype(int)\n",
    "submatrix[:,0]=coltmp0\n",
    "submatrix[:,1]=coltmp1\n",
    "X_OHE=np.append(X_OHE, submatrix, axis=1)\n",
    "\n",
    "#LepEtaCentrality OHE\n",
    "colbefore=X_total[:,indice_OHE[2]]\n",
    "submatrix=np.zeros((X_total.shape[0], 2))\n",
    "coltmp0=(colbefore < 0).astype(int)\n",
    "coltmp1=(colbefore >= 0).astype(int)\n",
    "submatrix[:,0]=coltmp0\n",
    "submatrix[:,1]=coltmp1\n",
    "X_OHE=np.append(X_OHE, submatrix, axis=1)\n",
    "    \n",
    "\n",
    "#removing old features\n",
    "    \n",
    "X_OHE=np.delete(X_OHE, indice_OHE, axis=1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (250000,27) into shape (27,36)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-161-2943d867b9ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mXmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#maximum array for the different features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0msubscale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubmatrix\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mXmin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXmin\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mXmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mXscale\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnewindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (250000,27) into shape (27,36)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Xscale=np.copy(X_OHE)\n",
    "Original_number=31  #number of features before OHE +1 column\n",
    "number_features_OHE=3 #again if the centrality features are still part of the X data array\n",
    "newindex=Original_number-number_features_OHE\n",
    "submatrix=Xscale[:,1:newindex]  #don't scale the 1 column and the OHE variables which start at index 31-3 because we removed 3 categorical variables\n",
    "Xmin=(submatrix.min(axis=0)) #minimum array for the different features\n",
    "Xmax=(submatrix.max(axis=0)) #maximum array for the different features\n",
    "subscale=(submatrix-Xmin)/(Xmin-Xmax)\n",
    "Xscale[1:newindex]=subscale\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
