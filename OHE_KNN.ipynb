{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN\n",
    "Using two possible distance measures and rangeof k from 1 to 20 ( maybe can check overfitting with decreasing K)\n",
    "try without standardization and try with mi-max (maybe try with Emmanuel's standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing train data and organizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = \"i8,S5,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,i8,f8,f8,f8,f8,f8,f8,f8\"\n",
    "with open('train.csv', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    headerstrain = next(reader)\n",
    "datatrain = np.genfromtxt('train.csv', delimiter=\",\",names=True, dtype=dtypes)\n",
    "\n",
    "#training (s=1 b=0)\n",
    "y=(datatrain['Prediction']==b's').astype(int)\n",
    "#transforming features X into a list of list\n",
    "trainxlist=[]\n",
    "for i in datatrain:\n",
    "    trainsublist=[]\n",
    "    for x in i:\n",
    "        trainsublist.append(x)\n",
    "    trainsublist=trainsublist[2:]\n",
    "    trainxlist.append(trainsublist)\n",
    "\n",
    "#adding column of 1 \n",
    "for x in trainxlist:\n",
    "    x.insert(0,1)\n",
    "#trainxlist is the extended X matrix\n",
    "\n",
    "#transform into np.array\n",
    "Y_total=np.array(y)\n",
    "X_total=np.array(trainxlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OHE\n",
    "Before comparing distances it doesn't make sense to keep categorical variabes as integers instead OHE them. From EDA 3 variabes could be split and considered as categorical. PRI_jet_num definitely since there are only 4 unique values. And the centrality look like they could be split into two distributions with the threshold below.\n",
    "\n",
    "DER_lep_eta_cenrality: nans, 0, 1 threshold 0.5 for 0 vs 1\n",
    "DER_met_phi_centrality: threshold above or below 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column indices. The headers as two extra coumn 'id' and 'predictions', X matrices have only 1 extra column of ones so indices should be 1 less than in the headers \n",
    "indice_OHE=[]\n",
    "indice_OHE.append(headerstrain.index('DER_prodeta_jet_jet')-1)\n",
    "indice_OHE.append(headerstrain.index('DER_prodeta_jet_jet')-1)\n",
    "indice_OHE.append(headerstrain.index('PRI_jet_num')-1)\n",
    "cat=[2,2,4]\n",
    "\n",
    "\n",
    "X_OHE=np.copy(X_total)  #X is the extanded \"30+1 column array\"\n",
    "\n",
    "#JetNumOHE\n",
    "colbefore=X_total[:,indice_OHE[0]] #extract column from Xtotal\n",
    "classes=[0,1,2,3]\n",
    "#initialize ohe sub-matrix for feature col\n",
    "submatrix=np.zeros((X_total.shape[0], 4))\n",
    "for i in range(4):\n",
    "    coltmp=(colbefore==classes[i]).astype(int)\n",
    "    submatrix[:,i]=coltmp\n",
    "    \n",
    "X_OHE=np.append(X_OHE, submatrix, axis=1)\n",
    "    #remove column from XOHE\n",
    "    #append new columns to XOHE\n",
    "\n",
    "#NOTE: ONLY for X_Cleaned when no NAN values for the following two features. Are these two features retained with the threshold \n",
    "#LepEtaCentrality OHE\n",
    "colbefore=X_total[:,indice_OHE[1]]\n",
    "submatrix=np.zeros((X_total.shape[0], 2))\n",
    "coltmp0=(colbefore < 0.5).astype(int)\n",
    "coltmp1=(colbefore >= 0.5).astype(int)\n",
    "submatrix[:,0]=coltmp0\n",
    "submatrix[:,1]=coltmp1\n",
    "X_OHE=np.append(X_OHE, submatrix, axis=1)\n",
    "\n",
    "#LepEtaCentrality OHE\n",
    "colbefore=X_total[:,indice_OHE[2]]\n",
    "submatrix=np.zeros((X_total.shape[0], 2))\n",
    "coltmp0=(colbefore < 0).astype(int)\n",
    "coltmp1=(colbefore >= 0).astype(int)\n",
    "submatrix[:,0]=coltmp0\n",
    "submatrix[:,1]=coltmp1\n",
    "X_OHE=np.append(X_OHE, submatrix, axis=1)\n",
    "    \n",
    "\n",
    "#removing old features\n",
    "    \n",
    "X_OHE=np.delete(X_OHE, indice_OHE, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Scaling non-categorical feature \n",
    "so that distances are comparable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xscale=np.copy(XOHE)\n",
    "Original_number=31  #number of features before OHE +1 column\n",
    "number_feature_OHE=3 #again if the centrality features are still part of the X data array\n",
    "newindex=Original_number-number_features_OHE\n",
    "submatrix=Xscale[:,1:newindex]  #don't scale the 1 column and the OHE variables which start at index 31-3 because we removed 3 categorical variables\n",
    "Xmin=(submatrix.min(axis=0)).reshape(-1,1) #minimum array for the different features\n",
    "Xmax=(submatrix.max(axis=0)).reshape(-1,1) #maximum array for the different features\n",
    "subscale=(submatrix-Xmin)/(Xmin-Xmax)\n",
    "Xscale[1:newindex]=subscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull maybe 1000 random samples\n",
    "# Euclidean distance between two vectors\n",
    "def dist_euclidean(row1, row2):\n",
    "    r1=row1[1:] #takes features after column of 1\n",
    "    r2=row2[1:]\n",
    "    dist=np.linalg.norm(r1-rb)\n",
    "    return dist\n",
    "\n",
    "# calculate the manhattan distance between two vectors\n",
    "def dist_manhattan(row1, row2):\n",
    "    r1=row1[1:] #takes features after column of 1\n",
    "    r2=row2[1:]\n",
    "    dist=np.sum(np.abs(r1-r2))\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNNmodel(Xtest,Xtrain,Ytrain,dist_func, k_n):\n",
    "    \"\"\" input are extended X matrices, returns ypredictions array for Xtest using k closest distances\n",
    "    calculated with given dist_func Given X's have been OHE and scales and y predictions are given as 0/1\"\"\"\n",
    "    ypredarray=np.zeros(Xtest.shape[0])\n",
    "    for j in range(Xtest.shape[0]): #prediction for j'th row of Xtest\n",
    "        rowdistances=np.zeros(Xtrain.shape[0])\n",
    "        for i in range(Xtrain.shape[0]):   #i denotes index of row of Xtrain\n",
    "            disttmp=dist_func(Xtest[j],Xtrain[i])  #computing distance between test point and each Xtrain row\n",
    "            rowdistances[i]=disttmp                  #storing distance\n",
    "\n",
    "        sortindex=np.argsort(rowdistances) #orders neighbor rows for closest distances \n",
    "        indexn=sortindex[:k_n]\n",
    "        yneighbours=Ytrain[indexn]\n",
    "        ypredclass=( yneighbours.mean()>0.5).astype(int)  #if average above 0.5 majority 1's classified as 1 otherwise classified as 0\n",
    "        ypredarray[j]=ypredclass\n",
    "    \n",
    "    return ypredarray\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE\n",
    "would it be possible to also (instead of splitting NAN - Non NANS and cleaning NAN's) Directly take X_total and clean NANs: remove features if more than 50% are NANs otherwise replace by median. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
