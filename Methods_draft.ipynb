{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define\n",
    "This notebook contains draft of methods (meant for troubleshooting).\n",
    "\n",
    "Methods to be used need to be placed in helpers.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_MSE(y, tx, w):\n",
    "    \"\"\"Calculate the loss using MSE.\"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    lv = np.mean(np.square(e))\n",
    "    return lv\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    tx_t = tx.transpose()\n",
    "    dl = -1/len(tx)*tx_t.dot(e)\n",
    "    return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        # Compute gradient and loss\n",
    "        dl = compute_gradient(y,tx,w)\n",
    "        loss = compute_loss_MSE(y,tx,w)\n",
    "        \n",
    "        # Update w by gradient\n",
    "        w = w - gamma*dl\n",
    "        \n",
    "        # Store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "\n",
    "    return losses, ws\n",
    "    raise NotImplementedError\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares SGD\n",
    "The function example from the project description does not include batch size, I'm guessing thats an error?\n",
    "\n",
    "Batch iters was taken from helpers.py in Lab2. Are we allowed to use it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient from just few examples n and their corresponding y_n labels.\"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    tx_t = tx.transpose()\n",
    "    dl = -1/len(tx)*tx_t.dot(e)\n",
    "    return dl\n",
    "\n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    raise NotImplementedError\n",
    "\n",
    "#Draft\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient and loss\n",
    "        txx = [xx[1] for xx in batch_iter(y,tx,10)][0]\n",
    "        tyy = [xx[0] for xx in batch_iter(y,tx,10)][0]\n",
    "        dl = compute_stoch_gradient(tyy,txx,w)\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        \n",
    "        # update w by gradient\n",
    "        w = w - gamma*dl\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        \n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares\n",
    "\n",
    "For the exercise I used np.linalg.inv, but for matrices with multiple solutions to AB = I this gives errors.\n",
    "\n",
    "np.lingal.solve, returns a solution regardless of whether there are multiple ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    XX = np.dot(tx.transpose(),tx)\n",
    "    B = np.dot(tx.transpose(),y)\n",
    "    \n",
    "    w_s = np.linalg.solve(XX,B)\n",
    "    \n",
    "    return w_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    XX = np.dot(tx.transpose(),tx)\n",
    "    I = np.identity(XX.shape[0])\n",
    "    XX = XX + lambda_*I\n",
    "    \n",
    "    B = np.dot(tx.transpose(),y)\n",
    "    \n",
    "    w_s = np.linalg.solve(XX,B)\n",
    "    return w_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    raise NotImplementedError\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reg Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
