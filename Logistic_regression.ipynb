{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from helpers import*\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = \"i8,S5,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,f8,i8,f8,f8,f8,f8,f8,f8,f8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading train data\n",
    "with open('Data/train.csv', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    headers = next(reader)\n",
    "train_data = np.genfromtxt('Data/train.csv', delimiter=\",\",names=True, dtype=dtypes)\n",
    "\n",
    "# Loading test data\n",
    "\n",
    "with open('Data/test.csv', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    headers = next(reader)\n",
    "test_data = np.genfromtxt('Data/test.csv', delimiter=\",\",names=True, dtype=dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_as_list_of_lists_training(xinput):\n",
    "    xlist=[]\n",
    "    for i in xinput:\n",
    "        sublist=[]\n",
    "        for x in i:\n",
    "            sublist.append(x)\n",
    "        sublist=sublist[2:]       # Should be 1: if prediction is removed from the input data\n",
    "        xlist.append(sublist)\n",
    "        \n",
    "    return xlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fake_feature(listx):\n",
    "    for x in listx:\n",
    "        x.insert(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_y(data):\n",
    "    y = []\n",
    "\n",
    "    for point in data:\n",
    "        if point[1] == b'b':\n",
    "            y.append(-1)\n",
    "        else:\n",
    "            y.append(1)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data\n",
    "\n",
    "y_tr = extract_y(train_data)\n",
    "y_te = extract_y(test_data)\n",
    "\n",
    "tx_tr = get_as_list_of_lists_training(train_data)\n",
    "add_fake_feature(tx_tr)\n",
    "tx_te = get_as_list_of_lists_training(test_data)\n",
    "add_fake_feature(tx_te)\n",
    "\n",
    "w = np.zeros(len(tx[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init variables\n",
    "\n",
    "max_iter = 10000\n",
    "threshold = 1e-8\n",
    "gamma = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: MOVE TO OTHER FILE\n",
    "\n",
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "            \n",
    "    # compute gradient and loss    \n",
    "    g = calculate_gradient(y, tx, w)\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "\n",
    "    # update w by gradient\n",
    "    w = w - gamma*g\n",
    "\n",
    "    return loss, w\n",
    "\n",
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    \n",
    "    return np.transpose(tx).dot(sigmoid(tx.dot(w)) - y)\n",
    "\n",
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the loss: negative log likelihood.\"\"\"\n",
    "\n",
    "    return np.sum(-y*tx.dot(w) + np.log(1 + np.exp(tx.dot(w))))\n",
    "\n",
    "def sigmoid(t):\n",
    "    \"\"\"apply the sigmoid function on t.\"\"\"\n",
    "    \n",
    "    return 1 / (1 + np.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=173286.79513998624\n",
      "Current iteration=1, loss=inf\n",
      "Current iteration=2, loss=inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\aless\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:31: RuntimeWarning: overflow encountered in exp\n",
      "c:\\users\\aless\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:26: RuntimeWarning: overflow encountered in exp\n",
      "c:\\users\\aless\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=3, loss=inf\n",
      "Current iteration=4, loss=inf\n",
      "Current iteration=5, loss=inf\n",
      "Current iteration=6, loss=inf\n",
      "Current iteration=7, loss=inf\n",
      "Current iteration=8, loss=inf\n",
      "Current iteration=9, loss=inf\n",
      "Current iteration=10, loss=inf\n",
      "Current iteration=11, loss=inf\n",
      "Current iteration=12, loss=inf\n",
      "Current iteration=13, loss=inf\n",
      "Current iteration=14, loss=inf\n",
      "Current iteration=15, loss=inf\n",
      "Current iteration=16, loss=inf\n",
      "Current iteration=17, loss=inf\n",
      "Current iteration=18, loss=inf\n",
      "Current iteration=19, loss=inf\n",
      "Current iteration=20, loss=inf\n",
      "Current iteration=21, loss=inf\n",
      "Current iteration=22, loss=inf\n",
      "Current iteration=23, loss=inf\n",
      "Current iteration=24, loss=inf\n",
      "Current iteration=25, loss=inf\n",
      "Current iteration=26, loss=inf\n",
      "Current iteration=27, loss=inf\n",
      "Current iteration=28, loss=inf\n",
      "Current iteration=29, loss=inf\n",
      "Current iteration=30, loss=inf\n",
      "Current iteration=31, loss=inf\n",
      "Current iteration=32, loss=inf\n",
      "Current iteration=33, loss=inf\n",
      "Current iteration=34, loss=inf\n",
      "Current iteration=35, loss=inf\n",
      "Current iteration=36, loss=inf\n",
      "Current iteration=37, loss=inf\n",
      "Current iteration=38, loss=inf\n",
      "Current iteration=39, loss=inf\n",
      "Current iteration=40, loss=inf\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-75b6488a060c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# get loss and update w.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_by_gradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;31m# log info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m#if iter % 100 == 0:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-74-2658d3b813f7>\u001b[0m in \u001b[0;36mlearning_by_gradient_descent\u001b[1;34m(y, tx, w, gamma)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# compute gradient and loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# update w by gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-74-2658d3b813f7>\u001b[0m in \u001b[0;36mcalculate_loss\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;34m\"\"\"compute the loss: negative log likelihood.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Apply logistic regression\n",
    "\n",
    "losses = []\n",
    "\n",
    "for iter in range(max_iter):\n",
    "    # get loss and update w.\n",
    "    loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "    # log info\n",
    "    #if iter % 100 == 0:\n",
    "    print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "    # converge criterion\n",
    "    losses.append(loss)\n",
    "    if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
