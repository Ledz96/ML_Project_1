{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build prediction for AICROWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils_predictions_manipulation import*\n",
    "from utils_nans_manipulation import*\n",
    "from cross_validation import*\n",
    "from utils_data_loading import*\n",
    "from utils_features_manipulation import*\n",
    "from time import time\n",
    "from logistic_regression import*\n",
    "#from standardization import*\n",
    "from least_squares import*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata,_ = load_data('Data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata,_ = load_data('Data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = structure_data(traindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test,_ = structure_data(testdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nans = replace_bad_data_with_nans(X_train, -999)\n",
    "X_nans_test = replace_bad_data_with_nans(X_test, -999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clean,del_indexes = replace_nans_with_median(X_nans, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_del = delete_nans_indexes(X_nans_test, del_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_clean = replace_test_nans_with_median(X_test_del, X_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logit fine tuned 0.782?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_poly = build_poly_multi(X_clean, 4)\n",
    "X_train_final,x_meab, x_std = standardize(X_train_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_poly = build_poly_multi(X_test_clean, 4)\n",
    "X_test_final,_,_=standardize(X_test_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.ones((X_train_final.shape[1]))*(-0.01)\n",
    "\n",
    "ws,loss = logistic_regression(Y_train, X_train_final, initial_w, max_iters=10000, gamma=0.45)\n",
    "\n",
    "Y_test = sigmoid(X_test_final.dot(ws))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_logit = probability_to_prediction(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = testdata[['Id']]\n",
    "\n",
    "\n",
    "\n",
    "Y_final = np.c_[np.array(ids, dtype=np.int64), Y_pred]\n",
    "\n",
    "np.savetxt(\"submission.csv\", Y_final, delimiter=',', header=\"Id,Prediction\", comments=\"\", fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROOTS 0.740 (new submission is 12% different)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deg_v = [[1], [0.16666666666666666], [1], [1], [0.2], [4], [1], [6], [0.3333333333333333], [7], [1], [6], [0.5],\n",
    "         #[1], [8], [6], [2], [0.16666666666666666], [0.16666666666666666], [6], [4], [2]]\n",
    "\n",
    "X_use = np.delete(X_clean,19,axis=1)\n",
    "X_use = np.delete(X_use,22,axis=1)\n",
    "\n",
    "Xt_use = np.delete(X_test_clean,19,axis=1)\n",
    "Xt_use = np.delete(Xt_use,22,axis=1)\n",
    "\n",
    "degree_test = [1/6, 1/5, 1/4, 1/3, 1/2, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "deg_v = [[1] for i in range(X_use.shape[1])]\n",
    "\n",
    "ind_v = (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 22)\n",
      "(568238, 22)\n"
     ]
    }
   ],
   "source": [
    "X_train_poly = build_poly_index(X_use, ind_v, deg_v)\n",
    "X_train_final,_,_=standardize(X_train_poly)\n",
    "\n",
    "X_test_poly = build_poly_index(Xt_use, ind_v, deg_v)\n",
    "X_test_final,_,_=standardize(X_test_poly)\n",
    "\n",
    "print(X_train_final.shape)\n",
    "print(X_test_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.6958195753523564\n",
      "Current iteration=1, loss=0.6462948261636919\n",
      "Current iteration=2, loss=0.6207457547604618\n",
      "Current iteration=3, loss=0.6040464938404676\n",
      "Current iteration=4, loss=0.5919833114504403\n",
      "Current iteration=5, loss=0.5828133685295538\n",
      "Current iteration=6, loss=0.5756143414757334\n",
      "Current iteration=7, loss=0.5698283900446031\n",
      "Current iteration=8, loss=0.5650912975205589\n",
      "Current iteration=9, loss=0.5611533009979847\n",
      "Current iteration=10, loss=0.5578369365908883\n",
      "Current iteration=11, loss=0.5550125392560153\n",
      "Current iteration=12, loss=0.5525831599856901\n",
      "Current iteration=13, loss=0.5504748875084748\n",
      "Current iteration=14, loss=0.54863043088706\n",
      "Current iteration=15, loss=0.5470047447181079\n",
      "Current iteration=16, loss=0.5455619734230095\n",
      "Current iteration=17, loss=0.5442732701561658\n",
      "Current iteration=18, loss=0.5431152094705551\n",
      "Current iteration=19, loss=0.5420686118143365\n",
      "Current iteration=20, loss=0.5411176593373033\n",
      "Current iteration=21, loss=0.5402492214927235\n",
      "Current iteration=22, loss=0.5394523342305612\n",
      "Current iteration=23, loss=0.5387177933291984\n",
      "Current iteration=24, loss=0.5380378337064297\n",
      "Current iteration=25, loss=0.5374058742986891\n",
      "Current iteration=26, loss=0.5368163135011653\n",
      "Current iteration=27, loss=0.5362643639887075\n",
      "Current iteration=28, loss=0.5357459184876648\n",
      "Current iteration=29, loss=0.5352574400720524\n",
      "Current iteration=30, loss=0.5347958720351056\n",
      "Current iteration=31, loss=0.5343585634901586\n",
      "Current iteration=32, loss=0.5339432076868382\n",
      "Current iteration=33, loss=0.5335477906626682\n",
      "Current iteration=34, loss=0.5331705483378656\n",
      "Current iteration=35, loss=0.5328099305393525\n",
      "Current iteration=36, loss=0.5324645707356811\n",
      "Current iteration=37, loss=0.5321332604973117\n",
      "Current iteration=38, loss=0.5318149278810954\n",
      "Current iteration=39, loss=0.5315086190848082\n",
      "Current iteration=40, loss=0.5312134828353815\n",
      "Current iteration=41, loss=0.5309287570693817\n",
      "Current iteration=42, loss=0.5306537575410696\n",
      "Current iteration=43, loss=0.5303878680558045\n",
      "Current iteration=44, loss=0.5301305320774901\n",
      "Current iteration=45, loss=0.5298812455004811\n",
      "Current iteration=46, loss=0.529639550410688\n",
      "Current iteration=47, loss=0.5294050296888809\n",
      "Current iteration=48, loss=0.5291773023326356\n",
      "Current iteration=49, loss=0.5289560193927626\n",
      "Current iteration=50, loss=0.5287408604362404\n",
      "Current iteration=51, loss=0.5285315304611656\n",
      "Current iteration=52, loss=0.5283277572005082\n",
      "Current iteration=53, loss=0.5281292887609393\n",
      "Current iteration=54, loss=0.5279358915509434\n",
      "Current iteration=55, loss=0.5277473484591404\n",
      "Current iteration=56, loss=0.5275634572493981\n",
      "Current iteration=57, loss=0.5273840291441104\n",
      "Current iteration=58, loss=0.5272088875710719\n",
      "Current iteration=59, loss=0.5270378670528313\n",
      "Current iteration=60, loss=0.5268708122203463\n",
      "Current iteration=61, loss=0.5267075769352526\n",
      "Current iteration=62, loss=0.5265480235072125\n",
      "Current iteration=63, loss=0.5263920219946204\n",
      "Current iteration=64, loss=0.5262394495785206\n",
      "Current iteration=65, loss=0.5260901900009307\n",
      "Current iteration=66, loss=0.525944133059914\n",
      "Current iteration=67, loss=0.5258011741547524\n",
      "Current iteration=68, loss=0.5256612138754048\n",
      "Current iteration=69, loss=0.5255241576311981\n",
      "Current iteration=70, loss=0.5253899153143214\n",
      "Current iteration=71, loss=0.5252584009942478\n",
      "Current iteration=72, loss=0.525129532639694\n",
      "Current iteration=73, loss=0.5250032318651358\n",
      "Current iteration=74, loss=0.5248794236992618\n",
      "Current iteration=75, loss=0.5247580363730588\n",
      "Current iteration=76, loss=0.5246390011254998\n",
      "Current iteration=77, loss=0.5245222520250375\n",
      "Current iteration=78, loss=0.5244077258053221\n",
      "Current iteration=79, loss=0.5242953617137339\n",
      "Current iteration=80, loss=0.5241851013714904\n",
      "Current iteration=81, loss=0.5240768886442209\n",
      "Current iteration=82, loss=0.5239706695220253\n",
      "Current iteration=83, loss=0.5238663920081404\n",
      "Current iteration=84, loss=0.5237640060154364\n",
      "Current iteration=85, loss=0.5236634632700398\n",
      "Current iteration=86, loss=0.5235647172214649\n",
      "Current iteration=87, loss=0.5234677229586892\n",
      "Current iteration=88, loss=0.523372437131672\n",
      "Current iteration=89, loss=0.5232788178778657\n",
      "Current iteration=90, loss=0.5231868247533129\n",
      "Current iteration=91, loss=0.5230964186679602\n",
      "Current iteration=92, loss=0.5230075618248615\n",
      "Current iteration=93, loss=0.5229202176629681\n",
      "Current iteration=94, loss=0.5228343508032354\n",
      "Current iteration=95, loss=0.5227499269977993\n",
      "Current iteration=96, loss=0.5226669130819989\n",
      "Current iteration=97, loss=0.5225852769290413\n",
      "Current iteration=98, loss=0.5225049874071228\n",
      "Current iteration=99, loss=0.5224260143388358\n",
      "Current iteration=100, loss=0.5223483284627091\n",
      "Current iteration=101, loss=0.5222719013967327\n",
      "Current iteration=102, loss=0.522196705603748\n",
      "Current iteration=103, loss=0.5221227143585692\n",
      "Current iteration=104, loss=0.5220499017167388\n",
      "Current iteration=105, loss=0.5219782424848081\n",
      "Current iteration=106, loss=0.521907712192049\n",
      "Current iteration=107, loss=0.521838287063516\n",
      "Current iteration=108, loss=0.5217699439943715\n",
      "Current iteration=109, loss=0.5217026605254045\n",
      "Current iteration=110, loss=0.5216364148196735\n",
      "Current iteration=111, loss=0.5215711856402075\n",
      "Current iteration=112, loss=0.5215069523287074\n",
      "Current iteration=113, loss=0.5214436947851935\n",
      "Current iteration=114, loss=0.5213813934485438\n",
      "Current iteration=115, loss=0.5213200292778797\n",
      "Current iteration=116, loss=0.5212595837347501\n",
      "Current iteration=117, loss=0.5212000387660748\n",
      "Current iteration=118, loss=0.5211413767878056\n",
      "Current iteration=119, loss=0.5210835806692707\n",
      "Current iteration=120, loss=0.521026633718165\n",
      "Current iteration=121, loss=0.5209705196661564\n",
      "Current iteration=122, loss=0.5209152226550764\n",
      "Current iteration=123, loss=0.5208607272236658\n",
      "Current iteration=124, loss=0.5208070182948513\n",
      "Current iteration=125, loss=0.5207540811635221\n",
      "Current iteration=126, loss=0.5207019014847893\n",
      "Current iteration=127, loss=0.5206504652627016\n",
      "Current iteration=128, loss=0.5205997588393955\n",
      "Current iteration=129, loss=0.5205497688846602\n",
      "Current iteration=130, loss=0.5205004823859034\n",
      "Current iteration=131, loss=0.5204518866384896\n",
      "Current iteration=132, loss=0.5204039692364455\n",
      "Current iteration=133, loss=0.5203567180635058\n",
      "Current iteration=134, loss=0.5203101212844927\n",
      "Current iteration=135, loss=0.5202641673370094\n",
      "Current iteration=136, loss=0.5202188449234352\n",
      "Current iteration=137, loss=0.5201741430032095\n",
      "Current iteration=138, loss=0.5201300507853935\n",
      "Current iteration=139, loss=0.520086557721494\n",
      "Current iteration=140, loss=0.5200436534985432\n",
      "Current iteration=141, loss=0.5200013280324197\n",
      "Current iteration=142, loss=0.519959571461402\n",
      "Current iteration=143, loss=0.519918374139947\n",
      "Current iteration=144, loss=0.5198777266326796\n",
      "Current iteration=145, loss=0.5198376197085898\n",
      "Current iteration=146, loss=0.5197980443354249\n",
      "Current iteration=147, loss=0.519758991674273\n",
      "Current iteration=148, loss=0.5197204530743238\n",
      "Current iteration=149, loss=0.5196824200678095\n",
      "Current iteration=150, loss=0.5196448843651058\n",
      "Current iteration=151, loss=0.51960783785\n",
      "Current iteration=152, loss=0.5195712725751095\n",
      "Current iteration=153, loss=0.5195351807574512\n",
      "Current iteration=154, loss=0.5194995547741516\n",
      "Current iteration=155, loss=0.5194643871582959\n",
      "Current iteration=156, loss=0.5194296705949081\n",
      "Current iteration=157, loss=0.5193953979170588\n",
      "Current iteration=158, loss=0.5193615621020943\n",
      "Current iteration=159, loss=0.5193281562679853\n",
      "Current iteration=160, loss=0.5192951736697867\n",
      "Current iteration=161, loss=0.5192626076962089\n",
      "Current iteration=162, loss=0.519230451866291\n",
      "Current iteration=163, loss=0.5191986998261803\n",
      "Current iteration=164, loss=0.5191673453460038\n",
      "Current iteration=165, loss=0.5191363823168382\n",
      "Current iteration=166, loss=0.5191058047477687\n",
      "Current iteration=167, loss=0.5190756067630351\n",
      "Current iteration=168, loss=0.5190457825992641\n",
      "Current iteration=169, loss=0.5190163266027807\n",
      "Current iteration=170, loss=0.5189872332270002\n",
      "Current iteration=171, loss=0.5189584970298947\n",
      "Current iteration=172, loss=0.5189301126715339\n",
      "Current iteration=173, loss=0.5189020749116955\n",
      "Current iteration=174, loss=0.5188743786075452\n",
      "Current iteration=175, loss=0.5188470187113818\n",
      "Current iteration=176, loss=0.5188199902684458\n",
      "Current iteration=177, loss=0.5187932884147908\n",
      "Current iteration=178, loss=0.5187669083752128\n",
      "Current iteration=179, loss=0.5187408454612389\n",
      "Current iteration=180, loss=0.5187150950691699\n",
      "Current iteration=181, loss=0.5186896526781781\n",
      "Current iteration=182, loss=0.5186645138484557\n",
      "Current iteration=183, loss=0.5186396742194157\n",
      "Current iteration=184, loss=0.5186151295079389\n",
      "Current iteration=185, loss=0.5185908755066707\n",
      "Current iteration=186, loss=0.5185669080823614\n",
      "Current iteration=187, loss=0.5185432231742514\n",
      "Current iteration=188, loss=0.5185198167925007\n",
      "Current iteration=189, loss=0.5184966850166568\n",
      "Current iteration=190, loss=0.5184738239941655\n",
      "Current iteration=191, loss=0.5184512299389198\n",
      "Current iteration=192, loss=0.5184288991298448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=193, loss=0.5184068279095223\n",
      "Current iteration=194, loss=0.5183850126828481\n",
      "Current iteration=195, loss=0.5183634499157249\n",
      "Current iteration=196, loss=0.518342136133788\n",
      "Current iteration=197, loss=0.5183210679211635\n",
      "Current iteration=198, loss=0.5183002419192579\n",
      "Current iteration=199, loss=0.5182796548255784\n",
      "Current iteration=200, loss=0.5182593033925806\n",
      "Current iteration=201, loss=0.5182391844265478\n",
      "Current iteration=202, loss=0.5182192947864969\n",
      "Current iteration=203, loss=0.5181996313831098\n",
      "Current iteration=204, loss=0.5181801911776924\n",
      "Current iteration=205, loss=0.5181609711811591\n",
      "Current iteration=206, loss=0.5181419684530417\n",
      "Current iteration=207, loss=0.5181231801005216\n",
      "Current iteration=208, loss=0.5181046032774855\n",
      "Current iteration=209, loss=0.5180862351836055\n",
      "Current iteration=210, loss=0.5180680730634372\n",
      "Current iteration=211, loss=0.5180501142055434\n",
      "Current iteration=212, loss=0.518032355941636\n",
      "Current iteration=213, loss=0.5180147956457394\n",
      "Current iteration=214, loss=0.5179974307333719\n",
      "Current iteration=215, loss=0.5179802586607473\n",
      "Current iteration=216, loss=0.5179632769239952\n",
      "Current iteration=217, loss=0.5179464830583989\n",
      "Current iteration=218, loss=0.5179298746376512\n",
      "Current iteration=219, loss=0.5179134492731249\n",
      "Current iteration=220, loss=0.5178972046131651\n",
      "Current iteration=221, loss=0.5178811383423925\n",
      "Current iteration=222, loss=0.5178652481810239\n",
      "Current iteration=223, loss=0.51784953188421\n",
      "Current iteration=224, loss=0.5178339872413855\n",
      "Current iteration=225, loss=0.5178186120756352\n",
      "Current iteration=226, loss=0.5178034042430735\n",
      "Current iteration=227, loss=0.5177883616322376\n",
      "Current iteration=228, loss=0.517773482163495\n",
      "Current iteration=229, loss=0.5177587637884634\n",
      "Current iteration=230, loss=0.5177442044894418\n",
      "Current iteration=231, loss=0.5177298022788576\n",
      "Current iteration=232, loss=0.517715555198722\n",
      "Current iteration=233, loss=0.5177014613200999\n",
      "Current iteration=234, loss=0.5176875187425881\n",
      "Current iteration=235, loss=0.5176737255938092\n",
      "Current iteration=236, loss=0.5176600800289118\n",
      "Current iteration=237, loss=0.5176465802300843\n",
      "Current iteration=238, loss=0.5176332244060773\n",
      "Current iteration=239, loss=0.5176200107917374\n",
      "Current iteration=240, loss=0.5176069376475493\n",
      "Current iteration=241, loss=0.5175940032591899\n",
      "Current iteration=242, loss=0.5175812059370882\n",
      "Current iteration=243, loss=0.5175685440159982\n",
      "Current iteration=244, loss=0.5175560158545771\n",
      "Current iteration=245, loss=0.5175436198349755\n",
      "Current iteration=246, loss=0.5175313543624331\n",
      "Current iteration=247, loss=0.5175192178648846\n",
      "Current iteration=248, loss=0.5175072087925731\n",
      "Current iteration=249, loss=0.5174953256176706\n",
      "Current iteration=250, loss=0.5174835668339081\n",
      "Current iteration=251, loss=0.5174719309562108\n",
      "Current iteration=252, loss=0.5174604165203425\n",
      "Current iteration=253, loss=0.517449022082556\n",
      "Current iteration=254, loss=0.5174377462192513\n",
      "Current iteration=255, loss=0.5174265875266391\n",
      "Current iteration=256, loss=0.5174155446204144\n",
      "Current iteration=257, loss=0.5174046161354312\n",
      "Current iteration=258, loss=0.5173938007253895\n",
      "Current iteration=259, loss=0.5173830970625232\n",
      "Current iteration=260, loss=0.517372503837298\n",
      "Current iteration=261, loss=0.517362019758113\n",
      "Current iteration=262, loss=0.5173516435510092\n",
      "Current iteration=263, loss=0.5173413739593823\n",
      "Current iteration=264, loss=0.5173312097437028\n",
      "Current iteration=265, loss=0.5173211496812408\n",
      "Current iteration=266, loss=0.517311192565794\n",
      "Current iteration=267, loss=0.517301337207426\n",
      "Current iteration=268, loss=0.5172915824322027\n",
      "Current iteration=269, loss=0.5172819270819403\n",
      "Current iteration=270, loss=0.5172723700139533\n",
      "Current iteration=271, loss=0.51726291010081\n",
      "Current iteration=272, loss=0.5172535462300913\n",
      "Current iteration=273, loss=0.5172442773041541\n",
      "Current iteration=274, loss=0.5172351022398998\n",
      "Current iteration=275, loss=0.5172260199685463\n",
      "Current iteration=276, loss=0.5172170294354049\n",
      "Current iteration=277, loss=0.5172081295996611\n",
      "Current iteration=278, loss=0.5171993194341586\n",
      "Current iteration=279, loss=0.5171905979251893\n",
      "Current iteration=280, loss=0.5171819640722838\n",
      "Current iteration=281, loss=0.5171734168880104\n",
      "Current iteration=282, loss=0.517164955397773\n",
      "Current iteration=283, loss=0.5171565786396156\n",
      "Current iteration=284, loss=0.5171482856640299\n",
      "Current iteration=285, loss=0.5171400755337648\n",
      "Current iteration=286, loss=0.5171319473236421\n",
      "Current iteration=287, loss=0.517123900120372\n",
      "Current iteration=288, loss=0.5171159330223759\n",
      "Current iteration=289, loss=0.5171080451396083\n",
      "Current iteration=290, loss=0.517100235593385\n",
      "Current iteration=291, loss=0.5170925035162128\n",
      "Current iteration=292, loss=0.5170848480516225\n",
      "Current iteration=293, loss=0.5170772683540041\n",
      "Current iteration=294, loss=0.5170697635884478\n",
      "Current iteration=295, loss=0.5170623329305833\n",
      "Current iteration=296, loss=0.5170549755664255\n",
      "Current iteration=297, loss=0.5170476906922207\n",
      "Current iteration=298, loss=0.5170404775142977\n",
      "Current iteration=299, loss=0.5170333352489189\n",
      "Current iteration=300, loss=0.5170262631221358\n",
      "Current iteration=301, loss=0.5170192603696465\n",
      "Current iteration=302, loss=0.5170123262366546\n",
      "Current iteration=303, loss=0.5170054599777327\n",
      "Current iteration=304, loss=0.5169986608566861\n",
      "Current iteration=305, loss=0.5169919281464199\n",
      "Current iteration=306, loss=0.5169852611288077\n",
      "Current iteration=307, loss=0.516978659094564\n",
      "Current iteration=308, loss=0.5169721213431163\n",
      "Current iteration=309, loss=0.5169656471824821\n",
      "Current iteration=310, loss=0.5169592359291444\n",
      "Current iteration=311, loss=0.5169528869079347\n",
      "Current iteration=312, loss=0.5169465994519112\n",
      "Current iteration=313, loss=0.5169403729022456\n",
      "Current iteration=314, loss=0.5169342066081059\n",
      "Current iteration=315, loss=0.5169280999265458\n",
      "Current iteration=316, loss=0.5169220522223938\n",
      "Current iteration=317, loss=0.5169160628681434\n",
      "Current iteration=318, loss=0.5169101312438468\n",
      "Current iteration=319, loss=0.5169042567370095\n",
      "Current iteration=320, loss=0.5168984387424864\n",
      "Current iteration=321, loss=0.5168926766623804\n",
      "Current iteration=322, loss=0.5168869699059413\n",
      "Current iteration=323, loss=0.5168813178894681\n",
      "Current iteration=324, loss=0.5168757200362114\n",
      "Current iteration=325, loss=0.5168701757762774\n",
      "Current iteration=326, loss=0.5168646845465349\n",
      "Current iteration=327, loss=0.5168592457905222\n",
      "Current iteration=328, loss=0.5168538589583564\n",
      "Current iteration=329, loss=0.5168485235066432\n",
      "Current iteration=330, loss=0.5168432388983901\n",
      "Current iteration=331, loss=0.516838004602918\n",
      "Current iteration=332, loss=0.5168328200957767\n",
      "Current iteration=333, loss=0.5168276848586606\n",
      "Current iteration=334, loss=0.516822598379327\n",
      "Current iteration=335, loss=0.5168175601515123\n",
      "Current iteration=336, loss=0.5168125696748543\n",
      "Current iteration=337, loss=0.5168076264548124\n",
      "Current iteration=338, loss=0.5168027300025889\n",
      "Current iteration=339, loss=0.5167978798350544\n",
      "Current iteration=340, loss=0.5167930754746708\n",
      "Current iteration=341, loss=0.516788316449418\n",
      "Current iteration=342, loss=0.5167836022927208\n",
      "Current iteration=343, loss=0.5167789325433767\n",
      "Current iteration=344, loss=0.5167743067454861\n",
      "Current iteration=345, loss=0.5167697244483809\n",
      "Current iteration=346, loss=0.5167651852065572\n",
      "Current iteration=347, loss=0.5167606885796074\n",
      "Current iteration=348, loss=0.5167562341321533\n",
      "Current iteration=349, loss=0.5167518214337808\n",
      "Current iteration=350, loss=0.5167474500589753\n",
      "Current iteration=351, loss=0.5167431195870578\n",
      "Current iteration=352, loss=0.5167388296021228\n",
      "Current iteration=353, loss=0.5167345796929755\n",
      "Current iteration=354, loss=0.516730369453073\n",
      "Current iteration=355, loss=0.5167261984804621\n",
      "Current iteration=356, loss=0.5167220663777213\n",
      "Current iteration=357, loss=0.5167179727519031\n",
      "Current iteration=358, loss=0.5167139172144762\n",
      "Current iteration=359, loss=0.5167098993812693\n",
      "Current iteration=360, loss=0.5167059188724149\n",
      "Current iteration=361, loss=0.5167019753122958\n",
      "Current iteration=362, loss=0.5166980683294893\n",
      "Current iteration=363, loss=0.516694197556716\n",
      "Current iteration=364, loss=0.5166903626307856\n",
      "Current iteration=365, loss=0.5166865631925467\n",
      "Current iteration=366, loss=0.5166827988868351\n",
      "Current iteration=367, loss=0.5166790693624237\n",
      "Current iteration=368, loss=0.516675374271973\n",
      "Current iteration=369, loss=0.516671713271983\n",
      "Current iteration=370, loss=0.5166680860227438\n",
      "Current iteration=371, loss=0.5166644921882909\n",
      "Current iteration=372, loss=0.5166609314363547\n",
      "Current iteration=373, loss=0.5166574034383186\n",
      "Current iteration=374, loss=0.51665390786917\n",
      "Current iteration=375, loss=0.5166504444074587\n",
      "Current iteration=376, loss=0.5166470127352506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=377, loss=0.5166436125380856\n",
      "Current iteration=378, loss=0.516640243504934\n",
      "Current iteration=379, loss=0.5166369053281548\n",
      "Current iteration=380, loss=0.5166335977034541\n",
      "Current iteration=381, loss=0.5166303203298432\n",
      "Current iteration=382, loss=0.5166270729096\n",
      "Current iteration=383, loss=0.5166238551482264\n",
      "Current iteration=384, loss=0.5166206667544115\n",
      "Current iteration=385, loss=0.5166175074399921\n",
      "Current iteration=386, loss=0.5166143769199135\n",
      "Current iteration=387, loss=0.516611274912192\n",
      "Current iteration=388, loss=0.5166082011378796\n",
      "Current iteration=389, loss=0.5166051553210248\n",
      "Current iteration=390, loss=0.5166021371886379\n",
      "Current iteration=391, loss=0.5165991464706547\n",
      "Current iteration=392, loss=0.5165961828999017\n",
      "Current iteration=393, loss=0.5165932462120617\n",
      "Current iteration=394, loss=0.5165903361456382\n",
      "Current iteration=395, loss=0.5165874524419235\n",
      "Current iteration=396, loss=0.5165845948449638\n",
      "Current iteration=397, loss=0.5165817631015275\n",
      "Current iteration=398, loss=0.516578956961072\n",
      "Current iteration=399, loss=0.5165761761757122\n",
      "Current iteration=400, loss=0.516573420500189\n",
      "Current iteration=401, loss=0.5165706896918377\n",
      "Current iteration=402, loss=0.516567983510558\n",
      "Current iteration=403, loss=0.5165653017187829\n",
      "Current iteration=404, loss=0.5165626440814499\n",
      "Current iteration=405, loss=0.5165600103659704\n",
      "Current iteration=406, loss=0.5165574003422017\n",
      "Current iteration=407, loss=0.5165548137824171\n",
      "Current iteration=408, loss=0.5165522504612791\n",
      "Current iteration=409, loss=0.5165497101558097\n",
      "Current iteration=410, loss=0.5165471926453645\n",
      "Current iteration=411, loss=0.5165446977116047\n",
      "Current iteration=412, loss=0.5165422251384701\n",
      "Current iteration=413, loss=0.5165397747121531\n",
      "Current iteration=414, loss=0.5165373462210722\n",
      "Current iteration=415, loss=0.5165349394558463\n",
      "Current iteration=416, loss=0.5165325542092695\n",
      "Current iteration=417, loss=0.5165301902762856\n",
      "Current iteration=418, loss=0.5165278474539637\n",
      "Current iteration=419, loss=0.5165255255414734\n",
      "Current iteration=420, loss=0.5165232243400606\n",
      "Current iteration=421, loss=0.5165209436530243\n",
      "Current iteration=422, loss=0.5165186832856923\n",
      "Current iteration=423, loss=0.5165164430453978\n",
      "Current iteration=424, loss=0.5165142227414581\n",
      "Current iteration=425, loss=0.51651202218515\n",
      "Current iteration=426, loss=0.5165098411896887\n",
      "Current iteration=427, loss=0.516507679570205\n",
      "Current iteration=428, loss=0.5165055371437243\n",
      "Current iteration=429, loss=0.5165034137291442\n",
      "Current iteration=430, loss=0.5165013091472146\n",
      "Current iteration=431, loss=0.516499223220515\n",
      "Current iteration=432, loss=0.5164971557734352\n",
      "Current iteration=433, loss=0.5164951066321549\n",
      "Current iteration=434, loss=0.5164930756246224\n",
      "Current iteration=435, loss=0.5164910625805358\n",
      "Current iteration=436, loss=0.5164890673313228\n",
      "Current iteration=437, loss=0.5164870897101217\n",
      "Current iteration=438, loss=0.5164851295517616\n",
      "Current iteration=439, loss=0.5164831866927446\n",
      "Current iteration=440, loss=0.5164812609712256\n",
      "Current iteration=441, loss=0.5164793522269954\n",
      "Current iteration=442, loss=0.5164774603014609\n",
      "Current iteration=443, loss=0.5164755850376289\n",
      "Current iteration=444, loss=0.5164737262800871\n",
      "Current iteration=445, loss=0.5164718838749867\n",
      "Current iteration=446, loss=0.5164700576700255\n",
      "Current iteration=447, loss=0.5164682475144303\n",
      "Current iteration=448, loss=0.5164664532589408\n",
      "Current iteration=449, loss=0.5164646747557918\n",
      "Current iteration=450, loss=0.5164629118586976\n",
      "Current iteration=451, loss=0.5164611644228353\n",
      "Current iteration=452, loss=0.5164594323048292\n",
      "Current iteration=453, loss=0.5164577153627342\n",
      "Current iteration=454, loss=0.5164560134560205\n",
      "Current iteration=455, loss=0.5164543264455584\n",
      "Current iteration=456, loss=0.5164526541936025\n",
      "Current iteration=457, loss=0.5164509965637772\n",
      "Current iteration=458, loss=0.5164493534210608\n",
      "Current iteration=459, loss=0.5164477246317714\n",
      "Current iteration=460, loss=0.5164461100635525\n",
      "Current iteration=461, loss=0.5164445095853585\n",
      "Current iteration=462, loss=0.5164429230674398\n",
      "Current iteration=463, loss=0.51644135038133\n",
      "Current iteration=464, loss=0.5164397913998305\n",
      "Current iteration=465, loss=0.5164382459969984\n",
      "Current iteration=466, loss=0.5164367140481317\n",
      "Current iteration=467, loss=0.5164351954297567\n",
      "Current iteration=468, loss=0.5164336900196139\n",
      "Current iteration=469, loss=0.5164321976966463\n",
      "Current iteration=470, loss=0.5164307183409851\n",
      "Current iteration=471, loss=0.5164292518339377\n",
      "Current iteration=472, loss=0.516427798057975\n",
      "Current iteration=473, loss=0.5164263568967191\n",
      "Current iteration=474, loss=0.5164249282349305\n",
      "Current iteration=475, loss=0.5164235119584962\n",
      "Current iteration=476, loss=0.516422107954418\n",
      "Current iteration=477, loss=0.5164207161108003\n",
      "Current iteration=478, loss=0.5164193363168383\n",
      "Current iteration=479, loss=0.5164179684628067\n",
      "Current iteration=480, loss=0.5164166124400482\n",
      "Current iteration=481, loss=0.5164152681409616\n",
      "Current iteration=482, loss=0.5164139354589914\n",
      "Current iteration=483, loss=0.5164126142886167\n",
      "Current iteration=484, loss=0.5164113045253396\n",
      "Current iteration=485, loss=0.5164100060656749\n",
      "Current iteration=486, loss=0.5164087188071393\n",
      "Current iteration=487, loss=0.5164074426482415\n",
      "Current iteration=488, loss=0.5164061774884701\n",
      "Current iteration=489, loss=0.5164049232282851\n",
      "Current iteration=490, loss=0.516403679769107\n",
      "Current iteration=491, loss=0.5164024470133063\n",
      "Current iteration=492, loss=0.5164012248641945\n",
      "Current iteration=493, loss=0.5164000132260133\n",
      "Current iteration=494, loss=0.5163988120039257\n",
      "Current iteration=495, loss=0.5163976211040056\n",
      "Current iteration=496, loss=0.5163964404332287\n",
      "Current iteration=497, loss=0.5163952698994642\n",
      "Current iteration=498, loss=0.5163941094114627\n",
      "Current iteration=499, loss=0.5163929588788503\n",
      "Current iteration=500, loss=0.516391818212117\n",
      "Current iteration=501, loss=0.5163906873226096\n",
      "Current iteration=502, loss=0.5163895661225211\n",
      "Current iteration=503, loss=0.516388454524884\n",
      "Current iteration=504, loss=0.516387352443559\n",
      "Current iteration=505, loss=0.5163862597932289\n",
      "Current iteration=506, loss=0.516385176489389\n",
      "Current iteration=507, loss=0.5163841024483389\n",
      "Current iteration=508, loss=0.5163830375871739\n",
      "Current iteration=509, loss=0.5163819818237769\n",
      "Current iteration=510, loss=0.5163809350768118\n",
      "Current iteration=511, loss=0.5163798972657124\n",
      "Current iteration=512, loss=0.516378868310678\n",
      "Current iteration=513, loss=0.5163778481326631\n",
      "Current iteration=514, loss=0.5163768366533701\n",
      "Current iteration=515, loss=0.5163758337952428\n",
      "Current iteration=516, loss=0.5163748394814578\n",
      "Current iteration=517, loss=0.5163738536359174\n",
      "Current iteration=518, loss=0.5163728761832418\n",
      "Current iteration=519, loss=0.5163719070487622\n",
      "Current iteration=520, loss=0.5163709461585139\n",
      "Current iteration=521, loss=0.5163699934392287\n",
      "Current iteration=522, loss=0.5163690488183271\n",
      "Current iteration=523, loss=0.5163681122239129\n",
      "Current iteration=524, loss=0.5163671835847651\n",
      "Current iteration=525, loss=0.5163662628303318\n",
      "Current iteration=526, loss=0.5163653498907228\n",
      "Current iteration=527, loss=0.5163644446967032\n",
      "Current iteration=528, loss=0.5163635471796869\n",
      "Current iteration=529, loss=0.5163626572717301\n",
      "Current iteration=530, loss=0.5163617749055246\n",
      "Current iteration=531, loss=0.5163609000143913\n",
      "Current iteration=532, loss=0.5163600325322744\n",
      "Current iteration=533, loss=0.5163591723937349\n",
      "Current iteration=534, loss=0.516358319533944\n",
      "Current iteration=535, loss=0.5163574738886777\n",
      "Current iteration=536, loss=0.5163566353943102\n",
      "Current iteration=537, loss=0.5163558039878084\n",
      "Current iteration=538, loss=0.516354979606725\n",
      "Current iteration=539, loss=0.5163541621891944\n",
      "Current iteration=540, loss=0.5163533516739248\n",
      "Current iteration=541, loss=0.5163525480001943\n",
      "Current iteration=542, loss=0.5163517511078441\n",
      "Current iteration=543, loss=0.5163509609372733\n",
      "Current iteration=544, loss=0.5163501774294333\n",
      "Current iteration=545, loss=0.5163494005258229\n",
      "Current iteration=546, loss=0.516348630168481\n",
      "Current iteration=547, loss=0.5163478662999841\n",
      "Current iteration=548, loss=0.5163471088634383\n",
      "Current iteration=549, loss=0.5163463578024757\n",
      "Current iteration=550, loss=0.5163456130612489\n",
      "Current iteration=551, loss=0.5163448745844251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=552, loss=0.5163441423171821\n",
      "Current iteration=553, loss=0.5163434162052025\n",
      "Current iteration=554, loss=0.5163426961946688\n",
      "Current iteration=555, loss=0.5163419822322594\n",
      "Current iteration=556, loss=0.516341274265142\n",
      "Current iteration=557, loss=0.5163405722409705\n",
      "Current iteration=558, loss=0.5163398761078796\n",
      "Current iteration=559, loss=0.5163391858144794\n",
      "Current iteration=560, loss=0.5163385013098518\n",
      "Current iteration=561, loss=0.516337822543545\n",
      "Current iteration=562, loss=0.5163371494655702\n",
      "Current iteration=563, loss=0.5163364820263954\n",
      "Current iteration=564, loss=0.5163358201769421\n",
      "Current iteration=565, loss=0.5163351638685807\n",
      "Current iteration=566, loss=0.5163345130531258\n",
      "Current iteration=567, loss=0.5163338676828324\n",
      "Current iteration=568, loss=0.5163332277103906\n",
      "Current iteration=569, loss=0.5163325930889225\n",
      "Current iteration=570, loss=0.5163319637719779\n",
      "Current iteration=571, loss=0.5163313397135287\n",
      "Current iteration=572, loss=0.5163307208679669\n",
      "Current iteration=573, loss=0.5163301071900996\n",
      "Current iteration=574, loss=0.5163294986351441\n",
      "Current iteration=575, loss=0.5163288951587254\n",
      "Current iteration=576, loss=0.5163282967168711\n",
      "Current iteration=577, loss=0.5163277032660087\n",
      "Current iteration=578, loss=0.5163271147629601\n",
      "Current iteration=579, loss=0.5163265311649397\n",
      "Current iteration=580, loss=0.5163259524295492\n",
      "Current iteration=581, loss=0.5163253785147741\n",
      "Current iteration=582, loss=0.516324809378981\n",
      "Current iteration=583, loss=0.5163242449809123\n",
      "Current iteration=584, loss=0.5163236852796836\n",
      "Current iteration=585, loss=0.5163231302347809\n",
      "Current iteration=586, loss=0.5163225798060552\n",
      "Current iteration=587, loss=0.5163220339537199\n",
      "Current iteration=588, loss=0.516321492638348\n",
      "Current iteration=589, loss=0.5163209558208675\n",
      "Current iteration=590, loss=0.5163204234625587\n",
      "Current iteration=591, loss=0.5163198955250509\n",
      "Current iteration=592, loss=0.5163193719703177\n",
      "Current iteration=593, loss=0.5163188527606767\n",
      "Current iteration=594, loss=0.5163183378587827\n",
      "Current iteration=595, loss=0.5163178272276271\n",
      "Current iteration=596, loss=0.516317320830533\n",
      "Current iteration=597, loss=0.5163168186311538\n",
      "Current iteration=598, loss=0.5163163205934678\n",
      "Current iteration=599, loss=0.5163158266817773\n",
      "Current iteration=600, loss=0.5163153368607042\n",
      "Current iteration=601, loss=0.5163148510951876\n",
      "Current iteration=602, loss=0.5163143693504797\n",
      "Current iteration=603, loss=0.516313891592145\n",
      "Current iteration=604, loss=0.5163134177860548\n",
      "Current iteration=605, loss=0.5163129478983861\n",
      "Current iteration=606, loss=0.5163124818956183\n",
      "Current iteration=607, loss=0.5163120197445296\n",
      "Current iteration=608, loss=0.5163115614121953\n",
      "Current iteration=609, loss=0.5163111068659836\n",
      "Current iteration=610, loss=0.5163106560735548\n",
      "Current iteration=611, loss=0.5163102090028566\n",
      "Current iteration=612, loss=0.5163097656221227\n",
      "Current iteration=613, loss=0.5163093258998691\n",
      "Current iteration=614, loss=0.5163088898048926\n",
      "Current iteration=615, loss=0.5163084573062667\n",
      "Current iteration=616, loss=0.5163080283733409\n",
      "Current iteration=617, loss=0.5163076029757364\n",
      "Current iteration=618, loss=0.5163071810833442\n",
      "Current iteration=619, loss=0.5163067626663225\n",
      "Current iteration=620, loss=0.5163063476950945\n",
      "Current iteration=621, loss=0.5163059361403461\n",
      "Current iteration=622, loss=0.5163055279730219\n",
      "Current iteration=623, loss=0.5163051231643252\n",
      "Current iteration=624, loss=0.5163047216857131\n",
      "Current iteration=625, loss=0.516304323508896\n",
      "Current iteration=626, loss=0.5163039286058348\n",
      "Current iteration=627, loss=0.5163035369487379\n",
      "Current iteration=628, loss=0.5163031485100594\n",
      "Current iteration=629, loss=0.5163027632624967\n",
      "Current iteration=630, loss=0.5163023811789885\n",
      "Current iteration=631, loss=0.5163020022327122\n",
      "Current iteration=632, loss=0.5163016263970813\n",
      "Current iteration=633, loss=0.5163012536457449\n",
      "Current iteration=634, loss=0.5163008839525834\n",
      "Current iteration=635, loss=0.5163005172917076\n",
      "Current iteration=636, loss=0.5163001536374561\n",
      "Current iteration=637, loss=0.5162997929643938\n",
      "Current iteration=638, loss=0.5162994352473083\n",
      "Current iteration=639, loss=0.5162990804612105\n",
      "Current iteration=640, loss=0.5162987285813294\n",
      "Current iteration=641, loss=0.5162983795831125\n",
      "Current iteration=642, loss=0.5162980334422229\n",
      "Current iteration=643, loss=0.5162976901345365\n",
      "Current iteration=644, loss=0.5162973496361418\n",
      "Current iteration=645, loss=0.5162970119233367\n",
      "Current iteration=646, loss=0.516296676972626\n",
      "Current iteration=647, loss=0.516296344760722\n",
      "Current iteration=648, loss=0.5162960152645397\n",
      "Current iteration=649, loss=0.5162956884611963\n",
      "Current iteration=650, loss=0.5162953643280097\n",
      "Current iteration=651, loss=0.516295042842496\n",
      "Current iteration=652, loss=0.5162947239823674\n",
      "Current iteration=653, loss=0.5162944077255316\n",
      "Current iteration=654, loss=0.5162940940500887\n",
      "Current iteration=655, loss=0.5162937829343303\n",
      "Current iteration=656, loss=0.5162934743567374\n",
      "Current iteration=657, loss=0.5162931682959786\n",
      "Current iteration=658, loss=0.5162928647309086\n",
      "Current iteration=659, loss=0.5162925636405665\n",
      "Current iteration=660, loss=0.5162922650041739\n",
      "Current iteration=661, loss=0.516291968801133\n",
      "Current iteration=662, loss=0.5162916750110261\n",
      "Current iteration=663, loss=0.5162913836136124\n",
      "Current iteration=664, loss=0.5162910945888278\n",
      "Current iteration=665, loss=0.516290807916782\n",
      "Current iteration=666, loss=0.5162905235777584\n",
      "Current iteration=667, loss=0.5162902415522104\n",
      "Current iteration=668, loss=0.516289961820762\n",
      "Current iteration=669, loss=0.5162896843642059\n",
      "Current iteration=670, loss=0.5162894091635002\n",
      "Current iteration=671, loss=0.516289136199769\n",
      "Current iteration=672, loss=0.5162888654542996\n",
      "Current iteration=673, loss=0.5162885969085419\n",
      "Current iteration=674, loss=0.5162883305441061\n",
      "Current iteration=675, loss=0.5162880663427614\n",
      "Current iteration=676, loss=0.5162878042864358\n",
      "Current iteration=677, loss=0.5162875443572126\n",
      "Current iteration=678, loss=0.5162872865373306\n",
      "Current iteration=679, loss=0.5162870308091818\n",
      "Current iteration=680, loss=0.516286777155311\n",
      "Current iteration=681, loss=0.5162865255584129\n",
      "Current iteration=682, loss=0.5162862760013324\n",
      "Current iteration=683, loss=0.5162860284670618\n",
      "Current iteration=684, loss=0.5162857829387408\n",
      "Current iteration=685, loss=0.5162855393996543\n",
      "Current iteration=686, loss=0.516285297833231\n",
      "Current iteration=687, loss=0.5162850582230424\n",
      "Current iteration=688, loss=0.5162848205528021\n",
      "Current iteration=689, loss=0.5162845848063637\n",
      "Current iteration=690, loss=0.5162843509677195\n",
      "Current iteration=691, loss=0.5162841190209998\n",
      "Current iteration=692, loss=0.5162838889504715\n",
      "Current iteration=693, loss=0.5162836607405363\n",
      "Current iteration=694, loss=0.5162834343757309\n",
      "Current iteration=695, loss=0.5162832098407236\n",
      "Current iteration=696, loss=0.5162829871203154\n",
      "Current iteration=697, loss=0.5162827661994376\n",
      "Current iteration=698, loss=0.5162825470631505\n",
      "Current iteration=699, loss=0.5162823296966427\n",
      "Current iteration=700, loss=0.5162821140852301\n",
      "Current iteration=701, loss=0.5162819002143539\n",
      "Current iteration=702, loss=0.516281688069581\n",
      "Current iteration=703, loss=0.5162814776366009\n",
      "Current iteration=704, loss=0.5162812689012265\n",
      "Current iteration=705, loss=0.5162810618493914\n",
      "Current iteration=706, loss=0.5162808564671503\n",
      "Current iteration=707, loss=0.5162806527406768\n",
      "Current iteration=708, loss=0.5162804506562627\n",
      "Current iteration=709, loss=0.516280250200317\n",
      "Current iteration=710, loss=0.5162800513593654\n",
      "Current iteration=711, loss=0.5162798541200478\n",
      "Current iteration=712, loss=0.5162796584691186\n",
      "Current iteration=713, loss=0.5162794643934454\n",
      "Current iteration=714, loss=0.5162792718800078\n",
      "Current iteration=715, loss=0.5162790809158964\n",
      "Current iteration=716, loss=0.5162788914883119\n",
      "Current iteration=717, loss=0.5162787035845641\n",
      "Current iteration=718, loss=0.5162785171920707\n",
      "Current iteration=719, loss=0.5162783322983571\n",
      "Current iteration=720, loss=0.5162781488910549\n",
      "Current iteration=721, loss=0.5162779669579005\n",
      "Current iteration=722, loss=0.5162777864867346\n",
      "Current iteration=723, loss=0.5162776074655021\n",
      "Current iteration=724, loss=0.51627742988225\n",
      "Current iteration=725, loss=0.5162772537251267\n",
      "Current iteration=726, loss=0.5162770789823813\n",
      "Current iteration=727, loss=0.5162769056423634\n",
      "Current iteration=728, loss=0.5162767336935208\n",
      "Current iteration=729, loss=0.5162765631243998\n",
      "Current iteration=730, loss=0.5162763939236436\n",
      "Current iteration=731, loss=0.516276226079992\n",
      "Current iteration=732, loss=0.5162760595822803\n",
      "Current iteration=733, loss=0.5162758944194383\n",
      "Current iteration=734, loss=0.51627573058049\n",
      "Current iteration=735, loss=0.5162755680545519\n",
      "Current iteration=736, loss=0.5162754068308327\n",
      "Current iteration=737, loss=0.516275246898633\n",
      "Current iteration=738, loss=0.5162750882473435\n",
      "Current iteration=739, loss=0.5162749308664446\n",
      "Current iteration=740, loss=0.5162747747455064\n",
      "Current iteration=741, loss=0.5162746198741865\n",
      "Current iteration=742, loss=0.5162744662422303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=743, loss=0.5162743138394695\n",
      "Current iteration=744, loss=0.516274162655822\n",
      "Current iteration=745, loss=0.516274012681291\n",
      "Current iteration=746, loss=0.5162738639059635\n",
      "Current iteration=747, loss=0.5162737163200108\n",
      "Current iteration=748, loss=0.5162735699136871\n",
      "Current iteration=749, loss=0.5162734246773282\n",
      "Current iteration=750, loss=0.5162732806013524\n",
      "Current iteration=751, loss=0.5162731376762574\n",
      "Current iteration=752, loss=0.5162729958926224\n",
      "Current iteration=753, loss=0.5162728552411048\n",
      "Current iteration=754, loss=0.5162727157124417\n",
      "Current iteration=755, loss=0.5162725772974477\n",
      "Current iteration=756, loss=0.5162724399870143\n",
      "Current iteration=757, loss=0.5162723037721104\n",
      "Current iteration=758, loss=0.5162721686437807\n",
      "Current iteration=759, loss=0.5162720345931447\n",
      "Current iteration=760, loss=0.5162719016113974\n",
      "Current iteration=761, loss=0.5162717696898066\n",
      "Current iteration=762, loss=0.5162716388197146\n",
      "Current iteration=763, loss=0.516271508992536\n",
      "Current iteration=764, loss=0.5162713801997576\n",
      "Current iteration=765, loss=0.5162712524329375\n",
      "Current iteration=766, loss=0.5162711256837045\n",
      "Current iteration=767, loss=0.5162709999437579\n",
      "Current iteration=768, loss=0.5162708752048667\n",
      "Current iteration=769, loss=0.516270751458869\n",
      "Current iteration=770, loss=0.5162706286976705\n",
      "Current iteration=771, loss=0.5162705069132459\n",
      "Current iteration=772, loss=0.516270386097636\n",
      "Current iteration=773, loss=0.516270266242949\n",
      "Current iteration=774, loss=0.5162701473413591\n",
      "Current iteration=775, loss=0.5162700293851059\n",
      "Current iteration=776, loss=0.5162699123664937\n",
      "Current iteration=777, loss=0.5162697962778916\n",
      "Current iteration=778, loss=0.5162696811117321\n",
      "Current iteration=779, loss=0.5162695668605115\n",
      "Current iteration=780, loss=0.5162694535167882\n",
      "Current iteration=781, loss=0.5162693410731829\n",
      "Current iteration=782, loss=0.5162692295223785\n",
      "Current iteration=783, loss=0.5162691188571183\n",
      "Current iteration=784, loss=0.5162690090702068\n",
      "Current iteration=785, loss=0.5162689001545083\n",
      "Current iteration=786, loss=0.5162687921029463\n",
      "Current iteration=787, loss=0.5162686849085036\n",
      "Current iteration=788, loss=0.5162685785642219\n",
      "Current iteration=789, loss=0.5162684730632002\n",
      "Current iteration=790, loss=0.5162683683985956\n",
      "Current iteration=791, loss=0.5162682645636218\n",
      "Current iteration=792, loss=0.5162681615515494\n",
      "Current iteration=793, loss=0.5162680593557051\n",
      "Current iteration=794, loss=0.5162679579694703\n",
      "Current iteration=795, loss=0.5162678573862826\n",
      "Current iteration=796, loss=0.5162677575996334\n",
      "Current iteration=797, loss=0.5162676586030689\n",
      "Current iteration=798, loss=0.5162675603901884\n",
      "Current iteration=799, loss=0.5162674629546451\n",
      "Current iteration=800, loss=0.5162673662901438\n",
      "Current iteration=801, loss=0.5162672703904427\n",
      "Current iteration=802, loss=0.5162671752493517\n",
      "Current iteration=803, loss=0.5162670808607318\n",
      "Current iteration=804, loss=0.5162669872184946\n",
      "Current iteration=805, loss=0.5162668943166032\n",
      "Current iteration=806, loss=0.5162668021490703\n",
      "Current iteration=807, loss=0.5162667107099582\n",
      "Current iteration=808, loss=0.5162666199933785\n",
      "Current iteration=809, loss=0.5162665299934919\n",
      "Current iteration=810, loss=0.5162664407045068\n",
      "Current iteration=811, loss=0.5162663521206805\n",
      "Current iteration=812, loss=0.5162662642363174\n",
      "Current iteration=813, loss=0.516266177045769\n",
      "Current iteration=814, loss=0.5162660905434338\n",
      "Current iteration=815, loss=0.5162660047237569\n",
      "Current iteration=816, loss=0.5162659195812284\n",
      "Current iteration=817, loss=0.5162658351103855\n",
      "Current iteration=818, loss=0.5162657513058091\n",
      "Current iteration=819, loss=0.516265668162126\n",
      "Current iteration=820, loss=0.516265585674007\n",
      "Current iteration=821, loss=0.5162655038361672\n",
      "Current iteration=822, loss=0.5162654226433644\n",
      "Current iteration=823, loss=0.5162653420904013\n",
      "Current iteration=824, loss=0.5162652621721223\n",
      "Current iteration=825, loss=0.516265182883415\n",
      "Current iteration=826, loss=0.516265104219209\n",
      "Current iteration=827, loss=0.5162650261744756\n",
      "Current iteration=828, loss=0.5162649487442279\n",
      "Current iteration=829, loss=0.5162648719235201\n",
      "Current iteration=830, loss=0.516264795707447\n",
      "Current iteration=831, loss=0.5162647200911438\n",
      "Current iteration=832, loss=0.5162646450697863\n",
      "Current iteration=833, loss=0.5162645706385894\n",
      "Current iteration=834, loss=0.5162644967928082\n",
      "Current iteration=835, loss=0.5162644235277359\n",
      "Current iteration=836, loss=0.5162643508387057\n",
      "Current iteration=837, loss=0.5162642787210879\n",
      "Current iteration=838, loss=0.5162642071702918\n",
      "Current iteration=839, loss=0.5162641361817643\n",
      "Current iteration=840, loss=0.5162640657509898\n",
      "Current iteration=841, loss=0.5162639958734896\n",
      "Current iteration=842, loss=0.5162639265448218\n",
      "Current iteration=843, loss=0.5162638577605819\n",
      "Current iteration=844, loss=0.5162637895164006\n",
      "Current iteration=845, loss=0.5162637218079446\n",
      "Current iteration=846, loss=0.5162636546309163\n",
      "Current iteration=847, loss=0.5162635879810542\n",
      "Current iteration=848, loss=0.5162635218541307\n",
      "Current iteration=849, loss=0.5162634562459535\n",
      "Current iteration=850, loss=0.5162633911523645\n",
      "Current iteration=851, loss=0.51626332656924\n",
      "Current iteration=852, loss=0.5162632624924898\n",
      "Current iteration=853, loss=0.5162631989180574\n",
      "Current iteration=854, loss=0.5162631358419193\n",
      "Current iteration=855, loss=0.5162630732600858\n",
      "Current iteration=856, loss=0.5162630111685991\n",
      "Current iteration=857, loss=0.5162629495635339\n",
      "Current iteration=858, loss=0.5162628884409978\n",
      "Current iteration=859, loss=0.5162628277971291\n",
      "Current iteration=860, loss=0.5162627676280989\n",
      "Current iteration=861, loss=0.5162627079301089\n",
      "Current iteration=862, loss=0.5162626486993921\n",
      "Current iteration=863, loss=0.5162625899322124\n",
      "Current iteration=864, loss=0.5162625316248644\n",
      "Current iteration=865, loss=0.5162624737736725\n",
      "Current iteration=866, loss=0.5162624163749919\n",
      "Current iteration=867, loss=0.5162623594252068\n",
      "Current iteration=868, loss=0.5162623029207317\n",
      "Current iteration=869, loss=0.51626224685801\n",
      "Current iteration=870, loss=0.5162621912335142\n",
      "Current iteration=871, loss=0.516262136043746\n",
      "Current iteration=872, loss=0.5162620812852347\n",
      "Current iteration=873, loss=0.5162620269545389\n",
      "Current iteration=874, loss=0.516261973048245\n",
      "Current iteration=875, loss=0.5162619195629673\n",
      "Current iteration=876, loss=0.5162618664953478\n",
      "Current iteration=877, loss=0.5162618138420552\n",
      "Current iteration=878, loss=0.5162617615997864\n",
      "Current iteration=879, loss=0.516261709765265\n",
      "Current iteration=880, loss=0.5162616583352407\n",
      "Current iteration=881, loss=0.5162616073064903\n",
      "Current iteration=882, loss=0.5162615566758165\n",
      "Current iteration=883, loss=0.5162615064400483\n",
      "Current iteration=884, loss=0.5162614565960406\n",
      "Current iteration=885, loss=0.5162614071406738\n",
      "Current iteration=886, loss=0.5162613580708535\n",
      "Current iteration=887, loss=0.5162613093835104\n",
      "Current iteration=888, loss=0.5162612610756011\n",
      "Current iteration=889, loss=0.5162612131441059\n",
      "Current iteration=890, loss=0.5162611655860304\n",
      "Current iteration=891, loss=0.5162611183984038\n",
      "Current iteration=892, loss=0.5162610715782805\n",
      "Current iteration=893, loss=0.5162610251227375\n",
      "Current iteration=894, loss=0.5162609790288771\n",
      "Current iteration=895, loss=0.516260933293824\n",
      "Current iteration=896, loss=0.5162608879147268\n",
      "Current iteration=897, loss=0.5162608428887568\n",
      "Current iteration=898, loss=0.516260798213109\n",
      "Current iteration=899, loss=0.5162607538850004\n",
      "Current iteration=900, loss=0.5162607099016714\n",
      "Current iteration=901, loss=0.516260666260384\n",
      "Current iteration=902, loss=0.5162606229584229\n",
      "Current iteration=903, loss=0.5162605799930947\n",
      "Current iteration=904, loss=0.5162605373617281\n",
      "Current iteration=905, loss=0.5162604950616727\n",
      "Current iteration=906, loss=0.5162604530903007\n",
      "Current iteration=907, loss=0.5162604114450048\n",
      "Current iteration=908, loss=0.5162603701231991\n",
      "Current iteration=909, loss=0.5162603291223187\n",
      "Current iteration=910, loss=0.5162602884398192\n",
      "Current iteration=911, loss=0.5162602480731774\n",
      "Current iteration=912, loss=0.5162602080198898\n",
      "Current iteration=913, loss=0.5162601682774736\n",
      "Current iteration=914, loss=0.5162601288434663\n",
      "Current iteration=915, loss=0.5162600897154247\n",
      "Current iteration=916, loss=0.5162600508909262\n",
      "Current iteration=917, loss=0.5162600123675668\n",
      "Current iteration=918, loss=0.5162599741429629\n",
      "Current iteration=919, loss=0.5162599362147496\n",
      "Current iteration=920, loss=0.5162598985805815\n",
      "Current iteration=921, loss=0.5162598612381316\n",
      "Current iteration=922, loss=0.5162598241850924\n",
      "Current iteration=923, loss=0.5162597874191746\n",
      "Current iteration=924, loss=0.5162597509381076\n",
      "Current iteration=925, loss=0.5162597147396388\n",
      "Current iteration=926, loss=0.5162596788215343\n",
      "Current iteration=927, loss=0.5162596431815779\n",
      "Current iteration=928, loss=0.5162596078175714\n",
      "Current iteration=929, loss=0.5162595727273345\n",
      "Current iteration=930, loss=0.5162595379087039\n",
      "Current iteration=931, loss=0.5162595033595346\n",
      "Current iteration=932, loss=0.5162594690776984\n",
      "Current iteration=933, loss=0.5162594350610843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=934, loss=0.5162594013075984\n",
      "Current iteration=935, loss=0.5162593678151634\n",
      "Current iteration=936, loss=0.5162593345817194\n",
      "Current iteration=937, loss=0.5162593016052225\n",
      "Current iteration=938, loss=0.5162592688836454\n",
      "Current iteration=939, loss=0.5162592364149776\n",
      "Current iteration=940, loss=0.5162592041972243\n",
      "Current iteration=941, loss=0.5162591722284063\n",
      "Current iteration=942, loss=0.5162591405065615\n",
      "Current iteration=943, loss=0.5162591090297428\n",
      "Current iteration=944, loss=0.5162590777960193\n",
      "Current iteration=945, loss=0.5162590468034747\n",
      "Current iteration=946, loss=0.516259016050209\n",
      "Current iteration=947, loss=0.5162589855343375\n",
      "Current iteration=948, loss=0.5162589552539902\n",
      "Current iteration=949, loss=0.5162589252073122\n",
      "Current iteration=950, loss=0.5162588953924636\n",
      "Current iteration=951, loss=0.5162588658076196\n",
      "Current iteration=952, loss=0.5162588364509695\n",
      "Current iteration=953, loss=0.5162588073207177\n",
      "Current iteration=954, loss=0.5162587784150823\n",
      "Current iteration=955, loss=0.5162587497322969\n",
      "Current iteration=956, loss=0.516258721270608\n",
      "Current iteration=957, loss=0.5162586930282768\n",
      "Current iteration=958, loss=0.5162586650035784\n",
      "Current iteration=959, loss=0.5162586371948018\n",
      "Current iteration=960, loss=0.5162586096002499\n",
      "Current iteration=961, loss=0.5162585822182387\n",
      "Current iteration=962, loss=0.516258555047098\n",
      "Current iteration=963, loss=0.5162585280851711\n",
      "Current iteration=964, loss=0.5162585013308146\n",
      "Current iteration=965, loss=0.516258474782398\n",
      "Current iteration=966, loss=0.5162584484383039\n",
      "Current iteration=967, loss=0.516258422296928\n",
      "Current iteration=968, loss=0.5162583963566794\n",
      "Current iteration=969, loss=0.5162583706159787\n",
      "Current iteration=970, loss=0.5162583450732601\n",
      "Current iteration=971, loss=0.5162583197269702\n",
      "Current iteration=972, loss=0.5162582945755677\n",
      "Current iteration=973, loss=0.5162582696175241\n",
      "Current iteration=974, loss=0.5162582448513229\n",
      "Current iteration=975, loss=0.5162582202754595\n",
      "Current iteration=976, loss=0.5162581958884417\n",
      "Current iteration=977, loss=0.516258171688789\n",
      "Current iteration=978, loss=0.5162581476750331\n",
      "Current iteration=979, loss=0.5162581238457171\n",
      "Current iteration=980, loss=0.5162581001993958\n",
      "Current iteration=981, loss=0.5162580767346355\n",
      "Current iteration=982, loss=0.5162580534500143\n",
      "Current iteration=983, loss=0.5162580303441212\n",
      "Current iteration=984, loss=0.5162580074155568\n",
      "Current iteration=985, loss=0.5162579846629329\n",
      "Current iteration=986, loss=0.516257962084872\n",
      "Current iteration=987, loss=0.5162579396800082\n",
      "Current iteration=988, loss=0.5162579174469859\n",
      "Current iteration=989, loss=0.5162578953844607\n",
      "Current iteration=990, loss=0.5162578734910989\n",
      "Current iteration=991, loss=0.5162578517655771\n",
      "Current iteration=992, loss=0.516257830206583\n",
      "Current iteration=993, loss=0.5162578088128145\n",
      "Current iteration=994, loss=0.5162577875829799\n",
      "Current iteration=995, loss=0.5162577665157974\n",
      "Current iteration=996, loss=0.5162577456099965\n",
      "Current iteration=997, loss=0.5162577248643154\n",
      "Current iteration=998, loss=0.5162577042775037\n",
      "Current iteration=999, loss=0.5162576838483202\n",
      "Current iteration=1000, loss=0.5162576635755333\n",
      "Current iteration=1001, loss=0.5162576434579222\n",
      "Current iteration=1002, loss=0.5162576234942752\n",
      "Current iteration=1003, loss=0.5162576036833904\n",
      "Current iteration=1004, loss=0.5162575840240751\n",
      "Current iteration=1005, loss=0.5162575645151464\n",
      "Current iteration=1006, loss=0.5162575451554312\n",
      "Current iteration=1007, loss=0.5162575259437652\n",
      "Current iteration=1008, loss=0.5162575068789933\n",
      "Current iteration=1009, loss=0.5162574879599701\n",
      "Current iteration=1010, loss=0.5162574691855589\n",
      "Current iteration=1011, loss=0.5162574505546321\n",
      "Current iteration=1012, loss=0.5162574320660712\n",
      "Current iteration=1013, loss=0.5162574137187667\n",
      "Current iteration=1014, loss=0.5162573955116173\n",
      "Current iteration=1015, loss=0.5162573774435313\n",
      "Current iteration=1016, loss=0.516257359513425\n",
      "Current iteration=1017, loss=0.5162573417202241\n",
      "Current iteration=1018, loss=0.5162573240628616\n",
      "Current iteration=1019, loss=0.5162573065402801\n",
      "Current iteration=1020, loss=0.5162572891514301\n",
      "Current iteration=1021, loss=0.5162572718952707\n",
      "Current iteration=1022, loss=0.5162572547707688\n",
      "Current iteration=1023, loss=0.5162572377768997\n",
      "Current iteration=1024, loss=0.5162572209126473\n",
      "Current iteration=1025, loss=0.5162572041770029\n",
      "Current iteration=1026, loss=0.5162571875689662\n",
      "Current iteration=1027, loss=0.5162571710875445\n",
      "Current iteration=1028, loss=0.5162571547317538\n",
      "Current iteration=1029, loss=0.5162571385006166\n",
      "Current iteration=1030, loss=0.5162571223931641\n",
      "Current iteration=1031, loss=0.516257106408435\n",
      "Current iteration=1032, loss=0.5162570905454754\n",
      "Current iteration=1033, loss=0.5162570748033393\n",
      "Current iteration=1034, loss=0.5162570591810877\n",
      "Current iteration=1035, loss=0.5162570436777894\n",
      "Current iteration=1036, loss=0.5162570282925206\n",
      "Current iteration=1037, loss=0.516257013024365\n",
      "Current iteration=1038, loss=0.5162569978724127\n",
      "Current iteration=1039, loss=0.516256982835762\n",
      "Current iteration=1040, loss=0.5162569679135178\n",
      "Current iteration=1041, loss=0.5162569531047921\n",
      "Current iteration=1042, loss=0.516256938408704\n",
      "Current iteration=1043, loss=0.5162569238243799\n",
      "Current iteration=1044, loss=0.5162569093509527\n",
      "Current iteration=1045, loss=0.516256894987562\n",
      "Current iteration=1046, loss=0.5162568807333547\n",
      "Current iteration=1047, loss=0.516256866587484\n",
      "Current iteration=1048, loss=0.5162568525491104\n",
      "Current iteration=1049, loss=0.5162568386174002\n",
      "Current iteration=1050, loss=0.5162568247915271\n",
      "Current iteration=1051, loss=0.5162568110706708\n",
      "Current iteration=1052, loss=0.5162567974540175\n",
      "Current iteration=1053, loss=0.5162567839407604\n",
      "Current iteration=1054, loss=0.5162567705300981\n",
      "Current iteration=1055, loss=0.5162567572212363\n",
      "Current iteration=1056, loss=0.516256744013387\n",
      "Current iteration=1057, loss=0.5162567309057675\n",
      "Current iteration=1058, loss=0.5162567178976026\n",
      "Current iteration=1059, loss=0.5162567049881222\n",
      "Current iteration=1060, loss=0.5162566921765628\n",
      "Current iteration=1061, loss=0.5162566794621665\n",
      "Current iteration=1062, loss=0.516256666844182\n",
      "Current iteration=1063, loss=0.5162566543218632\n",
      "Current iteration=1064, loss=0.5162566418944704\n",
      "Current iteration=1065, loss=0.5162566295612697\n",
      "Current iteration=1066, loss=0.5162566173215324\n",
      "Current iteration=1067, loss=0.5162566051745368\n",
      "Current iteration=1068, loss=0.5162565931195652\n",
      "Current iteration=1069, loss=0.5162565811559069\n",
      "Current iteration=1070, loss=0.5162565692828563\n",
      "Current iteration=1071, loss=0.5162565574997133\n",
      "Current iteration=1072, loss=0.5162565458057835\n",
      "Current iteration=1073, loss=0.5162565342003779\n",
      "Current iteration=1074, loss=0.5162565226828126\n",
      "Current iteration=1075, loss=0.5162565112524097\n",
      "Current iteration=1076, loss=0.5162564999084963\n",
      "Current iteration=1077, loss=0.5162564886504047\n",
      "Current iteration=1078, loss=0.5162564774774726\n",
      "Current iteration=1079, loss=0.516256466389043\n",
      "Current iteration=1080, loss=0.5162564553844637\n",
      "Current iteration=1081, loss=0.516256444463088\n",
      "Current iteration=1082, loss=0.5162564336242741\n",
      "Current iteration=1083, loss=0.5162564228673853\n",
      "Current iteration=1084, loss=0.5162564121917901\n",
      "Current iteration=1085, loss=0.5162564015968614\n",
      "Current iteration=1086, loss=0.5162563910819777\n",
      "Current iteration=1087, loss=0.516256380646522\n",
      "Current iteration=1088, loss=0.516256370289882\n",
      "Current iteration=1089, loss=0.516256360011451\n",
      "Current iteration=1090, loss=0.5162563498106261\n",
      "Current iteration=1091, loss=0.5162563396868096\n",
      "Current iteration=1092, loss=0.5162563296394082\n",
      "Current iteration=1093, loss=0.5162563196678336\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.ones((X_train_final.shape[1]))*(-0.01)\n",
    "\n",
    "ws,loss = logistic_regression(Y_train, X_train_final, initial_w, max_iters=10000, gamma=0.45, print_=True)\n",
    "\n",
    "Y_test = sigmoid(X_test_final.dot(ws))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, ...,  1,  1, -1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = probability_to_prediction(Y_test)\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, ...,  1,  1, -1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = testdata[['Id']]\n",
    "\n",
    "\n",
    "\n",
    "Y_final = np.c_[np.array(ids, dtype=np.int64), Y_pred]\n",
    "\n",
    "np.savetxt(\"submission.csv\", Y_final, delimiter=',', header=\"Id,Prediction\", comments=\"\", fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLIT DATA 0.805"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = [[19,0.5],\n",
    "          [19,1.5],\n",
    "          [19,2.5]]\n",
    "\n",
    "X_sets, Y_sets, thresholds = split_data_set(X_clean, Y_train, thresh)\n",
    "_, Test_ind, _ = split_data_set(X_clean, np.array(range(len(Y_train))), thresh)\n",
    "print(\"Train split sizes:\", [sets.shape[0] for sets in X_sets[-1]])\n",
    "\n",
    "X_sets_t, Train_ind, thresholds = split_data_set(X_test_clean, np.array(range(X_test_clean.shape[0])), thresh)\n",
    "print(\"Test split sizes:\", [sets.shape[0] for sets in X_sets_t[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming Train Data\n",
    "X_pass = X_sets[-1][0]\n",
    "X_pass = np.delete(X_pass,19,axis=1)\n",
    "X_pass = build_poly_multi(X_pass, 4)\n",
    "X_pass,_,_ = standardize(X_pass)\n",
    "\n",
    "Y_pass = Y_sets[-1][0]\n",
    "\n",
    "#Building model\n",
    "initial_w = np.ones((X_pass.shape[1]))*(-0.01)\n",
    "ws,loss = logistic_regression(Y_pass, X_pass, initial_w, max_iters=10000, gamma=1)\n",
    "\n",
    "#Transforming Test Data\n",
    "Xt_pass = X_sets_t[-1][0]\n",
    "Xt_pass = np.delete(Xt_pass,19,axis=1)\n",
    "Xt_pass = build_poly_multi(Xt_pass, 4)\n",
    "Xt_pass,_,_ = standardize(Xt_pass)\n",
    "\n",
    "#Building predictions\n",
    "Y_test = sigmoid(Xt_pass.dot(ws))\n",
    "Y_pred_G1 = probability_to_prediction(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming Train Data\n",
    "X_pass = X_sets[-1][1]\n",
    "X_pass = np.delete(X_pass,19,axis=1)\n",
    "X_pass = build_poly_multi(X_pass, 2)\n",
    "X_pass,_,_ = standardize(X_pass)\n",
    "\n",
    "Y_pass = Y_sets[-1][1]\n",
    "\n",
    "#Building model\n",
    "initial_w = np.ones((X_pass.shape[1]))*(-0.01)\n",
    "ws,loss = logistic_regression(Y_pass, X_pass, initial_w, max_iters=10000, gamma=1)\n",
    "\n",
    "#Transforming Test Data\n",
    "Xt_pass = X_sets_t[-1][1]\n",
    "Xt_pass = np.delete(Xt_pass,19,axis=1)\n",
    "Xt_pass = build_poly_multi(Xt_pass, 2)\n",
    "Xt_pass,_,_ = standardize(Xt_pass)\n",
    "\n",
    "#Building predictions\n",
    "Y_test = sigmoid(Xt_pass.dot(ws))\n",
    "Y_pred_G2 = probability_to_prediction(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming Train Data\n",
    "X_pass = X_sets[-1][2]\n",
    "X_pass = np.delete(X_pass,19,axis=1)\n",
    "X_pass = build_poly_multi(X_pass, 7)\n",
    "X_pass,_,_ = standardize(X_pass)\n",
    "\n",
    "Y_pass = Y_sets[-1][2]\n",
    "\n",
    "#Building model\n",
    "initial_w = np.ones((X_pass.shape[1]))*(-0.01)\n",
    "ws,loss = logistic_regression(Y_pass, X_pass, initial_w, max_iters=10000, gamma=1)\n",
    "\n",
    "#Transforming Test Data\n",
    "Xt_pass = X_sets_t[-1][2]\n",
    "Xt_pass = np.delete(Xt_pass,19,axis=1)\n",
    "Xt_pass = build_poly_multi(Xt_pass, 7)\n",
    "Xt_pass,_,_ = standardize(Xt_pass)\n",
    "\n",
    "#Building predictions\n",
    "Y_test = sigmoid(Xt_pass.dot(ws))\n",
    "Y_pred_G3 = probability_to_prediction(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming Train Data\n",
    "X_pass = X_sets[-1][3]\n",
    "X_pass = np.delete(X_pass,19,axis=1)\n",
    "X_pass = np.delete(X_pass,22,axis=1)\n",
    "X_pass = build_poly_multi(X_pass, 1)\n",
    "X_pass,_,_ = standardize(X_pass)\n",
    "\n",
    "Y_pass = Y_sets[-1][3]\n",
    "\n",
    "#Building model\n",
    "initial_w = np.ones((X_pass.shape[1]))*(-0.01)\n",
    "ws,loss = logistic_regression(Y_pass, X_pass, initial_w, max_iters=10000, gamma=0.01)\n",
    "\n",
    "#Transforming Test Data\n",
    "Xt_pass = X_sets_t[-1][3]\n",
    "Xt_pass = np.delete(Xt_pass,19,axis=1)\n",
    "Xt_pass = np.delete(Xt_pass,22,axis=1)\n",
    "Xt_pass = build_poly_multi(Xt_pass, 1)\n",
    "Xt_pass,_,_ = standardize(Xt_pass)\n",
    "\n",
    "#Building predictions\n",
    "Y_test = sigmoid(Xt_pass.dot(ws))\n",
    "Y_pred_G4 = probability_to_prediction(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test split sizes:\", [sets.shape[0] for sets in X_sets_t[-1]])\n",
    "print(\"Test pred. sizes:\", [len(Y_pred_G1), len(Y_pred_G2), len(Y_pred_G3), len(Y_pred_G4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = np.zeros(X_test.shape[0])\n",
    "\n",
    "for i,j in zip(Train_ind[-1][0], Y_pred_G1):\n",
    "    Y_pred[i]= j\n",
    "for i,j in zip(Train_ind[-1][1], Y_pred_G2):\n",
    "    Y_pred[i]= j\n",
    "for i,j in zip(Train_ind[-1][2], Y_pred_G3):\n",
    "    Y_pred[i]= j\n",
    "for i,j in zip(Train_ind[-1][3], Y_pred_G4):\n",
    "    Y_pred[i]= j\n",
    "\n",
    "any(Y_pred == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = testdata[['Id']]\n",
    "\n",
    "Y_final = np.c_[np.array(ids, dtype=np.int64), Y_pred]\n",
    "\n",
    "np.savetxt(\"submission.csv\", Y_final, delimiter=',', header=\"Id,Prediction\", comments=\"\", fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Algorithm Selection of Polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_s = standardize_data(X_clean)\n",
    "X_s_test = standardize_data(X_test_clean)\n",
    "\n",
    "#Obtained from genetic algorithm\n",
    "index = (1, 3, 5, 7, 13, 16, 18, 20, 23)\n",
    "degree = [(1, 0.25, 0.3333333333333333, 2), (1, 0.03333333333333333, 0.1, 0.2, 0.3333333333333333, 0.5, 2), (1, 0.1, 0.3333333333333333, 20), (1, 0.03333333333333333, 0.05, 0.2, 0.3333333333333333, 30), (1, 0.05, 10), (1, 0.2, 0.25, 0.3333333333333333, 3, 4, 5), (1, 3, 10, 20), (1, 0.1, 0.3333333333333333, 4), (1, 0.03333333333333333, 0.2), (1, 0.1, 2, 5), (1, 0.05, 0.1), (1, 0.2, 0.3333333333333333, 0.5, 2, 4, 10), (1, 0.1, 0.25, 0.3333333333333333, 5, 30), (1, 3, 30), (1, 0.1, 5, 10), (1, 0.25, 3, 30), (1, 0.1, 10, 30), (1, 0.2, 1, 3), (1, 0.05, 0.5, 30), (1, 3, 4), (1, 0.3333333333333333, 3, 5), (1, 2, 10), (1, 0.25, 0.3333333333333333), (1, 0.05, 0.2, 30)]\n",
    "\n",
    "X_poly = build_poly_index(X_s, index, degree)\n",
    "X_poly_test = build_poly_index(X_s_test, index, degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanacc_dtest_ls = []\n",
    "meanacc_dtrain_ls = []\n",
    "\n",
    "dtmp_tr,dtmp_te=cross_validation(Y_train,X_poly,k_fold=4,seed=1, function_name='least_squares')\n",
    "meanacc_dtest_ls.append(dtmp_te)\n",
    "meanacc_dtrain_ls.append(dtmp_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanacc_dtest_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws,loss = least_squares(Y_train, X_poly)\n",
    "\n",
    "Y_test = X_poly_test.dot(ws)\n",
    "\n",
    "Y_pred = probability_to_prediction(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = testdata[['Id']]\n",
    "\n",
    "\n",
    "\n",
    "Y_final = np.c_[np.array(ids, dtype=np.int64), Y_pred]\n",
    "\n",
    "np.savetxt(\"submission.csv\", Y_final, delimiter=',', header=\"Id,Prediction\", comments=\"\", fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
