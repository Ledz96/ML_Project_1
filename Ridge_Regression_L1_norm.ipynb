{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import compress\n",
    "\n",
    "from utils_nans_manipulation import*\n",
    "from cross_validation import*\n",
    "from utils_data_loading import*\n",
    "from utils_features_manipulation import*\n",
    "from standardization import*\n",
    "from L1_norm import*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata,_ = load_data('Data/train.csv')\n",
    "X_total, Y_total = structure_data(traindata)\n",
    "\n",
    "# Replacing undefined data with NaNs\n",
    "X_nans = replace_bad_data_with_nans(X_total, -999)\n",
    "\n",
    "X_nans, col = replace_nans_with_median(X_nans, threshold=0.5)\n",
    "\n",
    "X_nans = standardize_data(X_nans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 0.7339773333333333 0.7337959999999999\n"
     ]
    }
   ],
   "source": [
    "dtmp_tr,dtmp_te = cross_validation(Y_total, X_nans, k_fold=4, seed=2, function_name='least_squares')\n",
    "print(\"Baseline:\", dtmp_tr, dtmp_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 0.78872 0.788544\n"
     ]
    }
   ],
   "source": [
    "index_list = list(range(2,X_nans.shape[1]))\n",
    "degree_test = [1, 1/4,1/3, 1/2, 2, 3]\n",
    "degree = [degree_test for i in range(X_nans.shape[1])]\n",
    "\n",
    "X_poly = build_poly_index(X_nans, index_list, degree)\n",
    "\n",
    "dtmp_tr,dtmp_te = cross_validation(Y_total, X_poly, k_fold=4, seed=1, function_name='least_squares')\n",
    "print(\"Baseline:\", dtmp_tr, dtmp_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999): loss=0.171334\n",
      "Gradient Descent(1/999): loss=0.17132689577187044\n",
      "Gradient Descent(2/999): loss=0.17132001410766098\n",
      "Gradient Descent(3/999): loss=0.17131330255137775\n",
      "Gradient Descent(4/999): loss=0.17130672130781865\n",
      "Gradient Descent(5/999): loss=0.17130024018640608\n",
      "Gradient Descent(6/999): loss=0.17129383628275077\n",
      "Gradient Descent(7/999): loss=0.17128749221985443\n",
      "Gradient Descent(8/999): loss=0.17128119481386736\n",
      "Gradient Descent(9/999): loss=0.17127493406191388\n",
      "Gradient Descent(10/999): loss=0.17126870237424593\n",
      "Gradient Descent(11/999): loss=0.17126249399174623\n",
      "Gradient Descent(12/999): loss=0.1712563045440353\n",
      "Gradient Descent(13/999): loss=0.17125013071425127\n",
      "Gradient Descent(14/999): loss=0.1712439699847435\n",
      "Gradient Descent(15/999): loss=0.17123782044415561\n",
      "Gradient Descent(16/999): loss=0.17123168064107405\n",
      "Gradient Descent(17/999): loss=0.1712255494730063\n",
      "Gradient Descent(18/999): loss=0.17121942610215626\n",
      "Gradient Descent(19/999): loss=0.1712133098915325\n",
      "Gradient Descent(20/999): loss=0.17120720035647702\n",
      "Gradient Descent(21/999): loss=0.17120109712789666\n",
      "Gradient Descent(22/999): loss=0.1711949999243681\n",
      "Gradient Descent(23/999): loss=0.17118890853097743\n",
      "Gradient Descent(24/999): loss=0.17118282278326746\n",
      "Gradient Descent(25/999): loss=0.1711767425550594\n",
      "Gradient Descent(26/999): loss=0.17117066774921502\n",
      "Gradient Descent(27/999): loss=0.17116459829062813\n",
      "Gradient Descent(28/999): loss=0.171158534120908\n",
      "Gradient Descent(29/999): loss=0.17115247519434632\n",
      "Gradient Descent(30/999): loss=0.1711464214748574\n",
      "Gradient Descent(31/999): loss=0.17114037293365714\n",
      "Gradient Descent(32/999): loss=0.1711343295475021\n",
      "Gradient Descent(33/999): loss=0.17112829129735405\n",
      "Gradient Descent(34/999): loss=0.1711222581673662\n",
      "Gradient Descent(35/999): loss=0.17111623014411498\n",
      "Gradient Descent(36/999): loss=0.1711102072160163\n",
      "Gradient Descent(37/999): loss=0.1711041893728836\n",
      "Gradient Descent(38/999): loss=0.17109817660559187\n",
      "Gradient Descent(39/999): loss=0.17109216890582352\n",
      "Gradient Descent(40/999): loss=0.1710861662658749\n",
      "Gradient Descent(41/999): loss=0.17108016867850961\n",
      "Gradient Descent(42/999): loss=0.17107417613684786\n",
      "Gradient Descent(43/999): loss=0.17106818863428155\n",
      "Gradient Descent(44/999): loss=0.1710622061644109\n",
      "Gradient Descent(45/999): loss=0.1710562287209953\n",
      "Gradient Descent(46/999): loss=0.17105025629791715\n",
      "Gradient Descent(47/999): loss=0.17104428888915324\n",
      "Gradient Descent(48/999): loss=0.1710383264887541\n",
      "Gradient Descent(49/999): loss=0.17103236909082772\n",
      "Gradient Descent(50/999): loss=0.17102641668952717\n",
      "Gradient Descent(51/999): loss=0.17102046927904163\n",
      "Gradient Descent(52/999): loss=0.17101452685358928\n",
      "Gradient Descent(53/999): loss=0.17100858940741184\n",
      "Gradient Descent(54/999): loss=0.17100265693477076\n",
      "Gradient Descent(55/999): loss=0.1709967294299439\n",
      "Gradient Descent(56/999): loss=0.17099080688722323\n",
      "Gradient Descent(57/999): loss=0.17098488930091338\n",
      "Gradient Descent(58/999): loss=0.17097897666532988\n",
      "Gradient Descent(59/999): loss=0.1709730689747983\n",
      "Gradient Descent(60/999): loss=0.17096716622365357\n",
      "Gradient Descent(61/999): loss=0.1709612684062391\n",
      "Gradient Descent(62/999): loss=0.17095537551690695\n",
      "Gradient Descent(63/999): loss=0.17094948755001652\n",
      "Gradient Descent(64/999): loss=0.1709436044999353\n",
      "Gradient Descent(65/999): loss=0.17093772636103788\n",
      "Gradient Descent(66/999): loss=0.17093185312770628\n",
      "Gradient Descent(67/999): loss=0.17092598479432955\n",
      "Gradient Descent(68/999): loss=0.17092012135530388\n",
      "Gradient Descent(69/999): loss=0.17091426280503239\n",
      "Gradient Descent(70/999): loss=0.1709084091379251\n",
      "Gradient Descent(71/999): loss=0.170902560348399\n",
      "Gradient Descent(72/999): loss=0.17089671643087792\n",
      "Gradient Descent(73/999): loss=0.1708908773797923\n",
      "Gradient Descent(74/999): loss=0.1708850431895795\n",
      "Gradient Descent(75/999): loss=0.1708792138546837\n",
      "Gradient Descent(76/999): loss=0.1708733893695558\n",
      "Gradient Descent(77/999): loss=0.17086756972865336\n",
      "Gradient Descent(78/999): loss=0.1708617549264406\n",
      "Gradient Descent(79/999): loss=0.1708559449573887\n",
      "Gradient Descent(80/999): loss=0.17085013981597538\n",
      "Gradient Descent(81/999): loss=0.17084433949668493\n",
      "Gradient Descent(82/999): loss=0.17083854399400847\n",
      "Gradient Descent(83/999): loss=0.17083275330244377\n",
      "Gradient Descent(84/999): loss=0.17082696741649514\n",
      "Gradient Descent(85/999): loss=0.17082118633067364\n",
      "Gradient Descent(86/999): loss=0.17081541003949705\n",
      "Gradient Descent(87/999): loss=0.17080963853748946\n",
      "Gradient Descent(88/999): loss=0.1708038718191821\n",
      "Gradient Descent(89/999): loss=0.1707981098791122\n",
      "Gradient Descent(90/999): loss=0.17079235271182405\n",
      "Gradient Descent(91/999): loss=0.17078660031186832\n",
      "Gradient Descent(92/999): loss=0.17078085267380239\n",
      "Gradient Descent(93/999): loss=0.17077510979219004\n",
      "Gradient Descent(94/999): loss=0.17076937166160186\n",
      "Gradient Descent(95/999): loss=0.17076363827661473\n",
      "Gradient Descent(96/999): loss=0.1707579096318124\n",
      "Gradient Descent(97/999): loss=0.17075218572178483\n",
      "Gradient Descent(98/999): loss=0.17074646654112877\n",
      "Gradient Descent(99/999): loss=0.17074075208444742\n",
      "Gradient Descent(100/999): loss=0.17073504234635045\n",
      "Gradient Descent(101/999): loss=0.17072933732145404\n",
      "Gradient Descent(102/999): loss=0.17072363700438095\n",
      "Gradient Descent(103/999): loss=0.17071794138976046\n",
      "Gradient Descent(104/999): loss=0.17071225047222824\n",
      "Gradient Descent(105/999): loss=0.17070656424642644\n",
      "Gradient Descent(106/999): loss=0.17070088270700384\n",
      "Gradient Descent(107/999): loss=0.17069520584861544\n",
      "Gradient Descent(108/999): loss=0.17068953366592296\n",
      "Gradient Descent(109/999): loss=0.1706838661535943\n",
      "Gradient Descent(110/999): loss=0.17067820330630407\n",
      "Gradient Descent(111/999): loss=0.170672545118733\n",
      "Gradient Descent(112/999): loss=0.1706668915855686\n",
      "Gradient Descent(113/999): loss=0.1706612427015045\n",
      "Gradient Descent(114/999): loss=0.17065559846124104\n",
      "Gradient Descent(115/999): loss=0.17064995885948459\n",
      "Gradient Descent(116/999): loss=0.17064432389094816\n",
      "Gradient Descent(117/999): loss=0.17063869355035113\n",
      "Gradient Descent(118/999): loss=0.17063306783241924\n",
      "Gradient Descent(119/999): loss=0.17062744673188449\n",
      "Gradient Descent(120/999): loss=0.17062183024348537\n",
      "Gradient Descent(121/999): loss=0.1706162183619667\n",
      "Gradient Descent(122/999): loss=0.17061061108207956\n",
      "Gradient Descent(123/999): loss=0.1706050083985817\n",
      "Gradient Descent(124/999): loss=0.1705994103062366\n",
      "Gradient Descent(125/999): loss=0.1705938167998145\n",
      "Gradient Descent(126/999): loss=0.17058822787409192\n",
      "Gradient Descent(127/999): loss=0.17058264352385163\n",
      "Gradient Descent(128/999): loss=0.17057706374388268\n",
      "Gradient Descent(129/999): loss=0.1705714885289804\n",
      "Gradient Descent(130/999): loss=0.17056591787394648\n",
      "Gradient Descent(131/999): loss=0.17056035177358872\n",
      "Gradient Descent(132/999): loss=0.1705547902227214\n",
      "Gradient Descent(133/999): loss=0.17054923321616489\n",
      "Gradient Descent(134/999): loss=0.170543680748746\n",
      "Gradient Descent(135/999): loss=0.17053813281529764\n",
      "Gradient Descent(136/999): loss=0.17053258941065882\n",
      "Gradient Descent(137/999): loss=0.17052705052967518\n",
      "Gradient Descent(138/999): loss=0.1705215161671982\n",
      "Gradient Descent(139/999): loss=0.1705159863180858\n",
      "Gradient Descent(140/999): loss=0.17051046097720196\n",
      "Gradient Descent(141/999): loss=0.170504940139417\n",
      "Gradient Descent(142/999): loss=0.1704994237996074\n",
      "Gradient Descent(143/999): loss=0.17049391195265567\n",
      "Gradient Descent(144/999): loss=0.1704884045934506\n",
      "Gradient Descent(145/999): loss=0.17048290171688726\n",
      "Gradient Descent(146/999): loss=0.1704774033178667\n",
      "Gradient Descent(147/999): loss=0.17047190939129625\n",
      "Gradient Descent(148/999): loss=0.17046641993208927\n",
      "Gradient Descent(149/999): loss=0.17046093493516545\n",
      "Gradient Descent(150/999): loss=0.17045545439545046\n",
      "Gradient Descent(151/999): loss=0.17044997830787603\n",
      "Gradient Descent(152/999): loss=0.1704445066673801\n",
      "Gradient Descent(153/999): loss=0.17043903946890682\n",
      "Gradient Descent(154/999): loss=0.1704335767074062\n",
      "Gradient Descent(155/999): loss=0.17042811837783461\n",
      "Gradient Descent(156/999): loss=0.17042266447515425\n",
      "Gradient Descent(157/999): loss=0.17041721499433354\n",
      "Gradient Descent(158/999): loss=0.17041176993034704\n",
      "Gradient Descent(159/999): loss=0.17040632927817526\n",
      "Gradient Descent(160/999): loss=0.17040089303280467\n",
      "Gradient Descent(161/999): loss=0.170395461189228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(162/999): loss=0.17039003374244388\n",
      "Gradient Descent(163/999): loss=0.17038461068745706\n",
      "Gradient Descent(164/999): loss=0.1703791920192782\n",
      "Gradient Descent(165/999): loss=0.17037377773292414\n",
      "Gradient Descent(166/999): loss=0.17036836782341766\n",
      "Gradient Descent(167/999): loss=0.17036296228578746\n",
      "Gradient Descent(168/999): loss=0.17035756111506825\n",
      "Gradient Descent(169/999): loss=0.17035216430630085\n",
      "Gradient Descent(170/999): loss=0.170346771854532\n",
      "Gradient Descent(171/999): loss=0.1703413837548144\n",
      "Gradient Descent(172/999): loss=0.17033600000220656\n",
      "Gradient Descent(173/999): loss=0.1703306205917733\n",
      "Gradient Descent(174/999): loss=0.1703252455185852\n",
      "Gradient Descent(175/999): loss=0.17031987477771873\n",
      "Gradient Descent(176/999): loss=0.17031450836425627\n",
      "Gradient Descent(177/999): loss=0.17030914627328636\n",
      "Gradient Descent(178/999): loss=0.17030378849990319\n",
      "Gradient Descent(179/999): loss=0.17029843503920705\n",
      "Gradient Descent(180/999): loss=0.17029308588630404\n",
      "Gradient Descent(181/999): loss=0.1702877410363062\n",
      "Gradient Descent(182/999): loss=0.1702824004843314\n",
      "Gradient Descent(183/999): loss=0.17027706422550365\n",
      "Gradient Descent(184/999): loss=0.1702717322549524\n",
      "Gradient Descent(185/999): loss=0.17026640456781342\n",
      "Gradient Descent(186/999): loss=0.17026108115922794\n",
      "Gradient Descent(187/999): loss=0.1702557620243433\n",
      "Gradient Descent(188/999): loss=0.1702504471583128\n",
      "Gradient Descent(189/999): loss=0.17024513655629525\n",
      "Gradient Descent(190/999): loss=0.1702398302134554\n",
      "Gradient Descent(191/999): loss=0.17023452812496406\n",
      "Gradient Descent(192/999): loss=0.17022923028599765\n",
      "Gradient Descent(193/999): loss=0.17022393669173833\n",
      "Gradient Descent(194/999): loss=0.17021864733737424\n",
      "Gradient Descent(195/999): loss=0.17021336221809932\n",
      "Gradient Descent(196/999): loss=0.1702080813291132\n",
      "Gradient Descent(197/999): loss=0.17020280466562127\n",
      "Gradient Descent(198/999): loss=0.17019753222283482\n",
      "Gradient Descent(199/999): loss=0.17019226399597084\n",
      "Gradient Descent(200/999): loss=0.17018699998025205\n",
      "Gradient Descent(201/999): loss=0.17018174017090706\n",
      "Gradient Descent(202/999): loss=0.17017648456317014\n",
      "Gradient Descent(203/999): loss=0.17017123315228122\n",
      "Gradient Descent(204/999): loss=0.17016598593348622\n",
      "Gradient Descent(205/999): loss=0.17016074290203634\n",
      "Gradient Descent(206/999): loss=0.17015550405318902\n",
      "Gradient Descent(207/999): loss=0.17015026938220706\n",
      "Gradient Descent(208/999): loss=0.17014503888435908\n",
      "Gradient Descent(209/999): loss=0.17013981255491945\n",
      "Gradient Descent(210/999): loss=0.1701345903891681\n",
      "Gradient Descent(211/999): loss=0.17012937238239084\n",
      "Gradient Descent(212/999): loss=0.17012415852987897\n",
      "Gradient Descent(213/999): loss=0.17011894882692954\n",
      "Gradient Descent(214/999): loss=0.1701137432688453\n",
      "Gradient Descent(215/999): loss=0.17010854185093463\n",
      "Gradient Descent(216/999): loss=0.17010334456851148\n",
      "Gradient Descent(217/999): loss=0.17009815141689547\n",
      "Gradient Descent(218/999): loss=0.1700929623914121\n",
      "Gradient Descent(219/999): loss=0.17008777748739215\n",
      "Gradient Descent(220/999): loss=0.17008259670017217\n",
      "Gradient Descent(221/999): loss=0.17007742002509443\n",
      "Gradient Descent(222/999): loss=0.17007224745750665\n",
      "Gradient Descent(223/999): loss=0.17006707899276222\n",
      "Gradient Descent(224/999): loss=0.1700619146262201\n",
      "Gradient Descent(225/999): loss=0.1700567543532449\n",
      "Gradient Descent(226/999): loss=0.17005159816920673\n",
      "Gradient Descent(227/999): loss=0.17004644606948133\n",
      "Gradient Descent(228/999): loss=0.17004129804944995\n",
      "Gradient Descent(229/999): loss=0.1700361541044995\n",
      "Gradient Descent(230/999): loss=0.17003101423002234\n",
      "Gradient Descent(231/999): loss=0.17002587842141648\n",
      "Gradient Descent(232/999): loss=0.1700207466740854\n",
      "Gradient Descent(233/999): loss=0.17001561898343798\n",
      "Gradient Descent(234/999): loss=0.17001049534488902\n",
      "Gradient Descent(235/999): loss=0.1700053757538585\n",
      "Gradient Descent(236/999): loss=0.1700002602057718\n",
      "Gradient Descent(237/999): loss=0.1699951486960603\n",
      "Gradient Descent(238/999): loss=0.16999004122016048\n",
      "Gradient Descent(239/999): loss=0.16998493777351442\n",
      "Gradient Descent(240/999): loss=0.16997983835156974\n",
      "Gradient Descent(241/999): loss=0.16997474294977938\n",
      "Gradient Descent(242/999): loss=0.16996965156360203\n",
      "Gradient Descent(243/999): loss=0.16996456418850153\n",
      "Gradient Descent(244/999): loss=0.16995948081994733\n",
      "Gradient Descent(245/999): loss=0.16995440145341445\n",
      "Gradient Descent(246/999): loss=0.16994932608438315\n",
      "Gradient Descent(247/999): loss=0.1699442547083392\n",
      "Gradient Descent(248/999): loss=0.16993918732077382\n",
      "Gradient Descent(249/999): loss=0.16993412391718374\n",
      "Gradient Descent(250/999): loss=0.1699290644930709\n",
      "Gradient Descent(251/999): loss=0.16992400904394292\n",
      "Gradient Descent(252/999): loss=0.1699189575653125\n",
      "Gradient Descent(253/999): loss=0.16991391005269807\n",
      "Gradient Descent(254/999): loss=0.1699088665016233\n",
      "Gradient Descent(255/999): loss=0.16990382690761696\n",
      "Gradient Descent(256/999): loss=0.16989879126621393\n",
      "Gradient Descent(257/999): loss=0.16989375957295363\n",
      "Gradient Descent(258/999): loss=0.16988873182338146\n",
      "Gradient Descent(259/999): loss=0.16988370801304795\n",
      "Gradient Descent(260/999): loss=0.1698786881375088\n",
      "Gradient Descent(261/999): loss=0.16987367219232538\n",
      "Gradient Descent(262/999): loss=0.16986866017306432\n",
      "Gradient Descent(263/999): loss=0.16986365207529736\n",
      "Gradient Descent(264/999): loss=0.16985864789460176\n",
      "Gradient Descent(265/999): loss=0.16985364762656016\n",
      "Gradient Descent(266/999): loss=0.16984865126676027\n",
      "Gradient Descent(267/999): loss=0.16984365881079538\n",
      "Gradient Descent(268/999): loss=0.16983867025426394\n",
      "Gradient Descent(269/999): loss=0.16983368559276968\n",
      "Gradient Descent(270/999): loss=0.1698287048219217\n",
      "Gradient Descent(271/999): loss=0.16982372793733425\n",
      "Gradient Descent(272/999): loss=0.16981875493462692\n",
      "Gradient Descent(273/999): loss=0.1698137858094247\n",
      "Gradient Descent(274/999): loss=0.16980882055735755\n",
      "Gradient Descent(275/999): loss=0.16980385917406088\n",
      "Gradient Descent(276/999): loss=0.16979890165517542\n",
      "Gradient Descent(277/999): loss=0.169793947996347\n",
      "Gradient Descent(278/999): loss=0.1697889981932267\n",
      "Gradient Descent(279/999): loss=0.16978405224147086\n",
      "Gradient Descent(280/999): loss=0.16977911013674107\n",
      "Gradient Descent(281/999): loss=0.16977417187470403\n",
      "Gradient Descent(282/999): loss=0.1697692374510318\n",
      "Gradient Descent(283/999): loss=0.1697643068614015\n",
      "Gradient Descent(284/999): loss=0.1697593801014956\n",
      "Gradient Descent(285/999): loss=0.16975445716700144\n",
      "Gradient Descent(286/999): loss=0.16974953805361212\n",
      "Gradient Descent(287/999): loss=0.16974462275702523\n",
      "Gradient Descent(288/999): loss=0.16973971127294415\n",
      "Gradient Descent(289/999): loss=0.16973480359707693\n",
      "Gradient Descent(290/999): loss=0.1697298997251372\n",
      "Gradient Descent(291/999): loss=0.16972499965284332\n",
      "Gradient Descent(292/999): loss=0.16972010337591906\n",
      "Gradient Descent(293/999): loss=0.16971521089009348\n",
      "Gradient Descent(294/999): loss=0.1697103221911003\n",
      "Gradient Descent(295/999): loss=0.1697054372746788\n",
      "Gradient Descent(296/999): loss=0.16970055613657312\n",
      "Gradient Descent(297/999): loss=0.16969567877253275\n",
      "Gradient Descent(298/999): loss=0.169690805178312\n",
      "Gradient Descent(299/999): loss=0.16968593534967052\n",
      "Gradient Descent(300/999): loss=0.16968106928237298\n",
      "Gradient Descent(301/999): loss=0.16967620697218902\n",
      "Gradient Descent(302/999): loss=0.1696713484148936\n",
      "Gradient Descent(303/999): loss=0.16966649360626646\n",
      "Gradient Descent(304/999): loss=0.1696616425420927\n",
      "Gradient Descent(305/999): loss=0.1696567952181623\n",
      "Gradient Descent(306/999): loss=0.16965195163027044\n",
      "Gradient Descent(307/999): loss=0.16964711177421712\n",
      "Gradient Descent(308/999): loss=0.1696422756458077\n",
      "Gradient Descent(309/999): loss=0.1696374432408522\n",
      "Gradient Descent(310/999): loss=0.169632614555166\n",
      "Gradient Descent(311/999): loss=0.16962778958456948\n",
      "Gradient Descent(312/999): loss=0.16962296832488774\n",
      "Gradient Descent(313/999): loss=0.16961815077195125\n",
      "Gradient Descent(314/999): loss=0.1696133369215952\n",
      "Gradient Descent(315/999): loss=0.16960852676966015\n",
      "Gradient Descent(316/999): loss=0.16960372031199114\n",
      "Gradient Descent(317/999): loss=0.16959891754443868\n",
      "Gradient Descent(318/999): loss=0.16959411846285805\n",
      "Gradient Descent(319/999): loss=0.16958932306310948\n",
      "Gradient Descent(320/999): loss=0.16958453134105814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(321/999): loss=0.16957974329257441\n",
      "Gradient Descent(322/999): loss=0.16957495891353336\n",
      "Gradient Descent(323/999): loss=0.1695701781998151\n",
      "Gradient Descent(324/999): loss=0.16956540114730467\n",
      "Gradient Descent(325/999): loss=0.1695606277518922\n",
      "Gradient Descent(326/999): loss=0.16955585800947257\n",
      "Gradient Descent(327/999): loss=0.16955109191594556\n",
      "Gradient Descent(328/999): loss=0.16954632946721612\n",
      "Gradient Descent(329/999): loss=0.16954157065919379\n",
      "Gradient Descent(330/999): loss=0.16953681548779329\n",
      "Gradient Descent(331/999): loss=0.16953206394893408\n",
      "Gradient Descent(332/999): loss=0.16952731603854068\n",
      "Gradient Descent(333/999): loss=0.16952257175254237\n",
      "Gradient Descent(334/999): loss=0.1695178310868733\n",
      "Gradient Descent(335/999): loss=0.16951309403747256\n",
      "Gradient Descent(336/999): loss=0.16950836060028399\n",
      "Gradient Descent(337/999): loss=0.1695036307712567\n",
      "Gradient Descent(338/999): loss=0.1694989045463441\n",
      "Gradient Descent(339/999): loss=0.16949418192150476\n",
      "Gradient Descent(340/999): loss=0.1694894628927022\n",
      "Gradient Descent(341/999): loss=0.16948474745590467\n",
      "Gradient Descent(342/999): loss=0.169480035607085\n",
      "Gradient Descent(343/999): loss=0.1694753273422213\n",
      "Gradient Descent(344/999): loss=0.1694706226572962\n",
      "Gradient Descent(345/999): loss=0.1694659215482973\n",
      "Gradient Descent(346/999): loss=0.16946122401121683\n",
      "Gradient Descent(347/999): loss=0.16945653004205224\n",
      "Gradient Descent(348/999): loss=0.1694518396368052\n",
      "Gradient Descent(349/999): loss=0.16944715279148267\n",
      "Gradient Descent(350/999): loss=0.16944246950209613\n",
      "Gradient Descent(351/999): loss=0.16943778976466187\n",
      "Gradient Descent(352/999): loss=0.16943311357520105\n",
      "Gradient Descent(353/999): loss=0.1694284409297397\n",
      "Gradient Descent(354/999): loss=0.16942377182430826\n",
      "Gradient Descent(355/999): loss=0.1694191062549423\n",
      "Gradient Descent(356/999): loss=0.16941444421768198\n",
      "Gradient Descent(357/999): loss=0.16940978570857212\n",
      "Gradient Descent(358/999): loss=0.1694051307236626\n",
      "Gradient Descent(359/999): loss=0.1694004792590076\n",
      "Gradient Descent(360/999): loss=0.1693958313106663\n",
      "Gradient Descent(361/999): loss=0.16939118687470275\n",
      "Gradient Descent(362/999): loss=0.16938654594718527\n",
      "Gradient Descent(363/999): loss=0.16938190852418736\n",
      "Gradient Descent(364/999): loss=0.16937727460178698\n",
      "Gradient Descent(365/999): loss=0.16937264417606682\n",
      "Gradient Descent(366/999): loss=0.1693680172431143\n",
      "Gradient Descent(367/999): loss=0.16936339379902146\n",
      "Gradient Descent(368/999): loss=0.1693587738398852\n",
      "Gradient Descent(369/999): loss=0.1693541573618069\n",
      "Gradient Descent(370/999): loss=0.16934954436089272\n",
      "Gradient Descent(371/999): loss=0.16934493483325352\n",
      "Gradient Descent(372/999): loss=0.16934032877500463\n",
      "Gradient Descent(373/999): loss=0.1693357261822663\n",
      "Gradient Descent(374/999): loss=0.16933112705116332\n",
      "Gradient Descent(375/999): loss=0.169326531377825\n",
      "Gradient Descent(376/999): loss=0.16932193915838553\n",
      "Gradient Descent(377/999): loss=0.16931735038898343\n",
      "Gradient Descent(378/999): loss=0.16931276506576223\n",
      "Gradient Descent(379/999): loss=0.16930818318486976\n",
      "Gradient Descent(380/999): loss=0.16930360474245862\n",
      "Gradient Descent(381/999): loss=0.16929902973468594\n",
      "Gradient Descent(382/999): loss=0.16929445815771357\n",
      "Gradient Descent(383/999): loss=0.1692898900077078\n",
      "Gradient Descent(384/999): loss=0.1692853252808397\n",
      "Gradient Descent(385/999): loss=0.1692807639732848\n",
      "Gradient Descent(386/999): loss=0.16927620608122318\n",
      "Gradient Descent(387/999): loss=0.16927165160083962\n",
      "Gradient Descent(388/999): loss=0.16926710052832342\n",
      "Gradient Descent(389/999): loss=0.16926255285986838\n",
      "Gradient Descent(390/999): loss=0.169258008591673\n",
      "Gradient Descent(391/999): loss=0.16925346771994024\n",
      "Gradient Descent(392/999): loss=0.16924893024087764\n",
      "Gradient Descent(393/999): loss=0.16924439615069717\n",
      "Gradient Descent(394/999): loss=0.16923986544561542\n",
      "Gradient Descent(395/999): loss=0.16923533812185373\n",
      "Gradient Descent(396/999): loss=0.16923081417563757\n",
      "Gradient Descent(397/999): loss=0.16922629360319727\n",
      "Gradient Descent(398/999): loss=0.16922177640076735\n",
      "Gradient Descent(399/999): loss=0.16921726256458727\n",
      "Gradient Descent(400/999): loss=0.16921275209090053\n",
      "Gradient Descent(401/999): loss=0.16920824497595538\n",
      "Gradient Descent(402/999): loss=0.16920374121600448\n",
      "Gradient Descent(403/999): loss=0.1691992408073053\n",
      "Gradient Descent(404/999): loss=0.16919474374611917\n",
      "Gradient Descent(405/999): loss=0.1691902500287124\n",
      "Gradient Descent(406/999): loss=0.16918575965135577\n",
      "Gradient Descent(407/999): loss=0.16918127261032406\n",
      "Gradient Descent(408/999): loss=0.16917678890189702\n",
      "Gradient Descent(409/999): loss=0.16917230852235865\n",
      "Gradient Descent(410/999): loss=0.16916783146799738\n",
      "Gradient Descent(411/999): loss=0.16916335773510613\n",
      "Gradient Descent(412/999): loss=0.16915888731998216\n",
      "Gradient Descent(413/999): loss=0.16915442021892738\n",
      "Gradient Descent(414/999): loss=0.16914995642824787\n",
      "Gradient Descent(415/999): loss=0.16914549594425426\n",
      "Gradient Descent(416/999): loss=0.16914103876326164\n",
      "Gradient Descent(417/999): loss=0.16913658488158942\n",
      "Gradient Descent(418/999): loss=0.1691321342955615\n",
      "Gradient Descent(419/999): loss=0.1691276870015061\n",
      "Gradient Descent(420/999): loss=0.16912324299575596\n",
      "Gradient Descent(421/999): loss=0.16911880227464796\n",
      "Gradient Descent(422/999): loss=0.16911436483452358\n",
      "Gradient Descent(423/999): loss=0.16910993067172878\n",
      "Gradient Descent(424/999): loss=0.16910549978261355\n",
      "Gradient Descent(425/999): loss=0.16910107216353243\n",
      "Gradient Descent(426/999): loss=0.1690966478108445\n",
      "Gradient Descent(427/999): loss=0.16909222672091287\n",
      "Gradient Descent(428/999): loss=0.16908780889010522\n",
      "Gradient Descent(429/999): loss=0.1690833943147935\n",
      "Gradient Descent(430/999): loss=0.16907898299135402\n",
      "Gradient Descent(431/999): loss=0.16907457491616745\n",
      "Gradient Descent(432/999): loss=0.16907017008561873\n",
      "Gradient Descent(433/999): loss=0.1690657684960971\n",
      "Gradient Descent(434/999): loss=0.1690613701439964\n",
      "Gradient Descent(435/999): loss=0.1690569750257142\n",
      "Gradient Descent(436/999): loss=0.16905258313765303\n",
      "Gradient Descent(437/999): loss=0.16904819447621933\n",
      "Gradient Descent(438/999): loss=0.16904380903782396\n",
      "Gradient Descent(439/999): loss=0.16903942681888218\n",
      "Gradient Descent(440/999): loss=0.16903504781581322\n",
      "Gradient Descent(441/999): loss=0.16903067202504082\n",
      "Gradient Descent(442/999): loss=0.16902629944299302\n",
      "Gradient Descent(443/999): loss=0.16902193006610214\n",
      "Gradient Descent(444/999): loss=0.16901756389080458\n",
      "Gradient Descent(445/999): loss=0.16901320091354113\n",
      "Gradient Descent(446/999): loss=0.16900884113075695\n",
      "Gradient Descent(447/999): loss=0.16900448453890127\n",
      "Gradient Descent(448/999): loss=0.16900013113442763\n",
      "Gradient Descent(449/999): loss=0.1689957809137938\n",
      "Gradient Descent(450/999): loss=0.16899143387346188\n",
      "Gradient Descent(451/999): loss=0.1689870900098981\n",
      "Gradient Descent(452/999): loss=0.16898274931957286\n",
      "Gradient Descent(453/999): loss=0.1689784117989608\n",
      "Gradient Descent(454/999): loss=0.16897407744454102\n",
      "Gradient Descent(455/999): loss=0.1689697462527965\n",
      "Gradient Descent(456/999): loss=0.16896541822021463\n",
      "Gradient Descent(457/999): loss=0.16896109334328682\n",
      "Gradient Descent(458/999): loss=0.168956771618509\n",
      "Gradient Descent(459/999): loss=0.1689524530423809\n",
      "Gradient Descent(460/999): loss=0.16894813761140673\n",
      "Gradient Descent(461/999): loss=0.16894382532209465\n",
      "Gradient Descent(462/999): loss=0.16893951617095718\n",
      "Gradient Descent(463/999): loss=0.1689352101545109\n",
      "Gradient Descent(464/999): loss=0.16893090726927656\n",
      "Gradient Descent(465/999): loss=0.1689266075117793\n",
      "Gradient Descent(466/999): loss=0.16892231087854778\n",
      "Gradient Descent(467/999): loss=0.1689180173661156\n",
      "Gradient Descent(468/999): loss=0.1689137269710201\n",
      "Gradient Descent(469/999): loss=0.16890943968980274\n",
      "Gradient Descent(470/999): loss=0.16890515551900911\n",
      "Gradient Descent(471/999): loss=0.16890087445518906\n",
      "Gradient Descent(472/999): loss=0.1688965964948965\n",
      "Gradient Descent(473/999): loss=0.16889232163468937\n",
      "Gradient Descent(474/999): loss=0.16888804987112993\n",
      "Gradient Descent(475/999): loss=0.1688837812007843\n",
      "Gradient Descent(476/999): loss=0.1688795156202228\n",
      "Gradient Descent(477/999): loss=0.1688752531260201\n",
      "Gradient Descent(478/999): loss=0.16887099371475447\n",
      "Gradient Descent(479/999): loss=0.16886673738300859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(480/999): loss=0.16886248412736923\n",
      "Gradient Descent(481/999): loss=0.16885823394442714\n",
      "Gradient Descent(482/999): loss=0.16885398683077724\n",
      "Gradient Descent(483/999): loss=0.16884974278301826\n",
      "Gradient Descent(484/999): loss=0.16884550179775332\n",
      "Gradient Descent(485/999): loss=0.16884126387158951\n",
      "Gradient Descent(486/999): loss=0.16883702900113784\n",
      "Gradient Descent(487/999): loss=0.16883279718301342\n",
      "Gradient Descent(488/999): loss=0.16882856841383545\n",
      "Gradient Descent(489/999): loss=0.1688243426902272\n",
      "Gradient Descent(490/999): loss=0.16882012000881588\n",
      "Gradient Descent(491/999): loss=0.16881590036623273\n",
      "Gradient Descent(492/999): loss=0.16881168375911318\n",
      "Gradient Descent(493/999): loss=0.1688074701840964\n",
      "Gradient Descent(494/999): loss=0.16880325963782583\n",
      "Gradient Descent(495/999): loss=0.1687990521169488\n",
      "Gradient Descent(496/999): loss=0.1687948476181165\n",
      "Gradient Descent(497/999): loss=0.16879064613798447\n",
      "Gradient Descent(498/999): loss=0.168786447673212\n",
      "Gradient Descent(499/999): loss=0.1687822522204623\n",
      "Gradient Descent(500/999): loss=0.1687780597764028\n",
      "Gradient Descent(501/999): loss=0.16877387033770472\n",
      "Gradient Descent(502/999): loss=0.16876968390104327\n",
      "Gradient Descent(503/999): loss=0.16876550046309788\n",
      "Gradient Descent(504/999): loss=0.16876132002055144\n",
      "Gradient Descent(505/999): loss=0.16875714257009136\n",
      "Gradient Descent(506/999): loss=0.16875296810840856\n",
      "Gradient Descent(507/999): loss=0.1687487966321982\n",
      "Gradient Descent(508/999): loss=0.1687446281381592\n",
      "Gradient Descent(509/999): loss=0.16874046262299455\n",
      "Gradient Descent(510/999): loss=0.1687363000834112\n",
      "Gradient Descent(511/999): loss=0.16873214051611982\n",
      "Gradient Descent(512/999): loss=0.16872798391783514\n",
      "Gradient Descent(513/999): loss=0.16872383028527582\n",
      "Gradient Descent(514/999): loss=0.16871967961516446\n",
      "Gradient Descent(515/999): loss=0.16871553190422756\n",
      "Gradient Descent(516/999): loss=0.16871138714919545\n",
      "Gradient Descent(517/999): loss=0.16870724534680234\n",
      "Gradient Descent(518/999): loss=0.16870310649378645\n",
      "Gradient Descent(519/999): loss=0.16869897058688993\n",
      "Gradient Descent(520/999): loss=0.1686948376228586\n",
      "Gradient Descent(521/999): loss=0.16869070759844237\n",
      "Gradient Descent(522/999): loss=0.16868658051039498\n",
      "Gradient Descent(523/999): loss=0.16868245635547396\n",
      "Gradient Descent(524/999): loss=0.16867833513044075\n",
      "Gradient Descent(525/999): loss=0.16867421683206069\n",
      "Gradient Descent(526/999): loss=0.168670101457103\n",
      "Gradient Descent(527/999): loss=0.1686659890023406\n",
      "Gradient Descent(528/999): loss=0.16866187946455047\n",
      "Gradient Descent(529/999): loss=0.16865777284051325\n",
      "Gradient Descent(530/999): loss=0.1686536691270135\n",
      "Gradient Descent(531/999): loss=0.1686495683208396\n",
      "Gradient Descent(532/999): loss=0.1686454704187838\n",
      "Gradient Descent(533/999): loss=0.16864137541764224\n",
      "Gradient Descent(534/999): loss=0.16863728331421468\n",
      "Gradient Descent(535/999): loss=0.1686331941053047\n",
      "Gradient Descent(536/999): loss=0.16862910778771995\n",
      "Gradient Descent(537/999): loss=0.16862502435827173\n",
      "Gradient Descent(538/999): loss=0.16862094381377515\n",
      "Gradient Descent(539/999): loss=0.16861686615104896\n",
      "Gradient Descent(540/999): loss=0.168612791366916\n",
      "Gradient Descent(541/999): loss=0.16860871945820266\n",
      "Gradient Descent(542/999): loss=0.1686046504217392\n",
      "Gradient Descent(543/999): loss=0.1686005842543597\n",
      "Gradient Descent(544/999): loss=0.16859652095290206\n",
      "Gradient Descent(545/999): loss=0.16859246051420773\n",
      "Gradient Descent(546/999): loss=0.16858840293512206\n",
      "Gradient Descent(547/999): loss=0.16858434821249418\n",
      "Gradient Descent(548/999): loss=0.16858029634317698\n",
      "Gradient Descent(549/999): loss=0.16857624732402707\n",
      "Gradient Descent(550/999): loss=0.16857220115190472\n",
      "Gradient Descent(551/999): loss=0.16856815782367424\n",
      "Gradient Descent(552/999): loss=0.16856411733620316\n",
      "Gradient Descent(553/999): loss=0.16856007968636338\n",
      "Gradient Descent(554/999): loss=0.1685560448710299\n",
      "Gradient Descent(555/999): loss=0.16855201288708194\n",
      "Gradient Descent(556/999): loss=0.16854798373140217\n",
      "Gradient Descent(557/999): loss=0.16854395740087694\n",
      "Gradient Descent(558/999): loss=0.16853993389239666\n",
      "Gradient Descent(559/999): loss=0.168535913202855\n",
      "Gradient Descent(560/999): loss=0.16853189532914953\n",
      "Gradient Descent(561/999): loss=0.16852788026818158\n",
      "Gradient Descent(562/999): loss=0.16852386801685607\n",
      "Gradient Descent(563/999): loss=0.16851985857208165\n",
      "Gradient Descent(564/999): loss=0.16851585193077065\n",
      "Gradient Descent(565/999): loss=0.16851184808983907\n",
      "Gradient Descent(566/999): loss=0.1685078470462065\n",
      "Gradient Descent(567/999): loss=0.16850384879679653\n",
      "Gradient Descent(568/999): loss=0.16849985333853584\n",
      "Gradient Descent(569/999): loss=0.16849586066835529\n",
      "Gradient Descent(570/999): loss=0.1684918707831891\n",
      "Gradient Descent(571/999): loss=0.16848788367997541\n",
      "Gradient Descent(572/999): loss=0.16848389935565572\n",
      "Gradient Descent(573/999): loss=0.1684799178071752\n",
      "Gradient Descent(574/999): loss=0.16847593903148297\n",
      "Gradient Descent(575/999): loss=0.16847196302553136\n",
      "Gradient Descent(576/999): loss=0.1684679897862766\n",
      "Gradient Descent(577/999): loss=0.16846401931067848\n",
      "Gradient Descent(578/999): loss=0.16846005159570038\n",
      "Gradient Descent(579/999): loss=0.16845608663830935\n",
      "Gradient Descent(580/999): loss=0.16845212443547608\n",
      "Gradient Descent(581/999): loss=0.1684481649841746\n",
      "Gradient Descent(582/999): loss=0.16844420828138293\n",
      "Gradient Descent(583/999): loss=0.16844025432408244\n",
      "Gradient Descent(584/999): loss=0.16843630310925808\n",
      "Gradient Descent(585/999): loss=0.16843235463389855\n",
      "Gradient Descent(586/999): loss=0.16842840889499613\n",
      "Gradient Descent(587/999): loss=0.16842446588954646\n",
      "Gradient Descent(588/999): loss=0.1684205256145489\n",
      "Gradient Descent(589/999): loss=0.16841658806700643\n",
      "Gradient Descent(590/999): loss=0.1684126532439255\n",
      "Gradient Descent(591/999): loss=0.16840872114231623\n",
      "Gradient Descent(592/999): loss=0.1684047917591921\n",
      "Gradient Descent(593/999): loss=0.1684008650915704\n",
      "Gradient Descent(594/999): loss=0.1683969411364719\n",
      "Gradient Descent(595/999): loss=0.1683930198909206\n",
      "Gradient Descent(596/999): loss=0.1683891013519444\n",
      "Gradient Descent(597/999): loss=0.1683851855165749\n",
      "Gradient Descent(598/999): loss=0.1683812723818466\n",
      "Gradient Descent(599/999): loss=0.16837736194479805\n",
      "Gradient Descent(600/999): loss=0.16837345420247124\n",
      "Gradient Descent(601/999): loss=0.1683695491519115\n",
      "Gradient Descent(602/999): loss=0.16836564679016766\n",
      "Gradient Descent(603/999): loss=0.16836174711429253\n",
      "Gradient Descent(604/999): loss=0.1683578501213418\n",
      "Gradient Descent(605/999): loss=0.16835395580837498\n",
      "Gradient Descent(606/999): loss=0.168350064172455\n",
      "Gradient Descent(607/999): loss=0.16834617521064843\n",
      "Gradient Descent(608/999): loss=0.1683422889200252\n",
      "Gradient Descent(609/999): loss=0.16833840529765864\n",
      "Gradient Descent(610/999): loss=0.16833452434062574\n",
      "Gradient Descent(611/999): loss=0.16833064604600687\n",
      "Gradient Descent(612/999): loss=0.1683267704108859\n",
      "Gradient Descent(613/999): loss=0.1683228974323501\n",
      "Gradient Descent(614/999): loss=0.16831902710749025\n",
      "Gradient Descent(615/999): loss=0.16831515943340075\n",
      "Gradient Descent(616/999): loss=0.16831129440717912\n",
      "Gradient Descent(617/999): loss=0.16830743202592666\n",
      "Gradient Descent(618/999): loss=0.1683035722867478\n",
      "Gradient Descent(619/999): loss=0.16829971518675071\n",
      "Gradient Descent(620/999): loss=0.16829586072304686\n",
      "Gradient Descent(621/999): loss=0.16829200889275112\n",
      "Gradient Descent(622/999): loss=0.1682881596929818\n",
      "Gradient Descent(623/999): loss=0.1682843131208607\n",
      "Gradient Descent(624/999): loss=0.16828046917351303\n",
      "Gradient Descent(625/999): loss=0.16827662784806738\n",
      "Gradient Descent(626/999): loss=0.16827278914165572\n",
      "Gradient Descent(627/999): loss=0.16826895305141357\n",
      "Gradient Descent(628/999): loss=0.1682651195744797\n",
      "Gradient Descent(629/999): loss=0.16826128870799628\n",
      "Gradient Descent(630/999): loss=0.168257460449109\n",
      "Gradient Descent(631/999): loss=0.16825363479496688\n",
      "Gradient Descent(632/999): loss=0.1682498117427223\n",
      "Gradient Descent(633/999): loss=0.16824599128953105\n",
      "Gradient Descent(634/999): loss=0.1682421734325523\n",
      "Gradient Descent(635/999): loss=0.16823835816894855\n",
      "Gradient Descent(636/999): loss=0.1682345454958858\n",
      "Gradient Descent(637/999): loss=0.1682307354105333\n",
      "Gradient Descent(638/999): loss=0.16822692791006352\n",
      "Gradient Descent(639/999): loss=0.16822312299165273\n",
      "Gradient Descent(640/999): loss=0.16821932065248013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(641/999): loss=0.16821552088972844\n",
      "Gradient Descent(642/999): loss=0.1682117237005837\n",
      "Gradient Descent(643/999): loss=0.16820792908223536\n",
      "Gradient Descent(644/999): loss=0.16820413703187603\n",
      "Gradient Descent(645/999): loss=0.16820034754670196\n",
      "Gradient Descent(646/999): loss=0.16819656062391233\n",
      "Gradient Descent(647/999): loss=0.16819277626071\n",
      "Gradient Descent(648/999): loss=0.16818899445430097\n",
      "Gradient Descent(649/999): loss=0.1681852152018947\n",
      "Gradient Descent(650/999): loss=0.1681814385007037\n",
      "Gradient Descent(651/999): loss=0.16817766434794398\n",
      "Gradient Descent(652/999): loss=0.16817389274083488\n",
      "Gradient Descent(653/999): loss=0.16817012367659898\n",
      "Gradient Descent(654/999): loss=0.16816635715246217\n",
      "Gradient Descent(655/999): loss=0.16816259316565352\n",
      "Gradient Descent(656/999): loss=0.16815883171340573\n",
      "Gradient Descent(657/999): loss=0.16815507279295433\n",
      "Gradient Descent(658/999): loss=0.1681513164015385\n",
      "Gradient Descent(659/999): loss=0.16814756253640045\n",
      "Gradient Descent(660/999): loss=0.1681438111947858\n",
      "Gradient Descent(661/999): loss=0.1681400623739434\n",
      "Gradient Descent(662/999): loss=0.16813631607112545\n",
      "Gradient Descent(663/999): loss=0.16813257228358724\n",
      "Gradient Descent(664/999): loss=0.16812883100858741\n",
      "Gradient Descent(665/999): loss=0.16812509224338784\n",
      "Gradient Descent(666/999): loss=0.16812135598525382\n",
      "Gradient Descent(667/999): loss=0.16811762223145343\n",
      "Gradient Descent(668/999): loss=0.16811389097925858\n",
      "Gradient Descent(669/999): loss=0.16811016222594408\n",
      "Gradient Descent(670/999): loss=0.1681064359687879\n",
      "Gradient Descent(671/999): loss=0.16810271220507148\n",
      "Gradient Descent(672/999): loss=0.16809899093207928\n",
      "Gradient Descent(673/999): loss=0.16809527214709918\n",
      "Gradient Descent(674/999): loss=0.1680915558474221\n",
      "Gradient Descent(675/999): loss=0.16808784203034233\n",
      "Gradient Descent(676/999): loss=0.1680841306931571\n",
      "Gradient Descent(677/999): loss=0.1680804218331672\n",
      "Gradient Descent(678/999): loss=0.16807671544767652\n",
      "Gradient Descent(679/999): loss=0.16807301153399185\n",
      "Gradient Descent(680/999): loss=0.16806931008942344\n",
      "Gradient Descent(681/999): loss=0.16806561111128487\n",
      "Gradient Descent(682/999): loss=0.16806191459689254\n",
      "Gradient Descent(683/999): loss=0.1680582205435664\n",
      "Gradient Descent(684/999): loss=0.16805452894862924\n",
      "Gradient Descent(685/999): loss=0.1680508398094073\n",
      "Gradient Descent(686/999): loss=0.16804715312322976\n",
      "Gradient Descent(687/999): loss=0.16804346888742916\n",
      "Gradient Descent(688/999): loss=0.16803978709934106\n",
      "Gradient Descent(689/999): loss=0.1680361077563043\n",
      "Gradient Descent(690/999): loss=0.1680324308556609\n",
      "Gradient Descent(691/999): loss=0.16802875639475576\n",
      "Gradient Descent(692/999): loss=0.16802508437093733\n",
      "Gradient Descent(693/999): loss=0.16802141478155674\n",
      "Gradient Descent(694/999): loss=0.16801774762396868\n",
      "Gradient Descent(695/999): loss=0.1680140828955308\n",
      "Gradient Descent(696/999): loss=0.16801042059360377\n",
      "Gradient Descent(697/999): loss=0.16800676071555168\n",
      "Gradient Descent(698/999): loss=0.1680031032587414\n",
      "Gradient Descent(699/999): loss=0.1679994482205433\n",
      "Gradient Descent(700/999): loss=0.16799579559833042\n",
      "Gradient Descent(701/999): loss=0.16799214538947926\n",
      "Gradient Descent(702/999): loss=0.16798849759136938\n",
      "Gradient Descent(703/999): loss=0.1679848522013832\n",
      "Gradient Descent(704/999): loss=0.16798120921690662\n",
      "Gradient Descent(705/999): loss=0.16797756863532834\n",
      "Gradient Descent(706/999): loss=0.1679739304540403\n",
      "Gradient Descent(707/999): loss=0.16797029467043753\n",
      "Gradient Descent(708/999): loss=0.16796666128191792\n",
      "Gradient Descent(709/999): loss=0.16796303028588278\n",
      "Gradient Descent(710/999): loss=0.16795940167973628\n",
      "Gradient Descent(711/999): loss=0.16795577546088583\n",
      "Gradient Descent(712/999): loss=0.16795215162674168\n",
      "Gradient Descent(713/999): loss=0.16794853017471736\n",
      "Gradient Descent(714/999): loss=0.16794491110222934\n",
      "Gradient Descent(715/999): loss=0.16794129440669728\n",
      "Gradient Descent(716/999): loss=0.16793768008554363\n",
      "Gradient Descent(717/999): loss=0.16793406813619421\n",
      "Gradient Descent(718/999): loss=0.16793045855607777\n",
      "Gradient Descent(719/999): loss=0.16792685134262597\n",
      "Gradient Descent(720/999): loss=0.16792324649327373\n",
      "Gradient Descent(721/999): loss=0.16791964400545872\n",
      "Gradient Descent(722/999): loss=0.16791604387662207\n",
      "Gradient Descent(723/999): loss=0.16791244610420755\n",
      "Gradient Descent(724/999): loss=0.1679088506856622\n",
      "Gradient Descent(725/999): loss=0.16790525761843586\n",
      "Gradient Descent(726/999): loss=0.16790166689998162\n",
      "Gradient Descent(727/999): loss=0.16789807852775537\n",
      "Gradient Descent(728/999): loss=0.1678944924992162\n",
      "Gradient Descent(729/999): loss=0.16789090881182617\n",
      "Gradient Descent(730/999): loss=0.16788732746305013\n",
      "Gradient Descent(731/999): loss=0.16788374845035628\n",
      "Gradient Descent(732/999): loss=0.16788017177121572\n",
      "Gradient Descent(733/999): loss=0.1678765974231021\n",
      "Gradient Descent(734/999): loss=0.16787302540349278\n",
      "Gradient Descent(735/999): loss=0.16786945570986753\n",
      "Gradient Descent(736/999): loss=0.1678658883397094\n",
      "Gradient Descent(737/999): loss=0.16786232329050446\n",
      "Gradient Descent(738/999): loss=0.16785876055974147\n",
      "Gradient Descent(739/999): loss=0.1678552001449124\n",
      "Gradient Descent(740/999): loss=0.16785164204351205\n",
      "Gradient Descent(741/999): loss=0.1678480862530383\n",
      "Gradient Descent(742/999): loss=0.16784453277099196\n",
      "Gradient Descent(743/999): loss=0.16784098159487665\n",
      "Gradient Descent(744/999): loss=0.16783743272219928\n",
      "Gradient Descent(745/999): loss=0.1678338861504693\n",
      "Gradient Descent(746/999): loss=0.16783034187719945\n",
      "Gradient Descent(747/999): loss=0.16782679989990518\n",
      "Gradient Descent(748/999): loss=0.1678232602161049\n",
      "Gradient Descent(749/999): loss=0.1678197228233202\n",
      "Gradient Descent(750/999): loss=0.16781618771907525\n",
      "Gradient Descent(751/999): loss=0.16781265490089745\n",
      "Gradient Descent(752/999): loss=0.16780912436631698\n",
      "Gradient Descent(753/999): loss=0.16780559611286688\n",
      "Gradient Descent(754/999): loss=0.16780207013808318\n",
      "Gradient Descent(755/999): loss=0.1677985464395049\n",
      "Gradient Descent(756/999): loss=0.16779502501467386\n",
      "Gradient Descent(757/999): loss=0.16779150586113475\n",
      "Gradient Descent(758/999): loss=0.1677879889764354\n",
      "Gradient Descent(759/999): loss=0.16778447435812627\n",
      "Gradient Descent(760/999): loss=0.1677809620037609\n",
      "Gradient Descent(761/999): loss=0.16777745191089546\n",
      "Gradient Descent(762/999): loss=0.1677739440770894\n",
      "Gradient Descent(763/999): loss=0.16777043849990478\n",
      "Gradient Descent(764/999): loss=0.1677669351769064\n",
      "Gradient Descent(765/999): loss=0.16776343410566255\n",
      "Gradient Descent(766/999): loss=0.16775993528374367\n",
      "Gradient Descent(767/999): loss=0.1677564387087235\n",
      "Gradient Descent(768/999): loss=0.16775294437817861\n",
      "Gradient Descent(769/999): loss=0.16774945228968827\n",
      "Gradient Descent(770/999): loss=0.1677459624408347\n",
      "Gradient Descent(771/999): loss=0.16774247482920307\n",
      "Gradient Descent(772/999): loss=0.1677389894523813\n",
      "Gradient Descent(773/999): loss=0.16773550630796014\n",
      "Gradient Descent(774/999): loss=0.16773202539353313\n",
      "Gradient Descent(775/999): loss=0.16772854670669698\n",
      "Gradient Descent(776/999): loss=0.16772507024505084\n",
      "Gradient Descent(777/999): loss=0.16772159600619693\n",
      "Gradient Descent(778/999): loss=0.1677181239877402\n",
      "Gradient Descent(779/999): loss=0.16771465418728837\n",
      "Gradient Descent(780/999): loss=0.16771118660245232\n",
      "Gradient Descent(781/999): loss=0.1677077212308453\n",
      "Gradient Descent(782/999): loss=0.1677042580700837\n",
      "Gradient Descent(783/999): loss=0.16770079711778668\n",
      "Gradient Descent(784/999): loss=0.16769733837157594\n",
      "Gradient Descent(785/999): loss=0.16769388182907644\n",
      "Gradient Descent(786/999): loss=0.1676904274879155\n",
      "Gradient Descent(787/999): loss=0.1676869753457236\n",
      "Gradient Descent(788/999): loss=0.16768352540013381\n",
      "Gradient Descent(789/999): loss=0.16768007764878204\n",
      "Gradient Descent(790/999): loss=0.16767663208930697\n",
      "Gradient Descent(791/999): loss=0.16767318871935027\n",
      "Gradient Descent(792/999): loss=0.16766974753655597\n",
      "Gradient Descent(793/999): loss=0.16766630853857134\n",
      "Gradient Descent(794/999): loss=0.16766287172304611\n",
      "Gradient Descent(795/999): loss=0.16765943708763292\n",
      "Gradient Descent(796/999): loss=0.16765600462998723\n",
      "Gradient Descent(797/999): loss=0.1676525743477672\n",
      "Gradient Descent(798/999): loss=0.1676491462386336\n",
      "Gradient Descent(799/999): loss=0.16764572030025027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(800/999): loss=0.16764229653028362\n",
      "Gradient Descent(801/999): loss=0.1676388749264028\n",
      "Gradient Descent(802/999): loss=0.16763545548627976\n",
      "Gradient Descent(803/999): loss=0.16763203820758926\n",
      "Gradient Descent(804/999): loss=0.16762862308800872\n",
      "Gradient Descent(805/999): loss=0.1676252101252183\n",
      "Gradient Descent(806/999): loss=0.16762179931690083\n",
      "Gradient Descent(807/999): loss=0.16761839066074213\n",
      "Gradient Descent(808/999): loss=0.16761498415443046\n",
      "Gradient Descent(809/999): loss=0.16761157979565697\n",
      "Gradient Descent(810/999): loss=0.1676081775821155\n",
      "Gradient Descent(811/999): loss=0.1676047775115026\n",
      "Gradient Descent(812/999): loss=0.16760137958151758\n",
      "Gradient Descent(813/999): loss=0.16759798378986232\n",
      "Gradient Descent(814/999): loss=0.1675945901342416\n",
      "Gradient Descent(815/999): loss=0.16759119861236288\n",
      "Gradient Descent(816/999): loss=0.16758780922193614\n",
      "Gradient Descent(817/999): loss=0.1675844219606742\n",
      "Gradient Descent(818/999): loss=0.16758103682629272\n",
      "Gradient Descent(819/999): loss=0.1675776538165098\n",
      "Gradient Descent(820/999): loss=0.16757427292904625\n",
      "Gradient Descent(821/999): loss=0.16757089416162577\n",
      "Gradient Descent(822/999): loss=0.1675675175119746\n",
      "Gradient Descent(823/999): loss=0.1675641429778217\n",
      "Gradient Descent(824/999): loss=0.16756077055689864\n",
      "Gradient Descent(825/999): loss=0.1675574002469398\n",
      "Gradient Descent(826/999): loss=0.16755403204568212\n",
      "Gradient Descent(827/999): loss=0.1675506659508652\n",
      "Gradient Descent(828/999): loss=0.16754730196023138\n",
      "Gradient Descent(829/999): loss=0.16754394007152557\n",
      "Gradient Descent(830/999): loss=0.1675405802824955\n",
      "Gradient Descent(831/999): loss=0.16753722259089132\n",
      "Gradient Descent(832/999): loss=0.16753386699446612\n",
      "Gradient Descent(833/999): loss=0.16753051349097542\n",
      "Gradient Descent(834/999): loss=0.1675271620781773\n",
      "Gradient Descent(835/999): loss=0.16752381275383288\n",
      "Gradient Descent(836/999): loss=0.1675204655157054\n",
      "Gradient Descent(837/999): loss=0.16751712036156122\n",
      "Gradient Descent(838/999): loss=0.167513777289169\n",
      "Gradient Descent(839/999): loss=0.16751043629630016\n",
      "Gradient Descent(840/999): loss=0.16750709738072883\n",
      "Gradient Descent(841/999): loss=0.16750376054023158\n",
      "Gradient Descent(842/999): loss=0.16750042577258759\n",
      "Gradient Descent(843/999): loss=0.16749709307557892\n",
      "Gradient Descent(844/999): loss=0.16749376244698996\n",
      "Gradient Descent(845/999): loss=0.16749043388460785\n",
      "Gradient Descent(846/999): loss=0.1674871073862224\n",
      "Gradient Descent(847/999): loss=0.1674837829496258\n",
      "Gradient Descent(848/999): loss=0.16748046057261304\n",
      "Gradient Descent(849/999): loss=0.16747714025298163\n",
      "Gradient Descent(850/999): loss=0.16747382198853175\n",
      "Gradient Descent(851/999): loss=0.16747050577706601\n",
      "Gradient Descent(852/999): loss=0.16746719161638973\n",
      "Gradient Descent(853/999): loss=0.16746387950431077\n",
      "Gradient Descent(854/999): loss=0.16746056943863968\n",
      "Gradient Descent(855/999): loss=0.1674572614171894\n",
      "Gradient Descent(856/999): loss=0.1674539554377757\n",
      "Gradient Descent(857/999): loss=0.16745065149821656\n",
      "Gradient Descent(858/999): loss=0.16744734959633287\n",
      "Gradient Descent(859/999): loss=0.16744404972994797\n",
      "Gradient Descent(860/999): loss=0.16744075189688756\n",
      "Gradient Descent(861/999): loss=0.16743745609498037\n",
      "Gradient Descent(862/999): loss=0.1674341623220572\n",
      "Gradient Descent(863/999): loss=0.16743087057595157\n",
      "Gradient Descent(864/999): loss=0.16742758085449977\n",
      "Gradient Descent(865/999): loss=0.16742429315554036\n",
      "Gradient Descent(866/999): loss=0.1674210074769145\n",
      "Gradient Descent(867/999): loss=0.16741772381646602\n",
      "Gradient Descent(868/999): loss=0.16741444217204104\n",
      "Gradient Descent(869/999): loss=0.16741116254148847\n",
      "Gradient Descent(870/999): loss=0.16740788492265973\n",
      "Gradient Descent(871/999): loss=0.16740460931340853\n",
      "Gradient Descent(872/999): loss=0.16740133571159144\n",
      "Gradient Descent(873/999): loss=0.1673980641150672\n",
      "Gradient Descent(874/999): loss=0.16739479452169742\n",
      "Gradient Descent(875/999): loss=0.16739152692934586\n",
      "Gradient Descent(876/999): loss=0.16738826133587917\n",
      "Gradient Descent(877/999): loss=0.16738499773916618\n",
      "Gradient Descent(878/999): loss=0.16738173613707844\n",
      "Gradient Descent(879/999): loss=0.16737847652749\n",
      "Gradient Descent(880/999): loss=0.16737521890827725\n",
      "Gradient Descent(881/999): loss=0.1673719632773192\n",
      "Gradient Descent(882/999): loss=0.1673687096324974\n",
      "Gradient Descent(883/999): loss=0.16736545797169572\n",
      "Gradient Descent(884/999): loss=0.1673622082928007\n",
      "Gradient Descent(885/999): loss=0.16735896059370128\n",
      "Gradient Descent(886/999): loss=0.16735571487228884\n",
      "Gradient Descent(887/999): loss=0.16735247112645735\n",
      "Gradient Descent(888/999): loss=0.16734922935410318\n",
      "Gradient Descent(889/999): loss=0.16734598955312519\n",
      "Gradient Descent(890/999): loss=0.16734275172142468\n",
      "Gradient Descent(891/999): loss=0.16733951585690549\n",
      "Gradient Descent(892/999): loss=0.16733628195747388\n",
      "Gradient Descent(893/999): loss=0.1673330500210386\n",
      "Gradient Descent(894/999): loss=0.16732982004551067\n",
      "Gradient Descent(895/999): loss=0.16732659202880393\n",
      "Gradient Descent(896/999): loss=0.16732336596883438\n",
      "Gradient Descent(897/999): loss=0.16732014186352068\n",
      "Gradient Descent(898/999): loss=0.1673169197107836\n",
      "Gradient Descent(899/999): loss=0.16731369950854674\n",
      "Gradient Descent(900/999): loss=0.1673104812547359\n",
      "Gradient Descent(901/999): loss=0.1673072649472794\n",
      "Gradient Descent(902/999): loss=0.16730405058410808\n",
      "Gradient Descent(903/999): loss=0.167300838163155\n",
      "Gradient Descent(904/999): loss=0.16729762768235568\n",
      "Gradient Descent(905/999): loss=0.16729441913964846\n",
      "Gradient Descent(906/999): loss=0.16729121253297347\n",
      "Gradient Descent(907/999): loss=0.16728800786027379\n",
      "Gradient Descent(908/999): loss=0.16728480511949465\n",
      "Gradient Descent(909/999): loss=0.1672816043085837\n",
      "Gradient Descent(910/999): loss=0.16727840542549113\n",
      "Gradient Descent(911/999): loss=0.16727520846816937\n",
      "Gradient Descent(912/999): loss=0.1672720134345735\n",
      "Gradient Descent(913/999): loss=0.1672688203226608\n",
      "Gradient Descent(914/999): loss=0.1672656291303909\n",
      "Gradient Descent(915/999): loss=0.16726243985572598\n",
      "Gradient Descent(916/999): loss=0.16725925249663057\n",
      "Gradient Descent(917/999): loss=0.16725606705107168\n",
      "Gradient Descent(918/999): loss=0.1672528835170184\n",
      "Gradient Descent(919/999): loss=0.16724970189244256\n",
      "Gradient Descent(920/999): loss=0.1672465221753183\n",
      "Gradient Descent(921/999): loss=0.16724334436362182\n",
      "Gradient Descent(922/999): loss=0.16724016845533213\n",
      "Gradient Descent(923/999): loss=0.16723699444843043\n",
      "Gradient Descent(924/999): loss=0.16723382234090015\n",
      "Gradient Descent(925/999): loss=0.1672306521307273\n",
      "Gradient Descent(926/999): loss=0.16722748381590022\n",
      "Gradient Descent(927/999): loss=0.16722431739440952\n",
      "Gradient Descent(928/999): loss=0.1672211528642482\n",
      "Gradient Descent(929/999): loss=0.16721799022341163\n",
      "Gradient Descent(930/999): loss=0.16721482946989755\n",
      "Gradient Descent(931/999): loss=0.1672116706017061\n",
      "Gradient Descent(932/999): loss=0.16720851361683958\n",
      "Gradient Descent(933/999): loss=0.16720535851330284\n",
      "Gradient Descent(934/999): loss=0.167202205289103\n",
      "Gradient Descent(935/999): loss=0.16719905394224935\n",
      "Gradient Descent(936/999): loss=0.16719590447075397\n",
      "Gradient Descent(937/999): loss=0.16719275687263066\n",
      "Gradient Descent(938/999): loss=0.16718961114589612\n",
      "Gradient Descent(939/999): loss=0.16718646728856884\n",
      "Gradient Descent(940/999): loss=0.1671833252986702\n",
      "Gradient Descent(941/999): loss=0.16718018517422348\n",
      "Gradient Descent(942/999): loss=0.16717704691325444\n",
      "Gradient Descent(943/999): loss=0.16717391051379107\n",
      "Gradient Descent(944/999): loss=0.1671707759738639\n",
      "Gradient Descent(945/999): loss=0.16716764329150546\n",
      "Gradient Descent(946/999): loss=0.16716451246475078\n",
      "Gradient Descent(947/999): loss=0.16716138349163714\n",
      "Gradient Descent(948/999): loss=0.16715825637020423\n",
      "Gradient Descent(949/999): loss=0.16715513109849378\n",
      "Gradient Descent(950/999): loss=0.16715200767455002\n",
      "Gradient Descent(951/999): loss=0.1671488860964195\n",
      "Gradient Descent(952/999): loss=0.1671457663621509\n",
      "Gradient Descent(953/999): loss=0.16714264846979537\n",
      "Gradient Descent(954/999): loss=0.1671395324174062\n",
      "Gradient Descent(955/999): loss=0.167136418203039\n",
      "Gradient Descent(956/999): loss=0.16713330582475172\n",
      "Gradient Descent(957/999): loss=0.16713019528060452\n",
      "Gradient Descent(958/999): loss=0.16712708656865985\n",
      "Gradient Descent(959/999): loss=0.1671239796869825\n",
      "Gradient Descent(960/999): loss=0.16712087463363937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(961/999): loss=0.16711777140669976\n",
      "Gradient Descent(962/999): loss=0.1671146700042352\n",
      "Gradient Descent(963/999): loss=0.16711157042431946\n",
      "Gradient Descent(964/999): loss=0.16710847266502862\n",
      "Gradient Descent(965/999): loss=0.1671053767244409\n",
      "Gradient Descent(966/999): loss=0.16710228260063692\n",
      "Gradient Descent(967/999): loss=0.16709919029169953\n",
      "Gradient Descent(968/999): loss=0.1670960997957137\n",
      "Gradient Descent(969/999): loss=0.16709301111076658\n",
      "Gradient Descent(970/999): loss=0.167089924234948\n",
      "Gradient Descent(971/999): loss=0.16708683916634953\n",
      "Gradient Descent(972/999): loss=0.16708375590306518\n",
      "Gradient Descent(973/999): loss=0.1670806744431912\n",
      "Gradient Descent(974/999): loss=0.1670775947848261\n",
      "Gradient Descent(975/999): loss=0.16707451692607053\n",
      "Gradient Descent(976/999): loss=0.16707144086502745\n",
      "Gradient Descent(977/999): loss=0.16706836659980187\n",
      "Gradient Descent(978/999): loss=0.16706529412850132\n",
      "Gradient Descent(979/999): loss=0.16706222344923538\n",
      "Gradient Descent(980/999): loss=0.1670591545601157\n",
      "Gradient Descent(981/999): loss=0.16705608745925635\n",
      "Gradient Descent(982/999): loss=0.16705302214477347\n",
      "Gradient Descent(983/999): loss=0.16704995861478564\n",
      "Gradient Descent(984/999): loss=0.1670468968674134\n",
      "Gradient Descent(985/999): loss=0.16704383690077948\n",
      "Gradient Descent(986/999): loss=0.16704077871300904\n",
      "Gradient Descent(987/999): loss=0.16703772230222913\n",
      "Gradient Descent(988/999): loss=0.16703466766656927\n",
      "Gradient Descent(989/999): loss=0.16703161480416098\n",
      "Gradient Descent(990/999): loss=0.16702856371313815\n",
      "Gradient Descent(991/999): loss=0.16702551439163665\n",
      "Gradient Descent(992/999): loss=0.16702246683779465\n",
      "Gradient Descent(993/999): loss=0.1670194210497525\n",
      "Gradient Descent(994/999): loss=0.16701637702565264\n",
      "Gradient Descent(995/999): loss=0.16701333476363983\n",
      "Gradient Descent(996/999): loss=0.1670102942618609\n",
      "Gradient Descent(997/999): loss=0.1670072555184648\n",
      "Gradient Descent(998/999): loss=0.16700421853160272\n",
      "Gradient Descent(999/999): loss=0.1670011832994281\n"
     ]
    }
   ],
   "source": [
    "max_iters = 1000\n",
    "initial_w = np.ones((X_poly.shape[1]))*(0)\n",
    "gamma = 1e-7\n",
    "lambda_ =  1e-9\n",
    "w, loss = least_squares_L1(Y_total, X_poly, initial_w, max_iters, gamma, lambda_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sort = np.sort(abs(w))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(len(w)),sort)\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Weight absolute value\")\n",
    "plt.title(\"Sorted weights\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% ws filtered: 0.9925373134328358\n"
     ]
    }
   ],
   "source": [
    "# Filter weights by size\n",
    "filt = abs(w)>0.0005e-5\n",
    "print(\"% ws filtered:\", sum(filt)/len(w))\n",
    "\n",
    "# Create list containing degrees used\n",
    "id_list = []\n",
    "di = 0\n",
    "for i in range(len(X_nans[0])):\n",
    "    if i in index_list:\n",
    "        for deg in degree[di]:\n",
    "            id_list.append([i,deg])\n",
    "        di=+1\n",
    "    else:\n",
    "        id_list.append([i])\n",
    "\n",
    "# Apply filter to list\n",
    "id_filt = list(compress(id_list, filt))\n",
    "index_list_f = list(np.unique([i[0] for i in id_filt]))\n",
    "degree_f = [[] for i in range(X_nans.shape[1])]\n",
    "for i in id_filt:\n",
    "    if len(i)==1:\n",
    "        degree_f[i[0]].append(1)\n",
    "    else:\n",
    "        degree_f[i[0]].append(i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing reduced feature model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 0.7887013333333333 0.788172\n"
     ]
    }
   ],
   "source": [
    "X_poly_f = build_poly_index(X_nans, index_list_f, degree_f)\n",
    "\n",
    "dtmp_tr,dtmp_te = cross_validation(Y_total, X_poly_f, k_fold=4, seed=2, function_name='least_squares')\n",
    "print(\"Baseline:\", dtmp_tr, dtmp_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
