{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils_predictions_manipulation import*\n",
    "from utils_nans_manipulation import*\n",
    "from cross_validation import*\n",
    "from utils_data_loading import*\n",
    "from utils_features_manipulation import*\n",
    "from standardization import*\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import compress\n",
    "from least_squares import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_L1(y, tx, w, lambda_):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    \n",
    "    e = (y - tx.dot(w))\n",
    "    return -(tx.transpose().dot(e) / y.shape[0]) + lambda_*np.sign(w)\n",
    "\n",
    "def least_squares_L1(y, tx, initial_w, max_iters, gamma, lambda_):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient and loss\n",
    "        g = compute_gradient_L1(y, tx, w, lambda_)\n",
    "        loss = compute_mse(y, tx, w)\n",
    "\n",
    "        # update w by gradient\n",
    "        w = w - gamma*g\n",
    "\n",
    "        # store w and loss\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata,_ = load_data('Data/train.csv')\n",
    "X_total, Y_total = structure_data(traindata)\n",
    "\n",
    "# Replacing undefined data with NaNs\n",
    "X_nans = replace_bad_data_with_nans(X_total, -999)\n",
    "\n",
    "X_nans, col = replace_nans_with_median(X_nans, threshold=0.5)\n",
    "\n",
    "X_nans = standardize_data(X_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 0.7339773333333333 0.7337959999999999\n"
     ]
    }
   ],
   "source": [
    "dtmp_tr,dtmp_te = cross_validation(Y_total, X_nans, k_fold=4, seed=2, function_name='least_squares')\n",
    "print(\"Baseline:\", dtmp_tr, dtmp_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 0.78872 0.788544\n"
     ]
    }
   ],
   "source": [
    "index_list = list(range(2,X_nans.shape[1]))\n",
    "\n",
    "degree_test = [1, 1/4,1/3, 1/2, 2,3]\n",
    "degree = [degree_test for i in range(X_nans.shape[1])]\n",
    "\n",
    "#index_list = [1, 3, 5, 7, 13, 16, 18, 20, 23] \n",
    "#degree = [(0.25, 0.3333333333333333, 2), (0.03333333333333333, 0.1, 0.2, 0.3333333333333333, 0.5, 2), (0.1, 0.3333333333333333, 20), (0.03333333333333333, 0.05, 0.2, 0.3333333333333333, 30), (0.05, 10), (0.2, 0.25, 0.3333333333333333, 3, 4, 5), (3, 10, 20), (0.1, 0.3333333333333333, 4), (0.03333333333333333, 0.2, 1), (0.1, 2, 5), (0.05, 0.1, 1), (0.2, 0.3333333333333333, 0.5, 2, 4, 10), (0.1, 0.25, 0.3333333333333333, 1, 5, 30), (1, 3, 30), (0.1, 5, 10), (0.25, 3, 30), (0.1, 10, 30), (0.2, 1, 3), (0.05, 0.5, 30), (1, 3, 4), (0.3333333333333333, 3, 5), (2, 10), (0.25, 0.3333333333333333, 1), (0.05, 0.2, 30)]\n",
    "\n",
    "X_poly = build_poly_index(X_nans, index_list, degree)\n",
    "\n",
    "dtmp_tr,dtmp_te = cross_validation(Y_total, X_poly, k_fold=4, seed=1, function_name='least_squares')\n",
    "print(\"Baseline:\", dtmp_tr, dtmp_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999): loss=0.171334\n",
      "Gradient Descent(1/999): loss=0.17132689577187044\n",
      "Gradient Descent(2/999): loss=0.17132001410766098\n",
      "Gradient Descent(3/999): loss=0.17131330255137775\n",
      "Gradient Descent(4/999): loss=0.17130672130781865\n",
      "Gradient Descent(5/999): loss=0.17130024018640608\n",
      "Gradient Descent(6/999): loss=0.17129383628275077\n",
      "Gradient Descent(7/999): loss=0.17128749221985443\n",
      "Gradient Descent(8/999): loss=0.17128119481386736\n",
      "Gradient Descent(9/999): loss=0.17127493406191388\n",
      "Gradient Descent(10/999): loss=0.17126870237424593\n",
      "Gradient Descent(11/999): loss=0.17126249399174623\n",
      "Gradient Descent(12/999): loss=0.1712563045440353\n",
      "Gradient Descent(13/999): loss=0.17125013071425127\n",
      "Gradient Descent(14/999): loss=0.1712439699847435\n",
      "Gradient Descent(15/999): loss=0.17123782044415561\n",
      "Gradient Descent(16/999): loss=0.17123168064107405\n",
      "Gradient Descent(17/999): loss=0.1712255494730063\n",
      "Gradient Descent(18/999): loss=0.17121942610215626\n",
      "Gradient Descent(19/999): loss=0.1712133098915325\n",
      "Gradient Descent(20/999): loss=0.17120720035647702\n",
      "Gradient Descent(21/999): loss=0.17120109712789666\n",
      "Gradient Descent(22/999): loss=0.1711949999243681\n",
      "Gradient Descent(23/999): loss=0.17118890853097743\n",
      "Gradient Descent(24/999): loss=0.17118282278326746\n",
      "Gradient Descent(25/999): loss=0.1711767425550594\n",
      "Gradient Descent(26/999): loss=0.17117066774921502\n",
      "Gradient Descent(27/999): loss=0.17116459829062813\n",
      "Gradient Descent(28/999): loss=0.171158534120908\n",
      "Gradient Descent(29/999): loss=0.17115247519434632\n",
      "Gradient Descent(30/999): loss=0.1711464214748574\n",
      "Gradient Descent(31/999): loss=0.17114037293365714\n",
      "Gradient Descent(32/999): loss=0.1711343295475021\n",
      "Gradient Descent(33/999): loss=0.17112829129735405\n",
      "Gradient Descent(34/999): loss=0.1711222581673662\n",
      "Gradient Descent(35/999): loss=0.17111623014411498\n",
      "Gradient Descent(36/999): loss=0.1711102072160163\n",
      "Gradient Descent(37/999): loss=0.1711041893728836\n",
      "Gradient Descent(38/999): loss=0.17109817660559187\n",
      "Gradient Descent(39/999): loss=0.17109216890582352\n",
      "Gradient Descent(40/999): loss=0.1710861662658749\n",
      "Gradient Descent(41/999): loss=0.17108016867850961\n",
      "Gradient Descent(42/999): loss=0.17107417613684786\n",
      "Gradient Descent(43/999): loss=0.17106818863428155\n",
      "Gradient Descent(44/999): loss=0.1710622061644109\n",
      "Gradient Descent(45/999): loss=0.1710562287209953\n",
      "Gradient Descent(46/999): loss=0.17105025629791715\n",
      "Gradient Descent(47/999): loss=0.17104428888915324\n",
      "Gradient Descent(48/999): loss=0.1710383264887541\n",
      "Gradient Descent(49/999): loss=0.17103236909082772\n",
      "Gradient Descent(50/999): loss=0.17102641668952717\n",
      "Gradient Descent(51/999): loss=0.17102046927904163\n",
      "Gradient Descent(52/999): loss=0.17101452685358928\n",
      "Gradient Descent(53/999): loss=0.17100858940741184\n",
      "Gradient Descent(54/999): loss=0.17100265693477076\n",
      "Gradient Descent(55/999): loss=0.1709967294299439\n",
      "Gradient Descent(56/999): loss=0.17099080688722323\n",
      "Gradient Descent(57/999): loss=0.17098488930091338\n",
      "Gradient Descent(58/999): loss=0.17097897666532988\n",
      "Gradient Descent(59/999): loss=0.1709730689747983\n",
      "Gradient Descent(60/999): loss=0.17096716622365357\n",
      "Gradient Descent(61/999): loss=0.1709612684062391\n",
      "Gradient Descent(62/999): loss=0.17095537551690695\n",
      "Gradient Descent(63/999): loss=0.17094948755001652\n",
      "Gradient Descent(64/999): loss=0.1709436044999353\n",
      "Gradient Descent(65/999): loss=0.17093772636103788\n",
      "Gradient Descent(66/999): loss=0.17093185312770628\n",
      "Gradient Descent(67/999): loss=0.17092598479432955\n",
      "Gradient Descent(68/999): loss=0.17092012135530388\n",
      "Gradient Descent(69/999): loss=0.17091426280503239\n",
      "Gradient Descent(70/999): loss=0.1709084091379251\n",
      "Gradient Descent(71/999): loss=0.170902560348399\n",
      "Gradient Descent(72/999): loss=0.17089671643087792\n",
      "Gradient Descent(73/999): loss=0.1708908773797923\n",
      "Gradient Descent(74/999): loss=0.1708850431895795\n",
      "Gradient Descent(75/999): loss=0.1708792138546837\n",
      "Gradient Descent(76/999): loss=0.1708733893695558\n",
      "Gradient Descent(77/999): loss=0.17086756972865336\n",
      "Gradient Descent(78/999): loss=0.1708617549264406\n",
      "Gradient Descent(79/999): loss=0.1708559449573887\n",
      "Gradient Descent(80/999): loss=0.17085013981597538\n",
      "Gradient Descent(81/999): loss=0.17084433949668493\n",
      "Gradient Descent(82/999): loss=0.17083854399400847\n",
      "Gradient Descent(83/999): loss=0.17083275330244377\n",
      "Gradient Descent(84/999): loss=0.17082696741649514\n",
      "Gradient Descent(85/999): loss=0.17082118633067364\n",
      "Gradient Descent(86/999): loss=0.17081541003949705\n",
      "Gradient Descent(87/999): loss=0.17080963853748946\n",
      "Gradient Descent(88/999): loss=0.1708038718191821\n",
      "Gradient Descent(89/999): loss=0.1707981098791122\n",
      "Gradient Descent(90/999): loss=0.17079235271182405\n",
      "Gradient Descent(91/999): loss=0.17078660031186832\n",
      "Gradient Descent(92/999): loss=0.17078085267380239\n",
      "Gradient Descent(93/999): loss=0.17077510979219004\n",
      "Gradient Descent(94/999): loss=0.17076937166160186\n",
      "Gradient Descent(95/999): loss=0.17076363827661473\n",
      "Gradient Descent(96/999): loss=0.1707579096318124\n",
      "Gradient Descent(97/999): loss=0.17075218572178483\n",
      "Gradient Descent(98/999): loss=0.17074646654112877\n",
      "Gradient Descent(99/999): loss=0.17074075208444742\n",
      "Gradient Descent(100/999): loss=0.17073504234635045\n",
      "Gradient Descent(101/999): loss=0.17072933732145404\n",
      "Gradient Descent(102/999): loss=0.17072363700438095\n",
      "Gradient Descent(103/999): loss=0.17071794138976046\n",
      "Gradient Descent(104/999): loss=0.17071225047222824\n",
      "Gradient Descent(105/999): loss=0.17070656424642644\n",
      "Gradient Descent(106/999): loss=0.17070088270700384\n",
      "Gradient Descent(107/999): loss=0.17069520584861544\n",
      "Gradient Descent(108/999): loss=0.17068953366592296\n",
      "Gradient Descent(109/999): loss=0.1706838661535943\n",
      "Gradient Descent(110/999): loss=0.17067820330630407\n",
      "Gradient Descent(111/999): loss=0.170672545118733\n",
      "Gradient Descent(112/999): loss=0.1706668915855686\n",
      "Gradient Descent(113/999): loss=0.1706612427015045\n",
      "Gradient Descent(114/999): loss=0.17065559846124104\n",
      "Gradient Descent(115/999): loss=0.17064995885948459\n",
      "Gradient Descent(116/999): loss=0.17064432389094816\n",
      "Gradient Descent(117/999): loss=0.17063869355035113\n",
      "Gradient Descent(118/999): loss=0.17063306783241924\n",
      "Gradient Descent(119/999): loss=0.17062744673188449\n",
      "Gradient Descent(120/999): loss=0.17062183024348537\n",
      "Gradient Descent(121/999): loss=0.1706162183619667\n",
      "Gradient Descent(122/999): loss=0.17061061108207956\n",
      "Gradient Descent(123/999): loss=0.1706050083985817\n",
      "Gradient Descent(124/999): loss=0.1705994103062366\n",
      "Gradient Descent(125/999): loss=0.1705938167998145\n",
      "Gradient Descent(126/999): loss=0.17058822787409192\n",
      "Gradient Descent(127/999): loss=0.17058264352385163\n",
      "Gradient Descent(128/999): loss=0.17057706374388268\n",
      "Gradient Descent(129/999): loss=0.1705714885289804\n",
      "Gradient Descent(130/999): loss=0.17056591787394648\n",
      "Gradient Descent(131/999): loss=0.17056035177358872\n",
      "Gradient Descent(132/999): loss=0.1705547902227214\n",
      "Gradient Descent(133/999): loss=0.17054923321616489\n",
      "Gradient Descent(134/999): loss=0.170543680748746\n",
      "Gradient Descent(135/999): loss=0.17053813281529764\n",
      "Gradient Descent(136/999): loss=0.17053258941065882\n",
      "Gradient Descent(137/999): loss=0.17052705052967518\n",
      "Gradient Descent(138/999): loss=0.1705215161671982\n",
      "Gradient Descent(139/999): loss=0.1705159863180858\n",
      "Gradient Descent(140/999): loss=0.17051046097720196\n",
      "Gradient Descent(141/999): loss=0.170504940139417\n",
      "Gradient Descent(142/999): loss=0.1704994237996074\n",
      "Gradient Descent(143/999): loss=0.17049391195265567\n",
      "Gradient Descent(144/999): loss=0.1704884045934506\n",
      "Gradient Descent(145/999): loss=0.17048290171688726\n",
      "Gradient Descent(146/999): loss=0.1704774033178667\n",
      "Gradient Descent(147/999): loss=0.17047190939129625\n",
      "Gradient Descent(148/999): loss=0.17046641993208927\n",
      "Gradient Descent(149/999): loss=0.17046093493516545\n",
      "Gradient Descent(150/999): loss=0.17045545439545046\n",
      "Gradient Descent(151/999): loss=0.17044997830787603\n",
      "Gradient Descent(152/999): loss=0.1704445066673801\n",
      "Gradient Descent(153/999): loss=0.17043903946890682\n",
      "Gradient Descent(154/999): loss=0.1704335767074062\n",
      "Gradient Descent(155/999): loss=0.17042811837783461\n",
      "Gradient Descent(156/999): loss=0.17042266447515425\n",
      "Gradient Descent(157/999): loss=0.17041721499433354\n",
      "Gradient Descent(158/999): loss=0.17041176993034704\n",
      "Gradient Descent(159/999): loss=0.17040632927817526\n",
      "Gradient Descent(160/999): loss=0.17040089303280467\n",
      "Gradient Descent(161/999): loss=0.170395461189228\n",
      "Gradient Descent(162/999): loss=0.17039003374244388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(163/999): loss=0.17038461068745706\n",
      "Gradient Descent(164/999): loss=0.1703791920192782\n",
      "Gradient Descent(165/999): loss=0.17037377773292414\n",
      "Gradient Descent(166/999): loss=0.17036836782341766\n",
      "Gradient Descent(167/999): loss=0.17036296228578746\n",
      "Gradient Descent(168/999): loss=0.17035756111506825\n",
      "Gradient Descent(169/999): loss=0.17035216430630085\n",
      "Gradient Descent(170/999): loss=0.170346771854532\n",
      "Gradient Descent(171/999): loss=0.1703413837548144\n",
      "Gradient Descent(172/999): loss=0.17033600000220656\n",
      "Gradient Descent(173/999): loss=0.1703306205917733\n",
      "Gradient Descent(174/999): loss=0.1703252455185852\n",
      "Gradient Descent(175/999): loss=0.17031987477771873\n",
      "Gradient Descent(176/999): loss=0.17031450836425627\n",
      "Gradient Descent(177/999): loss=0.17030914627328636\n",
      "Gradient Descent(178/999): loss=0.17030378849990319\n",
      "Gradient Descent(179/999): loss=0.17029843503920705\n",
      "Gradient Descent(180/999): loss=0.17029308588630404\n",
      "Gradient Descent(181/999): loss=0.1702877410363062\n",
      "Gradient Descent(182/999): loss=0.1702824004843314\n",
      "Gradient Descent(183/999): loss=0.17027706422550365\n",
      "Gradient Descent(184/999): loss=0.1702717322549524\n",
      "Gradient Descent(185/999): loss=0.17026640456781342\n",
      "Gradient Descent(186/999): loss=0.17026108115922794\n",
      "Gradient Descent(187/999): loss=0.1702557620243433\n",
      "Gradient Descent(188/999): loss=0.1702504471583128\n",
      "Gradient Descent(189/999): loss=0.17024513655629525\n",
      "Gradient Descent(190/999): loss=0.1702398302134554\n",
      "Gradient Descent(191/999): loss=0.17023452812496406\n",
      "Gradient Descent(192/999): loss=0.17022923028599765\n",
      "Gradient Descent(193/999): loss=0.17022393669173833\n",
      "Gradient Descent(194/999): loss=0.17021864733737424\n",
      "Gradient Descent(195/999): loss=0.17021336221809932\n",
      "Gradient Descent(196/999): loss=0.1702080813291132\n",
      "Gradient Descent(197/999): loss=0.17020280466562127\n",
      "Gradient Descent(198/999): loss=0.17019753222283482\n",
      "Gradient Descent(199/999): loss=0.17019226399597084\n",
      "Gradient Descent(200/999): loss=0.17018699998025205\n",
      "Gradient Descent(201/999): loss=0.17018174017090706\n",
      "Gradient Descent(202/999): loss=0.17017648456317014\n",
      "Gradient Descent(203/999): loss=0.17017123315228122\n",
      "Gradient Descent(204/999): loss=0.17016598593348622\n",
      "Gradient Descent(205/999): loss=0.17016074290203634\n",
      "Gradient Descent(206/999): loss=0.17015550405318902\n",
      "Gradient Descent(207/999): loss=0.17015026938220706\n",
      "Gradient Descent(208/999): loss=0.17014503888435908\n",
      "Gradient Descent(209/999): loss=0.17013981255491945\n",
      "Gradient Descent(210/999): loss=0.1701345903891681\n",
      "Gradient Descent(211/999): loss=0.17012937238239084\n",
      "Gradient Descent(212/999): loss=0.17012415852987897\n",
      "Gradient Descent(213/999): loss=0.17011894882692954\n",
      "Gradient Descent(214/999): loss=0.1701137432688453\n",
      "Gradient Descent(215/999): loss=0.17010854185093463\n",
      "Gradient Descent(216/999): loss=0.17010334456851148\n",
      "Gradient Descent(217/999): loss=0.17009815141689547\n",
      "Gradient Descent(218/999): loss=0.1700929623914121\n",
      "Gradient Descent(219/999): loss=0.17008777748739215\n",
      "Gradient Descent(220/999): loss=0.17008259670017217\n",
      "Gradient Descent(221/999): loss=0.17007742002509443\n",
      "Gradient Descent(222/999): loss=0.17007224745750665\n",
      "Gradient Descent(223/999): loss=0.17006707899276222\n",
      "Gradient Descent(224/999): loss=0.1700619146262201\n",
      "Gradient Descent(225/999): loss=0.1700567543532449\n",
      "Gradient Descent(226/999): loss=0.17005159816920673\n",
      "Gradient Descent(227/999): loss=0.17004644606948133\n",
      "Gradient Descent(228/999): loss=0.17004129804944995\n",
      "Gradient Descent(229/999): loss=0.1700361541044995\n",
      "Gradient Descent(230/999): loss=0.17003101423002234\n",
      "Gradient Descent(231/999): loss=0.17002587842141648\n",
      "Gradient Descent(232/999): loss=0.1700207466740854\n",
      "Gradient Descent(233/999): loss=0.17001561898343798\n",
      "Gradient Descent(234/999): loss=0.17001049534488902\n",
      "Gradient Descent(235/999): loss=0.1700053757538585\n",
      "Gradient Descent(236/999): loss=0.1700002602057718\n",
      "Gradient Descent(237/999): loss=0.1699951486960603\n",
      "Gradient Descent(238/999): loss=0.16999004122016048\n",
      "Gradient Descent(239/999): loss=0.16998493777351442\n",
      "Gradient Descent(240/999): loss=0.16997983835156974\n",
      "Gradient Descent(241/999): loss=0.16997474294977938\n",
      "Gradient Descent(242/999): loss=0.16996965156360203\n",
      "Gradient Descent(243/999): loss=0.16996456418850153\n",
      "Gradient Descent(244/999): loss=0.16995948081994733\n",
      "Gradient Descent(245/999): loss=0.16995440145341445\n",
      "Gradient Descent(246/999): loss=0.16994932608438315\n",
      "Gradient Descent(247/999): loss=0.1699442547083392\n",
      "Gradient Descent(248/999): loss=0.16993918732077382\n",
      "Gradient Descent(249/999): loss=0.16993412391718374\n",
      "Gradient Descent(250/999): loss=0.1699290644930709\n",
      "Gradient Descent(251/999): loss=0.16992400904394292\n",
      "Gradient Descent(252/999): loss=0.1699189575653125\n",
      "Gradient Descent(253/999): loss=0.16991391005269807\n",
      "Gradient Descent(254/999): loss=0.1699088665016233\n",
      "Gradient Descent(255/999): loss=0.16990382690761696\n",
      "Gradient Descent(256/999): loss=0.16989879126621393\n",
      "Gradient Descent(257/999): loss=0.16989375957295363\n",
      "Gradient Descent(258/999): loss=0.16988873182338146\n",
      "Gradient Descent(259/999): loss=0.16988370801304795\n",
      "Gradient Descent(260/999): loss=0.1698786881375088\n",
      "Gradient Descent(261/999): loss=0.16987367219232538\n",
      "Gradient Descent(262/999): loss=0.16986866017306432\n",
      "Gradient Descent(263/999): loss=0.16986365207529736\n",
      "Gradient Descent(264/999): loss=0.16985864789460176\n",
      "Gradient Descent(265/999): loss=0.16985364762656016\n",
      "Gradient Descent(266/999): loss=0.16984865126676027\n",
      "Gradient Descent(267/999): loss=0.16984365881079538\n",
      "Gradient Descent(268/999): loss=0.16983867025426394\n",
      "Gradient Descent(269/999): loss=0.16983368559276968\n",
      "Gradient Descent(270/999): loss=0.1698287048219217\n",
      "Gradient Descent(271/999): loss=0.16982372793733425\n",
      "Gradient Descent(272/999): loss=0.16981875493462692\n",
      "Gradient Descent(273/999): loss=0.1698137858094247\n",
      "Gradient Descent(274/999): loss=0.16980882055735755\n",
      "Gradient Descent(275/999): loss=0.16980385917406088\n",
      "Gradient Descent(276/999): loss=0.16979890165517542\n",
      "Gradient Descent(277/999): loss=0.169793947996347\n",
      "Gradient Descent(278/999): loss=0.1697889981932267\n",
      "Gradient Descent(279/999): loss=0.16978405224147086\n",
      "Gradient Descent(280/999): loss=0.16977911013674107\n",
      "Gradient Descent(281/999): loss=0.16977417187470403\n",
      "Gradient Descent(282/999): loss=0.1697692374510318\n",
      "Gradient Descent(283/999): loss=0.1697643068614015\n",
      "Gradient Descent(284/999): loss=0.1697593801014956\n",
      "Gradient Descent(285/999): loss=0.16975445716700144\n",
      "Gradient Descent(286/999): loss=0.16974953805361212\n",
      "Gradient Descent(287/999): loss=0.16974462275702523\n",
      "Gradient Descent(288/999): loss=0.16973971127294415\n",
      "Gradient Descent(289/999): loss=0.16973480359707693\n",
      "Gradient Descent(290/999): loss=0.1697298997251372\n",
      "Gradient Descent(291/999): loss=0.16972499965284332\n",
      "Gradient Descent(292/999): loss=0.16972010337591906\n",
      "Gradient Descent(293/999): loss=0.16971521089009348\n",
      "Gradient Descent(294/999): loss=0.1697103221911003\n",
      "Gradient Descent(295/999): loss=0.1697054372746788\n",
      "Gradient Descent(296/999): loss=0.16970055613657312\n",
      "Gradient Descent(297/999): loss=0.16969567877253275\n",
      "Gradient Descent(298/999): loss=0.169690805178312\n",
      "Gradient Descent(299/999): loss=0.16968593534967052\n",
      "Gradient Descent(300/999): loss=0.16968106928237298\n",
      "Gradient Descent(301/999): loss=0.16967620697218902\n",
      "Gradient Descent(302/999): loss=0.1696713484148936\n",
      "Gradient Descent(303/999): loss=0.16966649360626646\n",
      "Gradient Descent(304/999): loss=0.1696616425420927\n",
      "Gradient Descent(305/999): loss=0.1696567952181623\n",
      "Gradient Descent(306/999): loss=0.16965195163027044\n",
      "Gradient Descent(307/999): loss=0.16964711177421712\n",
      "Gradient Descent(308/999): loss=0.1696422756458077\n",
      "Gradient Descent(309/999): loss=0.1696374432408522\n",
      "Gradient Descent(310/999): loss=0.169632614555166\n",
      "Gradient Descent(311/999): loss=0.16962778958456948\n",
      "Gradient Descent(312/999): loss=0.16962296832488774\n",
      "Gradient Descent(313/999): loss=0.16961815077195125\n",
      "Gradient Descent(314/999): loss=0.1696133369215952\n",
      "Gradient Descent(315/999): loss=0.16960852676966015\n",
      "Gradient Descent(316/999): loss=0.16960372031199114\n",
      "Gradient Descent(317/999): loss=0.16959891754443868\n",
      "Gradient Descent(318/999): loss=0.16959411846285805\n",
      "Gradient Descent(319/999): loss=0.16958932306310948\n",
      "Gradient Descent(320/999): loss=0.16958453134105814\n",
      "Gradient Descent(321/999): loss=0.16957974329257441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(322/999): loss=0.16957495891353336\n",
      "Gradient Descent(323/999): loss=0.1695701781998151\n",
      "Gradient Descent(324/999): loss=0.16956540114730467\n",
      "Gradient Descent(325/999): loss=0.1695606277518922\n",
      "Gradient Descent(326/999): loss=0.16955585800947257\n",
      "Gradient Descent(327/999): loss=0.16955109191594556\n",
      "Gradient Descent(328/999): loss=0.16954632946721612\n",
      "Gradient Descent(329/999): loss=0.16954157065919379\n",
      "Gradient Descent(330/999): loss=0.16953681548779329\n",
      "Gradient Descent(331/999): loss=0.16953206394893408\n",
      "Gradient Descent(332/999): loss=0.16952731603854068\n",
      "Gradient Descent(333/999): loss=0.16952257175254237\n",
      "Gradient Descent(334/999): loss=0.1695178310868733\n",
      "Gradient Descent(335/999): loss=0.16951309403747256\n",
      "Gradient Descent(336/999): loss=0.16950836060028399\n",
      "Gradient Descent(337/999): loss=0.1695036307712567\n",
      "Gradient Descent(338/999): loss=0.1694989045463441\n",
      "Gradient Descent(339/999): loss=0.16949418192150476\n",
      "Gradient Descent(340/999): loss=0.1694894628927022\n",
      "Gradient Descent(341/999): loss=0.16948474745590467\n",
      "Gradient Descent(342/999): loss=0.169480035607085\n",
      "Gradient Descent(343/999): loss=0.1694753273422213\n",
      "Gradient Descent(344/999): loss=0.1694706226572962\n",
      "Gradient Descent(345/999): loss=0.1694659215482973\n",
      "Gradient Descent(346/999): loss=0.16946122401121683\n",
      "Gradient Descent(347/999): loss=0.16945653004205224\n",
      "Gradient Descent(348/999): loss=0.1694518396368052\n",
      "Gradient Descent(349/999): loss=0.16944715279148267\n",
      "Gradient Descent(350/999): loss=0.16944246950209613\n",
      "Gradient Descent(351/999): loss=0.16943778976466187\n",
      "Gradient Descent(352/999): loss=0.16943311357520105\n",
      "Gradient Descent(353/999): loss=0.1694284409297397\n",
      "Gradient Descent(354/999): loss=0.16942377182430826\n",
      "Gradient Descent(355/999): loss=0.1694191062549423\n",
      "Gradient Descent(356/999): loss=0.16941444421768198\n",
      "Gradient Descent(357/999): loss=0.16940978570857212\n",
      "Gradient Descent(358/999): loss=0.1694051307236626\n",
      "Gradient Descent(359/999): loss=0.1694004792590076\n",
      "Gradient Descent(360/999): loss=0.1693958313106663\n",
      "Gradient Descent(361/999): loss=0.16939118687470275\n",
      "Gradient Descent(362/999): loss=0.16938654594718527\n",
      "Gradient Descent(363/999): loss=0.16938190852418736\n",
      "Gradient Descent(364/999): loss=0.16937727460178698\n",
      "Gradient Descent(365/999): loss=0.16937264417606682\n",
      "Gradient Descent(366/999): loss=0.1693680172431143\n",
      "Gradient Descent(367/999): loss=0.16936339379902146\n",
      "Gradient Descent(368/999): loss=0.1693587738398852\n",
      "Gradient Descent(369/999): loss=0.1693541573618069\n",
      "Gradient Descent(370/999): loss=0.16934954436089272\n",
      "Gradient Descent(371/999): loss=0.16934493483325352\n",
      "Gradient Descent(372/999): loss=0.16934032877500463\n",
      "Gradient Descent(373/999): loss=0.1693357261822663\n",
      "Gradient Descent(374/999): loss=0.16933112705116332\n",
      "Gradient Descent(375/999): loss=0.169326531377825\n",
      "Gradient Descent(376/999): loss=0.16932193915838553\n",
      "Gradient Descent(377/999): loss=0.16931735038898343\n",
      "Gradient Descent(378/999): loss=0.16931276506576223\n",
      "Gradient Descent(379/999): loss=0.16930818318486976\n",
      "Gradient Descent(380/999): loss=0.16930360474245862\n",
      "Gradient Descent(381/999): loss=0.16929902973468594\n",
      "Gradient Descent(382/999): loss=0.16929445815771357\n",
      "Gradient Descent(383/999): loss=0.1692898900077078\n",
      "Gradient Descent(384/999): loss=0.1692853252808397\n",
      "Gradient Descent(385/999): loss=0.1692807639732848\n",
      "Gradient Descent(386/999): loss=0.16927620608122318\n",
      "Gradient Descent(387/999): loss=0.16927165160083962\n",
      "Gradient Descent(388/999): loss=0.16926710052832342\n",
      "Gradient Descent(389/999): loss=0.16926255285986838\n",
      "Gradient Descent(390/999): loss=0.169258008591673\n",
      "Gradient Descent(391/999): loss=0.16925346771994024\n",
      "Gradient Descent(392/999): loss=0.16924893024087764\n",
      "Gradient Descent(393/999): loss=0.16924439615069717\n",
      "Gradient Descent(394/999): loss=0.16923986544561542\n",
      "Gradient Descent(395/999): loss=0.16923533812185373\n",
      "Gradient Descent(396/999): loss=0.16923081417563757\n",
      "Gradient Descent(397/999): loss=0.16922629360319727\n",
      "Gradient Descent(398/999): loss=0.16922177640076735\n",
      "Gradient Descent(399/999): loss=0.16921726256458727\n",
      "Gradient Descent(400/999): loss=0.16921275209090053\n",
      "Gradient Descent(401/999): loss=0.16920824497595538\n",
      "Gradient Descent(402/999): loss=0.16920374121600448\n",
      "Gradient Descent(403/999): loss=0.1691992408073053\n",
      "Gradient Descent(404/999): loss=0.16919474374611917\n",
      "Gradient Descent(405/999): loss=0.1691902500287124\n",
      "Gradient Descent(406/999): loss=0.16918575965135577\n",
      "Gradient Descent(407/999): loss=0.16918127261032406\n",
      "Gradient Descent(408/999): loss=0.16917678890189702\n",
      "Gradient Descent(409/999): loss=0.16917230852235865\n",
      "Gradient Descent(410/999): loss=0.16916783146799738\n",
      "Gradient Descent(411/999): loss=0.16916335773510613\n",
      "Gradient Descent(412/999): loss=0.16915888731998216\n",
      "Gradient Descent(413/999): loss=0.16915442021892738\n",
      "Gradient Descent(414/999): loss=0.16914995642824787\n",
      "Gradient Descent(415/999): loss=0.16914549594425426\n",
      "Gradient Descent(416/999): loss=0.16914103876326164\n",
      "Gradient Descent(417/999): loss=0.16913658488158942\n",
      "Gradient Descent(418/999): loss=0.1691321342955615\n",
      "Gradient Descent(419/999): loss=0.1691276870015061\n",
      "Gradient Descent(420/999): loss=0.16912324299575596\n",
      "Gradient Descent(421/999): loss=0.16911880227464796\n",
      "Gradient Descent(422/999): loss=0.16911436483452358\n",
      "Gradient Descent(423/999): loss=0.16910993067172878\n",
      "Gradient Descent(424/999): loss=0.16910549978261355\n",
      "Gradient Descent(425/999): loss=0.16910107216353243\n",
      "Gradient Descent(426/999): loss=0.1690966478108445\n",
      "Gradient Descent(427/999): loss=0.16909222672091287\n",
      "Gradient Descent(428/999): loss=0.16908780889010522\n",
      "Gradient Descent(429/999): loss=0.1690833943147935\n",
      "Gradient Descent(430/999): loss=0.16907898299135402\n",
      "Gradient Descent(431/999): loss=0.16907457491616745\n",
      "Gradient Descent(432/999): loss=0.16907017008561873\n",
      "Gradient Descent(433/999): loss=0.1690657684960971\n",
      "Gradient Descent(434/999): loss=0.1690613701439964\n",
      "Gradient Descent(435/999): loss=0.1690569750257142\n",
      "Gradient Descent(436/999): loss=0.16905258313765303\n",
      "Gradient Descent(437/999): loss=0.16904819447621933\n",
      "Gradient Descent(438/999): loss=0.16904380903782396\n",
      "Gradient Descent(439/999): loss=0.16903942681888218\n",
      "Gradient Descent(440/999): loss=0.16903504781581322\n",
      "Gradient Descent(441/999): loss=0.16903067202504082\n",
      "Gradient Descent(442/999): loss=0.16902629944299302\n",
      "Gradient Descent(443/999): loss=0.16902193006610214\n",
      "Gradient Descent(444/999): loss=0.16901756389080458\n",
      "Gradient Descent(445/999): loss=0.16901320091354113\n",
      "Gradient Descent(446/999): loss=0.16900884113075695\n",
      "Gradient Descent(447/999): loss=0.16900448453890127\n",
      "Gradient Descent(448/999): loss=0.16900013113442763\n",
      "Gradient Descent(449/999): loss=0.1689957809137938\n",
      "Gradient Descent(450/999): loss=0.16899143387346188\n",
      "Gradient Descent(451/999): loss=0.1689870900098981\n",
      "Gradient Descent(452/999): loss=0.16898274931957286\n",
      "Gradient Descent(453/999): loss=0.1689784117989608\n",
      "Gradient Descent(454/999): loss=0.16897407744454102\n",
      "Gradient Descent(455/999): loss=0.1689697462527965\n",
      "Gradient Descent(456/999): loss=0.16896541822021463\n",
      "Gradient Descent(457/999): loss=0.16896109334328682\n",
      "Gradient Descent(458/999): loss=0.168956771618509\n",
      "Gradient Descent(459/999): loss=0.1689524530423809\n",
      "Gradient Descent(460/999): loss=0.16894813761140673\n",
      "Gradient Descent(461/999): loss=0.16894382532209465\n",
      "Gradient Descent(462/999): loss=0.16893951617095718\n",
      "Gradient Descent(463/999): loss=0.1689352101545109\n",
      "Gradient Descent(464/999): loss=0.16893090726927656\n",
      "Gradient Descent(465/999): loss=0.1689266075117793\n",
      "Gradient Descent(466/999): loss=0.16892231087854778\n",
      "Gradient Descent(467/999): loss=0.1689180173661156\n",
      "Gradient Descent(468/999): loss=0.1689137269710201\n",
      "Gradient Descent(469/999): loss=0.16890943968980274\n",
      "Gradient Descent(470/999): loss=0.16890515551900911\n",
      "Gradient Descent(471/999): loss=0.16890087445518906\n",
      "Gradient Descent(472/999): loss=0.1688965964948965\n",
      "Gradient Descent(473/999): loss=0.16889232163468937\n",
      "Gradient Descent(474/999): loss=0.16888804987112993\n",
      "Gradient Descent(475/999): loss=0.1688837812007843\n",
      "Gradient Descent(476/999): loss=0.1688795156202228\n",
      "Gradient Descent(477/999): loss=0.1688752531260201\n",
      "Gradient Descent(478/999): loss=0.16887099371475447\n",
      "Gradient Descent(479/999): loss=0.16886673738300859\n",
      "Gradient Descent(480/999): loss=0.16886248412736923\n",
      "Gradient Descent(481/999): loss=0.16885823394442714\n",
      "Gradient Descent(482/999): loss=0.16885398683077724\n",
      "Gradient Descent(483/999): loss=0.16884974278301826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(484/999): loss=0.16884550179775332\n",
      "Gradient Descent(485/999): loss=0.16884126387158951\n",
      "Gradient Descent(486/999): loss=0.16883702900113784\n",
      "Gradient Descent(487/999): loss=0.16883279718301342\n",
      "Gradient Descent(488/999): loss=0.16882856841383545\n",
      "Gradient Descent(489/999): loss=0.1688243426902272\n",
      "Gradient Descent(490/999): loss=0.16882012000881588\n",
      "Gradient Descent(491/999): loss=0.16881590036623273\n",
      "Gradient Descent(492/999): loss=0.16881168375911318\n",
      "Gradient Descent(493/999): loss=0.1688074701840964\n",
      "Gradient Descent(494/999): loss=0.16880325963782583\n",
      "Gradient Descent(495/999): loss=0.1687990521169488\n",
      "Gradient Descent(496/999): loss=0.1687948476181165\n",
      "Gradient Descent(497/999): loss=0.16879064613798447\n",
      "Gradient Descent(498/999): loss=0.168786447673212\n",
      "Gradient Descent(499/999): loss=0.1687822522204623\n",
      "Gradient Descent(500/999): loss=0.1687780597764028\n",
      "Gradient Descent(501/999): loss=0.16877387033770472\n",
      "Gradient Descent(502/999): loss=0.16876968390104327\n",
      "Gradient Descent(503/999): loss=0.16876550046309788\n",
      "Gradient Descent(504/999): loss=0.16876132002055144\n",
      "Gradient Descent(505/999): loss=0.16875714257009136\n",
      "Gradient Descent(506/999): loss=0.16875296810840856\n",
      "Gradient Descent(507/999): loss=0.1687487966321982\n",
      "Gradient Descent(508/999): loss=0.1687446281381592\n",
      "Gradient Descent(509/999): loss=0.16874046262299455\n",
      "Gradient Descent(510/999): loss=0.1687363000834112\n",
      "Gradient Descent(511/999): loss=0.16873214051611982\n",
      "Gradient Descent(512/999): loss=0.16872798391783514\n",
      "Gradient Descent(513/999): loss=0.16872383028527582\n",
      "Gradient Descent(514/999): loss=0.16871967961516446\n",
      "Gradient Descent(515/999): loss=0.16871553190422756\n",
      "Gradient Descent(516/999): loss=0.16871138714919545\n",
      "Gradient Descent(517/999): loss=0.16870724534680234\n",
      "Gradient Descent(518/999): loss=0.16870310649378645\n",
      "Gradient Descent(519/999): loss=0.16869897058688993\n",
      "Gradient Descent(520/999): loss=0.1686948376228586\n",
      "Gradient Descent(521/999): loss=0.16869070759844237\n",
      "Gradient Descent(522/999): loss=0.16868658051039498\n",
      "Gradient Descent(523/999): loss=0.16868245635547396\n",
      "Gradient Descent(524/999): loss=0.16867833513044075\n",
      "Gradient Descent(525/999): loss=0.16867421683206069\n",
      "Gradient Descent(526/999): loss=0.168670101457103\n",
      "Gradient Descent(527/999): loss=0.1686659890023406\n",
      "Gradient Descent(528/999): loss=0.16866187946455047\n",
      "Gradient Descent(529/999): loss=0.16865777284051325\n",
      "Gradient Descent(530/999): loss=0.1686536691270135\n",
      "Gradient Descent(531/999): loss=0.1686495683208396\n",
      "Gradient Descent(532/999): loss=0.1686454704187838\n",
      "Gradient Descent(533/999): loss=0.16864137541764224\n",
      "Gradient Descent(534/999): loss=0.16863728331421468\n",
      "Gradient Descent(535/999): loss=0.1686331941053047\n",
      "Gradient Descent(536/999): loss=0.16862910778771995\n",
      "Gradient Descent(537/999): loss=0.16862502435827173\n",
      "Gradient Descent(538/999): loss=0.16862094381377515\n",
      "Gradient Descent(539/999): loss=0.16861686615104896\n",
      "Gradient Descent(540/999): loss=0.168612791366916\n",
      "Gradient Descent(541/999): loss=0.16860871945820266\n",
      "Gradient Descent(542/999): loss=0.1686046504217392\n",
      "Gradient Descent(543/999): loss=0.1686005842543597\n",
      "Gradient Descent(544/999): loss=0.16859652095290206\n",
      "Gradient Descent(545/999): loss=0.16859246051420773\n",
      "Gradient Descent(546/999): loss=0.16858840293512206\n",
      "Gradient Descent(547/999): loss=0.16858434821249418\n",
      "Gradient Descent(548/999): loss=0.16858029634317698\n",
      "Gradient Descent(549/999): loss=0.16857624732402707\n",
      "Gradient Descent(550/999): loss=0.16857220115190472\n",
      "Gradient Descent(551/999): loss=0.16856815782367424\n",
      "Gradient Descent(552/999): loss=0.16856411733620316\n",
      "Gradient Descent(553/999): loss=0.16856007968636338\n",
      "Gradient Descent(554/999): loss=0.1685560448710299\n",
      "Gradient Descent(555/999): loss=0.16855201288708194\n",
      "Gradient Descent(556/999): loss=0.16854798373140217\n",
      "Gradient Descent(557/999): loss=0.16854395740087694\n",
      "Gradient Descent(558/999): loss=0.16853993389239666\n",
      "Gradient Descent(559/999): loss=0.168535913202855\n",
      "Gradient Descent(560/999): loss=0.16853189532914953\n",
      "Gradient Descent(561/999): loss=0.16852788026818158\n",
      "Gradient Descent(562/999): loss=0.16852386801685607\n",
      "Gradient Descent(563/999): loss=0.16851985857208165\n",
      "Gradient Descent(564/999): loss=0.16851585193077065\n",
      "Gradient Descent(565/999): loss=0.16851184808983907\n",
      "Gradient Descent(566/999): loss=0.1685078470462065\n",
      "Gradient Descent(567/999): loss=0.16850384879679653\n",
      "Gradient Descent(568/999): loss=0.16849985333853584\n",
      "Gradient Descent(569/999): loss=0.16849586066835529\n",
      "Gradient Descent(570/999): loss=0.1684918707831891\n",
      "Gradient Descent(571/999): loss=0.16848788367997541\n",
      "Gradient Descent(572/999): loss=0.16848389935565572\n",
      "Gradient Descent(573/999): loss=0.1684799178071752\n",
      "Gradient Descent(574/999): loss=0.16847593903148297\n",
      "Gradient Descent(575/999): loss=0.16847196302553136\n",
      "Gradient Descent(576/999): loss=0.1684679897862766\n",
      "Gradient Descent(577/999): loss=0.16846401931067848\n",
      "Gradient Descent(578/999): loss=0.16846005159570038\n",
      "Gradient Descent(579/999): loss=0.16845608663830935\n",
      "Gradient Descent(580/999): loss=0.16845212443547608\n",
      "Gradient Descent(581/999): loss=0.1684481649841746\n",
      "Gradient Descent(582/999): loss=0.16844420828138293\n",
      "Gradient Descent(583/999): loss=0.16844025432408244\n",
      "Gradient Descent(584/999): loss=0.16843630310925808\n",
      "Gradient Descent(585/999): loss=0.16843235463389855\n",
      "Gradient Descent(586/999): loss=0.16842840889499613\n",
      "Gradient Descent(587/999): loss=0.16842446588954646\n",
      "Gradient Descent(588/999): loss=0.1684205256145489\n",
      "Gradient Descent(589/999): loss=0.16841658806700643\n",
      "Gradient Descent(590/999): loss=0.1684126532439255\n",
      "Gradient Descent(591/999): loss=0.16840872114231623\n",
      "Gradient Descent(592/999): loss=0.1684047917591921\n",
      "Gradient Descent(593/999): loss=0.1684008650915704\n",
      "Gradient Descent(594/999): loss=0.1683969411364719\n",
      "Gradient Descent(595/999): loss=0.1683930198909206\n",
      "Gradient Descent(596/999): loss=0.1683891013519444\n",
      "Gradient Descent(597/999): loss=0.1683851855165749\n",
      "Gradient Descent(598/999): loss=0.1683812723818466\n",
      "Gradient Descent(599/999): loss=0.16837736194479805\n",
      "Gradient Descent(600/999): loss=0.16837345420247124\n",
      "Gradient Descent(601/999): loss=0.1683695491519115\n",
      "Gradient Descent(602/999): loss=0.16836564679016766\n",
      "Gradient Descent(603/999): loss=0.16836174711429253\n",
      "Gradient Descent(604/999): loss=0.1683578501213418\n",
      "Gradient Descent(605/999): loss=0.16835395580837498\n",
      "Gradient Descent(606/999): loss=0.168350064172455\n",
      "Gradient Descent(607/999): loss=0.16834617521064843\n",
      "Gradient Descent(608/999): loss=0.1683422889200252\n",
      "Gradient Descent(609/999): loss=0.16833840529765864\n",
      "Gradient Descent(610/999): loss=0.16833452434062574\n",
      "Gradient Descent(611/999): loss=0.16833064604600687\n",
      "Gradient Descent(612/999): loss=0.1683267704108859\n",
      "Gradient Descent(613/999): loss=0.1683228974323501\n",
      "Gradient Descent(614/999): loss=0.16831902710749025\n",
      "Gradient Descent(615/999): loss=0.16831515943340075\n",
      "Gradient Descent(616/999): loss=0.16831129440717912\n",
      "Gradient Descent(617/999): loss=0.16830743202592666\n",
      "Gradient Descent(618/999): loss=0.1683035722867478\n",
      "Gradient Descent(619/999): loss=0.16829971518675071\n",
      "Gradient Descent(620/999): loss=0.16829586072304686\n",
      "Gradient Descent(621/999): loss=0.16829200889275112\n",
      "Gradient Descent(622/999): loss=0.1682881596929818\n",
      "Gradient Descent(623/999): loss=0.1682843131208607\n",
      "Gradient Descent(624/999): loss=0.16828046917351303\n",
      "Gradient Descent(625/999): loss=0.16827662784806738\n",
      "Gradient Descent(626/999): loss=0.16827278914165572\n",
      "Gradient Descent(627/999): loss=0.16826895305141357\n",
      "Gradient Descent(628/999): loss=0.1682651195744797\n",
      "Gradient Descent(629/999): loss=0.16826128870799628\n",
      "Gradient Descent(630/999): loss=0.168257460449109\n",
      "Gradient Descent(631/999): loss=0.16825363479496688\n",
      "Gradient Descent(632/999): loss=0.1682498117427223\n",
      "Gradient Descent(633/999): loss=0.16824599128953105\n",
      "Gradient Descent(634/999): loss=0.1682421734325523\n",
      "Gradient Descent(635/999): loss=0.16823835816894855\n",
      "Gradient Descent(636/999): loss=0.1682345454958858\n",
      "Gradient Descent(637/999): loss=0.1682307354105333\n",
      "Gradient Descent(638/999): loss=0.16822692791006352\n",
      "Gradient Descent(639/999): loss=0.16822312299165273\n",
      "Gradient Descent(640/999): loss=0.16821932065248013\n",
      "Gradient Descent(641/999): loss=0.16821552088972844\n",
      "Gradient Descent(642/999): loss=0.1682117237005837\n",
      "Gradient Descent(643/999): loss=0.16820792908223536\n",
      "Gradient Descent(644/999): loss=0.16820413703187603\n",
      "Gradient Descent(645/999): loss=0.16820034754670196\n",
      "Gradient Descent(646/999): loss=0.16819656062391233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(647/999): loss=0.16819277626071\n",
      "Gradient Descent(648/999): loss=0.16818899445430097\n",
      "Gradient Descent(649/999): loss=0.1681852152018947\n",
      "Gradient Descent(650/999): loss=0.1681814385007037\n",
      "Gradient Descent(651/999): loss=0.16817766434794398\n",
      "Gradient Descent(652/999): loss=0.16817389274083488\n",
      "Gradient Descent(653/999): loss=0.16817012367659898\n",
      "Gradient Descent(654/999): loss=0.16816635715246217\n",
      "Gradient Descent(655/999): loss=0.16816259316565352\n",
      "Gradient Descent(656/999): loss=0.16815883171340573\n",
      "Gradient Descent(657/999): loss=0.16815507279295433\n",
      "Gradient Descent(658/999): loss=0.1681513164015385\n",
      "Gradient Descent(659/999): loss=0.16814756253640045\n",
      "Gradient Descent(660/999): loss=0.1681438111947858\n",
      "Gradient Descent(661/999): loss=0.1681400623739434\n",
      "Gradient Descent(662/999): loss=0.16813631607112545\n",
      "Gradient Descent(663/999): loss=0.16813257228358724\n",
      "Gradient Descent(664/999): loss=0.16812883100858741\n",
      "Gradient Descent(665/999): loss=0.16812509224338784\n",
      "Gradient Descent(666/999): loss=0.16812135598525382\n",
      "Gradient Descent(667/999): loss=0.16811762223145343\n",
      "Gradient Descent(668/999): loss=0.16811389097925858\n",
      "Gradient Descent(669/999): loss=0.16811016222594408\n",
      "Gradient Descent(670/999): loss=0.1681064359687879\n",
      "Gradient Descent(671/999): loss=0.16810271220507148\n",
      "Gradient Descent(672/999): loss=0.16809899093207928\n",
      "Gradient Descent(673/999): loss=0.16809527214709918\n",
      "Gradient Descent(674/999): loss=0.1680915558474221\n",
      "Gradient Descent(675/999): loss=0.16808784203034233\n",
      "Gradient Descent(676/999): loss=0.1680841306931571\n",
      "Gradient Descent(677/999): loss=0.1680804218331672\n",
      "Gradient Descent(678/999): loss=0.16807671544767652\n",
      "Gradient Descent(679/999): loss=0.16807301153399185\n",
      "Gradient Descent(680/999): loss=0.16806931008942344\n",
      "Gradient Descent(681/999): loss=0.16806561111128487\n",
      "Gradient Descent(682/999): loss=0.16806191459689254\n",
      "Gradient Descent(683/999): loss=0.1680582205435664\n",
      "Gradient Descent(684/999): loss=0.16805452894862924\n",
      "Gradient Descent(685/999): loss=0.1680508398094073\n",
      "Gradient Descent(686/999): loss=0.16804715312322976\n",
      "Gradient Descent(687/999): loss=0.16804346888742916\n",
      "Gradient Descent(688/999): loss=0.16803978709934106\n",
      "Gradient Descent(689/999): loss=0.1680361077563043\n",
      "Gradient Descent(690/999): loss=0.1680324308556609\n",
      "Gradient Descent(691/999): loss=0.16802875639475576\n",
      "Gradient Descent(692/999): loss=0.16802508437093733\n",
      "Gradient Descent(693/999): loss=0.16802141478155674\n",
      "Gradient Descent(694/999): loss=0.16801774762396868\n",
      "Gradient Descent(695/999): loss=0.1680140828955308\n",
      "Gradient Descent(696/999): loss=0.16801042059360377\n",
      "Gradient Descent(697/999): loss=0.16800676071555168\n",
      "Gradient Descent(698/999): loss=0.1680031032587414\n",
      "Gradient Descent(699/999): loss=0.1679994482205433\n",
      "Gradient Descent(700/999): loss=0.16799579559833042\n",
      "Gradient Descent(701/999): loss=0.16799214538947926\n",
      "Gradient Descent(702/999): loss=0.16798849759136938\n",
      "Gradient Descent(703/999): loss=0.1679848522013832\n",
      "Gradient Descent(704/999): loss=0.16798120921690662\n",
      "Gradient Descent(705/999): loss=0.16797756863532834\n",
      "Gradient Descent(706/999): loss=0.1679739304540403\n",
      "Gradient Descent(707/999): loss=0.16797029467043753\n",
      "Gradient Descent(708/999): loss=0.16796666128191792\n",
      "Gradient Descent(709/999): loss=0.16796303028588278\n",
      "Gradient Descent(710/999): loss=0.16795940167973628\n",
      "Gradient Descent(711/999): loss=0.16795577546088583\n",
      "Gradient Descent(712/999): loss=0.16795215162674168\n",
      "Gradient Descent(713/999): loss=0.16794853017471736\n",
      "Gradient Descent(714/999): loss=0.16794491110222934\n",
      "Gradient Descent(715/999): loss=0.16794129440669728\n",
      "Gradient Descent(716/999): loss=0.16793768008554363\n",
      "Gradient Descent(717/999): loss=0.16793406813619421\n",
      "Gradient Descent(718/999): loss=0.16793045855607777\n",
      "Gradient Descent(719/999): loss=0.16792685134262597\n",
      "Gradient Descent(720/999): loss=0.16792324649327373\n",
      "Gradient Descent(721/999): loss=0.16791964400545872\n",
      "Gradient Descent(722/999): loss=0.16791604387662207\n",
      "Gradient Descent(723/999): loss=0.16791244610420755\n",
      "Gradient Descent(724/999): loss=0.1679088506856622\n",
      "Gradient Descent(725/999): loss=0.16790525761843586\n",
      "Gradient Descent(726/999): loss=0.16790166689998162\n",
      "Gradient Descent(727/999): loss=0.16789807852775537\n",
      "Gradient Descent(728/999): loss=0.1678944924992162\n",
      "Gradient Descent(729/999): loss=0.16789090881182617\n",
      "Gradient Descent(730/999): loss=0.16788732746305013\n",
      "Gradient Descent(731/999): loss=0.16788374845035628\n",
      "Gradient Descent(732/999): loss=0.16788017177121572\n",
      "Gradient Descent(733/999): loss=0.1678765974231021\n",
      "Gradient Descent(734/999): loss=0.16787302540349278\n",
      "Gradient Descent(735/999): loss=0.16786945570986753\n",
      "Gradient Descent(736/999): loss=0.1678658883397094\n",
      "Gradient Descent(737/999): loss=0.16786232329050446\n",
      "Gradient Descent(738/999): loss=0.16785876055974147\n",
      "Gradient Descent(739/999): loss=0.1678552001449124\n",
      "Gradient Descent(740/999): loss=0.16785164204351205\n",
      "Gradient Descent(741/999): loss=0.1678480862530383\n",
      "Gradient Descent(742/999): loss=0.16784453277099196\n",
      "Gradient Descent(743/999): loss=0.16784098159487665\n",
      "Gradient Descent(744/999): loss=0.16783743272219928\n",
      "Gradient Descent(745/999): loss=0.1678338861504693\n",
      "Gradient Descent(746/999): loss=0.16783034187719945\n",
      "Gradient Descent(747/999): loss=0.16782679989990518\n",
      "Gradient Descent(748/999): loss=0.1678232602161049\n",
      "Gradient Descent(749/999): loss=0.1678197228233202\n",
      "Gradient Descent(750/999): loss=0.16781618771907525\n",
      "Gradient Descent(751/999): loss=0.16781265490089745\n",
      "Gradient Descent(752/999): loss=0.16780912436631698\n",
      "Gradient Descent(753/999): loss=0.16780559611286688\n",
      "Gradient Descent(754/999): loss=0.16780207013808318\n",
      "Gradient Descent(755/999): loss=0.1677985464395049\n",
      "Gradient Descent(756/999): loss=0.16779502501467386\n",
      "Gradient Descent(757/999): loss=0.16779150586113475\n",
      "Gradient Descent(758/999): loss=0.1677879889764354\n",
      "Gradient Descent(759/999): loss=0.16778447435812627\n",
      "Gradient Descent(760/999): loss=0.1677809620037609\n",
      "Gradient Descent(761/999): loss=0.16777745191089546\n",
      "Gradient Descent(762/999): loss=0.1677739440770894\n",
      "Gradient Descent(763/999): loss=0.16777043849990478\n",
      "Gradient Descent(764/999): loss=0.1677669351769064\n",
      "Gradient Descent(765/999): loss=0.16776343410566255\n",
      "Gradient Descent(766/999): loss=0.16775993528374367\n",
      "Gradient Descent(767/999): loss=0.1677564387087235\n",
      "Gradient Descent(768/999): loss=0.16775294437817861\n",
      "Gradient Descent(769/999): loss=0.16774945228968827\n",
      "Gradient Descent(770/999): loss=0.1677459624408347\n",
      "Gradient Descent(771/999): loss=0.16774247482920307\n",
      "Gradient Descent(772/999): loss=0.1677389894523813\n",
      "Gradient Descent(773/999): loss=0.16773550630796014\n",
      "Gradient Descent(774/999): loss=0.16773202539353313\n",
      "Gradient Descent(775/999): loss=0.16772854670669698\n",
      "Gradient Descent(776/999): loss=0.16772507024505084\n",
      "Gradient Descent(777/999): loss=0.16772159600619693\n",
      "Gradient Descent(778/999): loss=0.1677181239877402\n",
      "Gradient Descent(779/999): loss=0.16771465418728837\n",
      "Gradient Descent(780/999): loss=0.16771118660245232\n",
      "Gradient Descent(781/999): loss=0.1677077212308453\n",
      "Gradient Descent(782/999): loss=0.1677042580700837\n",
      "Gradient Descent(783/999): loss=0.16770079711778668\n",
      "Gradient Descent(784/999): loss=0.16769733837157594\n",
      "Gradient Descent(785/999): loss=0.16769388182907644\n",
      "Gradient Descent(786/999): loss=0.1676904274879155\n",
      "Gradient Descent(787/999): loss=0.1676869753457236\n",
      "Gradient Descent(788/999): loss=0.16768352540013381\n",
      "Gradient Descent(789/999): loss=0.16768007764878204\n",
      "Gradient Descent(790/999): loss=0.16767663208930697\n",
      "Gradient Descent(791/999): loss=0.16767318871935027\n",
      "Gradient Descent(792/999): loss=0.16766974753655597\n",
      "Gradient Descent(793/999): loss=0.16766630853857134\n",
      "Gradient Descent(794/999): loss=0.16766287172304611\n",
      "Gradient Descent(795/999): loss=0.16765943708763292\n",
      "Gradient Descent(796/999): loss=0.16765600462998723\n",
      "Gradient Descent(797/999): loss=0.1676525743477672\n",
      "Gradient Descent(798/999): loss=0.1676491462386336\n",
      "Gradient Descent(799/999): loss=0.16764572030025027\n",
      "Gradient Descent(800/999): loss=0.16764229653028362\n",
      "Gradient Descent(801/999): loss=0.1676388749264028\n",
      "Gradient Descent(802/999): loss=0.16763545548627976\n",
      "Gradient Descent(803/999): loss=0.16763203820758926\n",
      "Gradient Descent(804/999): loss=0.16762862308800872\n",
      "Gradient Descent(805/999): loss=0.1676252101252183\n",
      "Gradient Descent(806/999): loss=0.16762179931690083\n",
      "Gradient Descent(807/999): loss=0.16761839066074213\n",
      "Gradient Descent(808/999): loss=0.16761498415443046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(809/999): loss=0.16761157979565697\n",
      "Gradient Descent(810/999): loss=0.1676081775821155\n",
      "Gradient Descent(811/999): loss=0.1676047775115026\n",
      "Gradient Descent(812/999): loss=0.16760137958151758\n",
      "Gradient Descent(813/999): loss=0.16759798378986232\n",
      "Gradient Descent(814/999): loss=0.1675945901342416\n",
      "Gradient Descent(815/999): loss=0.16759119861236288\n",
      "Gradient Descent(816/999): loss=0.16758780922193614\n",
      "Gradient Descent(817/999): loss=0.1675844219606742\n",
      "Gradient Descent(818/999): loss=0.16758103682629272\n",
      "Gradient Descent(819/999): loss=0.1675776538165098\n",
      "Gradient Descent(820/999): loss=0.16757427292904625\n",
      "Gradient Descent(821/999): loss=0.16757089416162577\n",
      "Gradient Descent(822/999): loss=0.1675675175119746\n",
      "Gradient Descent(823/999): loss=0.1675641429778217\n",
      "Gradient Descent(824/999): loss=0.16756077055689864\n",
      "Gradient Descent(825/999): loss=0.1675574002469398\n",
      "Gradient Descent(826/999): loss=0.16755403204568212\n",
      "Gradient Descent(827/999): loss=0.1675506659508652\n",
      "Gradient Descent(828/999): loss=0.16754730196023138\n",
      "Gradient Descent(829/999): loss=0.16754394007152557\n",
      "Gradient Descent(830/999): loss=0.1675405802824955\n",
      "Gradient Descent(831/999): loss=0.16753722259089132\n",
      "Gradient Descent(832/999): loss=0.16753386699446612\n",
      "Gradient Descent(833/999): loss=0.16753051349097542\n",
      "Gradient Descent(834/999): loss=0.1675271620781773\n",
      "Gradient Descent(835/999): loss=0.16752381275383288\n",
      "Gradient Descent(836/999): loss=0.1675204655157054\n",
      "Gradient Descent(837/999): loss=0.16751712036156122\n",
      "Gradient Descent(838/999): loss=0.167513777289169\n",
      "Gradient Descent(839/999): loss=0.16751043629630016\n",
      "Gradient Descent(840/999): loss=0.16750709738072883\n",
      "Gradient Descent(841/999): loss=0.16750376054023158\n",
      "Gradient Descent(842/999): loss=0.16750042577258759\n",
      "Gradient Descent(843/999): loss=0.16749709307557892\n",
      "Gradient Descent(844/999): loss=0.16749376244698996\n",
      "Gradient Descent(845/999): loss=0.16749043388460785\n",
      "Gradient Descent(846/999): loss=0.1674871073862224\n",
      "Gradient Descent(847/999): loss=0.1674837829496258\n",
      "Gradient Descent(848/999): loss=0.16748046057261304\n",
      "Gradient Descent(849/999): loss=0.16747714025298163\n",
      "Gradient Descent(850/999): loss=0.16747382198853175\n",
      "Gradient Descent(851/999): loss=0.16747050577706601\n",
      "Gradient Descent(852/999): loss=0.16746719161638973\n",
      "Gradient Descent(853/999): loss=0.16746387950431077\n",
      "Gradient Descent(854/999): loss=0.16746056943863968\n",
      "Gradient Descent(855/999): loss=0.1674572614171894\n",
      "Gradient Descent(856/999): loss=0.1674539554377757\n",
      "Gradient Descent(857/999): loss=0.16745065149821656\n",
      "Gradient Descent(858/999): loss=0.16744734959633287\n",
      "Gradient Descent(859/999): loss=0.16744404972994797\n",
      "Gradient Descent(860/999): loss=0.16744075189688756\n",
      "Gradient Descent(861/999): loss=0.16743745609498037\n",
      "Gradient Descent(862/999): loss=0.1674341623220572\n",
      "Gradient Descent(863/999): loss=0.16743087057595157\n",
      "Gradient Descent(864/999): loss=0.16742758085449977\n",
      "Gradient Descent(865/999): loss=0.16742429315554036\n",
      "Gradient Descent(866/999): loss=0.1674210074769145\n",
      "Gradient Descent(867/999): loss=0.16741772381646602\n",
      "Gradient Descent(868/999): loss=0.16741444217204104\n",
      "Gradient Descent(869/999): loss=0.16741116254148847\n",
      "Gradient Descent(870/999): loss=0.16740788492265973\n",
      "Gradient Descent(871/999): loss=0.16740460931340853\n",
      "Gradient Descent(872/999): loss=0.16740133571159144\n",
      "Gradient Descent(873/999): loss=0.1673980641150672\n",
      "Gradient Descent(874/999): loss=0.16739479452169742\n",
      "Gradient Descent(875/999): loss=0.16739152692934586\n",
      "Gradient Descent(876/999): loss=0.16738826133587917\n",
      "Gradient Descent(877/999): loss=0.16738499773916618\n",
      "Gradient Descent(878/999): loss=0.16738173613707844\n",
      "Gradient Descent(879/999): loss=0.16737847652749\n",
      "Gradient Descent(880/999): loss=0.16737521890827725\n",
      "Gradient Descent(881/999): loss=0.1673719632773192\n",
      "Gradient Descent(882/999): loss=0.1673687096324974\n",
      "Gradient Descent(883/999): loss=0.16736545797169572\n",
      "Gradient Descent(884/999): loss=0.1673622082928007\n",
      "Gradient Descent(885/999): loss=0.16735896059370128\n",
      "Gradient Descent(886/999): loss=0.16735571487228884\n",
      "Gradient Descent(887/999): loss=0.16735247112645735\n",
      "Gradient Descent(888/999): loss=0.16734922935410318\n",
      "Gradient Descent(889/999): loss=0.16734598955312519\n",
      "Gradient Descent(890/999): loss=0.16734275172142468\n",
      "Gradient Descent(891/999): loss=0.16733951585690549\n",
      "Gradient Descent(892/999): loss=0.16733628195747388\n",
      "Gradient Descent(893/999): loss=0.1673330500210386\n",
      "Gradient Descent(894/999): loss=0.16732982004551067\n",
      "Gradient Descent(895/999): loss=0.16732659202880393\n",
      "Gradient Descent(896/999): loss=0.16732336596883438\n",
      "Gradient Descent(897/999): loss=0.16732014186352068\n",
      "Gradient Descent(898/999): loss=0.1673169197107836\n",
      "Gradient Descent(899/999): loss=0.16731369950854674\n",
      "Gradient Descent(900/999): loss=0.1673104812547359\n",
      "Gradient Descent(901/999): loss=0.1673072649472794\n",
      "Gradient Descent(902/999): loss=0.16730405058410808\n",
      "Gradient Descent(903/999): loss=0.167300838163155\n",
      "Gradient Descent(904/999): loss=0.16729762768235568\n",
      "Gradient Descent(905/999): loss=0.16729441913964846\n",
      "Gradient Descent(906/999): loss=0.16729121253297347\n",
      "Gradient Descent(907/999): loss=0.16728800786027379\n",
      "Gradient Descent(908/999): loss=0.16728480511949465\n",
      "Gradient Descent(909/999): loss=0.1672816043085837\n",
      "Gradient Descent(910/999): loss=0.16727840542549113\n",
      "Gradient Descent(911/999): loss=0.16727520846816937\n",
      "Gradient Descent(912/999): loss=0.1672720134345735\n",
      "Gradient Descent(913/999): loss=0.1672688203226608\n",
      "Gradient Descent(914/999): loss=0.1672656291303909\n",
      "Gradient Descent(915/999): loss=0.16726243985572598\n",
      "Gradient Descent(916/999): loss=0.16725925249663057\n",
      "Gradient Descent(917/999): loss=0.16725606705107168\n",
      "Gradient Descent(918/999): loss=0.1672528835170184\n",
      "Gradient Descent(919/999): loss=0.16724970189244256\n",
      "Gradient Descent(920/999): loss=0.1672465221753183\n",
      "Gradient Descent(921/999): loss=0.16724334436362182\n",
      "Gradient Descent(922/999): loss=0.16724016845533213\n",
      "Gradient Descent(923/999): loss=0.16723699444843043\n",
      "Gradient Descent(924/999): loss=0.16723382234090015\n",
      "Gradient Descent(925/999): loss=0.1672306521307273\n",
      "Gradient Descent(926/999): loss=0.16722748381590022\n",
      "Gradient Descent(927/999): loss=0.16722431739440952\n",
      "Gradient Descent(928/999): loss=0.1672211528642482\n",
      "Gradient Descent(929/999): loss=0.16721799022341163\n",
      "Gradient Descent(930/999): loss=0.16721482946989755\n",
      "Gradient Descent(931/999): loss=0.1672116706017061\n",
      "Gradient Descent(932/999): loss=0.16720851361683958\n",
      "Gradient Descent(933/999): loss=0.16720535851330284\n",
      "Gradient Descent(934/999): loss=0.167202205289103\n",
      "Gradient Descent(935/999): loss=0.16719905394224935\n",
      "Gradient Descent(936/999): loss=0.16719590447075397\n",
      "Gradient Descent(937/999): loss=0.16719275687263066\n",
      "Gradient Descent(938/999): loss=0.16718961114589612\n",
      "Gradient Descent(939/999): loss=0.16718646728856884\n",
      "Gradient Descent(940/999): loss=0.1671833252986702\n",
      "Gradient Descent(941/999): loss=0.16718018517422348\n",
      "Gradient Descent(942/999): loss=0.16717704691325444\n",
      "Gradient Descent(943/999): loss=0.16717391051379107\n",
      "Gradient Descent(944/999): loss=0.1671707759738639\n",
      "Gradient Descent(945/999): loss=0.16716764329150546\n",
      "Gradient Descent(946/999): loss=0.16716451246475078\n",
      "Gradient Descent(947/999): loss=0.16716138349163714\n",
      "Gradient Descent(948/999): loss=0.16715825637020423\n",
      "Gradient Descent(949/999): loss=0.16715513109849378\n",
      "Gradient Descent(950/999): loss=0.16715200767455002\n",
      "Gradient Descent(951/999): loss=0.1671488860964195\n",
      "Gradient Descent(952/999): loss=0.1671457663621509\n",
      "Gradient Descent(953/999): loss=0.16714264846979537\n",
      "Gradient Descent(954/999): loss=0.1671395324174062\n",
      "Gradient Descent(955/999): loss=0.167136418203039\n",
      "Gradient Descent(956/999): loss=0.16713330582475172\n",
      "Gradient Descent(957/999): loss=0.16713019528060452\n",
      "Gradient Descent(958/999): loss=0.16712708656865985\n",
      "Gradient Descent(959/999): loss=0.1671239796869825\n",
      "Gradient Descent(960/999): loss=0.16712087463363937\n",
      "Gradient Descent(961/999): loss=0.16711777140669976\n",
      "Gradient Descent(962/999): loss=0.1671146700042352\n",
      "Gradient Descent(963/999): loss=0.16711157042431946\n",
      "Gradient Descent(964/999): loss=0.16710847266502862\n",
      "Gradient Descent(965/999): loss=0.1671053767244409\n",
      "Gradient Descent(966/999): loss=0.16710228260063692\n",
      "Gradient Descent(967/999): loss=0.16709919029169953\n",
      "Gradient Descent(968/999): loss=0.1670960997957137\n",
      "Gradient Descent(969/999): loss=0.16709301111076658\n",
      "Gradient Descent(970/999): loss=0.167089924234948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(971/999): loss=0.16708683916634953\n",
      "Gradient Descent(972/999): loss=0.16708375590306518\n",
      "Gradient Descent(973/999): loss=0.1670806744431912\n",
      "Gradient Descent(974/999): loss=0.1670775947848261\n",
      "Gradient Descent(975/999): loss=0.16707451692607053\n",
      "Gradient Descent(976/999): loss=0.16707144086502745\n",
      "Gradient Descent(977/999): loss=0.16706836659980187\n",
      "Gradient Descent(978/999): loss=0.16706529412850132\n",
      "Gradient Descent(979/999): loss=0.16706222344923538\n",
      "Gradient Descent(980/999): loss=0.1670591545601157\n",
      "Gradient Descent(981/999): loss=0.16705608745925635\n",
      "Gradient Descent(982/999): loss=0.16705302214477347\n",
      "Gradient Descent(983/999): loss=0.16704995861478564\n",
      "Gradient Descent(984/999): loss=0.1670468968674134\n",
      "Gradient Descent(985/999): loss=0.16704383690077948\n",
      "Gradient Descent(986/999): loss=0.16704077871300904\n",
      "Gradient Descent(987/999): loss=0.16703772230222913\n",
      "Gradient Descent(988/999): loss=0.16703466766656927\n",
      "Gradient Descent(989/999): loss=0.16703161480416098\n",
      "Gradient Descent(990/999): loss=0.16702856371313815\n",
      "Gradient Descent(991/999): loss=0.16702551439163665\n",
      "Gradient Descent(992/999): loss=0.16702246683779465\n",
      "Gradient Descent(993/999): loss=0.1670194210497525\n",
      "Gradient Descent(994/999): loss=0.16701637702565264\n",
      "Gradient Descent(995/999): loss=0.16701333476363983\n",
      "Gradient Descent(996/999): loss=0.1670102942618609\n",
      "Gradient Descent(997/999): loss=0.1670072555184648\n",
      "Gradient Descent(998/999): loss=0.16700421853160272\n",
      "Gradient Descent(999/999): loss=0.1670011832994281\n"
     ]
    }
   ],
   "source": [
    "max_iters = 1000\n",
    "initial_w = np.ones((X_poly.shape[1]))*(0)\n",
    "gamma = 1e-7\n",
    "lambda_ =  1e-9\n",
    "w, loss = least_squares_L1(Y_total, X_poly, initial_w, max_iters, gamma, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort = np.sort(abs(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa293405588>]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(w)),sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9925373134328358"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filt = abs(w)>0.0005e-5\n",
    "sum(filt)/len(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = []\n",
    "di = 0\n",
    "for i in range(len(X_nans[0])):\n",
    "    if i in index_list:\n",
    "        for deg in degree[di]:\n",
    "            id_list.append([i,deg])\n",
    "        di=+1\n",
    "    else:\n",
    "        id_list.append([i])\n",
    "        \n",
    "id_filt = list(compress(id_list, filt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list_f = list(np.unique([i[0] for i in id_filt]))\n",
    "\n",
    "degree_f = [[] for i in range(X_nans.shape[1])]\n",
    "for i in id_filt:\n",
    "    if len(i)==1:\n",
    "        degree_f[i[0]].append(1)\n",
    "    else:\n",
    "        degree_f[i[0]].append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 0.7887013333333333 0.788172\n"
     ]
    }
   ],
   "source": [
    "X_poly_f = build_poly_index(X_nans, index_list_f, degree_f)\n",
    "\n",
    "dtmp_tr,dtmp_te = cross_validation(Y_total, X_poly_f, k_fold=4, seed=2, function_name='least_squares')\n",
    "print(\"Baseline:\", dtmp_tr, dtmp_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = X_poly\n",
    "y = Y_total\n",
    "w = np.random.randn((X_poly.shape[1]))*(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx.dot(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(X_poly, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
